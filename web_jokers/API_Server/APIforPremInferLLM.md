---
layout: default
title:  地端LLM的API伺服器
parent: API Servers
grand_parent: Web Jokers
last_modified_date: 2024-10-16 08:16:17
tags: API_Server 
---

# 地端LLM的API伺服器
{: .no_toc }

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
- TOC
{:toc}
</details>
---

## 背景

> 地端語言模型生產階段的推論服務，如何有效面對並發請求？


在地端語言模型的生產環境中，面對並發請求時，推論服務需要具備高效的處理能力，以確保性能、可擴展性和穩定性。以下是一些有效應對並發請求的策略和技術：

1. 模型並行化

	•	多線程或多進程處理：在單台機器上，為了充分利用多核 CPU，可以使用多線程或多進程來處理推論請求。Python 中可以使用 multiprocessing 模組來啟動多個進程，或使用基於異步的框架（如 asyncio）來實現非阻塞的 I/O 操作。
	•	GPU/TPU 加速：如果推論任務高度依賴於矩陣運算，使用 GPU 或 TPU 可以顯著提升推論效率。將模型部署到多個 GPU/TPU 上，分散請求到不同的硬體資源處理，可以有效降低每個請求的響應時間。

2. 模型裁剪與壓縮

	•	量化模型：將浮點數權重轉換為低精度數字（如 8-bit 整數），可以大幅降低模型的記憶體需求和計算成本，從而提高推論速度，特別是在硬體資源有限的情況下。
	•	蒸餾模型：使用模型蒸餾技術，將大型語言模型的知識轉移到較小的模型中。這樣的輕量模型可以在處理並發請求時快速響應，減少延遲。

3. 水平擴展

	•	多實例負載均衡：可以通過在多台伺服器上運行多個語言模型實例，然後使用負載均衡器（如 Nginx、HAProxy）來分配請求。這樣能夠有效應對大量並發請求，並避免單個伺服器成為性能瓶頸。
	•	容器化部署：使用容器技術（如 Docker、Kubernetes）將語言模型推論服務封裝成容器，並且根據流量動態擴展服務實例。Kubernetes 的自動擴展功能可以根據實際需求自動增加或減少容器數量，以應對突發的並發請求。

4. 請求批處理

	•	批量處理請求（Batching）：可以將多個推論請求打包成一個批次，然後一次性在模型中進行推理。這可以充分利用 GPU 資源，提高處理效率。許多深度學習框架（如 TensorFlow、PyTorch）都支持批處理推論。
	•	動態批處理：使用動態批處理技術，根據當前系統的負載情況，自動將一定時間內到達的請求進行打包處理，以最大限度提高模型的利用率和吞吐量。

5. 異步處理

	•	非同步推論：使用異步框架（如 FastAPI、Tornado、Sanic）來處理推論請求，允許模型在處理一個請求時同時接收並處理其他請求，避免阻塞。這種方式特別適合 I/O 密集型的操作，例如需要檢索資料或進行多步推論的場景。

6. 推論結果緩存

	•	結果快取（Caching）：對於重複性較高的查詢或推論結果，可以使用快取系統（如 Redis、Memcached）來緩存之前的推論結果，減少模型的重複計算。這對於一些常見問題或高頻查詢特別有效。
	•	特定片段緩存：可以針對部分推論過程進行緩存，將一些固定不變的部分預先計算好，僅對動態變化的部分進行推理，從而減少整體推理時間。

7. 流量預測和資源預留

	•	預測高峰流量：通過分析歷史數據，預測可能出現的高流量時間段，提前擴展服務資源，以應對高併發需求。
	•	資源預留：對於關鍵應用，可以考慮使用雲服務提供商的預留實例（Reserved Instances），確保在高負載情況下有足夠的計算資源來處理請求。

8. 針對不同任務使用多個模型

	•	任務分離：如果不同的請求對語言模型的需求不同，可以根據請求的複雜度或任務類型，將請求分配給不同的模型。例如，簡單任務使用較小的模型，複雜任務使用較大的模型。這可以減少每個請求的平均處理時間，提升整體系統效率。

9. 模型服務框架

	•	使用專門的模型服務框架（如 TensorFlow Serving、TorchServe、ONNX Runtime），這些框架針對高效推理進行了優化，能夠自動處理並發請求，並提供批量處理、動態擴展等功能，減少開發和部署的負擔。

10. 推論時間優化

	•	推理過程優化：減少推理過程中的冗餘操作，如避免不必要的重新加載模型，提前將模型加載到記憶體中持續使用。這樣可以減少推理延遲並提高響應速度。
	•	Lazy Loading：對於較大或使用頻率較低的模型，採用 Lazy Loading（延遲加載）技術，在接收到相關請求時才加載模型，從而節省系統資源。

綜合這些方法，可以顯著提升地端語言模型在生產環境下處理高併發請求的效率，並確保在不同負載情況下的穩定性和性能。選擇具體策略時，應根據實際應用場景的特性、推論需求以及可用資源做出最佳化的決策。

## RAG 建議方案

- [source](https://ragflow.io/docs/dev/deploy_local_llm)

## API Server 方案比較

> 

針對語言模型推論服務的 API 伺服器，在處理並發請求時，需要選擇穩定且高效的方案。以下是一些常見且經驗證的 API 伺服器框架及其優缺點比較，供你選擇最合適的方案：

1. FastAPI

	•	優點：
	•	支持高併發和非同步處理，使用 asyncio 來實現非阻塞 I/O，能夠很好地處理多個並發請求。
	•	性能非常高，在許多基準測試中，FastAPI 都能與 Node.js、Go 等語言的伺服器表現媲美。
	•	內建自動生成的 OpenAPI 和 Swagger 文件，方便與其他服務整合和測試。
	•	缺點：
	•	對於非常高的併發需求，可能需要進一步優化，如與 Gunicorn 和 Uvicorn 結合使用來進行多進程部署。

穩定性：FastAPI 是當前應用比較廣泛且穩定的框架之一，並且社群活躍，適合大多數語言模型的推論服務。

2. Flask + Gunicorn

	•	優點：
	•	Flask 是一個輕量的 WSGI 框架，非常靈活，適合小型應用或需要高度定制化的應用。
	•	使用 Gunicorn 等 WSGI 伺服器可以實現多進程處理，提升並發請求的處理能力。
	•	缺點：
	•	Flask 本身不支持異步，若需要異步處理，可能需要額外配置，如引入 gevent 或 eventlet 等協程庫。
	•	在面對高併發時性能不如 FastAPI 或其他原生支持非同步的框架。

穩定性：Flask 結合 Gunicorn 非常穩定，但可能需要進行額外優化來處理高併發需求。

3. Django + Daphne/Channels

	•	優點：
	•	Django 作為一個全功能的 web 框架，內建了非常豐富的功能和生態系統，適合構建複雜的 API 和應用。
	•	使用 Django Channels，可以讓 Django 支持 WebSocket 和異步請求處理，適應高併發需求。
	•	缺點：
	•	Django 本身較為沉重，可能對於輕量級或單純的 API 伺服器需求來說有些過度設計。
	•	設定和學習曲線相對較陡，對於單純處理推論請求的需求可能不如 FastAPI 效率高。

穩定性：Django 是非常穩定的框架，但對於簡單的推論服務可能略顯複雜。

4. Tornado

	•	優點：
	•	Tornado 是一個原生支持異步非阻塞 I/O 的 Python web 框架，適合高併發應用場景。
	•	對於需要長時間處理或 WebSocket 支持的應用來說，Tornado 的設計非常合適。
	•	缺點：
	•	社群不如 FastAPI 或 Django 活躍，且生態系統較小。
	•	雖然 Tornado 性能高，但使用起來比 FastAPI 稍微複雜，對於一般應用來說未必是首選。

穩定性：Tornado 對於並發處理非常穩定，但學習曲線較陡。

5. Sanic

	•	優點：
	•	Sanic 是一個快速的異步 Python 框架，針對非同步處理進行了深度優化，適合高併發的 API 服務。
	•	提供了高效的事件循環模型和簡單的 API，性能與 FastAPI 相近。
	•	缺點：
	•	社群相對較小，與 FastAPI 相比，生態系統不如後者豐富。
	•	不如 FastAPI 那麼靈活，對於複雜應用來說可能限制較多。

穩定性：Sanic 穩定且高效，適合單純追求高性能的推論服務。

6. Flask + FastCGI

	•	優點：
	•	使用 Flask 與 FastCGI 結合，可以實現與 FastCGI 伺服器的高效溝通，適合有特殊需求的應用。
	•	缺點：
	•	配置和部署相對複雜，維護成本高，不適合大多數標準應用場景。
	•	性能優勢有限，並且與 Flask 本身的同步特性不完全契合。

穩定性：這個組合在特定場景下穩定，但在語言模型推論應用中不如其他選擇靈活或高效。

7. Node.js（非 Python 框架）

[[2024-11-03]]

穩定性：Node.js 非常穩定，但需考慮跨語言整合的複雜性。

綜合考量

如果你使用了 FastChat 並遇到並發請求問題，可以考慮切換到 FastAPI 或 Sanic，這兩個框架原生支持異步處理，並且性能高，應該能夠有效改善你的並發處理問題。同時，將 Uvicorn 與 Gunicorn 結合使用，可以幫助進一步優化性能和穩定性。

如果需求較為複雜，像是涉及 WebSocket 或長時間連接的情況，可以考慮使用 Tornado 或 Django Channels。

## Instances 

- [rag_api](https://github.com/AshishSinha5/rag_api)

