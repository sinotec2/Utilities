 1/1: import netcdf4
 1/2: import netCDF4
 1/3: fname='CCTM_ACONC_v53_gcc_1601_run5_20151230_sChina_27k_10.nc'
 1/4: nc = netcdf(fname,'r')
 1/5: nc = netCDF4.Dataset(fname,'r')
 1/6: nc = netCDF4.Dataset(fname,'r+')
 1/7: x='TFLAG'
 1/8: lst
 1/9: nc.variables[x][:,0,:]
1/10: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
1/11:  nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
1/12: nt,nlay,nrow,ncol
1/13: fname='CCTM_ACONC_v53_gcc_1601_run5_20151231_sChina_27k_10.nc'
1/14: nc = netCDF4.Dataset(fname,'r+')
1/15:  nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
1/16: nt,nlay,nrow,ncol
1/17: fname='CCTM_ACONC_v53_gcc_1601_run5_20160101_sChina_27k_10.nc'
1/18: nc = netCDF4.Dataset(fname,'r+')
1/19:  nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
1/20: nt,nlay,nrow,ncol
1/21: qut
 2/1: import netCDF4
 2/2: fname='CCTM_ACONC_v53_gcc_1601_run5_20151230_sChina_27k_10.nc'
 2/3: nc = netCDF4.Dataset(fname,'r+')
 2/4: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
 2/5:  nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
 2/6: x='TFLAG'
 2/7: nc.variables[x][:,0,:]
 2/8: nc.variables[x][:13,0,:]
 2/9: nc.variables[x][:12,0,:]
2/10: nc.variables[x][:12,:,0]=nc.variables[x][:12,:,0]-1
2/11: nc.variables[x][:12,0,:]
2/12: nc.variables[x][12:,:,0]=2015365
2/13: nc.variables[x][12:,:,0]
2/14: nc.variables[x][12:,0,0]
2/15: nc.variables[x][12:,0,:]
2/16: nc.variables[x][:,0,:]
2/17: nc.close()
 3/1: import netCDF4
 3/2: fname='teds10X.1601_run10.const.nc'
 3/3: nc = netCDF4.Dataset(fname,'r')
 3/4: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
 3/5: V[3]
 3/6: x='STKVE'
 3/7: nc.variables[x]
 3/8: import numpy as np
 3/9: np.max( nc.variables[x])
3/10: from pandas import *
3/11: y='STKHT';df=DataFrame({x: nc.variables[x][0,0,:,],y: nc.variables[y][0,0,:,0]})
3/12: y='STKHT';df=DataFrame({x: nc.variables[x][0,0,:,0],y:list(nc.variables[y][0,0,:,0])})
3/13: df.head()
3/14: np.max( nc.variables[x])/3600.
3/15: ls *py
3/16: !vi pt_con_const.py
3/17: !vi pt_const.py
3/18: !which ncks
3/19: !vi pt_const.py
 4/1: import numpy as np
 4/2: import netCDF4
 4/3: from pandas import *
 4/4: fname='teds10.1601.const.nc'
 4/5: nc = netCDF4.Dataset(fname,'r')
 4/6: x='STKVE'
 4/7: np.max( nc.variables[x])
 4/8: np.max( nc.variables[x])/3600.
 4/9: !vi pt_const.py
4/10: fname
4/11: nc = netCDF4.Dataset(fname,'r')
4/12: x
4/13: np.max( nc.variables[x])
 6/1: run ~/mac/kuang/bin/imNC 1601d210.S.nc T
 6/2: vr
 6/3: V
 6/4: vr[V]
 6/5: type(vr[V])
 6/6: run ~/mac/kuang/bin/imNC 1601d210.S.nc T
 6/7: fname[0]
 6/8: fname[1]
 6/9: path
6/10: run ~/mac/kuang/bin/imNC 1601d210.S.nc T
6/11: ax[V]
6/12: mnv.shape
6/13: v
6/14: nc[0][v].shape
6/15: nc[1][v].shape
6/16: nc[1][v][:].shape
6/17: nt,nlay,nrow,ncol
6/18: run ~/mac/kuang/bin/imNC 1601d210.S.nc T
6/19: run ~/mac/kuang/bin/imNC 1601d210.S.nc T
6/20: run ~/mac/kuang/bin/imNC 1601d210.S.nc I
 7/1: import netCDF4
 7/2: fname='ICON_d2_201536500.nc'
 7/3: x='TFLAG'
 7/4: nc.variables[x][0,:,1]=[0 for i in range(nc.NVARS)]
 7/5: nc = netCDF4.Dataset(fname,'r+')
 7/6: nc.variables[x][0,:,1]=[0 for i in range(nc.NVARS)]
 7/7: nc.variables[x][0,:,1]=[60000 for i in range(nc.NVARS)]
 7/8: nc.STIME=0
 7/9: nc.SDATE
7/10: nc.close()
 8/1:
import netCDF4
fname='ICON_d1_201601.nc'
nc = netCDF4.Dataset(fname,'r')
 8/2: tflag=nc.variables['TFLAG'][:,0,:]
 8/3: tflag[:5,:]
 9/1:
import netCDF4
fname='ICON_d1_201601.nc'
nc = netCDF4.Dataset(fname,'r')
 9/2: tflag=nc.variables['TFLAG'][:,0,:]
 9/3: tflag[:5,:]
10/1:
import netCDF4
fname='ICON_d1_201602_run5.nc'
nc = netCDF4.Dataset(fname,'r')
10/2: x='TFLAG'
10/3: nc.variables[x][:,0,:]=
10/4: nc.variables[x][:,0,:]
10/5:
import netCDF4
fname='ICON_d1_201602_run6.nc'
nc = netCDF4.Dataset(fname,'r')
10/6: nc.variables[x][:,0,:]
11/1:
import netCDF4
fname='teds10.timvar.1602.nc'
nc = netCDF4.Dataset(fname,'r')
11/2: ls
11/3:
import netCDF4
fname='teds10.1602.timvar.nc'
nc = netCDF4.Dataset(fname,'r')
11/4: !ncdump -h teds10.1601.timvar.nc|H
11/5: !ncdump -h teds10.1602.timvar.nc|H
11/6: !ncdump -h teds10.1602.const.nc|H
12/1: import netCDF4,os,sys
12/2: YMD='20151231'
12/3: yestd=subprocess.check_output('date -c "'+YMD+" -1 day" +%Y%m%d',shell=True).decode('utf8').strip('\n').split('/')[1]
12/4: yestd=subprocess.check_output('date -c "'+YMD+' -1 day" +%Y%m%d',shell=True).decode('utf8').strip('\n').split('/')[1]
12/5: import subprocess
12/6: yestd=subprocess.check_output('date -c "'+YMD+' -1 day" +%Y%m%d',shell=True).decode('utf8').strip('\n').split('/')[1]
12/7: yestd=subprocess.check_output('date -c "'+YMD+' -1 days" +%Y%m%d',shell=True).decode('utf8').strip('\n').split('/')[1]
12/8: !date -c "20151231 -1 days" +%Y%m%d
12/9: !date -d "20151231 -1 days" +%Y%m%d
12/10: yestd=subprocess.check_output('date -d "'+YMD+' -1 days" +%Y%m%d',shell=True).decode('utf8').strip('\n').split('/')[1]
12/11: yestd=subprocess.check_output('date -d "'+YMD+' -1 days" +%Y%m%d',shell=True).decode('utf8').strip('\n').split('/')[0]
12/12: yestd
13/1:
import netCDF4
fname='data/sites/1601/const.nc'
nc = netCDF4.Dataset(fname,'r')
13/2: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
13/3: V[0]
13/4: V[1]
13/5: V[2]
13/6: V[3]
14/1:
import netCDF4
fname='/home/cmaqruns/2016base/data/bcon/BCON_v53_1601_run5_regrid_20151231_TWN_3X3'
nc = netCDF4.Dataset(fname,'r')
14/2: v='TFLAG'
14/3: nc.variables[x][:30,0,:]
14/4: x='TFLAG'
14/5: nc.variables[x][:30,0,:]
15/1:
import netCDF4
fname='/home/cmaqruns/2016base/data/bcon/ACON_d2_201601.nc'
nc = netCDF4.Dataset(fname,'r')
15/2: x='TFLAG'
15/3: nc.variables[x][:30,0,:]
16/1:
import netCDF4
fname='/home/cmaqruns/2016base/data/bcon/ACON_d2_201601.nc'
nc = netCDF4.Dataset(fname,'r')
16/2:
import netCDF4
fname='"/home/cmaqruns/2016base/data/bcon/BCON_v53_1601_run5_regrid_20151231_TWN_3X3"'
nc = netCDF4.Dataset(fname,'r')
16/3:
import netCDF4
fname="/home/cmaqruns/2016base/data/bcon/BCON_v53_1601_run5_regrid_20151231_TWN_3X3"
nc = netCDF4.Dataset(fname,'r')
16/4: x='TFLAG'
16/5: nc.variables[x][-30:,0,:]
16/6: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
16/7: v='TFLAG;
16/8: v='TFLAG'
16/9: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
16/10: !rm CTM* &
16/11: nvars
16/12:
for v in V[4]:
  nc.variables[v][nt,:,:,:]=nc.variables[v][nt-24,:,:,:]
16/13: x='TFLAG'
16/14: nc.variables[x][-3:,0,:]
16/15: import datetime
16/16:
def dt2jul(dt):
  yr=dt.year
  return yr*1000+(dt-datetime.datetime(yr,1,1)).days+1

def jul2dt(jul):
  yr=int(jul/1000)
  return datetime.datetime(yr,1,1)+datetime.timedelta(jul-yr*1000-1)
16/17: dend=jul2dt(int(nc.variables[x][nt-1,0,0]))
16/18: dend
16/19:
def dt2jul(dt):
  yr=dt.year
  return yr*1000+(dt-datetime.datetime(yr,1,1)).days+1

def jul2dt(jul,tm):
  yr=int(jul/1000)
  ih=int(tm/10000.)
  return datetime.datetime(yr,1,1,0,0)+datetime.timedelta(days=jul-yr*1000-1)+datetime.timedelta(hour=ih)
16/20: dend=jul2dt(nc.variables[x][nt-1,0,:])
16/21: dend=jul2dt(list(nc.variables[x][nt-1,0,:]))
16/22:
def dt2jul(dt):
  yr=dt.year
  return yr*1000+(dt-datetime.datetime(yr,1,1)).days+1

def jul2dt([jul,tm]):
  yr=int(jul/1000)
  ih=int(tm/10000.)
  return datetime.datetime(yr,1,1,0,0)+datetime.timedelta(days=jul-yr*1000-1)+datetime.timedelta(hour=ih)
16/23:
def dt2jul(dt):
  yr=dt.year
  return yr*1000+(dt-datetime.datetime(yr,1,1)).days+1

def jul2dt(jultm):
  jul=jultm[0];tm=jultm[1]  
  yr=int(jul/1000)
  ih=int(tm/10000.)
  return datetime.datetime(yr,1,1,0,0)+datetime.timedelta(days=jul-yr*1000-1)+datetime.timedelta(hour=ih)
16/24: dend=jul2dt(list(nc.variables[x][nt-1,0,:]))
16/25: jultm=list(nc.variables[x][nt-1,0,:])
16/26: jul=jultm[0];tm=jultm[1]
16/27: yr=int(jul/1000)
16/28: ih=int(tm/10000.)
16/29: datetime.datetime(yr,1,1,0,0)+datetime.timedelta(days=jul-yr*1000-1)
16/30: jul-yr*1000-1
16/31: datetime.datetime(yr,1,1,0,0)+datetime.timedelta(jul-yr*1000-1)
16/32: jul
16/33: print (yr)
16/34: yr
16/35: datetime.datetime(yr,1,1,0,0)+datetime.timedelta(2)
16/36: datetime.datetime(yr,1,1,0,0)+datetime.timedelta(hour=2)
16/37: datetime.datetime(yr,1,1,0,0)+datetime.timedelta(hours=2)
16/38: dd=jul-yr*1000-1
16/39: datetime.datetime(yr,1,1,0,0)+datetime.timedelta(dd)
16/40: type(dd)
16/41: type(int(dd))
16/42: datetime.datetime(yr,1,1,0,0)+datetime.timedelta(int(dd))
16/43: jul,tm=jultm[:]
16/44: jul,tm
16/45: datetime.datetime(yr,1,1)+datetime.timedelta(days=int(jul-yr*1000-1))+datetime.timedelta(hours=ih)
16/46:
def jul2dt(jultm):
  jul,tm=jultm[:]
  yr=int(jul/1000)
  ih=int(tm/10000.)
  return datetime.datetime(yr,1,1)+datetime.timedelta(days=int(jul-yr*1000-1))+datetime.timedelta(hours=ih)
16/47: dend=jul2dt(list(nc.variables[x][nt-1,0,:]))
16/48: dend
16/49: nc = netCDF4.Dataset(fname,'r+')
16/50: nt
16/51: SDATE=dt2jul(dend+datetime.timedelta(hours=1))
16/52: SDATE
16/53: dend+datetime.timedelta(hours=1)
16/54:
def dt2jul(dt):
  yr=dt.year
  return (yr*1000+(dt-datetime.datetime(yr,1,1)).days+1,dt-datetime.datetime(yr,1,1,0,0).hours*10000)
16/55: SDATE=dt2jul(dend+datetime.timedelta(hours=1))
16/56:
def dt2jul(dt):
  yr=dt.year
  return (yr*1000+(dt-datetime.datetime(yr,1,1)).days+1,dt-datetime.datetime(yr,1,1,0,0).hour*10000)
16/57: SDATE=dt2jul(dend+datetime.timedelta(hours=1))
16/58:
def dt2jul(dt):
  yr=dt.year
  return (yr*1000+(dt-datetime.datetime(yr,1,1)).days+1,(dt-datetime.datetime(yr,1,1,0,0)).hour*10000)
16/59: SDATE=dt2jul(dend+datetime.timedelta(hours=1))
16/60: dir(datetime.datetime)
16/61: dend.hour
16/62:
def dt2jul(dt):
  yr=dt.year
  return (yr*1000+(dt-datetime.datetime(yr,1,1)).days+1,(dt-datetime.datetime(yr,1,1,0,0)).hour*10000)
16/63: SDATE=dt2jul(dend+datetime.timedelta(hours=1))
16/64: SDATE=dt2jul(dend+datetime.datetime.timedelta(hours=1))
16/65: dir(datetime)
16/66: SDATE=dt2jul(dend+datetime.timedelta(hour=1))
16/67: dir(datetime.timedelta)
16/68: SDATE=dt2jul(dend+datetime.timedelta(days=1./24.))
16/69: SDATE=dt2jul(dend+datetime.timedelta(days=1./24.))
16/70:
def dt2jul(dt):
  yr=dt.year
  deltaT=dt-datetime.datetime(yr,1,1)
  deltaH=(deltaT-(deltaT.days)*24).seconds/3600.
  return (yr*1000+deltaT.days+1,deltaH*10000)
16/71: SDATE=dt2jul(dend+datetime.timedelta(days=1./24.))
16/72:
def dt2jul(dt):
  yr=dt.year
  deltaT=dt-datetime.datetime(yr,1,1)
  deltaH=(deltaT.seconds-deltaT.days*24*3600)/3600.
  return (yr*1000+deltaT.days+1,deltaH*10000)
16/73: SDATE=dt2jul(dend+datetime.timedelta(days=1./24.))
16/74: SDATE
16/75: dt=dend+datetime.timedelta(days=1./24.)
16/76: dt
16/77: yr=dt.year
16/78: deltaT=dt-datetime.datetime(yr,1,1)
16/79: deltaH=(deltaT.seconds-deltaT.days*24*3600)/3600.
16/80: deltaT.seconds
16/81: dir(datetime.deltatime)
16/82: dir(datetime.timedelta)
16/83:
def dt2jul(dt):
  yr=dt.year
  deltaT=dt-datetime.datetime(yr,1,1)
  deltaH=int((deltaT.tota-_seconds-deltaT.days*24*3600)/3600.)
  return (yr*1000+deltaT.days+1,deltaH*10000)
16/84: SDATE=dt2jul(dend+datetime.timedelta(days=1./24.))
16/85:
def dt2jul(dt):
  yr=dt.year
  deltaT=dt-datetime.datetime(yr,1,1)
  deltaH=int((deltaT.total_seconds-deltaT.days*24*3600)/3600.)
  return (yr*1000+deltaT.days+1,deltaH*10000)
16/86: SDATE=dt2jul(dend+datetime.timedelta(days=1./24.))
16/87: deltaT.total_seconds
16/88: deltaT.total_seconds()
16/89: deltaT.days*24*3600
16/90:
def dt2jul(dt):
  yr=dt.year
  deltaT=dt-datetime.datetime(yr,1,1)
  deltaH=int((deltaT.total_seconds()-deltaT.days*24*3600)/3600.)
  return (yr*1000+deltaT.days+1,deltaH*10000)
16/91: SDATE=dt2jul(dend+datetime.timedelta(days=1./24.))
16/92: SDATE
16/93: history
16/94: SDATE
16/95: nc.variables[x][nt,:,0]=[SDATE[0] for i in range(nvars)]
16/96: nc.variables[x][nt,:,1]=[SDATE[1] for i in range(nvars)]
16/97:
for v in V[4]:
  nc.variables[v][nt,:,:,:]=nc.variables[v][nt-24,:,:,:]
16/98:
for v in V[3]:
  nc.variables[v][nt,:,:,:]=nc.variables[v][nt-24,:,:,:]
16/99: nc.close()
17/1:
import netCDF4
fname="/home/cmaqruns/2016base/data/bcon/BCON_v53_1601_run5_regrid_20151231_TWN_3X3"
nc = netCDF4.Dataset(fname,'r')
17/2: x='TFLAG'
17/3: nc.variables[x][-30:,0,:]
18/1: nc.variables[x][-30:,0,:]
19/1:
import netCDF4
fname="/home/cmaqruns/2016base/data/bcon/BCON_v53_1601_run5_regrid_20151231_TWN_3X3"
nc = netCDF4.Dataset(fname,'r')
19/2: v='TFLAG'
19/3: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
19/4: nc.variables[x][-30:,0,:]
19/5: x='TFLAG'
19/6: nc.variables[x][-30:,0,:]
19/7: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
19/8:
for v in V[3]:
  print np.max(nc.variables[v][nt-1,:,:,:])
19/9: import numpy as np
19/10:
for v in V[3]:
  print np.max(nc.variables[v][nt-1,:,:,:])
19/11:
for v in V[3]:
  print (v,np.max(nc.variables[v][nt-1,:,:,:]))
19/12: V[3]
19/13: [len(V[i]) for i in range(4)]
19/14: nc = netCDF4.Dataset(fname,'r+')
19/15:
for v in V[2]:
  nc.variables[v][nt-1,:,:,:]=nc.variables[v][nt-25,:,:,:]
19/16: nt
19/17: V[2]
19/18: v
19/19:
for v in V[2]:
    print(v)
    nc.variables[v][nt-1,:,:,:]=nc.variables[v][nt-25,:,:,:]
19/20:
for v in V[2]:
    print(v)
    if v==x:continue
    nc.variables[v][nt-1,:,:]=nc.variables[v][nt-25,:,:]
19/21: nc.close()
20/1: import numpy as np
20/2:
import netCDF4
fname='EPA57.20160110.timvar.nc_12'
nc = netCDF4.Dataset(fname,'r')
20/3: fname="/home/cmaqruns/2016base/data/sites/1601/EPA57.20160110.timvar.nc_12"
20/4: nc = netCDF4.Dataset(fname,'r+')
20/5: x='TFLAG'
20/6: nc.variables[x][-30:,0,:]
20/7: fname="/home/cmaqruns/2016base/data/sites/1601/EPA57.20160109.timvar.nc_12"
20/8: nc.variables[x][-30:,0,:]
20/9: nc = netCDF4.Dataset(fname,'r+')
20/10: nc.variables[x][-30:,0,:]
20/11: fname="/home/cmaqruns/2016base/data/sites/1601/EPA57.20160111.timvar.nc_12"
20/12: nc = netCDF4.Dataset(fname,'r+')
20/13: nc.variables[x][-30:,0,:]
21/1:
import netCDF4
fname='EPA57.1601.timvar.nc'
nc = netCDF4.Dataset(fname,'r')
21/2: x='TFLAG'
21/3: nc.variables[x][-30:,0,:]
21/4: v=x
21/5: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
21/6: nt,nvars,ndt
21/7: [nc.variables[x][i,0,:] for i in range(0,nt,24)]
22/1:
import netCDF4
fname='EPA57.1601.timvar.nc_12'
nc = netCDF4.Dataset(fname,'r')
22/2: [nc.variables[x][i,0,:] for i in range(0,nt,24)]
22/3: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
22/4: v='TFLAG'
22/5: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
22/6: [nc.variables[x][i,0,:] for i in range(0,nt,24)]
22/7: x=v
22/8: [nc.variables[x][i,0,:] for i in range(0,nt,24)]
23/1:
import netCDF4
fname='EPA57.1602.timvar.nc_12'
nc = netCDF4.Dataset(fname,'r')
23/2: x='TFLAG'
23/3: v='TFLAG'
23/4: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/5: [nc.variables[x][i,0,:] for i in range(0,nt,24)]
23/6: [list(nc.variables[x][i,0,:]) for i in range(0,nt,24)]
23/7: !lst
23/8: fname="/home/cmaqruns/2016base/data/sites/EPA57.201602.timvar.nc"
23/9: nc = netCDF4.Dataset(fname,'r+')
23/10: !lst
23/11: fname="/home/cmaqruns/2016base/data/sites/EPA57.1602.timvar.nc"
23/12: nc = netCDF4.Dataset(fname,'r+')
23/13: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/14: [list(nc.variables[x][i,0,:]) for i in range(0,nt,24)]
23/15: nc.variables[x][-30:,0,:]
23/16: nc.variables[x][-60:-30,0,:]
23/17: nc.variables[x][-90:-60,0,:]
23/18: fname="/home/cmaqruns/2016base/data/sites/EPA57.1603.timvar.nc"
23/19: nc = netCDF4.Dataset(fname,'r+')
23/20: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/21: nt
23/22: [list(nc.variables[x][i,0,:]) for i in range(0,nt,24)]
23/23: v
23/24: run ovm2cmaq_d4.py 1602
23/25: nt
23/26: v='TFLAG'
23/27: x='TFLAG'
23/28: [list(nc.variables[x][i,0,:]) for i in range(0,nt,24)]
23/29: fname="/home/cmaqruns/2016base/data/sites/EPA57.1602.timvar.nc"
23/30: nc = netCDF4.Dataset(fname,'r+')
23/31: [list(nc.variables[x][i,0,:]) for i in range(0,nt,24)]
23/32: fname="/home/cmaqruns/2016base/data/sites/EPA57.1602.timvar.nc_12"
23/33: nc = netCDF4.Dataset(fname,'r+')
23/34: [list(nc.variables[x][i,0,:]) for i in range(0,nt,24)]
23/35: fname="/home/cmaqruns/2016base/data/sites/EPA57.1603.timvar.nc_12"
23/36: nc = netCDF4.Dataset(fname,'r+')
23/37: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/38: [list(nc.variables[x][i,0,:]) for i in range(0,nt,24)]
23/39: fname="/home/cmaqruns/2016base/data/sites/EPA57.1602.timvar.nc_12"
23/40: nc = netCDF4.Dataset(fname,'r+')
23/41: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/42: [list(nc.variables[x][i,0,:]) for i in range(0,nt,24)]
23/43: fname="/home/cmaqruns/2016base/data/sites/EPA57.1602.timvar.nc"
23/44: nc = netCDF4.Dataset(fname,'r+')
23/45: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/46: v
23/47: x
23/48: [list(nc.variables[x][i,0,:]) for i in range(0,nt,24)]
23/49: run ovm2cmaq_d4.py 1602
23/50: nt
23/51: nc = netCDF4.Dataset(fname,'r+')
23/52: fname="/home/cmaqruns/2016base/data/sites/EPA57.1602.timvar.nc"
23/53: nc = netCDF4.Dataset(fname,'r+')
23/54: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/55: v='TFLAG'
23/56: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/57: nt
23/58: [list(nc.variables[x][i,0,:]) for i in range(0,nt,24)]
23/59: x='TFLAG'
23/60: [list(nc.variables[x][i,0,:]) for i in range(0,nt,24)]
23/61: len(juli)
23/62: len(df)*nt
23/63: len(df)
23/64: len(mon)
23/65: ntm
23/66: nt
23/67: len(dfm)
23/68: len(dfm)/len(df)
23/69: len(df)*ntm
23/70: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/71: nt
23/72: run ovm2cmaq_d4.py 1602
23/73: fname="/home/cmaqruns/2016base/data/sites/EPA57.1602.timvar.nc"
23/74: nc = netCDF4.Dataset(fname,'r+')
23/75: v='TFLAG'
23/76: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/77: nt
23/78: run ovm2cmaq_d4.py 1602
23/79: run ovm2cmaq_d4.py 1602
23/80: fname="/home/cmaqruns/2016base/data/sites/EPA57.1602.timvar.nc"
23/81: v='TFLAG'
23/82: nc = netCDF4.Dataset(fname,'r+')
23/83: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/84: nt
23/85: x='TFLAG'
23/86: [list(nc.variables[x][i,0,:]) for i in range(0,nt,24)]
23/87: run ovm2cmaq_d4.py 1605
23/88: fname="/home/cmaqruns/2016base/data/sites/EPA57.1605.timvar.nc"
23/89: nc = netCDF4.Dataset(fname,'r+')
23/90: v='TFLAG'
23/91: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/92: [list(nc.variables[v][i,0,:]) for i in range(0,nt,24)]
23/93: fname="/home/cmaqruns/2016base/data/sites/EPA57.1602.timvar.nc_12"
23/94: nc = netCDF4.Dataset(fname,'r+')
23/95: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/96: nt
23/97: [list(nc.variables[v][i,0,:]) for i in range(0,nt,24)]
23/98: run add_12hr.py EPA57.1602.timvar.nc
23/99: deld
23/100: endd
23/101: dend
23/102: (endd-dend).hours
23/103: (endd-dend).hour
23/104: (endd-dend)
23/105: (endd-dend).seconds/3600.
23/106: (endd-dend).seconds_tot()/3600.
23/107: (endd-dend).second_tot()/3600.
23/108: (endd-dend).total_seconds()/3600.
23/109: run add_12hr.py EPA57.1602.timvar.nc
23/110: fname="/home/cmaqruns/2016base/data/sites/EPA57.1602.timvar.nc_12"
23/111: nc = netCDF4.Dataset(fname,'r+')
23/112: v='TFLAG'
23/113: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/114: [list(nc.variables[v][i,0,:]) for i in range(0,nt,24)]
23/115: run add_12hr.py EPA57.1602.timvar.nc
23/116: run add_12hr.py EPA57.1602.timvar.nc
23/117: fname="/home/cmaqruns/2016base/data/sites/EPA57.1602.timvar.nc_12"
23/118: nc = netCDF4.Dataset(fname,'r+')
23/119: v='TFLAG'
23/120: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/121: [list(nc.variables[v][i,0,:]) for i in range(0,nt,24)]
23/122: nc.variables[x][-60:,0,:]
23/123: nc.variables[x][-40:,0,:]
23/124: nc.variables[v][-40:,0,:]
23/125: [list(nc.variables[v][i,0,:]) for i in range(0,nt,24)]
23/126: nc.variables[v][-60:-40,0,:]
23/127: nc.variables[v][-40:,0,:]
23/128: nc.variables[v][-48:-24,0,:]
23/129: nc.variables[v][-72:-48,0,:]
23/130: nc.variables[v][-96:-60,0,:]
23/131: run add_12hr.py EPA57.1602.timvar.nc
23/132: fname="/home/cmaqruns/2016base/data/sites/EPA57.1602.timvar.nc_12"
23/133: nc = netCDF4.Dataset(fname,'r+')
23/134: nc.variables[v][-96:-60,0,:]
23/135: nc.variables[v][-96:-48,0,:]
23/136: nc.variables[v][-48:,0,:]
23/137: [list(nc.variables[v][i,0,:]) for i in range(0,nt,24)]
23/138: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
23/139: [list(nc.variables[v][i,0,:]) for i in range(0,nt,24)]
24/1: fname='teds10.1604.timvar.nc_12'
24/2: import netCDF4,os,sys
24/3: nc = netCDF4.Dataset(fname,'r+')
24/4: v='TFLAG'
24/5: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
24/6: [list(nc.variables[v][i,0,:]) for i in range(0,nt,24)]
24/7: fname='teds10.1601.timvar.nc_12'
24/8: nc = netCDF4.Dataset(fname,'r+')
24/9: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
24/10: [list(nc.variables[v][i,0,:]) for i in range(0,nt,24)]
24/11: fname='teds10.1602.timvar.nc_12'
24/12: nc = netCDF4.Dataset(fname,'r+')
24/13: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
24/14: nt
24/15: [list(nc.variables[v][i,0,:]) for i in range(0,nt,24)]
24/16: cd ../emis
24/17: ls *12
24/18: fname='fortBE.213.teds10.base01.nc_12'
24/19: nc = netCDF4.Dataset(fname,'r+')
24/20: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
24/21: nt
24/22: [list(nc.variables[v][i,0,:]) for i in range(0,nt,24)]
25/1: import netCDF4,os,sys
25/2: fname="/home/cmaqruns/2016base/data/ptse/1601/teds10.20151231.timvar.nc_12"
25/3: nc = netCDF4.Dataset(fname,'r+')
25/4: x='TFLAG'
25/5: nc.variables[x][-60:,0,:]
25/6: nc.variables[x][:,:,0]
26/1: run ../acc_prob.py
26/2: i
26/3: pv.J[i]
26/4: pv.I[i]
26/5: len(y_mesh)
26/6: run ../acc_prob.py
26/7: run ../acc_prob.py
26/8: ex
26/9: tex
26/10: run ../acc_prob.py
26/11: ex
26/12: tex
27/1: import netCDF4,os,sys
27/2: fname="tmplateD1_27km.nc"
27/3: nc = netCDF4.Dataset(fname,'r+')
27/4: nc.XCELL=27000
27/5: nc.YCELL=27000
27/6: nc.NROWS=175
27/7: nc.NCOLS=175
28/1:
import netCDF4
import numpy as np
import datetime
import sys,os

def dt2jul(dt):
  yr=dt.year
  deltaT=dt-datetime.datetime(yr,1,1)
  deltaH=int((deltaT.total_seconds()-deltaT.days*24*3600)/3600.)
  return (yr*1000+deltaT.days+1,deltaH*10000)

def jul2dt(jultm):
  jul,tm=jultm[:]
  yr=int(jul/1000)
  ih=int(tm/10000.)
  return datetime.datetime(yr,1,1)+datetime.timedelta(days=int(jul-yr*1000-1))+datetime.timedelta(hours=ih)
28/2: mm='03';d='2'
28/3:
Dbeg=[-1,10,20,-1]
tbeg=[0,0,0,16] #(nextm,1)-0~1 are duplicated
beg0=[datetime.datetime(2016,int(mm),1) for j in range(3)]
nextd=beg0[0]+datetime.timedelta(days=31)
nexty,nextm=nextd.year,nextd.month
beg0=beg0+[datetime.datetime(nexty,nextm,1)]
ndays=(datetime.datetime(nexty,nextm,10)-beg0[0]).days+2
28/4: ndays
28/5:
tlst,vlst=[],[]
nta=0
for j in range(4):
  begd=(beg0[j]+datetime.timedelta(days=Dbeg[j])).strftime('%Y%m%d')
  fname='QC'+begd+'d0'+d+'.m3.nc'
  nc = netCDF4.Dataset(fname,'r')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
  var =np.zeros(shape=(len(V[3]),nt, nlay, nrow, ncol))
  var1=np.zeros(shape=((nt-tbeg[j])*3,len(V[3]), nlay, nrow, ncol))
  for v in V[3]:
    iv=V[3].index(v)
    var[iv,:,:,:,:]=nc.variables[v][:,:,:,:]
  for t in range(tbeg[j],nt-1):#skip the duplicated period
    tt=(t-tbeg[j])*3
    var1[tt+0,:,:,:,:]=(var[:,t,:,:,:]*3+var[:,t+1,:,:,:]*0)/3
    var1[tt+1,:,:,:,:]=(var[:,t,:,:,:]*2+var[:,t+1,:,:,:]*1)/3
    var1[tt+2,:,:,:,:]=(var[:,t,:,:,:]*1+var[:,t+1,:,:,:]*2)/3
  t=nt-1-tbeg[j];tt=t*3;var1[tt,:,:,:,:]=var[:,t,:,:,:]
  vlst.append(list(var1.flatten()))
  tlst.append(nc.variables['TFLAG'][tbeg[j]:,0,:])
  nta+=nt-tbeg[j]
28/6: nta/8
28/7: idx=np.where(var1==np.nan)
28/8: len(idx[0])
28/9: np.max(var1)
28/10: a=np.max(var1)
28/11: idx=np.where(var1==a)
28/12: len(idx[0])
28/13: len(var1.flatte())
28/14: len(var1.flatten())
28/15: 144664000/182520000
28/16:
tlst,vlst=[],[]
nta=0
for j in range(1):
  begd=(beg0[j]+datetime.timedelta(days=Dbeg[j])).strftime('%Y%m%d')
  fname='QC'+begd+'d0'+d+'.m3.nc'
  nc = netCDF4.Dataset(fname,'r')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
  var =np.zeros(shape=(len(V[3]),nt, nlay, nrow, ncol))
  var1=np.zeros(shape=((nt-tbeg[j])*3,len(V[3]), nlay, nrow, ncol))
  for v in V[3]:
    iv=V[3].index(v)
    var[iv,:,:,:,:]=nc.variables[v][:,:,:,:]
  for t in range(tbeg[j],nt-1):#skip the duplicated period
    tt=(t-tbeg[j])*3
    var1[tt+0,:,:,:,:]=(var[:,t,:,:,:]*3+var[:,t+1,:,:,:]*0)/3
    var1[tt+1,:,:,:,:]=(var[:,t,:,:,:]*2+var[:,t+1,:,:,:]*1)/3
    var1[tt+2,:,:,:,:]=(var[:,t,:,:,:]*1+var[:,t+1,:,:,:]*2)/3
  t=nt-1-tbeg[j];tt=t*3;var1[tt,:,:,:,:]=var[:,t,:,:,:]
  vlst.append(list(var1.flatten()))
  tlst.append(nc.variables['TFLAG'][tbeg[j]:,0,:])
  nta+=nt-tbeg[j]
28/17: np.max(var1)
28/18: np.max(var)
28/19: !lst
29/1: run fill_clouds.py 03 2
29/2: nta/8,ndays
29/3: run fill_clouds.py 03 2
30/1: run a.py 01
30/2:
for c in c2v:
  if c not in df.columns:continue
  if sum(df[c])==0.:continue
  if c2v[c] not in V[3]:continue
  #T/Y to gram/hour (gmole for SNC and VOCs)
  df[c]=df[c]*fac/c2m[c]
  dfc=df.loc[df[c]>0]
  #since the number of time schemes are limited, the filling process are done
  #by each values as a batch.
  #If one attempt to loop the df axis to the point of the record, it will be very slow to do so.
  ss=set(dfc[c])
  imn,imx=min(dfc.IX),max(dfc.IX)+1
  jmn,jmx=min(dfc.IY),max(dfc.IY)+1
  if imn<0 and imx+imn<ncol:sys.exit('negative indexing error in i')
  if jmn<0 and jmx+jmn<nrow:sys.exit('negative indexing error in j')

  z=np.zeros(shape=(ntm,jmx,imx))
  for s in ss:
    boo=(dfc[c]==s)
    idx=dfc.loc[boo].index
    idt=np.array(dfc.loc[idx,'idt'])
    iy=np.array(dfc.loc[idx,'IY'])
    ix=np.array(dfc.loc[idx,'IX'])
    z[idt,iy,ix]+=s
  #also mapping whole matrix, NOT by parts
    nc.variables[c2v[c]][:,0,:,:]+=z[:,:nrow,:ncol]
  #Take 24-averged grid values and fill the header and/or tailor
  if max(sdatetime)>mx_dfdt or min(sdatetime)<mn_dfdt:
    z=np.zeros(shape=(24,nrow,ncol))
    t0=ddt[mn_dfdt]
    t1=ddt[mx_dfdt]
    for t in range(24):
      z[t,:,:]=np.mean(nc.variables[c2v[c]][t0+t:t1:24,0,:,:],axis=0)
    #the header
    if max(sdatetime)>mx_dfdt:
      for t in range(t0):
        nc.variables[c2v[c]][t,0,:,:]=z[(t-t0)%24,:,:]
    #the tailor
    if min(sdatetime)<mn_dfdt:
      for t in range(t1,ntm):
        nc.variables[c2v[c]][t,0,:,:]=z[(t-t0)%24,:,:]
    print (c)
30/3: dfc.head()
30/4: dfc.columns
30/5:
if len(dfT)==0:
  nc.close()
  sys.exit('no timvar parts, fine!')
#Following is dealing with the time flags.
df=dfT
df['YJ']=df.JJJHH//100+2016000
if mm=='01':
  idx=df.loc[df.JJJHH>30000].index
  df.loc[idx,'YJ']=df.loc[idx,'YJ']-1000
df['dt']=[jul2dt([i,j]) for i,j in zip(df.YJ,df.JJJHH%100*10000)]
#It will take a long time to implement the Max/Min function in a large data frame.
mn_dfdt=min(df.dt)
mx_dfdt=max(df.dt)
if max(sdatetime)<mx_dfdt or min(sdatetime)>mn_dfdt:sys.exit('dt not right')
#Indexing the long list of SDATETIME is exhausted, just doing so once.
sdt=set(df.dt)
ddt={i:sdatetime.index(i) for i in sdt}
df['idt']=[ddt[i] for i in df.dt]
30/6:
for c in c2v:
  if c not in df.columns:continue
  if sum(df[c])==0.:continue
  if c2v[c] not in V[3]:continue
  #T/Y to gram/hour (gmole for SNC and VOCs)
  df[c]=df[c]*fac/c2m[c]
  dfc=df.loc[df[c]>0]
  #since the number of time schemes are limited, the filling process are done
  #by each values as a batch.
  #If one attempt to loop the df axis to the point of the record, it will be very slow to do so.
  ss=set(dfc[c])
  imn,imx=min(dfc.IX),max(dfc.IX)+1
  jmn,jmx=min(dfc.IY),max(dfc.IY)+1
  if imn<0 and imx+imn<ncol:sys.exit('negative indexing error in i')
  if jmn<0 and jmx+jmn<nrow:sys.exit('negative indexing error in j')

  z=np.zeros(shape=(ntm,jmx,imx))
  for s in ss:
    boo=(dfc[c]==s)
    idx=dfc.loc[boo].index
    idt=np.array(dfc.loc[idx,'idt'])
    iy=np.array(dfc.loc[idx,'IY'])
    ix=np.array(dfc.loc[idx,'IX'])
    z[idt,iy,ix]+=s
  #also mapping whole matrix, NOT by parts
    nc.variables[c2v[c]][:,0,:,:]+=z[:,:nrow,:ncol]
  #Take 24-averged grid values and fill the header and/or tailor
  if max(sdatetime)>mx_dfdt or min(sdatetime)<mn_dfdt:
    z=np.zeros(shape=(24,nrow,ncol))
    t0=ddt[mn_dfdt]
    t1=ddt[mx_dfdt]
    for t in range(24):
      z[t,:,:]=np.mean(nc.variables[c2v[c]][t0+t:t1:24,0,:,:],axis=0)
    #the header
    if max(sdatetime)>mx_dfdt:
      for t in range(t0):
        nc.variables[c2v[c]][t,0,:,:]=z[(t-t0)%24,:,:]
    #the tailor
    if min(sdatetime)<mn_dfdt:
      for t in range(t1,ntm):
        nc.variables[c2v[c]][t,0,:,:]=z[(t-t0)%24,:,:]
    print (c)
30/7: len(ss)
30/8: len(df)
30/9: s
30/10:
    boo=(dfc[c]==s)
    idx=dfc.loc[boo].index
    idt=np.array(dfc.loc[idx,'idt'])
    iy=np.array(dfc.loc[idx,'IY'])
    ix=np.array(dfc.loc[idx,'IX'])
    z[idt,iy,ix]+=s
  #also mapping whole matrix, NOT by parts
    nc.variables[c2v[c]][:,0,:,:]+=z[:,:nrow,:ncol]
30/11: ss=list(ss)
30/12: ss.index(s)
30/13:
  for s in ss:
    boo=(dfc[c]==s)
    idx=dfc.loc[boo].index
    idt=np.array(dfc.loc[idx,'idt'])
    iy=np.array(dfc.loc[idx,'IY'])
    ix=np.array(dfc.loc[idx,'IX'])
    z[idt,iy,ix]+=s
  #also mapping whole matrix, NOT by parts
    nc.variables[c2v[c]][:,0,:,:]+=z[:,:nrow,:ncol]
    print(ss.index(s))
30/14:
  for s in ss:
    boo=(dfc[c]==s)
    idx=dfc.loc[boo].index
    idt=np.array(dfc.loc[idx,'idt'])
    iy=np.array(dfc.loc[idx,'IY'])
    ix=np.array(dfc.loc[idx,'IX'])
    z[idt,iy,ix]+=s
  #also mapping whole matrix, NOT by parts
    nc.variables[c2v[c]][:,0,:,:]+=z[:,:nrow,:ncol]
    if ss.index(s)//500==0:print(ss.index(s))
30/15:
  for s in ss:
    boo=(dfc[c]==s)
    idx=dfc.loc[boo].index
    idt=np.array(dfc.loc[idx,'idt'])
    iy=np.array(dfc.loc[idx,'IY'])
    ix=np.array(dfc.loc[idx,'IX'])
    z[idt,iy,ix]+=s
  #also mapping whole matrix, NOT by parts
    nc.variables[c2v[c]][:,0,:,:]+=z[:,:nrow,:ncol]
    if ss.index(s)%500==0:print(ss.index(s))
30/16: idx=dfc.index
30/17:
    iy=np.array(dfc.loc[idx,'IY'])
    ix=np.array(dfc.loc[idx,'IX'])
    z[idt,iy,ix]+=dfc[c]
30/18: idt=np.array(dfc.loc[idx,'idt'])
30/19: s=np.array(dfc.loc[idx,c])
30/20: z[idt,iy,ix]+=s
30/21:
for c in c2v:
  if c not in df.columns:continue
  if sum(df[c])==0.:continue
  if c2v[c] not in V[3]:continue
  #T/Y to gram/hour (gmole for SNC and VOCs)
  df[c]=df[c]*fac/c2m[c]
  dfc=df.loc[df[c]>0]
  #since the number of time schemes are limited, the filling process are done
  #by each values as a batch.
  #If one attempt to loop the df axis to the point of the record, it will be very slow to do so.
  ss=set(dfc[c])
  imn,imx=min(dfc.IX),max(dfc.IX)+1
  jmn,jmx=min(dfc.IY),max(dfc.IY)+1
  if imn<0 and imx+imn<ncol:sys.exit('negative indexing error in i')
  if jmn<0 and jmx+jmn<nrow:sys.exit('negative indexing error in j')

  z=np.zeros(shape=(ntm,jmx,imx))
  idx=dfc.index
  idt=np.array(dfc.loc[idx,'idt'])
  iy=np.array(dfc.loc[idx,'IY'])
  ix=np.array(dfc.loc[idx,'IX'])
  ss=np.array(dfc.loc[idx,c])
  z[idt,iy,ix]+=ss
  #also mapping whole matrix, NOT by parts
  nc.variables[c2v[c]][:,0,:,:]+=z[:,:nrow,:ncol]
  print (c)
30/22:
for c in c2v:
  if c not in df.columns:continue
  if sum(df[c])==0.:continue
  if c2v[c] not in V[3]:continue
  #T/Y to gram/hour (gmole for SNC and VOCs)
  df[c]=df[c]*fac/c2m[c]
  dfc=df.loc[df[c]>0]
  #since the number of time schemes are limited, the filling process are done
  #by each values as a batch.
  #If one attempt to loop the df axis to the point of the record, it will be very slow to do so.
  ss=set(dfc[c])
  imn,imx=min(dfc.IX),max(max(dfc.IX)+1,ncol)
  jmn,jmx=min(dfc.IY),max(max(dfc.IY)+1,nrow)
  if imn<0 and imx+imn<ncol:sys.exit('negative indexing error in i')
  if jmn<0 and jmx+jmn<nrow:sys.exit('negative indexing error in j')

  z=np.zeros(shape=(ntm,jmx,imx))
  idx=dfc.index
  idt=np.array(dfc.loc[idx,'idt'])
  iy=np.array(dfc.loc[idx,'IY'])
  ix=np.array(dfc.loc[idx,'IX'])
  ss=np.array(dfc.loc[idx,c])
  z[idt,iy,ix]+=ss
  #also mapping whole matrix, NOT by parts
  nc.variables[c2v[c]][:,0,:,:]+=z[:,:nrow,:ncol]
  print(c)
30/23:
for c in c2v:
  if c not in df.columns:continue
  if sum(df[c])==0.:continue
  if c2v[c] not in V[3]:continue
  #T/Y to gram/hour (gmole for SNC and VOCs)
  df[c]=df[c]*fac/c2m[c]
  dfc=df.loc[df[c]>0]
  #since the number of time schemes are limited, the filling process are done
  #by each values as a batch.
  #If one attempt to loop the df axis to the point of the record, it will be very slow to do so.
  ss=set(dfc[c])
  imn,imx=min(dfc.IX),max(max(dfc.IX)+1,ncol)
  jmn,jmx=min(dfc.IY),max(max(dfc.IY)+1,nrow)
  if imn<0 and imx+imn<ncol:sys.exit('negative indexing error in i')
  if jmn<0 and jmx+jmn<nrow:sys.exit('negative indexing error in j')

  z=np.zeros(shape=(ntm,jmx,imx))
  idx=dfc.index
  idt=np.array(dfc.loc[idx,'idt'])
  iy=np.array(dfc.loc[idx,'IY'])
  ix=np.array(dfc.loc[idx,'IX'])
  ss=np.array(dfc.loc[idx,c])
  z[idt,iy,ix]+=ss
  #also mapping whole matrix, NOT by parts
  nc.variables[c2v[c]][:,0,:,:]+=z[:,:nrow,:ncol]
  #Take 24-averged grid values and fill the header and/or tailor
  if max(sdatetime)>mx_dfdt or min(sdatetime)<mn_dfdt:
    z=np.zeros(shape=(24,nrow,ncol))
    t0=ddt[mn_dfdt]
    t1=ddt[mx_dfdt]
    for t in range(24):
      z[t,:,:]=np.mean(nc.variables[c2v[c]][t0+t:t1:24,0,:,:],axis=0)
    #the header
    if max(sdatetime)>mx_dfdt:
      for t in range(t0):
        nc.variables[c2v[c]][t,0,:,:]=z[(t-t0)%24,:,:]
    #the tailor
    if min(sdatetime)<mn_dfdt:
      for t in range(t1,ntm):
        nc.variables[c2v[c]][t,0,:,:]=z[(t-t0)%24,:,:]
30/24: posC=[v for v in V[3] if np.sum(nc.variables[v][:])>0]
30/25: len(V[3]),len(posC)
30/26:
  nv=len(posC)
  z=np.zeros(shape=(nv,24,nrow,ncol))
  var=np.zeros(shape=(nv,ntm,nrow,ncol))
  for v in posC:
    iv=posC.index(v)
    var[iv,:,:,:]=nc.variables[v][:,0,:,:]
30/27:
  for t in range(24):
    z[:,t,:,:]=np.mean(var[:,t0+t:t1:24,0,:,:],axis=1)
30/28: a=np.mean(var[:,t0+t:t1:24,0,:,:],axis=1)
30/29: var.shape
30/30: t
30/31:
  t0=ddt[mn_dfdt]
  t1=ddt[mx_dfdt]
30/32:
  for t in range(24):
    z[:,t,:,:]=np.mean(var[:,t0+t:t1:24,:,:],axis=1)
30/33:
  if max(sdatetime)>mx_dfdt:
    for t in range(t0):
      nc.variables[c2v[c]][t,0,:,:]=z[(t-t0)%24,:,:]
    #the tailor
  if min(sdatetime)<mn_dfdt:
    for t in range(t1,ntm):
      nc.variables[c2v[c]][t,0,:,:]=z[(t-t0)%24,:,:]
30/34:
  if max(sdatetime)>mx_dfdt:
    for t in range(t0):
      var[:,t,:,:]=z[(t-t0)%24,:,:]
    #the tailor
  if min(sdatetime)<mn_dfdt:
    for t in range(t1,ntm):
      var[:,t,:,:]=z[(t-t0)%24,:,:]
  for v in posC:
    iv=posC.index(v)
    nc.variables[v][:,0,:,:]=var[iv,:,:,:]
31/1: run a.py 01
31/2: !tail fnames01.txt
31/3: len(dtT)
31/4: len(dfT)
31/5: 'idt' in df.columns
31/6: 'idt' in dfT.columns
31/7:
df=dfT
df['YJ']=df.JJJHH//100+2016000
if mm=='01':
  idx=df.loc[df.JJJHH>30000].index
  df.loc[idx,'YJ']=df.loc[idx,'YJ']-1000
df['dt']=[jul2dt([i,j]) for i,j in zip(df.YJ,df.JJJHH%100*10000)]
#It will take a long time to implement the Max/Min function in a large data frame.
mn_dfdt=min(df.dt)
mx_dfdt=max(df.dt)
if max(sdatetime)<mx_dfdt or min(sdatetime)>mn_dfdt:sys.exit('dt not right')
#Indexing the long list of SDATETIME is exhausted, just doing so once.
sdt=set(df.dt)
ddt={i:sdatetime.index(i) for i in sdt}
df['idt']=[ddt[i] for i in df.dt]
31/8: posC
31/9: fname
32/1: run wrtE.py 01
32/2: nc.close()
32/3: NCfname
32/4: nc = netCDF4.Dataset(NCfname,'r+')
32/5: nc.variables['CP_NO'][:,:8]=np.array(list(pv.CP_NOb)).flatten().reshape(nopts,8)
32/6: nox=nc.variables['NO2'][:,:]
32/7: nox=nc.variables['NO2'][:,:]
32/8: sum(nc.variables['NO2'][:50,:50])
32/9: np.sum(nc.variables['NO2'][:50,:50])
32/10: np.sum(nc.variables['NO2'][50,:])
32/11: np.sum(nc.variables['NO2'][750,:])
32/12: np.sum(nc.variables['NO2'][:,:])
32/13: a=nc.variables['NO2'].mask
32/14: a,shape
32/15: a.shape
32/16: a
32/17: a=nc.variables['SO2'].mask
32/18: a
32/19: idx=np.where(nox.mask==True)
32/20: nox.shape
32/21: nc.variables['NO2']
32/22: nc.close()
32/23: nc = netCDF4.Dataset(NCfname,'r+')
32/24: nc.variables['NO2']
32/25: nc.close()
32/26: run wrtE.py 01
32/27: nc.variables['NO2']
32/28: nc.variables['SO2']
32/29: nc.close()
32/30: nopts
32/31: nc = netCDF4.Dataset(NCfname,'r')
32/32: nc.variables['SO2'][0,7779]
32/33: nc.variables['SO2'][0,7778]
32/34: nc.close()
32/35: nc = netCDF4.Dataset(NCfname,'r+')
32/36:
nc.variables['CP_NO'][:,:8]=np.array(list(pv.CP_NOb)).flatten().reshape(nopts,8)
nox=nc.variables['NO2'][:,:nopts]
nc.variables['NO'][:,:nopts]=nox[:,:nopts]*0.9
nc.variables['NO2'][:,:nopts]=nox-nc.variables['NO'][:,:nopts]
32/37: nc.close()
32/38: nc = netCDF4.Dataset(NCfname,'r')
32/39:
for v in V[1]:
    print(v,nc.variables[v].shape)
32/40:
for v in V[2]:
    print(v,nc.variables[v].shape)
32/41:
for v in V[0]:
    print(v,nc.variables[v].shape)
32/42:
for v in V[3]:
    print(v,nc.variables[v].shape)
33/1: run pt2em_d04.py fortBE.413_teds10.ptsE10.nc
33/2: len(pv)
33/3: len(dfT)
33/4: IX[:5]
34/1: import Nio
34/2: fname='fortBE.413_teds10.ptsE01.nc'
34/3: f = Nio.open_file(fname, format="netcdf")
34/4: nc = Nio.open_file(fname, format="netcdf")
34/5: v='CO'
34/6: a=nc.variables[v][:]
34/7: v='NO'
34/8: a=nc.variables[v][:]
34/9: v='CO'
34/10: a[:5,:5]
34/11: a=nc.variables[v][:]
35/1: from pandas import *
35/2:
with open('LOCAT.txt') as f:
    ll=[l for l in f]
35/3: X,Y=([float(l.split()[j]) for l in ll] for j in [3,4])
35/4: X
35/5: Y
35/6: II=[int(i/1000)+int((83*3/2)) for i in X]
35/7: JJ=[int(i/1000)+int((137*3/2)) for i in Y]
35/8: II
35/9: JJ
35/10:
from pyproj import Proj
import numpy as np
from pandas import *

Latitude_Pole, Longitude_Pole = 23.61000, 120.990
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
35/11: import twd97
35/12: Xcent, Ycent = twd97.fromwgs84(Latitude_Pole, Longitude_Pole)
35/13: II=[int(i-Xcent/1000)+int((83*3/2)) for i in X]
35/14: JJ=[int(i-Ycent/1000)+int((137*3/2)) for i in Y]
35/15: II
35/16: II=[int((i-Xcent)/1000)+int((83*3/2)) for i in X]
35/17: JJ=[int((i-Ycent/1000))+int((137*3/2)) for i in Y]
35/18: II
35/19: JJ
35/20: JJ=[int((i-Ycent)/1000)+int((137*3/2)) for i in Y]
35/21: JJ
35/22: nams='twsteel,dalinPP,kandingInc,linkoPP,hepingPP,hepingPP,shulinInc,muzhaInc,ZhongHuaPaper,XieHePP,'.split(',')
35/23: nams
35/24: del nams[-1]
35/25: nams
35/26: IJs={n:i*1000+j for n,i,j in zip(nams,II,JJ)}
35/27: IJs
35/28: cat LOCAT.txt
35/29: wc LOCAT.txt
35/30: !wc LOCAT.txt
35/31: !lst
35/32: !wc LOCAT.txt
35/33: !wc LOCAT.txt
35/34: len(X)
35/35: len(nams)
35/36:
with open('LOCAT.txt') as f:
    ll=[l for l in f]
35/37: X,Y=([float(l.split()[j]) for l in ll] for j in [3,4])
35/38: II=[int((i-Xcent)/1000)+int((83*3/2)) for i in X]
35/39: JJ=[int((i-Ycent)/1000)+int((137*3/2)) for i in Y]
35/40: nams
35/41: ll
35/42: IJs={n:i*1000+j for n,i,j in zip(nams,II,JJ)}
35/43: IJs
35/44: pwd
35/45: IJ
35/46: IJs=[i*1000+j for i,j in zip(II,JJ)]
35/47: IJs={n:i*1000+j for n,i,j in zip(nams,II,JJ)}
35/48: IJ=[i*1000+j for i,j in zip(II,JJ)]
35/49: IJ
35/50: IS.sort()
35/51: IJ.sort()
35/52: IJ
35/53: history
35/54: !which compgen
36/1: run ../gen_inp.py /nas1/aermruns/terr_results/271200_40_320_2765700_40_320/gd3 271200 40 320 2765700 40 320
37/1: run ../gen_inp.py /nas1/aermruns/terr_results/271200_40_320_2765700_40_320/gd3 271200 40 320 2765700 40 320
38/1: run ../gen_inp.py /nas1/aermruns/terr_results/271200_40_320_2765700_40_320/gd3 271200 40 320 2765700 40 320
38/2: pwd
39/1:
with open('inp.txt','r') as f:
  STR = [l.strip('\n') for l in f][0]
if '\n' in STR:STR=STR.replace('\n','')
39/2: STR
39/3: loc='LOCATION'
39/4: reg not in STR and loc not in STR
39/5:
reg='GRIDCART'
loc='LOCATION'
39/6: reg not in STR and loc not in STR
39/7: loc in STR
39/8: from terrainLOCAT import rd_SO, terrainLOCAT
39/9: snamo,xs,ys,hs=rd_SO(STR)
39/10: snamo,xs,ys
39/11: hs
39/12: snamo=terrainLOCAT(snamo,xs,ys,hs)
39/13:
  loc='LOCATION'
  lloc=len(loc)
  nsrc=STR.count(loc)
  ibeg,iend=[lloc],[len(STR)]
  snamo,pav,orig=([] for i in range(3))
39/14: snamo,pav,orig
39/15: STR
39/16: nsrc
39/17: ibeg,iend
39/18:
  for isrc in range(nsrc):
    try:
      inp=STR[ibeg[isrc]:iend[isrc]].split()
    except:
     sys.exit("parameter errors!")
    pav_i=inp[1]
    if pav_i in ['AREA','POINT','VOLUME']:
      snamo.append(inp[0])
      pav.append(inp[1]) #POINT/AREA/VOLUME tag
      orig.append([float(inp[i].replace('SO','')) for i in range(2,4)])
    else:
      sys.exit("path not right: "+pav_i)
39/19: orig
39/20: pav
39/21: P=[165980.0, 2552833.0]
39/22:
  CGI='/Library/WebServer/CGI-Executables/isc/'
  OUT='>> isc.out'
  geninp='/opt/local/bin/gen_inp.py'
  WAITM='/opt/local/bin/wait_map.cs'
  CSV=WEB+'terr_results/TWN_1X1REC.csv'
  reg='GRIDCART'
39/23:
  WEB='/Library/WebServer/Documents/'
  CGI='/Library/WebServer/CGI-Executables/isc/'
  OUT='>> isc.out'
  geninp='/opt/local/bin/gen_inp.py'
  WAITM='/opt/local/bin/wait_map.cs'
  CSV=WEB+'terr_results/TWN_1X1REC.csv'
  reg='GRIDCART'
39/24:
  GEO='/nas1/WRF4.1/WPS/geo_em.d04_333m.nc'
  nc = netCDF4.Dataset(GEO, 'r')
  v='HGT_M'
  c=np.array(nc.variables[v][0,:,:])
  for v in ['CLAT','CLONG']:
    exec(v+'=nc.variables[v][0,:,:]',locals())
  xg,yg=pnyc(CLONG,CLAT, inverse=False)
  xg+=Xcent
  yg+=Ycent
  d=(xg-P[0])**2+(yg-P[1])**2
  idx=np.where(d==np.min(d))
39/25: import netCDF4
39/26:
  GEO='/nas1/WRF4.1/WPS/geo_em.d04_333m.nc'
  nc = netCDF4.Dataset(GEO, 'r')
  v='HGT_M'
  c=np.array(nc.variables[v][0,:,:])
  for v in ['CLAT','CLONG']:
    exec(v+'=nc.variables[v][0,:,:]',locals())
  xg,yg=pnyc(CLONG,CLAT, inverse=False)
  xg+=Xcent
  yg+=Ycent
  d=(xg-P[0])**2+(yg-P[1])**2
  idx=np.where(d==np.min(d))
39/27: import numpy
39/28: import numpy as np
39/29:
  GEO='/nas1/WRF4.1/WPS/geo_em.d04_333m.nc'
  nc = netCDF4.Dataset(GEO, 'r')
  v='HGT_M'
  c=np.array(nc.variables[v][0,:,:])
  for v in ['CLAT','CLONG']:
    exec(v+'=nc.variables[v][0,:,:]',locals())
  xg,yg=pnyc(CLONG,CLAT, inverse=False)
  xg+=Xcent
  yg+=Ycent
  d=(xg-P[0])**2+(yg-P[1])**2
  idx=np.where(d==np.min(d))
39/30:
import numpy as np
import twd97
import os, sys
from pyproj import Proj
from terrainXYINC import terrainXYINC
Latitude_Pole, Longitude_Pole = 23.61000, 120.990
Xcent, Ycent = twd97.fromwgs84(Latitude_Pole, Longitude_Pole)
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
39/31:
  GEO='/nas1/WRF4.1/WPS/geo_em.d04_333m.nc'
  nc = netCDF4.Dataset(GEO, 'r')
  v='HGT_M'
  c=np.array(nc.variables[v][0,:,:])
  for v in ['CLAT','CLONG']:
    exec(v+'=nc.variables[v][0,:,:]',locals())
  xg,yg=pnyc(CLONG,CLAT, inverse=False)
  xg+=Xcent
  yg+=Ycent
  d=(xg-P[0])**2+(yg-P[1])**2
  idx=np.where(d==np.min(d))
39/32: idx
39/33: !tail nohup.out
39/34: !tail nohup.out
39/35: !tail nohup.out
39/36: !tail nohup.out
39/37: !tail nohup.out
39/38: !tail nohup.out
39/39: !tail nohup.out
39/40: !tail nohup.out
39/41: !tail nohup.out
39/42: !tail nohup.out
39/43: !tail nohup.out
39/44: !tail nohup.out
39/45: !tail nohup.out
39/46: !tail nohup.out
39/47: !tail nohup.out
39/48: !tail nohup.out
39/49: !tail nohup.out
39/50: !tail nohup.out
39/51: !tail nohup.out
39/52: !tail nohup.out
39/53: !tail nohup.out
39/54: !tail nohup.out
39/55: !tail nohup.out
39/56: !tail nohup.out
39/57: !tail nohup.out
39/58: !tail nohup.out
39/59: !tail nohup.out
39/60: !tail nohup.out
39/61: !tail nohup.out
39/62: !tail nohup.out
39/63: !topu
39/64: !lsd
39/65: !psg aermap
39/66: ls -lh 167600_40_20_2564400_40_20/
39/67: !lsd
39/68: !lsd
39/69: !lsd
39/70: !lsd
39/71: !tail nohup.out
39/72: !tail nohup.out
39/73: lsd
39/74: !lsd
39/75: !grep -n R8704115P001 point_ij.csv
39/76: !wc nohup.out
39/77: !grep LOCA nohup.out|wc -l
39/78: !grep LOCA nohup.out|wc -l
39/79: !wc TWN_1X1REC.csv
39/80: !lsd
39/81: !grep LOCA nohup.out|wc -l
39/82: !lsd
39/83: !grep LOCA nohup.out|wc -l
39/84: !top
39/85: !topu
39/86: !wc TWN_1X1REC.csv
39/87: !grep LOCA nohup.out|wc -l
39/88: !grep LOCA nohup.out|wc -l
39/89: lsd
39/90: !lsd
39/91: !lsd
39/92: !topu
39/93: !topu
39/94: !grep LOCA nohup.out|wc -l
39/95: !topu
39/96: !wc TWN_1X1REC.csv
39/97: !lsd|wc
39/98: !grep LOCA nohup.out|wc -l
39/99: !topu
39/100: !grep LOCA nohup.out|wc -l
39/101: df.head()
39/102: pwd
39/103: from pandas import *
39/104: df=read_csv('TWN_1X1REC.csv')
39/105: df.head()
39/106: df.nx[0]
39/107: ls -l *csv
39/108: pts=read_csv('point_QC.csv')
39/109: pts.head()
39/110: pts=read_csv('point_ij.csv')
39/111: pts.head()
40/1: from pandas import *
40/2:
from pyproj import Proj
import numpy as np
from pandas import *

Latitude_Pole, Longitude_Pole = 23.61000, 120.990
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
40/3: df=read_csv('point_QC.csv')
40/4: df.columns
40/5: df['name']=df.CP_NO
40/6: col=['lon','lat','name','desc']
40/7: lon,lat,name,desc=([] for i in range(4))
40/8:
for i in range(len(df)):
    x,y=df.UTME[i],df.UTMN[i]
    loni, lati = pnyc(x, y, inverse=True)
    for s in ['lon','lat']:
        exec(s+'.append('+s+'i)')
40/9: df['XY']=[]
40/10: df.UTM_E=[int(i) for i in df.UTM_E]
40/11: df.UTM_N=[int(i) for i in df.UTM_N]
40/12: df['XY']=[(i,j) for i,j in zip(list(df.UTM_E),lsit(df.UTM_N))]
40/13: df['XY']=[(i,j) for i,j in zip(list(df.UTM_E),list(df.UTM_N))]
40/14: len(df),len(set(df.XY))
40/15: df.columns
40/16: df['C_NO']=[i[:-4] for i in df.CP_NO]
40/17: df.head()
40/18: lon,lat,name,desc=([] for i in range(4))
40/19:
for c in set(df.C_NO):
    dfc=df.loc[df.C_NO==c].reset_index(drop=True)
    x,y=np.mean(dfc.UTM_E),np.mean(dfc.UTM_N)
    loni, lati = pnyc(x, y, inverse=True)
    for s in ['lon','lat']:
        exec(s+'.append('+s+'i)')
    name.append(c)
    dfc['X'],dfc['Y']=dfc.UTM_E+Xcent,dfc.UTM_N+Ycent    
    dfc['STR']=['SO LOCATION {:s} POINT {:.0f} {:.0f} 0.\n SRCPARAM {:s} {:.3f} {:.1f} {:.1f} {:f} {:.1f}'.format(c,x,y,s,h,t,v,d) for \
    c,x,y,s,h,t,v,d in zip(list(dfc.CP_NO),list(dfc.X),list(dfc.Y),\
    list(dfc.CP_NO),list(dfc.PM25_EMI),list(dfc.HEI),list(dfc.TEMP),list(dfc.DIA),list(dfc.VEL))]
    ss=''
    for s in dfc.STR:
        ss+=s+' '
    desc.append(ss)
40/20: Xcent, Ycent = twd97.fromwgs84(Latitude_Pole, Longitude_Pole)
40/21: import twd97
40/22: Xcent, Ycent = twd97.fromwgs84(Latitude_Pole, Longitude_Pole)
40/23: lon,lat,name,desc=([] for i in range(4))
40/24:
for c in set(df.C_NO):
    dfc=df.loc[df.C_NO==c].reset_index(drop=True)
    x,y=np.mean(dfc.UTM_E),np.mean(dfc.UTM_N)
    loni, lati = pnyc(x, y, inverse=True)
    for s in ['lon','lat']:
        exec(s+'.append('+s+'i)')
    name.append(c)
    dfc['X'],dfc['Y']=dfc.UTM_E+Xcent,dfc.UTM_N+Ycent    
    dfc['STR']=['SO LOCATION {:s} POINT {:.0f} {:.0f} 0.\n SRCPARAM {:s} {:.3f} {:.1f} {:.1f} {:f} {:.1f}'.format(c,x,y,s,h,t,v,d) for \
    c,x,y,s,h,t,v,d in zip(list(dfc.CP_NO),list(dfc.X),list(dfc.Y),\
    list(dfc.CP_NO),list(dfc.PM25_EMI),list(dfc.HEI),list(dfc.TEMP),list(dfc.DIA),list(dfc.VEL))]
    ss=''
    for s in dfc.STR:
        ss+=s+' '
    desc.append(ss)
40/25:
for c in set(df.C_NO):
    dfc=df.loc[df.C_NO==c].reset_index(drop=True)
    x,y=np.mean(dfc.UTM_E),np.mean(dfc.UTM_N)
    loni, lati = pnyc(x, y, inverse=True)
    for s in ['lon','lat']:
        exec(s+'.append('+s+'i)')
    name.append(c)
    dfc['X'],dfc['Y']=dfc.UTM_E+Xcent,dfc.UTM_N+Ycent    
    dfc['STR']=['SO LOCATION {:s} POINT {:.0f} {:.0f} 0.\n SRCPARAM {:s} {:.3f} {:.1f} {:.1f} {:f} {:.1f}'.format(c,x,y,s,p,h,t,v,d) for \
    c,x,y,s,p,h,t,v,d in zip(list(dfc.CP_NO),list(dfc.X),list(dfc.Y),\
    list(dfc.CP_NO),list(dfc.PM25_EMI),list(dfc.HEI),list(dfc.TEMP),list(dfc.DIA),list(dfc.VEL))]
    ss=''
    for s in dfc.STR:
        ss+=s+' '
    desc.append(ss)
40/26:
for s in col:
    exec('print (len('+s+'))')
40/27: lon,lat,name,desc=([] for i in range(4))
40/28:
for c in set(df.C_NO):
    dfc=df.loc[df.C_NO==c].reset_index(drop=True)
    x,y=np.mean(dfc.UTM_E),np.mean(dfc.UTM_N)
    loni, lati = pnyc(x, y, inverse=True)
    for s in ['lon','lat']:
        exec(s+'.append('+s+'i)')
    name.append(c)
    dfc['X'],dfc['Y']=dfc.UTM_E+Xcent,dfc.UTM_N+Ycent    
    dfc['STR']=['SO LOCATION {:s} POINT {:.0f} {:.0f} 0.\n SRCPARAM {:s} {:.3f} {:.1f} {:.1f} {:f} {:.1f}'.format(c,x,y,s,p,h,t,v,d) for \
    c,x,y,s,p,h,t,v,d in zip(list(dfc.CP_NO),list(dfc.X),list(dfc.Y),\
    list(dfc.CP_NO),list(dfc.PM25_EMI),list(dfc.HEI),list(dfc.TEMP),list(dfc.DIA),list(dfc.VEL))]
    ss=''
    for s in dfc.STR:
        ss+=s+' '
    desc.append(ss)
40/29:
for s in col:
    exec('print (len('+s+'))')
40/30: DD={}
40/31:
for s in ['lon','lat','name','desc']:
    exec('DD.update({"'+s+'":'+s+'})')
df=DataFrame(DD)
40/32: df[col].set_index('lon').to_csv('tedsSRCPARAM.csv')
40/33: more tedsSRCPARAM.csv
40/34: df=read_csv('point_QC.csv')
40/35: lon,lat,name,desc=([] for i in range(4))
40/36:
for c in set(df.C_NO):
    dfc=df.loc[df.C_NO==c].reset_index(drop=True)
    x,y=np.mean(dfc.UTM_E),np.mean(dfc.UTM_N)
    loni, lati = pnyc(x, y, inverse=True)
    for s in ['lon','lat']:
        exec(s+'.append('+s+'i)')
    name.append(c)
    dfc['X'],dfc['Y']=dfc.UTM_E+Xcent,dfc.UTM_N+Ycent    
    dfc['STR']=['SO LOCATION {:s} POINT {:.0f} {:.0f} 0.\n SRCPARAM {:s} {:.3f} {:.1f} {:.1f} {:.2f} {:.1f}'.format(c,x,y,s,p,h,t+273,v,d) for \
    c,x,y,s,p,h,t,v,d in zip(list(dfc.CP_NO),list(dfc.X),list(dfc.Y),\
    list(dfc.CP_NO),list(dfc.PM25_EMI),list(dfc.HEI),list(dfc.TEMP),list(dfc.DIA),list(dfc.VEL)) if p>0.001]
    ss=''
    for s in dfc.STR:
        ss+=s+' '
    desc.append(ss)
40/37: df['C_NO']=[i[:-4] for i in df.CP_NO]
40/38:
for c in set(df.C_NO):
    dfc=df.loc[df.C_NO==c].reset_index(drop=True)
    x,y=np.mean(dfc.UTM_E),np.mean(dfc.UTM_N)
    loni, lati = pnyc(x, y, inverse=True)
    for s in ['lon','lat']:
        exec(s+'.append('+s+'i)')
    name.append(c)
    dfc['X'],dfc['Y']=dfc.UTM_E+Xcent,dfc.UTM_N+Ycent    
    dfc['STR']=['SO LOCATION {:s} POINT {:.0f} {:.0f} 0.\n SRCPARAM {:s} {:.3f} {:.1f} {:.1f} {:.2f} {:.1f}'.format(c,x,y,s,p,h,t+273,v,d) for \
    c,x,y,s,p,h,t,v,d in zip(list(dfc.CP_NO),list(dfc.X),list(dfc.Y),\
    list(dfc.CP_NO),list(dfc.PM25_EMI),list(dfc.HEI),list(dfc.TEMP),list(dfc.DIA),list(dfc.VEL)) if p>0.001]
    ss=''
    for s in dfc.STR:
        ss+=s+' '
    desc.append(ss)
40/39: lon,lat,name,desc=([] for i in range(4))
40/40:
for c in set(df.C_NO):
    dfc=df.loc[(df.C_NO==c) & (df.PM25_EMI>0.001)].reset_index(drop=True)
    x,y=np.mean(dfc.UTM_E),np.mean(dfc.UTM_N)
    loni, lati = pnyc(x, y, inverse=True)
    for s in ['lon','lat']:
        exec(s+'.append('+s+'i)')
    name.append(c)
    dfc['X'],dfc['Y']=dfc.UTM_E+Xcent,dfc.UTM_N+Ycent    
    dfc['STR']=['SO LOCATION {:s} POINT {:.0f} {:.0f} 0.\n SRCPARAM {:s} {:.3f} {:.1f} {:.1f} {:.2f} {:.1f}'.format(c,x,y,s,p,h,t+273,v,d) for \
    c,x,y,s,p,h,t,v,d in zip(list(dfc.CP_NO),list(dfc.X),list(dfc.Y),\
    list(dfc.CP_NO),list(dfc.PM25_EMI),list(dfc.HEI),list(dfc.TEMP),list(dfc.DIA),list(dfc.VEL))]
    ss=''
    for s in dfc.STR:
        ss+=s+' '
    desc.append(ss)
40/41:
for s in col:
    exec('print (len('+s+'))')
40/42: DD={}
40/43:
for s in ['lon','lat','name','desc']:
    exec('DD.update({"'+s+'":'+s+'})')
dfd=DataFrame(DD)
40/44: dfd[col].set_index('lon').to_csv('tedsSRCPARAM.csv')
40/45: !which csv2kml.py
40/46: !/home/kuang/bin/csv2kml.py -f tedsSRCPARAM.csv -n N -g LL
40/47: run /home/kuang/bin/csv2kml.py -f tedsSRCPARAM.csv -n N -g LL
40/48: !scp tedsSRCPARAM.csv imackuang:~
40/49: !scp imackuang:/opt/local/bin/csv2kml.py ~/bin
40/50: run /home/kuang/bin/csv2kml.py -f tedsSRCPARAM.csv -n N -g LL
40/51: more tedsSRCPARAM.csv
40/52: !grep J55B1594 point_QC.csv
40/53: lon,lat,name,desc=([] for i in range(4))
40/54:
for c in set(df.C_NO):
    dfc=df.loc[(df.C_NO==c) & (df.PM25_EMI>0.001)].reset_index(drop=True)
    if len(dfc)==0:continue
    x,y=np.mean(dfc.UTM_E),np.mean(dfc.UTM_N)
    loni, lati = pnyc(x, y, inverse=True)
    for s in ['lon','lat']:
        exec(s+'.append('+s+'i)')
    name.append(c)
    dfc['X'],dfc['Y']=dfc.UTM_E+Xcent,dfc.UTM_N+Ycent    
    dfc['STR']=['SO LOCATION {:s} POINT {:.0f} {:.0f} 0.\n SRCPARAM {:s} {:.3f} {:.1f} {:.1f} {:.2f} {:.1f}\n'.format(c,x,y,s,p,h,t+273,v,d) for \
    c,x,y,s,p,h,t,v,d in zip(list(dfc.CP_NO),list(dfc.X),list(dfc.Y),\
    list(dfc.CP_NO),list(dfc.PM25_EMI),list(dfc.HEI),list(dfc.TEMP),list(dfc.DIA),list(dfc.VEL))]
    ss=''
    for s in dfc.STR:
        ss+=s+' '
    desc.append(ss)
40/55: df=read_csv('point_QC.csv')
40/56: df['C_NO']=[i[:-4] for i in df.CP_NO]
40/57: lon,lat,name,desc=([] for i in range(4))
40/58:
for c in set(df.C_NO):
    dfc=df.loc[(df.C_NO==c) & (df.PM25_EMI>0.001)].reset_index(drop=True)
    if len(dfc)==0:continue
    x,y=np.mean(dfc.UTM_E),np.mean(dfc.UTM_N)
    loni, lati = pnyc(x, y, inverse=True)
    for s in ['lon','lat']:
        exec(s+'.append('+s+'i)')
    name.append(c)
    dfc['X'],dfc['Y']=dfc.UTM_E+Xcent,dfc.UTM_N+Ycent    
    dfc['STR']=['SO LOCATION {:s} POINT {:.0f} {:.0f} 0.\n SRCPARAM {:s} {:.3f} {:.1f} {:.1f} {:.2f} {:.1f}\n'.format(c,x,y,s,p,h,t+273,v,d) for \
    c,x,y,s,p,h,t,v,d in zip(list(dfc.CP_NO),list(dfc.X),list(dfc.Y),\
    list(dfc.CP_NO),list(dfc.PM25_EMI),list(dfc.HEI),list(dfc.TEMP),list(dfc.DIA),list(dfc.VEL))]
    ss=''
    for s in dfc.STR:
        ss+=s+' '
    desc.append(ss)
40/59:
for s in col:
    exec('print (len('+s+'))')
40/60: DD={}
40/61:
for s in ['lon','lat','name','desc']:
    exec('DD.update({"'+s+'":'+s+'})')
dfd=DataFrame(DD)
40/62: dfd[col].set_index('lon').to_csv('tedsSRCPARAM.csv')
40/63: run /home/kuang/bin/csv2kml.py -f tedsSRCPARAM.csv -n N -g LL
40/64: !lst
40/65: lon,lat,name,desc=([] for i in range(4))
40/66: history
40/67: !vi mk_kml.py
40/68: !vi mk_kml.py
40/69: run mk_kml.py
40/70: !vi mk_kml.py
40/71: run mk_kml.py
40/72: !lst
40/73: !vi mk_kml.py
40/74: run mk_kml.py
40/75:
GEO='/nas1/WRF4.1/WPS/geo_em.d04_333m.nc'
nc = netCDF4.Dataset(GEO, 'r')
v='HGT_M'
c=np.array(nc.variables[v][0,:,:])
for v in ['CLAT','CLONG']:
    exec(v+'=nc.variables[v][0,:,:]',locals())
xg,yg=pnyc(CLONG,CLAT, inverse=False)
40/76: xg[0,0],yg[0,0]
40/77: len(df)
40/78: xg[0,1]-xg[0,0]
40/79: xg[0,100]-xg[0,99]
40/80: xg.shape
40/81: run mk_kml.py
40/82: xg.shape
40/83: run mk_kml.py
40/84: run mk_kml.py
40/85: !lst
40/86: !head ../terr_results/point_ij.csv
41/1: import netCDF4
41/2: nc = netCDF4.Dataset(NCfname,'r')
41/3: NCfname='1601FX2.S.nc8'
41/4: nc = netCDF4.Dataset(NCfname,'r')
41/5: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
41/6: V[3]
41/7: v=V[3][0]
41/8: import numpy as np
41/9: np.max(nc.variable[v][:])
41/10: np.max(nc.variables[v][:])
42/1: run fil_rean.py 1601 n2
42/2: var.shape
42/3: dt0[16*8]
42/4: 16*8
42/5: var=np.zeros(shape=(nt0-16*8,nlay0,nrow0,ncol0))
42/6: var[:,:,:,:]=nc0.variables[v][16*8:,:,:,:]
42/7: bnd=var[:,:,idx[0][:],idx[1][:]]
42/8: nt0
42/9: !lst
42/10: !lst /nas1/ecmwf/reanalysis/gribs
42/11: !lst /nas1/ecmwf/reanalysis/gribs
42/12: run fil_rean.py 1601 n2
42/13: var[:,:,:,:]=nc0.variables[v][:,:,:,:]
43/1: run fil_rean.py 1601 n2
43/2: var/shape
43/3: var.shape
43/4: nc0.variables[v].shape
43/5: var[:,:,:,:]=nc0.variables[v][:,:,:,:]
44/1: run fil_rean.py 1601 n2
44/2: var.shape
44/3: dt0[:5]
44/4: dt0[-5:]
44/5: a=nc0.variables[v][:]
44/6: a.shape
44/7: import xarray as xr
44/8: !pip install xarray
44/9: import xarray as xr
46/1: import xarray as xr
46/2: run fil_rean.py 1601 n2
46/3: fname
46/4: ds =  xr.open_dataset(fname)
46/5: var=ds.[v][:]
46/6: var=ds[v][:]
46/7: var.shape
46/8: var[:5,:5,:5,:5]
46/9: dir(ds)
46/10: nc0 = xr.open_dataset(fname)
46/11: V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
46/12: V0
46/13: run fil_rean.py 1601 n2
46/14: run fil_rean.py 1601 n2
47/1: run fil_rean.py 1601 n2
47/2: var=nc0[v][:]
47/3: bnd=var[:,:,idx[0][:],idx[1][:]]
47/4: run fil_rean.py 1601 n2
47/5: run fil_rean.py 1601 n2
48/1: run fil_rean.py 1601 n2
48/2: nm in V[2]
48/3: v
48/4: v in V[2]
48/5: fname
48/6:   if v not in V[2]:sys.exit(v+' not in BCON file')
48/7:   nc.variables[v][:]=0
48/8:   tb,te=(np.where(dt0==dti[i])[0][0] for i in [0,-1])
48/9: tb,te
48/10:
  for t0 in range(tb,te+1):
    ymdh=dt0[t0]
    if ymdh not in dti:continue
    ti=np.where(dti==ymdh)[0]
    for t in ti:
      nc.variables[v][t,:,:,0]=bnd[t0,:,:]
48/11: ti
48/12: ymdh
48/13: dti[0]
48/14: t0
48/15: nc.variables[v][t,:,:,0].shape
48/16: t
48/17: v
48/18: nc.variables[v].shape
48/19:
    for t in ti:
      nc.variables[v][t,:,:]=bnd[t0,:,:]
48/20: bnd[t0,:,:].shape
48/21: bnd=var[:,:,idx[0][:],idx[1][:]]
48/22: bnd.shape
48/23: bnd=np.zeros(shape=nt,nlay,nbd)
48/24: bnd=np.zeros(shape=(nt,nlay,nbd))
48/25: bnd=np.zeros(shape=(nt,nlay,nbnd))
48/26: bnd.shape
48/27: bnd=np.zeros(shape=nt0,nlay0,nbd)
48/28: bnd=np.zeros(shape=(nt0,nlay0,nbnd))
48/29: bnd.shape
48/30: nbnd=(ncNCOLS+ncNROWS)*2+4
48/31: nbnd
48/32: bnd=np.zeros(shape=(nt0,nlay0,nbnd))
48/33:
for i in range(nbnd):
  bnd[:,:,i]=var[:,:,idx[0][i],idx[1][i]]
49/1: run fil_rean.py 1601 n2
49/2: bnd=var[:,:,idx[0][:],idx[1][:]]
49/3: bnd.shape
49/4: bnd[0,0,:5,:5]
49/5: [bnd[0,0,i,i] for i in range(5)]
49/6: var=np.array(nc0[v][:])
50/1: run fil_rean.py 1601 n2
50/2: bnd=var[:,:,idx[0][:],idx[1][:]]
50/3: [bnd[0,0,i,i] for i in range(5)]
50/4: [bnd[0,0,i,i].values for i in range(5)]
50/5: var=nc0[v][:].values
51/1: run fil_rean.py 1601 n2
51/2: bnd=var[:,:,idx[0][i],idx[1][i]]
51/3: bnd=var[:,:,idx[0][:],idx[1][:]]
51/4: a=[bnd[:,:,i,i].values for i in range(nbnd)]
52/1: run fil_rean.py 1601 n2
52/2: t0=0
52/3: bnd[t0,:,:].shape
52/4: bnd.shape
52/5: idx[0][0],idx[1][0]
52/6: bnd[0,0,0,0]
52/7: var[0,0,251,223]
52/8: tmp=var[:,:,idx[0][:],idx[1][:]]
52/9: bnd=np.zeros(shape=(nt0,nlay0,nbnd))
52/10:
for n in range(nbnd)
  bnd[:,:,n]=tmp[:,:,n,n]
52/11:
for n in range(nbnd):
  bnd[:,:,n]=tmp[:,:,n,n]
52/12: bnd=np.diagonal(tmp,axis1=2, axis2=3)
52/13: bnd.shape
52/14: bnd[0,0,0]
52/15: run fil_rean.py 1601 n2
52/16: t
52/17: t0
52/18: bnd[t0,0,0]
52/19: nc.variables[v][t,0,0]
52/20: nc.variables[v]
52/21: nc0.variables[v]
52/22: run fil_rean.py 1601 n2
52/23: bnd.shape
52/24: var.shape
52/25: nc.variables[V[2][0]].shape
52/26: V[2][0]
52/27: run fil_rean.py 1601 n2
52/28:
  var2=var[:]
  var2[:]=0.
52/29: run fil_rean.py 1601 n2
52/30: nc.variables[v][:24,0,0]
52/31: var2[:24,0,0]
52/32: var2[100:124,0,0]
52/33: var2.shape
52/34: var2[10:34,0,0]
52/35: var2[10:34,2,1]
52/36: var[10:34,2,1]
52/37: np.max(var)
52/38: np.max(bnd)
52/39:
for t0 in range(tb,te+1):
    ymdh=dt0[t0]
    if ymdh not in dti:continue
    print(ymdh)
    ti=np.where(dti==ymdh)[0]
    for t in ti:
        print(t)
52/40: var.shape
52/41: tb,te
52/42: np.max(bnd[tb:te+1,:,:])
52/43: np.mean(bnd[tb:te+1,:,:])
52/44: np.mean(var[:,:,:])
52/45: np.max(var[:,:,:])
52/46:
  for t0 in range(tb,te+1):
    ymdh=dt0[t0]
    if ymdh not in dti:continue
    ti=np.where(dti==ymdh)[0]
    for t in ti:
      var[t,:,:]=bnd[t0,:,:]
52/47: np.max(var[:,:,:])
52/48:
  var2=var[:]
  var2[:]=0.
  for t in range(0,nt-3,3):
    var2[t+1,:,:]=var[t,:,:]*2/3+var[t+3,:,:]*1/3
    var2[t+2,:,:]=var[t,:,:]*1/3+var[t+3,:,:]*2/3
  nc.variables[v][:]=var2[:]
52/49: nc.variables[v][:24,0,0]
52/50: np.max(var2[:,:,:])
52/51: nt
52/52: var2=np.zeros(shape=var.shape)
52/53:
  for t in range(0,nt-3,3):
    var2[t+0,:,:]=var[t,:,:]
    var2[t+1,:,:]=var[t,:,:]*2/3+var[t+3,:,:]*1/3
    var2[t+2,:,:]=var[t,:,:]*1/3+var[t+3,:,:]*2/3
52/54: np.max(var2[:,:,:])
52/55: np.max(var[:,:,:])
52/56:
  var=np.zeros(shape=(nt,nlay,nbnd))
  for t0 in range(tb,te+1):
    ymdh=dt0[t0]
    if ymdh not in dti:continue
    ti=np.where(dti==ymdh)[0]
    for t in ti:
      var[t,:,:]=bnd[t0,:,:]
  var2=np.zeros(shape=var.shape)
  for t in range(0,nt-3,3):
    var2[t+0,:,:]=var[t,:,:]
    var2[t+1,:,:]=var[t,:,:]*2/3+var[t+3,:,:]*1/3
    var2[t+2,:,:]=var[t,:,:]*1/3+var[t+3,:,:]*2/3
52/57: np.max(var[:,:,:])
52/58: np.max(var2[:,:,:])
52/59: run fil_rean.py 1601 n2
52/60: nc.variables[v][:24,0,0]
52/61: bnd[:24,0,0]
52/62: nc0.variables[v][:24,0,0]
52/63: nc0.variables[v][:24,0,0,0]
53/1: import netCDF4
53/2: fname='BCON_v53_1601_run10_regrid_20160120_TWN_3X3'
53/3: nc = netCDF4.Dataset(fname,'r')
53/4: nc.variables[v][:24,0,0]
53/5: v='NO2'
53/6: nc.variables[v][:24,0,0]
53/7: v='O3'
53/8: nc.variables[v][:24,0,0]
53/9: fname='/nas1/ecmwf/reanalysis/gribs/o31603D2.m3.nc'
53/10: nc = netCDF4.Dataset(fname,'r')
53/11: v=
53/12: v
53/13: nc.variables[v][:24,0,0]
53/14: nc.variables[v][:24,0,0,0]
53/15: fname='/nas1/ecmwf/reanalysis/gribs/o31603.nc'
53/16: nc = netCDF4.Dataset(fname,'r')
53/17: nc.variables[v][:24,0,0,0]
53/18: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
53/19: V
53/20: v='VAR_192_210_203_P0_L105_GLL0'
53/21: nc.variables[v][:24,0,0,0]
53/22: 5.0024555e-06*28/48*10**6
53/23: run fil_rean.py 1601 n2
53/24: run ../acc_prob.py
53/25: run fil_rean.py 1601 n2
53/26: var[10:34,2,1]
53/27: var2[10:34,2,1]
54/1: run fil_rean.py 1602 o3
54/2: dti[0],dti[-1]
54/3: 2016022400 in dt0
54/4: len(dt0)
54/5: run fil_rean.py 1604 o3
54/6: len(dt0)
54/7: run fil_rean.py 1608 o3
54/8:
len(['21', '22', '23', '24', '25', '26', '27',
     '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38',
     '39', '40', '42', '43', '44', '46', '47', '48', '49', '50', '51',
     '53', '54', '56', '57', '59'])
54/9:
l34=['21', '22', '23', '24', '25', '26', '27',
     '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38',
     '39', '40', '42', '43', '44', '46', '47', '48', '49', '50', '51',
     '53', '54', '56', '57', '59']
54/10:
l40=['21','21', '22','22', '23','24', '24', '25','25', '26', '27',
     '28','28', '29', '30', '31','32', '32', '33', '34', '35', '36', '37', '38',
     '39', '40', '42', '43', '44', '46', '47', '48', '49', '50', '51',
     '53', '54', '56', '57', '59']
54/11: d40_23=[l40[k]:l34.index(l40[k]) for k in range(4)]
54/12: d40_23={l40[k]:l34.index(l40[k]) for k in range(4)}
54/13: d40_23={40-k:l34.index(l40[k]) for k in range(40)}
54/14: d40_23
54/15: d40_23={39-k:l34.index(l40[k]) for k in range(40)}
54/16: d40_23
54/17:
len(['21', '22', '23', '24', '25', '26', '27',
     '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38',
     '39', '40', '42', '43', '44', '46', '47', '48', '49', '50', '51',
     '53', '54', '56', '57', '59'])
54/18: l34[33]
54/19: !psg fil
54/20: !lst
54/21: !kil fil_rean.py
54/22: !psg fil
54/23: !kill -9 207129207129 207129 207133 207139 207141 207149
54/24: !psg fil
54/25: run fil_rean.py 1601 o3
55/1: run fil_rean.py 1601 o3
55/2:
for fname in fnames[1:]:
  print (fname)
  nc = netCDF4.Dataset(fname,'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  if v not in V[2]:sys.exit(v+' not in BCON file')
  nt,nlay,nbnd=nc.variables[v].shape#=(nc.NCOLS+nc.NROWS)*2+4
  julies,times=(np.array([nc.variables['TFLAG'][i,0,j] for i in range(nt)]) for j in range(2))
  times=[int((i/10000+1.5)/3)*30000 for i in times]
  dt=[jul2dt([i,j]) for i,j in zip(julies,times)]
  dti=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
  tb,te=(np.where(dt0==dti[i])[0][0] for i in [0,-1])
  var=np.zeros(shape=(nt,nlay,nbnd))
  for t0 in range(tb,te+1):
    ymdh=dt0[t0]
    if ymdh not in dti:continue
    ti=np.where(dti==ymdh)[0]
    for t in ti:
      var[t,:,:]=bnd[t0,:,:]
  var2=np.zeros(shape=var.shape)
  # interpolate the
  for t in range(0,nt-3,3):
    var2[t+0,:,:]=var[t,:,:]
    var2[t+1,:,:]=var[t,:,:]*2/3+var[t+3,:,:]*1/3
    var2[t+2,:,:]=var[t,:,:]*1/3+var[t+3,:,:]*2/3
  nc.variables[v][:]=var2[:]
  nc.close()
55/3: fname='BCON_v53_1601_run11_regrid_20160124_TWN_3X3'
55/4: nc = netCDF4.Dataset(fname,'r')
55/5: v
55/6: nc.variables[v][:24,0,0,0]
55/7: nc.variables[v][:24,0,0]
56/1: run fil_rean.py 1601 o3
56/2: !pip install xarray
56/3: run fil_rean.py 1601 o3
56/4: nt,nvars,ndt=(nc.variables[v].shape[i] for i in range(3))
56/5: fname='BCON_v53_1601_run10_regrid_20160120_TWN_3X3'
56/6: nc = netCDF4.Dataset(fname,'r')
56/7: fname='01/BCON_v53_1601_run10_regrid_20160120_TWN_3X3'
56/8: nc1 = netCDF4.Dataset(fname,'r')
56/9: v
56/10: nc.variables[v][:24,0,0]
56/11: nc1.variables[v][:24,0,0]
56/12: (nc1.variables[v][:24,0,0]==nc.variables[v][:24,0,0]).all()
56/13: (nc1.variables[v][:24,10,10]==nc.variables[v][:24,10,10]).all()
56/14: (nc1.variables[v][124:148,10,10]==nc.variables[v][124:148,10,10]).all()
56/15: tb,te
56/16: ymdh
56/17: ymdh not in dti
56/18: ti=np.where(dti==ymdh)[0]
56/19: ti
56/20: t0
56/21: dt0[t0]
56/22: y
56/23: t
56/24: np.mean(var[t,:,:])
56/25: np.mean(var[t,0,:])
56/26: np.mean(nc.variables[v][:])
56/27: np.mean(var2[:])
56/28: np.mean(nc1.variables[v][:])
56/29: np.mean(var[:])
56/30: np.mean(bnd[:])
56/31: run fil_rean.py 1601 n2
56/32: run fil_rean.py 1604 n2
57/1: run fil_rean.py 1612 o3
57/2: cd /nas1/cmaqruns/2016base/data/emis/TEDS
57/3: cd /nas1/cmaqruns/2016base/data/emis/TEDS/1601
57/4: fname='ptsG_TWN_3X3.20160109.nc'
57/5: nc = netCDF4.Dataset(fname,'r')
57/6: v='TFLAG'
57/7: nc.variables[v][:,0,:]
57/8: fname='ptse_TWN_3X3.20160109.nc'
57/9: nc1 = netCDF4.Dataset(fname,'r')
57/10: nc1.variables[v][:,0,:]
57/11: run ~/bin/pr_tflag.py ptse_TWN_3X3.20160110.nc
57/12: run ~/bin/pr_tflag.py ptse_TWN_3X3.20160111.nc
57/13: run ~/bin/pr_tflag.py ptse_TWN_3X3.20160112.nc
57/14: run ~/bin/pr_tflag.py ptse_TWN_3X3.20160113.nc
57/15: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
57/16: [i for i in V[3] if 'E' in i]
57/17: cd /nas1/cmaqruns/2016base/data/emis/TEDS
57/18:
pth='area biog line ptse ship'.split()
ext={i:i for i in pth}
ext.update({'ship':'51Ab','ptse':'ptsG'})
home='/nas1/TEDS/teds10_camx/HourlyWeighted'
57/19: ipth=3
57/20: p=pth[ipth]
57/21: p
57/22: m=0
57/23: m=1
57/24:
    mm='{:02d}'.format(m)
    fname=home+'/'+p+'/'+'fortBE.413_teds10.'+ext[p]+mm+'.nc'
    fnameO=p+'_TWN_3X3.16'+mm+'.nc'
57/25:
    fname=home+'/'+p+'/'+'fortBE.413_teds10.'+ext[p]+mm+'.nc'
    fnameO=p+'_TWN_3X3.16'+mm+'.nc'
57/26:
    nc0= netCDF4.Dataset(fname, 'r')
    nc = netCDF4.Dataset(fnameO, 'r+')
    V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
    V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
    nv=len(V[3])
    nt,nlay,nrow,ncol=nc0.variables[V0[3][0]].shape
    sdatetime=[jul2dt(nc0.variables['TFLAG'][t,0,:]) for t in range(nt)]
    jul2=[dt2jul(i+datetime.timedelta(hours=-8)) for i in sdatetime]
57/27: o in jul2
57/28: 0 in jul2
57/29: jul2
57/30: jul2[:50]
57/31: jul2[:30]
57/32: jul2[30:60]
57/33: jul2[60:90]
57/34: jul2[90:120]
57/35: jul2[120:150]
57/36: jul2[150:180]
57/37: jul2[180:210]
57/38: jul2[210:240]
57/39: jul2[240:270]
57/40: [i for i in V[3] if 'P' in i]
57/41: pwd
57/42: cd ../../bcon
57/43: fname='/home/cmaqruns/2016base/data/bcon/BCON_v53_1612_run5_regrid_20161201_TWN_3X3'
57/44: nc = netCDF4.Dataset(fname,'r')
57/45: run ~/bin/pr_tflag.py /home/cmaqruns/2016base/data/bcon/BCON_v53_1612_run5_regrid_20161201_TWN_3X3
57/46: ls -l /home/cmaqruns/2016base/data/bcon/BCON_v53_1612_run5_regrid_*
57/47: run ~/bin/pr_tflag.py /home/cmaqruns/2016base/data/bcon/BCON_v53_1612_run5_regrid_20161201_sChina_27k
57/48: ls -l /home/cmaqruns/2016base/data/bcon/BCON_v53_1612_run6_regrid_*
57/49: run ~/bin/pr_tflag.py /nas1/cmaqruns/2016base/data/bcon/BCON_v53_1612_run6_regrid_20161205_TWN_3X3
58/1: run fil_rean.py 1612 o3
59/1: run fil_rean.py 1612 o3
59/2: fname='/nas1/ecmwf/reanalysis/gribs/o31612D2.m3.nc'
59/3: nc = netCDF4.Dataset(fname,'r+')
59/4: v='TFLAG'
59/5: nc.variables[v][-5:,0,:]
59/6: tb,te
59/7: dti[i]
59/8: i=-1
59/9: dti[i]
59/10: i,j,k=nc.variables[v].shape
59/11: i
59/12: nc.variables[v][i,:,0]=2017003
59/13: nc.variables[v][i,:,1]=0
59/14: nc.variables['O3'][i-1,0,0,0]
59/15: nc.variables['O3'][i-1,:,0,0]
59/16: nc.variables['O3'][i-2,:,0,0]
59/17: nc.variables['O3'][2,:,0,0]
59/18: np.max(nc.variables['O3'][:])
59/19: fname='/nas1/ecmwf/reanalysis/gribs/o31601D2.m3.nc'
59/20: nc.close()
59/21: nc = netCDF4.Dataset(fname,'r+')
59/22: np.max(nc.variables['O3'][:])
59/23: !top
59/24: pwd
59/25: cd /nas1/ecmwf/reanalysis/gribs
59/26: run ../grb2m3.py 2 o31601.nc
59/27: !lst
59/28: run ../grb2m3.py 2 o31612.nc
59/29: fname
59/30: run ~/bin/mxNC o31601D2.m3.nc
59/31: run ~/bin/mxNC o31612D2.m3.nc
59/32: nm
59/33: c.shape
59/34: nlay
59/35: zz.shape
59/36: nlay1
59/37: type(xyc)
59/38: len(xyc)
59/39: type(x1)
59/40: x1.shape
59/41: y1.shape
59/42: nt1,nlay1,nrow1,ncol1
59/43: maxx - minx
59/44: 585*3000
59/45: (maxx - minx) /2+nc1.XCELL*10
59/46: (maxx - minx) /2+3000*10
59/47: fnameO
59/48: nc1= netCDF4.Dataset(fnameO,'r+')
59/49: nc1.XCELL
59/50: (maxx - minx) /2+nc1.XCELL*10
59/51: (maxx - minx) /2+nc1.XCELL*10 *2
59/52: ((maxx - minx) /2+nc1.XCELL*10 )*2
59/53: np.max(zz)
59/54: np.where(zz=np.isnan)
59/55: np.where(zz==np.isnan)
59/56: np.max(zz[:])
59/57: np.mean(zz[:])
59/58: zz.shape
59/59: zz[:5,:5,:5,:5]
59/60: pwd
59/61: run ../grb2m3.py 2 o31612.nc
59/62: !lst
59/63: cd /nas1/cmaqruns/2016base/data/bcon
59/64: run fil_rean.py 1612 o3
59/65: [dti[i] for i in [0,-1]]
59/66: fname=o31612D2.m3.nc
59/67: fname='/nas1/ecmwf/reanalysis/gribs/o31612D2.m3.nc'
59/68: nc = netCDF4.Dataset(fname,'r+')
59/69: v='TFLAG'
59/70: i,j,k=nc.variables[v].shape
59/71: nc.variables[v][i-1,0,:]
59/72: nc.variables[v][i,:,1]=0
59/73: nc.variables[v][i,:,0]=2017003
59/74: nc.variables['O3'][i,:,:,:]=nc.variables['O3'][i-1,:,:,:]
59/75: nc.close()
59/76: run fil_rean.py 1612 o3
59/77: [dti[i] for i in [0,-1]]
59/78: dt0[-5:]
59/79: pwd
59/80: fname
59/81: fname='/nas1/ecmwf/reanalysis/gribs/o31612D2.m3.nc'
59/82: nc = netCDF4.Dataset(fname,'r+')
59/83: fname='/nas1/ecmwf/reanalysis/gribs/'+sp+ym+'D2.m3.nc'
59/84: fname
59/85:
nc0 = xr.open_dataset(fname)
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nrow0,ncol0=(nc0.variables[V0[3][0]].shape[i] for i in range(4))
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt0=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
59/86: dt0[-5:]
59/87: ls -lrth /nas1/ecmwf/reanalysis/gribs/o31612D2.m3.nc
59/88: date
59/89: !date
59/90: fname
59/91: nc = netCDF4.Dataset(fname,'r+')
59/92: v='TFLAG'
59/93: i,j,k=nc.variables[v].shape
59/94: i,j,k
59/95: nt0
59/96: run fil_rean.py 1612 o3
59/97: nt0
59/98: run fil_rean.py 1612 o3
59/99: run fil_rean.py 1612 o3
59/100: var.shape
59/101: tmp.shape
59/102: run fil_rean.py 1612 o3
59/103: run ~/bin/mxNC BCON_v53_1612_run10_regrid_20161221_TWN_3X3
59/104: fname='BCON_v53_1612_run10_regrid_20161221_TWN_3X3'
59/105: nc = netCDF4.Dataset(fname,'r')
59/106: v='O3'
59/107: nc.variables[v][:5,:5,:5]
   1: !vi fil_rean.py
   2: run fil_rean.py 1601 n2
   3: [dti[i] for i in [0,-1]]
   4: 2016020200 in dt0
   5: !vi fil_rean.py
   6: dt0[-1]
   7: !vi fil_rean.py
   8: run fil_rean.py 1601 n2
   9: !vi fil_rean.py
  10: run fil_rean.py 1601 n2
  11: run fil_rean.py 1601 o3
  12: run fil_rean.py 1601 propane_
  13: run fil_rean.py 1601 nitrogen_monoxide__
  14: run fil_rean.py 1601 nitrogen_monoxide_
  15: run fil_rean.py 1601 is
  16: pwd
61/1: import netCDF4
61/2: !ls
61/3: fname='BCON_v53_1902_run7_regrid_20190208_TWN_3X31'
61/4: nc = netCDF4.Dataset(fname,'r')
61/5:
import numpy as np
avg=[]
for i in 'IJK':
  avg.append(np.mean(nc.variables['ASO4'+i][:]))
61/6: avg
61/7:
avg=[0.07304604, 1.4140079, 0.2478455]
avgr=[avg[i]/sum(avg) for i in range(3)]
SO4rate={'IJK'[i]:j for i,j in zip([0,1,2],avrg)}
61/8:
avg=[0.07304604, 1.4140079, 0.2478455]
avrg=[avg[i]/sum(avg) for i in range(3)]
SO4rate={'IJK'[i]:j for i,j in zip([0,1,2],avrg)}
61/9: SO4rate
61/10: i,j=2,3
61/11: i*=j
61/12: i
61/13: i,j=[2,4],3
61/14: i*=j
61/15: i
61/16: i,j=np.array([2,4]),3
61/17: i*=j
61/18: i
61/19: pwd
62/1: from PseudoNetCDF.camxfiles.Memmaps import lateral_boundary
62/2: ls
62/3: ls base*
62/4: fname='base.grd02.1903.bc'
62/5: nc=lateral_boundary(fname,'r')
62/6: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
62/7: vn=[i for i in V[2] if 'NO2' in i]
62/8: vn
62/9: nc.variables[vn[0]].shape
62/10: import numpy as np
62/11: np.max(nc.variables[vn[0]][100:110,:,:])
62/12:
for i in range(4):
    print(np.max(nc.variables[vn[i]][100:110,:,:]))
62/13:
for i in range(4):
    print(np.max(nc.variables[vn[i]][:,:,:]))
62/14:
for i in range(4):
    print(np.max(nc.variables[vn[i]][:200,:,:]))
62/15:
for i in range(4):
    print(np.max(nc.variables[vn[i]][100:200,:,:]))
62/16:
for i in range(4):
    print(np.max(nc.variables[vn[i]][100:150,:,:]))
62/17:
for i in range(4):
    print(np.max(nc.variables[vn[i]][120:150,:,:]))
62/18:
for i in range(4):
    print(np.max(nc.variables[vn[i]][110:120,:,:]))
62/19:
for i in range(4):
    print(np.max(nc.variables[vn[i]][100:120,:,:]))
62/20:
for i in range(4):
    print(np.max(nc.variables[vn[i]][110:120,:,:]))
62/21:
for i in range(4):
    print(np.max(nc.variables[vn[i]][100:110,:,:]))
62/22:
for i in range(4):
    print(np.max(nc.variables[vn[i]][110:115,:,:]))
62/23:
for i in range(4):
    print(np.max(nc.variables[vn[i]][115:117,:,:]))
62/24:
for t in range(117,120):
  for i in range(4):
    print(np.max(nc.variables[vn[i]][t,:,:]))
62/25:
for t in range(115,120):
  for i in range(4):
    print(np.max(nc.variables[vn[i]][t,:,:]))
62/26: t=118
62/27:
for v in V[2]:
    print(np.max(nc.variables[v][t,:,:]))
62/28:
for v in V[2]:
    print(v,np.array(np.max(nc.variables[v][t,:,:])))
62/29: t=119
62/30:
for v in V[2][:20]:
    print(v,np.array(np.max(nc.variables[v][t,:,:])))
62/31: v='TFLAG'
62/32: nc.variables[v][t,0,:]
62/33: v='ETFLAG'
62/34: nc.variables[v][t,0,:]
62/35: fname='base.grd02.1904.bc'
62/36: nc=lateral_boundary(fname,'r')
62/37:
for v in ['TFLAG','ETFLAG']:
    print(nc.variables[v][t,0,:])
62/38: pwd
62/39: cd 1903
62/40: ls
62/41: nc='20190305'
62/42: fname='20190305'
62/43: nc = netCDF4.Dataset(fname,'r')
62/44: import netCDF4
62/45: nc = netCDF4.Dataset(fname,'r')
62/46: v='NO2'
62/47:
for t in range(24):
    print(t,np.array(np.max(nc.variables[v][t,:,:])))
62/48: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
62/49: len(V)
62/50: len(V[2])
62/51:
for v in V[2]:
  for t in range(24):
    print(v,t,np.array(np.max(nc.variables[v][t,:,:])))
62/52:
with open('a','w') as f:
  for v in V[2]:
    for t in range(24):
      f.write('{:s} {:d} {:f}\n'.format(v,t,np.array(np.max(nc.variables[v][t,:,:]))))
62/53: !vi a
62/54: !vi out3.txt
62/55: !lst
62/56: cd .
62/57: !lst
62/58: cd ..
62/59: !lst
62/60: !vi out3.txt
62/61: fname='base.grd02.1903.bc'
62/62: nc=lateral_boundary(fname,'r')
62/63: t=119
62/64: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
62/65:
for v in V[2][:20]:
    print(v,np.array(np.max(nc.variables[v][t,:,:])))
62/66: t=118
62/67:
for v in V[2][:20]:
    print(v,np.array(np.max(nc.variables[v][t,:,:])))
62/68: t=117
62/69:
for v in V[2][:20]:
    print(v,np.array(np.max(nc.variables[v][t,:,:])))
62/70: t=0
62/71:
for v in V[2][:20]:
    print(v,np.array(np.max(nc.variables[v][t,:,:])))
62/72: v
62/73:
for t in range(117):
    print(t,np.array(np.max(nc.variables[v][t,:,:])))
62/74:
for t in range(117,200):
    print(t,np.array(np.max(nc.variables[v][t,:,:])))
62/75: v='O'
62/76: v='NO'
62/77:
for t in range(117):
    print(t,np.array(np.max(nc.variables[v][t,:,:])))
62/78: v='WEST_NO'
62/79:
for t in range(117):
    print(t,np.array(np.max(nc.variables[v][t,:,:])))
62/80: t=117
62/81:
for v in V[2][:20]:
    print(v,np.array(np.max(nc.variables[v][t,:,:])))
62/82: v='WEST_NO'
62/83:
for t in range(117,200):
    print(t,np.array(np.max(nc.variables[v][t,:,:])))
62/84:
for v in V[2][:20]:
  for t in range(116,121)    
    print(v,t,np.array(np.max(nc.variables[v][t,:,:])))
62/85:
for v in V[2][:20]:
  for t in range(116,121):
    print(v,t,np.array(np.max(nc.variables[v][t,:,:])))
62/86:
for v in V[2][20,40,4]:
  for t in range(116,121):
    print(v,t,np.array(np.max(nc.variables[v][t,:,:])))
62/87:
for v in V[2][20:40:4]:
  for t in range(116,121):
    print(v,t,np.array(np.max(nc.variables[v][t,:,:])))
62/88:
for v in V[2][40:120:4]:
  for t in range(116,121):
    print(v,t,np.array(np.max(nc.variables[v][t,:,:])))
62/89: ls -lh /nas1/cmaqruns/2019base/data/bcon/BCON_v53_1903_run5*
62/90: fname='/nas1/cmaqruns/2019base/data/bcon/BCON_v53_1903_run5_regrid_20190302_TWN_3X3'
62/91: pwd
62/92: fname='BCON_v53_1903_run5_regrid_20190302_TWN_3X3'
62/93: nc = netCDF4.Dataset(fname,'r')
62/94: v='TFLAG'
62/95:
import datetime
import subprocess
import sys
import xarray as xr

def dt2jul(dt):
  yr=dt.year
  deltaT=dt-datetime.datetime(yr,1,1)
  deltaH=int((deltaT.total_seconds()-deltaT.days*24*3600)/3600.)
  return (yr*1000+deltaT.days+1,deltaH*10000)
def jul2dt(jultm):
  jul,tm=jultm[:]
  yr,ih=int(jul/1000), int(tm/10000.)
  return datetime.datetime(yr,1,1)+datetime.timedelta(days=int(jul-yr*1000-1))+datetime.timedelta(hours=ih)
62/96:
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nrow0,ncol0=(nc0.variables[V0[3][0]].shape[i] for i in range(4))
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt0=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
62/97: nt0,nlay0,nbnd0=(nc0.variables[V0[2][0]].shape[i] for i in range(3))
62/98:
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt0=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
62/99: len(dt0)
62/100: dt0[:5]
62/101: fname
62/102: fname='BCON_v53_1903_run5_regrid_20190302_TWN_3X31'
62/103:
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nbnd0=(nc0.variables[V0[2][0]].shape[i] for i in range(3))
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt1=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
62/104: dt1[:5]
62/105: v='O3'
62/106: nc.variables[v][:5,0,0]
62/107: pwd
62/108: cd 1903
62/109: fname='bcon'
62/110: nc = netCDF4.Dataset(fname,'r')
62/111:
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nbnd0=(nc0.variables[V0[2][0]].shape[i] for i in range(3))
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt1=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
62/112: dt1[:5]
62/113: np.where(dt1==2019030500)
62/114: v='O3'
62/115: nc.variables[v][118:125,0,0]
62/116: fname='20190305'
62/117:
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nbnd0=(nc0.variables[V0[2][0]].shape[i] for i in range(3))
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt1=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
62/118: fname='bcon'
62/119:
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nbnd0=(nc0.variables[V0[2][0]].shape[i] for i in range(3))
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt1=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
62/120: nc0.variables[v][118:125,0,0]
62/121: dt1[118:125]
62/122: nc0.variables[v][115:125,0,0]
62/123: dt1[115:125]
62/124: fname='20190304'
62/125:
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nbnd0=(nc0.variables[V0[2][0]].shape[i] for i in range(3))
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt1=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
62/126: nc0.variables[v][:5,0,0]
62/127: nc0.variables[v][-5:,0,0]
62/128: pwd
62/129: cd ../
62/130: fname='BCON_v53_1903_run5_regrid_20190302_TWN_3X31'
62/131:
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nbnd0=(nc0.variables[V0[2][0]].shape[i] for i in range(3))
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt1=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
62/132: np.where(dt1==2019030422)
62/133: nc.variables[v][40:50,0,0]
62/134:
for v in V0[2]:
    print(v,np.array(np.max(nc.variables[v][40:50,:,:])))
62/135: fname
62/136: fname='20190304'
62/137:
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nbnd0=(nc0.variables[V0[2][0]].shape[i] for i in range(3))
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt1=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
62/138: pwd
62/139: cd 1903
62/140: !lst
62/141:
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nbnd0=(nc0.variables[V0[2][0]].shape[i] for i in range(3))
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt1=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
62/142: nc.variables[v][:5,0,0]
62/143: v
62/144: v='O3'
62/145: nc.variables[v][-5:,0,:]
62/146: nc.variables[v][-5:,0,0]
62/147: np.max(nc.variables[v][:,:,:])
62/148: np.max(nc0.variables[v][:,:,:])
62/149: fname='20190305'
62/150:
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nbnd0=(nc0.variables[V0[2][0]].shape[i] for i in range(3))
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt1=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
62/151: np.max(nc0.variables[v][:,:,:])
62/152: pwd
62/153: fname='bcon'
62/154:
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nbnd0=(nc0.variables[V0[2][0]].shape[i] for i in range(3))
dt=[jul2dt(nc0.variables['TFLAG'][i,0,:]) for i in range(nt0)]
dt1=np.array([int(str(d.strftime("%Y%m%d%H"))) for d in dt])
62/155: v
62/156: np.max(nc0.variables[v][:,:,:])
62/157: cd ../
62/158: fname='base.grd02.1903.bc'
62/159: nc=lateral_boundary(fname,'r')
62/160: v='WEST_NO'
62/161: np.max(nc.variables[v][:,:,:])
62/162: cd /nas1/ecmwf/reanalysis/gribs19
62/163: fname='1901.nc'
62/164: nc = netCDF4.Dataset(fname,'r')
62/165: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
62/166: len(V[3])
62/167: V[3]
62/168: spec=[i.split('_')[3] for i in V[3]]
62/169: spec
62/170: spec.sort()
62/171: spec
62/172: spec=[int(i.split('_')[3]) for i in V[3]]
62/173: spec=[int(i.split('_')[3]) for i in V[3]].sort()
62/174: spec
62/175: spec=[int(i.split('_')[3]) for i in V[3]]
62/176: spec.sort()
62/177: spec
62/178: spec1=[int(i.split('_')[3]) for i in V[3]]
62/179: spec0=[int(i.split('_')[2]) for i in V[3]]
62/180: spec1=[i.split('_')[3] for i in V[3]]
62/181: spec0=[i.split('_')[2] for i in V[3]]
62/182: spec=[i+'_'+j for i,j in zip(spec0,spec1)]
62/183: spec.sort()
62/184: spec
62/185: len(set(spec))
62/186:
dic={
"VAR_192_217_21_P0_L105_GLL0":"ammonium",
"VAR_192_210_123_P0_L105_GLL0":"carbon_monoxide",
"VAR_192_210_4_P0_L105_GLL0":"dust_aerosol_0.03-0.55um_mixing_ratio",
"VAR_192_210_5_P0_L105_GLL0":"dust_aerosol_0.55-0.9um_mixing_ratio",
"VAR_192_210_6_P0_L105_GLL0":"dust_aerosol_0.9-20um_mixing_ratio",
"VAR_192_217_45_P0_L105_GLL0":"ethane",
"VAR_192_210_124_P0_L105_GLL0":"formaldehyde",
"VAR_192_210_9_P0_L105_GLL0":"hydrophilic_black_carbon_aerosol_mixing_ratio",
"VAR_192_210_7_P0_L105_GLL0":"hydrophilic_organic_matter_aerosol_mixing_ratio",
"VAR_192_210_10_P0_L105_GLL0":"hydrophobic_black_carbon_aerosol_mixing_ratio",
"VAR_192_210_8_P0_L105_GLL0":"hydrophobic_organic_matter_aerosol_mixing_ratio",
"VAR_192_217_16_P0_L105_GLL0":"isoprene",
"VAR_192_217_51_P0_L105_GLL0":"nitrate",
"VAR_192_217_6_P0_L105_GLL0":"nitric_acid",
"VAR_192_210_121_P0_L105_GLL0":"nitrogen_dioxide",
"VAR_192_217_27_P0_L105_GLL0":"nitrogen_monoxide",
"VAR_192_217_11_P0_L105_GLL0":"olefins",
"VAR_192_210_203_P0_L105_GLL0":"ozone",
"VAR_192_217_13_P0_L105_GLL0":"peroxyacetyl_nitrate",
"VAR_192_217_47_P0_L105_GLL0":"propane",
"VAR_192_210_1_P0_L105_GLL0":"sea_salt_aerosol_0.03-0.5um_mixing_ratio",
"VAR_192_210_2_P0_L105_GLL0":"sea_salt_aerosol_0.5-5um_mixing_ratio",
"VAR_192_210_3_P0_L105_GLL0":"sea_salt_aerosol_5-20um_mixing_ratio",
"VAR_192_210_11_P0_L105_GLL0":"sulphate_aerosol_mixing_ratio",
"VAR_192_210_122_P0_L105_GLL0":"sulphur_dioxide",
}
62/187: gas=dic.values()
62/188: gas[:5]
62/189: gas=list(dic.values())
62/190: gas[:5]
62/191: gas=[i for i in list(dic.values()) if 'mix' not in i]
62/192: gas
62/193: gas=[i for i in list(dic.values()) if 'mix' not in i and 'nitrate' not in i]
62/194: gas
62/195: gas=[i for i in list(dic.values()) if 'mix' not in i and i!='nitrate']
62/196: [i for i in list(dic.values()) if i not in gas]
62/197:
mws={
 'ammonium':18,
 'carbon_monoxide':28,
 'ethane':16,
 'formaldehyde':30,
 'isoprene':68.12,
 'nitric_acid':63,
 'nitrogen_dioxide':46,
 'nitrogen_monoxide':30,
 'olefins':42.1,
 'ozone':48,
 'paraffins':72.1,
 "peroxyacetyl_nitrate":121,
 'propane':44,
 'sulphur_dioxide':64,
}
mws.update({p:24.5E9 for p in [i for i in list(dic.values()) if i not in gas]})
62/198: len(mws)
62/199: mws
62/200:
SPECs =['carbon_monoxide', 'ethane', 'formaldehyde', 'isoprene', 'nitrogen_dioxide', 'nitrogen_monoxide', 'propane', 'sulphur_dioxide' ]
SPECs+=['ozone', 'ammonium', 'nitrate', 'olefins', 'organic_nitrates', 'paraffins']
PARTs =[
            'dust_aerosol_0.03-0.55um_mixing_ratio', 'dust_aerosol_0.55-0.9um_mixing_ratio', 'dust_aerosol_0.9-20um_mixing_ratio',
            'hydrophilic_black_carbon_aerosol_mixing_ratio', 'hydrophilic_organic_matter_aerosol_mixing_ratio', 'hydrophobic_black_carbon_aerosol_mixing_ratio',
            'hydrophobic_organic_matter_aerosol_mixing_ratio', 'nitric_acid', 'peroxyacetyl_nitrate',
            'sea_salt_aerosol_0.03-0.5um_mixing_ratio', 'sea_salt_aerosol_0.5-5um_mixing_ratio', 'sea_salt_aerosol_5-20um_mixing_ratio',
            'sulphate_aerosol_mixing_ratio',
        ]
62/201: len(SPECs+PARTs)
62/202: gas=[i for i in list(dic.values()) if 'mix' not in i and i not in ['ammonium','nitrate','organic_nitrates']]
62/203: gas_nm=['CO','ETH','FORM','ISOP','HNO3','NO2','NO','OLE','O3','PAR','PAN','PRPA','SO2']
62/204: len(gas),len(gas_nm)
62/205: 'PAR' in gas
62/206: gas
62/207:
gas=list(mws={
 'carbon_monoxide':28,
 'ethane':16,
 'formaldehyde':30,
 'isoprene':68.12,
 'nitric_acid':63,
 'nitrogen_dioxide':46,
 'nitrogen_monoxide':30,
 'olefins':42.1,
 'ozone':48,
 'paraffins':72.1,
 "peroxyacetyl_nitrate":121,
 'propane':44,
 'sulphur_dioxide':64,
}
)
62/208: gas=list(mws.keys())
62/209: len(gas),len(gas_nm)
62/210:
mws={
 'carbon_monoxide':28,
 'ethane':16,
 'formaldehyde':30,
 'isoprene':68.12,
 'nitric_acid':63,
 'nitrogen_dioxide':46,
 'nitrogen_monoxide':30,
 'olefins':42.1,
 'ozone':48,
 'paraffins':72.1,
 "peroxyacetyl_nitrate":121,
 'propane':44,
 'sulphur_dioxide':64,
}
62/211: len(gas),len(gas_nm)
62/212: gas=list(mws.keys())
62/213: len(gas),len(gas_nm)
62/214: dici={i:j for i,j in zip(dic.values(),dic.keys())}
62/215: dici
62/216: nms={dici[i]:j for i,j in zip(gas,gas_nm)}
62/217: par=[i for i in list(dic.values()) if i not in gas]
62/218: par
62/219: par=[i for i in list(dic.values()) if i not in gas]+['organic_nitrates']
62/220: gas
62/221: len(gas+par)
62/222: par.sort()
62/223: par
62/224: V[2][:5]
62/225: fname='/nas1/cmaqruns/2019base/data/bcon/BCON_v53_1903_run5_regrid_20190302_TWN_3X3'
62/226: nc = netCDF4.Dataset(fname,'r')
62/227: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
62/228: [i for i in V[2] if 'FE' in i]
62/229: a='AFEJ AALJ ASIJ ACAJ AMGJ AKJ AMNJ'.split()
62/230:
for v in a:
    print ([i for i in V[2] if i==v])
62/231: len(a)
62/232: a='ACORS ASOIL'.split()
62/233:
for v in a:
    print ([i for i in V[2] if i==v])
62/234: [i for i in V[2] if 'NO3' in i]
62/235: vlist='APOCI APNCOMI APOCJ AOTHRJ AXYL1J AXYL2J AXYL3JATOL1J ATOL2J ATOL3J ABNZ1J ABNZ2J ABNZ3J AISO1JAISO2J AISO3J ATRP1J ATRP2J ASQTJ AALK1J AALK2J AORGCJ AOLGBJ AOLGAJ APAH1J APAH2J APAH3J APN-COMJ'.split()
62/236:
for v in vlist:
    print ([i for i in V[2] if i==v])
62/237:
vlist=['APOCI','APNCOMI','APOCJ','AOTHRJ','AISO3J', 'ASQTJ', 'AORGCJ', 'AOLGBJ', 'AOLGAJ']
nms.update({
 dici['ammonium']:['ANH4'+i for i in 'IJK'],
 dici['dust_aerosol_0.03-0.55um_mixing_ratio']:'AFEJ AALJ ASIJ ACAJ AMGJ AKJ AMNJ'.split(),
 dici['dust_aerosol_0.55-0.9um_mixing_ratio']:'AFEJ AALJ ASIJ ACAJ AMGJ AKJ AMNJ'.split(),
 dici['dust_aerosol_0.9-20um_mixing_ratio']:['ACORS','ASOIL'],
 dici['hydrophilic_black_carbon_aerosol_mixing_ratio']:['AEC'+i for i in 'IJ'],
 dici['hydrophilic_organic_matter_aerosol_mixing_ratio']:vlist,
 dici['hydrophobic_black_carbon_aerosol_mixing_ratio']:['AEC'+i for i in 'IJ'],
 dici['hydrophobic_organic_matter_aerosol_mixing_ratio']:vlist,
 dici['nitrate']:['ANO3'+i for i in 'IJK'],
 dici['organic_nitrates']:['AMTNO3J','AMTHYDJ'],
 dici['sea_salt_aerosol_0.03-0.5um_mixing_ratio']:['ANAI', 'ACLI'],
 dici['sea_salt_aerosol_0.5-5um_mixing_ratio']:['ANAJ','ACLJ'],
 dici['sea_salt_aerosol_5-20um_mixing_ratio']:['ACLK','ASEACAT'],
 dici['sulphate_aerosol_mixing_ratio']:['ASO4'+i for i in 'IJK'],
})
62/238: nms={dici[i]:j for i,j in zip(gas,gas_nm)}
62/239: gas+=['paraffins']
62/240: nms={dici[i]:j for i,j in zip(gas,gas_nm)}
62/241: len(gas),len(gas_nm)
62/242: gas
62/243: [i for i in V[2] if 'NTR1' in i]
62/244:
vlist=['APOCI','APNCOMI','APOCJ','AOTHRJ','AISO3J', 'ASQTJ', 'AORGCJ', 'AOLGBJ', 'AOLGAJ']
nms_part={
 dici['ammonium']:['ANH4'+i for i in 'IJK'],
 dici['dust_aerosol_0.03-0.55um_mixing_ratio']:'AFEJ AALJ ASIJ ACAJ AMGJ AKJ AMNJ'.split(),
 dici['dust_aerosol_0.55-0.9um_mixing_ratio']:'AFEJ AALJ ASIJ ACAJ AMGJ AKJ AMNJ'.split(),
 dici['dust_aerosol_0.9-20um_mixing_ratio']:['ACORS','ASOIL'],
 dici['hydrophilic_black_carbon_aerosol_mixing_ratio']:['AEC'+i for i in 'IJ'],
 dici['hydrophilic_organic_matter_aerosol_mixing_ratio']:vlist,
 dici['hydrophobic_black_carbon_aerosol_mixing_ratio']:['AEC'+i for i in 'IJ'],
 dici['hydrophobic_organic_matter_aerosol_mixing_ratio']:vlist,
 dici['nitrate']:['ANO3'+i for i in 'IJK'],
 dici['sea_salt_aerosol_0.03-0.5um_mixing_ratio']:['ANAI', 'ACLI'],
 dici['sea_salt_aerosol_0.5-5um_mixing_ratio']:['ANAJ','ACLJ'],
 dici['sea_salt_aerosol_5-20um_mixing_ratio']:['ACLK','ASEACAT'],
 dici['sulphate_aerosol_mixing_ratio']:['ASO4'+i for i in 'IJK'],
}
62/245:
par_nms=[]
for i in nms_part:
    par_nms+=nms_par[i]
62/246:
par_nms=[]
for i in nms_part:
    par_nms+=nms_part[i]
62/247: len(set(par_nms))
62/248: begd
62/249: dt=datetime.datetime(2016,1,1)
62/250: dt
62/251: dt.strftime('%m/%d/%Y (%H:%M)')
62/252: [i.byte() for i in dt.strftime('%m/%d/%Y (%H:%M)')]
62/253: [byte(i) for i in dt.strftime('%m/%d/%Y (%H:%M)')]
62/254: grep byte ~/bin/*py
62/255: !grep byte ~/bin/*py
62/256: [bytes(i,encoding='utf-8') for i in dt.strftime('%m/%d/%Y (%H:%M)')]
62/257: a=[bytes(i,encoding='utf-8') for i in dt.strftime('%m/%d/%Y (%H:%M)')]
62/258: len(a)
62/259: pwd
62/260: !lst
62/261: !lst
62/262: !du -ch *_1901.nc
62/263: !lst
62/264: !lst
62/265: run FillBcon.py 2 1901.nc
62/266: run FillBcon.py 2 1901.nc
62/267: run FillBcon.py 2 1901.nc
62/268: !ncdump -h 1901.nc|M
62/269: !ncks -O --mk_rec_dmn initial_time0_hours 1901.nc tmp.nc
62/270: run FillBcon.py 2 1901.nc
62/271: !ncdump -h 1901.nc|M
62/272: mv tmp.nc 1901.nc
62/273: run FillBcon.py 2 1901.nc
62/274: run FillBcon.py 2 1901.nc
62/275: run FillBcon.py 2 1901.nc
62/276: run FillBcon.py 2 1901.nc
62/277: run FillBcon.py 2 1901.nc
62/278: mws
62/279: run FillBcon.py 2 1901.nc
62/280: run FillBcon.py 2 1901.nc
62/281: pwd
63/1: import netCDF4
63/2: fname='/nas1/cmaqruns/2019base/data/bcon/BCON_v53_1901_run10_regrid_20190120_TWN_3X3'
63/3: nc = netCDF4.Dataset(fname,'r')
63/4: v='CO'
63/5: nc.variables[v][:,0,0]
63/6: fname='/nas1/cmaqruns/2019base/data/bcon/BCON_v53_1902_run10_regrid_20190220_TWN_3X3'
63/7: nc = netCDF4.Dataset(fname,'r')
63/8: nc.variables[v][:,0,0]
63/9: fname='/nas1/cmaqruns/2016base/data/bcon/BCON_v53_1602_run10_regrid_20160220_EAsia_81K'
63/10: nc = netCDF4.Dataset(fname,'r')
63/11: nc.variables[v][:,0,0]
63/12: run FillBcon.py 2 1901.nc
63/13: dic
63/14: len(dic)
63/15: dici
63/16: len(gas)
63/17: gas
63/18: len(par_nms)
63/19: par_nms[:5]
63/20: list(nms_gas.values())
63/21: len(set(par_nms))
63/22: fname=fnames[0]
63/23: nc = netCDF4.Dataset(fname,'r')
63/24:
    V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
    rate={}
    for v in nms_part:
      nms=nms_part[v]
      avg=[]
      for nm in nms:
        avg.append(np.mean(nc.variables[nm][:]))
      sum_avg=sum(avg)
      if sum_avg==0:sys.exit('sum_avg==0')
      ratev=[avg[i]/sum_avg for i in range(len(avg))]
      rate.update({v:ratev})
63/25: v
63/26: nms_part[v]
63/27: rate[v]
63/28: dici['nitrate']
63/29: rate['VAR_192_217_51_P0_L105_GLL0']
63/30: nms_part[v]
63/31: v='VAR_192_217_51_P0_L105_GLL0'
63/32: v='VAR_192_217_51_P0_L105_GLL0'
63/33: nms_part[v]
63/34: dici['dust_aerosol_0.9-20um_mixing_ratio']
63/35: v='VAR_192_210_6_P0_L105_GLL0'
63/36: nms_part[v]
63/37: rate[v]
63/38: v=dici['ammonium']
63/39: rate[v]
63/40: v=dici['hydrophobic_black_carbon_aerosol_mixing_ratio']
63/41: rate[v]
63/42: v=dici['hydrophobic_organic_matter_aerosol_mixing_ratio']
63/43: rate[v]
63/44: v=dici['sea_salt_aerosol_0.5-5um_mixing_ratio']
63/45: rate[v]
63/46: v=dici['sea_salt_aerosol_5-20um_mixing_ratio']
63/47: rate[v]
63/48: pwd
63/49: run FillBcon.py 2 1901.nc
63/50: run FillBcon.py 2 1901.nc
63/51: pwd
63/52: cat FillBcon.py
63/53: cat FillBcon.py|wc
63/54: cat -n FillBcon.py
63/55: cat -n FillBcon.py|head 60
63/56: cat -n FillBcon.py|head -n60
63/57: pwd
63/58: ls /nas1/cmaqruns/2019base/data/bcon/*py
63/59: pwd
63/60: cat merge.cs
63/61: fname='/nas1/cmaqruns/2019base/data/bcon/BCON_v53_1910_run9_regrid_20191018_TWN_3X3'
63/62: nc = netCDF4.Dataset(fname,'r')
63/63: v='ASO4J'
63/64: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
63/65: v in V[2]
63/66: nc.variables[v][:,0,0]
63/67: fname='/nas1/cmaqruns/2016base/data/bcon/BCON_v53_1610_run5_regrid_20161001_TWN_3X3'
63/68: nc = netCDF4.Dataset(fname,'r')
63/69: nc.variables[v][:,0,0]
63/70: v='ANH4J'
63/71: v in V[2]
63/72: nc.variables[v][:,0,0]
63/73: fname='/nas1/cmaqruns/2019base/data/bcon/BCON_v53_1910_run9_regrid_20191018_TWN_3X3'
63/74: nc1 = netCDF4.Dataset(fname,'r')
63/75: nc1.variables[v][:,0,0]
63/76: np.mean(nc.variables[v][:,0,0])
63/77: np.mean(nc1.variables[v][:,0,0])
63/78: np.mean(nc1.variables[v][:,0,0])*1E12
63/79: np.mean(nc1.variables[v][:,0,0])*1E15
63/80: v='ANH4I'
63/81: np.mean(nc1.variables[v][:,0,0])*1E12
63/82: np.mean(nc.variables[v][:,0,0])
63/83: np.mean(nc.variables[v][:,1,0])
63/84: np.mean(nc.variables[v][:,:,:])
63/85: np.mean(nc1.variables[v][:,:,:])
63/86: np.mean(nc1.variables[v][:,:,:])*1.E12
63/87: set(par_nms)
63/88: list(set(par_nms))
63/89: a=list(set(par_nms))
63/90: a.sort()
63/91: a
63/92: fname='/nas1/cmaqruns/2019base/data/bcon/BCON_v53_1910_run9_regrid_20191018_TWN_3X3'
63/93: ls -lh /nas1/cmaqruns/2019base/data/bcon/BCON_v53_1910_run9_regrid_20191018_TWN_3X3
63/94: ls -lh /nas1/cmaqruns/2019base/data/bcon/BCON_v53_1910_run9_regrid_20191018_TWN_3X3
63/95: ls -lh /nas1/cmaqruns/2019base/data/bcon/BCON_v53_1901_run5_regrid_20181231_TWN_3X3
63/96: fname='/nas1/cmaqruns/2019base/data/bcon/BCON_v53_1901_run5_regrid_20181231_TWN_3X3'
63/97: nc1 = netCDF4.Dataset(fname,'r')
63/98: nc1.variables[v][:,0,0]
63/99: v
63/100: v='ASOIL'
63/101: np.mean(nc1.variables[v][:,:,:])
63/102: np.mean(nc.variables[v][:,:,:])
63/103: v='ASO4J'
63/104: np.mean(nc1.variables[v][:,:,:])
63/105: np.mean(nc.variables[v][:,:,:])
63/106: v='ASO4I'
63/107: np.mean(nc.variables[v][:,:,:])
63/108: np.mean(nc1.variables[v][:,:,:])
63/109: v='ANO3J'
63/110: np.mean(nc1.variables[v][:,:,:])
63/111: np.mean(nc.variables[v][:,:,:])
63/112: from PseudoNetCDF.camxfiles.Memmaps import lateral_boundary
63/113: nc=lateral_boundary(fname,'r')
63/114: fname='/nas1/camxruns/2019/ICBC/EC_REAN/base.grd02.1901.bc'
63/115: nc1=lateral_boundary(fname,'r')
63/116: fname='/nas1/camxruns/2019/ICBC/EC_REAN/base.grd02.1901.bc_Gas'
63/117: nc=lateral_boundary(fname,'r')
63/118: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
63/119: V
63/120:
for v in V[2]:
    print(v,np.mean(nc.variables[v][:]),np.mean(nc1.variables[v][:]))
63/121:
for v in V[2]:
    print(v,np.mean(nc.variables[v][:].flatten()),np.mean(nc1.variables[v][:].flatten()))
63/122:
for v in V[2]:
    print(v,np.mean(nc.variables[v][:].flatten())-np.mean(nc1.variables[v][:].flatten()))
63/123:
for v in V[2]:
    print(v,list(np.mean(nc.variables[v][:].flatten())-np.mean(nc1.variables[v][:].flatten()))[0])
63/124:
for v in V[2]:
    print(v,list(np.mean(nc.variables[v][:].flatten())-np.mean(nc1.variables[v][:].flatten())))
63/125:
for v in V[2]:
    print(v,list(np.mean(nc.variables[v][:].flatten())),list(np.mean(nc1.variables[v][:].flatten())))
63/126:
for v in V[2]:
    print(v,np.array(np.mean(nc.variables[v][:].flatten())),np.array(np.mean(nc1.variables[v][:].flatten())))
63/127:
for v in V[2]:
    d=np.array(np.mean(nc.variables[v][:].flatten()))-np.array(np.mean(nc1.variables[v][:].flatten()))
    if d != 0.:print (v,d)
63/128:
for v in V[2]:    
    a=np.array(np.mean(nc.variables[v][:].flatten()))
    b=np.array(np.mean(nc1.variables[v][:].flatten()))   
    if b-a != 0.:print (v,b,a)
63/129: v='WEST_NO'
63/130: np.mean(nc.variables[v][:,:,:])
63/131: np.mean(nc1.variables[v][:,:,:])
63/132: fname
63/133: np.mean(nc1.variables[v][:,0,0])
63/134: nc1.variables[v][:5,:4,:4]
63/135: fname='/nas1/camxruns/2019/ICBC/EC_REAN/base.grd02.1905.bc'
63/136: nc=lateral_boundary(fname,'r')
63/137: nc.variables[v][:5,:5,:5]
63/138: v
63/139: v='WEST_NO2'
63/140: nc.variables[v][:5,:5,:5]
63/141: nc.variables[v][:5,:5,:5]*1000
63/142: v='WEST_O3'
63/143: nc.variables[v][:5,:5,:5]*1000
63/144: nc1.variables[v][:5,:5,:5]*1000
63/145: nc.variables[v][:5,:5,:5]*1000
63/146: np.mean(nc1.variables[v][:,0,0])
63/147: np.mean(nc.variables[v][:,:,:])
63/148: np.mean(nc1.variables[v][:,:,:])
63/149:
for v in V[2]:    
    a=np.array(np.mean(nc.variables[v][:].flatten()))
    b=np.array(np.mean(nc1.variables[v][:].flatten()))   
    if b-a != 0.:print (v,b,a)
63/150: len(V[2])
63/151:
for v in V[2][125]:    
    a=np.array(np.mean(nc.variables[v][:].flatten()))
    b=np.array(np.mean(nc1.variables[v][:].flatten()))   
    if b-a != 0.:print (v,b,a)
63/152:
for v in V[2][:125]:    
    a=np.array(np.mean(nc.variables[v][:].flatten()))
    b=np.array(np.mean(nc1.variables[v][:].flatten()))   
    if b-a != 0.:print (v,b,a)
63/153: fname
63/154: fname='/nas1/camxruns/2019/ICBC/EC_REAN/base.grd02.1901.bc_Gas'
63/155: nc=lateral_boundary(fname,'r')
63/156:
for v in V[2][:125]:    
    a=np.array(np.mean(nc.variables[v][:].flatten()))
    b=np.array(np.mean(nc1.variables[v][:].flatten()))   
    if b-a != 0.:print (v,b,a)
63/157: v='SOUTH_SO2'
63/158: np.mean(nc.variables[v][:,0,0])
63/159: nt0,nlay0,nbnd0=(nc0.variables[V0[2][0]].shape[i] for i in range(3))
63/160: nt0,nlay0,nbnd0=(nc0.variables[V[2][0]].shape[i] for i in range(3))
63/161: nt0,nlay0,nbnd0=(nc.variables[V[2][0]].shape[i] for i in range(3))
63/162: nt0
63/163: np.mean(nc.variables[v][:300,0,0])
63/164: np.mean(nc.variables[v][300:,0,0])
63/165: np.mean(nc.variables[v][450:,0,0])
63/166: np.mean(nc.variables[v][300:450,0,0])
63/167: np.mean(nc.variables[v][600:,0,0])
63/168: np.mean(nc.variables[v][300:450,0,0])
63/169: np.mean(nc.variables[v][600:700,0,0])
63/170: np.mean(nc.variables[v][700:790,0,0])
63/171: nt0
63/172: nc.variables[v][780:,0,0]
63/173: v='TFLAG'
63/174: nc.variables[v][780:,0,:]
63/175: pwd
63/176: fname='/nas1/camxruns/2019/ICBC/EC_REAN/base.grd02.1902.bc'
63/177: nc=lateral_boundary(fname,'r')
63/178: v='SOUTH_SO2'
63/179: nc.variables[v][780:,0,:]
63/180: nt0,nlay0,nbnd0=(nc.variables[V[2][0]].shape[i] for i in range(3))
63/181: nt0
63/182: nc.variables[v][-5:,0,0]
63/183: np.mean(nc.variables[v][:,0,0])
63/184: np.mean(nc.variables[v][:,:,:])
63/185: pwd
63/186: fname='/nas1/cmaqruns/2019base/data/bcon/BCON_v53_1906_run5_regrid_20190601_TWN_3X3'
63/187: nc = netCDF4.Dataset(fname,'r')
63/188: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
63/189: v='O3'
63/190: np.mean(nc.variables[v][:,:,:])
63/191: nms_gas
63/192: nms_part
63/193: pwd
63/194: run FillBcon.py 2 1901.nc
63/195: run FillBcon.py 2 1901.nc
63/196: pwd
63/197: run FillBcon.py 2 1901.nc
63/198: !lst
63/199: run FillBcon.py 2 1901.nc
64/1: from PseudoNetCDF.camxfiles.Memmaps import lateral_boundary
64/2: fname='base.grd02.1906.bc'
64/3: nc=lateral_boundary(fname,'r')
64/4: v='SOUTH_SO2'
64/5: nc.variables[v][:,0,0]
64/6: from PseudoNetCDF.camxfiles.Memmaps import uamiv
64/7: ls ../../outputs/con06
64/8: fname='../../outputs/con06/1906base.S.grd01'
64/9: nc=uamiv(fname,'r')
64/10: fname='base.grd02.1906.bc'
64/11: nc=lateral_boundary(fname,'r')
64/12: so2=np.array(nc.variables[v][:,0,0])
64/13: import numpy as np
64/14: np.mean(nc.variables[v][:,:,:])
64/15: so2=np.array(nc.variables[v][:,:,:])
64/16: np.where(so2==np.max(so2))
64/17: v='TFLAG'
64/18: nc.variables[v][70,0,:]
64/19: !jul2cal 19154
64/20: v='SOUTH_SO2'
64/21: nc.variables[v][65:75,0,0]
64/22: v='TFLAG'
64/23: nc.variables[v][65:75,0,0]
64/24: nc.variables[v][65:75,0,1]
64/25: cd 1906
64/26: ls
64/27: fname='20190603'
64/28: nc = netCDF4.Dataset(fname,'r')
64/29: import netCDF4
64/30: nc = netCDF4.Dataset(fname,'r')
64/31: v='SO2'
64/32: v='TFLAG'
64/33: nc.variables[v][:,0,:]
64/34: fname='20190602'
64/35: nc = netCDF4.Dataset(fname,'r')
64/36: v='SO2'
64/37: nc.variables[v][-3:,0,0]
64/38: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
64/39:
for v in V[2]:
    print(np.array(nc.variables[v][-3:,0,0]))
64/40: ls ../1905
65/1: import netCDF4
65/2: import numpy as np
65/3: fname='wrfout_d04_6'
65/4: nc = netCDF4.Dataset(fname,'r')
65/5: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
65/6: !grep nt ~/bin/*py|M
65/7: nt,nlay,nrow,ncol=nc.variables[V[4][0]].shape
65/8: V[4][:5]
65/9: V
65/10: nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
65/11: fname
65/12: nc0=nc
65/13: v_ac='ACGRDFLX,ACSNOM,RAINC,RAINSH,RAINNC,SNOWNC,GRAUPELNC,HAILNC,ACHFX,ACLHF'.split(',')
65/14: set(v_ac) in set(V[3])
65/15: set(v_ac) in set(V[2])
65/16: set(v_ac) - set(V[3])
65/17: set(v_ac) - set(V[2])
65/18: len(set(v_ac) - set(V[3]))
65/19: len(v_ac)
65/20: len(set(v_ac) - set(V[1]))
65/21: len(set(v_ac) - set(V[2]))
65/22: set(v_ac) in set(V[2])
65/23: fname='wrfout_d04_7'
65/24: nc = netCDF4.Dataset(fname,'r')
65/25: nc0.START_DATE
65/26: nc.START_DATE
65/27: x='XTIME'
65/28: x in V[1]
65/29: x in V[0]
65/30: nc.variables[v][:][:5]
65/31: nc.variables[x][:][:5]
65/32: fname='wrfout_d04_1'
65/33: nc = netCDF4.Dataset(fname,'r')
65/34: nc.variables[x][:][:5]
65/35: nc.variables[x][:][-5:]
65/36: fname='wrfout_d04_2'
65/37: nc = netCDF4.Dataset(fname,'r')
65/38: nc.variables[x][:][:5]
65/39:
acc=['ACGRDFLX', 'ACSNOM', 'RAINC', 'RAINSH', 'RAINNC', 'SNOWNC', 'GRAUPELNC', 'HAILNC', 'ACHFX', 'ACLHF']
acmx={ac:np.array(nc0.variables[ac][-1,:,:]) for ac in acc}
for f in range(7,10):
  fname='wrfout_d04_'+str(f)
  nc = netCDF4.Dataset(fname,'r+')        
  nc.SIMULATION_START_DATE=nc0.SIMULATION_START_DATE
  nc.START_DATE           =nc0.START_DATE
  nc.JULYR                =nc0.JULYR
  nc.JULDAY               =nc0.JULDAY
  nc.TITLE                =nc0.TITLE  
  for x in ['XTIME','ITIMESTEP']:
    x0=nc0.variables[x][:][-1]      
    nc.variables[x][:]+=x0
  for ac in acc:
    var=np.array(nc.variables[ac][:])
    nc.variables[ac][:,:,:]=var[:,:,:]+acmx[Null,:,:]
  nc.close()
65/40: fname
65/41: nc = netCDF4.Dataset(fname,'r')
65/42: nc.SIMULATION_START_DATE==nc0.SIMULATION_START_DATE
65/43:
acc=['ACGRDFLX', 'ACSNOM', 'RAINC', 'RAINSH', 'RAINNC', 'SNOWNC', 'GRAUPELNC', 'HAILNC', 'ACHFX', 'ACLHF']
acmx={ac:np.array(nc0.variables[ac][-1,:,:]) for ac in acc}
for f in range(7,10):
  fname='wrfout_d04_'+str(f)
  nc = netCDF4.Dataset(fname,'r+')        
  nc.SIMULATION_START_DATE=nc0.SIMULATION_START_DATE
  nc.START_DATE           =nc0.START_DATE
  nc.JULYR                =nc0.JULYR
  nc.JULDAY               =nc0.JULDAY
  nc.TITLE                =nc0.TITLE  
  for x in ['XTIME','ITIMESTEP']:
    x0=nc0.variables[x][:][-1]      
    nc.variables[x][:]+=x0
  for ac in acc:
    var=np.array(nc.variables[ac][:])
    nc.variables[ac][:,:,:]=var[:,:,:]+acmx[Nul,:,:]
  nc.close()
65/44:
acc=['ACGRDFLX', 'ACSNOM', 'RAINC', 'RAINSH', 'RAINNC', 'SNOWNC', 'GRAUPELNC', 'HAILNC', 'ACHFX', 'ACLHF']
acmx={ac:np.array(nc0.variables[ac][-1,:,:]) for ac in acc}
for f in range(7,10):
  fname='wrfout_d04_'+str(f)
  nc = netCDF4.Dataset(fname,'r+')        
  nc.SIMULATION_START_DATE=nc0.SIMULATION_START_DATE
  nc.START_DATE           =nc0.START_DATE
  nc.JULYR                =nc0.JULYR
  nc.JULDAY               =nc0.JULDAY
  nc.TITLE                =nc0.TITLE  
  for x in ['XTIME','ITIMESTEP']:
    x0=nc0.variables[x][:][-1]      
    nc.variables[x][:]+=x0
  for ac in acc:
    var=np.array(nc.variables[ac][:])
    nc.variables[ac][:,:,:]=var[:,:,:]+acmx[NULL,:,:]
  nc.close()
65/45:
acc=['ACGRDFLX', 'ACSNOM', 'RAINC', 'RAINSH', 'RAINNC', 'SNOWNC', 'GRAUPELNC', 'HAILNC', 'ACHFX', 'ACLHF']
acmx={ac:np.array(nc0.variables[ac][-1,:,:]) for ac in acc}
for f in range(7,10):
  fname='wrfout_d04_'+str(f)
  nc = netCDF4.Dataset(fname,'r+')        
  nc.SIMULATION_START_DATE=nc0.SIMULATION_START_DATE
  nc.START_DATE           =nc0.START_DATE
  nc.JULYR                =nc0.JULYR
  nc.JULDAY               =nc0.JULDAY
  nc.TITLE                =nc0.TITLE  
  for x in ['XTIME','ITIMESTEP']:
    x0=nc0.variables[x][:][-1]      
    nc.variables[x][:]+=x0
  for ac in acc:
    var=np.array(nc.variables[ac][:])
    nc.variables[ac][:,:,:]=var[:,:,:]+acmx[acc][NULL,:,:]
  nc.close()
65/46: acc
65/47:
acc=['ACGRDFLX', 'ACSNOM', 'RAINC', 'RAINSH', 'RAINNC', 'SNOWNC', 'GRAUPELNC', 'HAILNC', 'ACHFX', 'ACLHF']
acmx={ac:np.array(nc0.variables[ac][-1,:,:]) for ac in acc}
for f in range(7,10):
  fname='wrfout_d04_'+str(f)
  nc = netCDF4.Dataset(fname,'r+')        
  nc.SIMULATION_START_DATE=nc0.SIMULATION_START_DATE
  nc.START_DATE           =nc0.START_DATE
  nc.JULYR                =nc0.JULYR
  nc.JULDAY               =nc0.JULDAY
  nc.TITLE                =nc0.TITLE  
  for x in ['XTIME','ITIMESTEP']:
    x0=nc0.variables[x][:][-1]      
    nc.variables[x][:]+=x0
  for ac in acc:
    var=np.array(nc.variables[ac][:])
    nc.variables[ac][:,:,:]=var[:,:,:]+acmx[ac][NULL,:,:]
  nc.close()
65/48:
acc=['ACGRDFLX', 'ACSNOM', 'RAINC', 'RAINSH', 'RAINNC', 'SNOWNC', 'GRAUPELNC', 'HAILNC', 'ACHFX', 'ACLHF']
acmx={ac:np.array(nc0.variables[ac][-1,:,:]) for ac in acc}
for f in range(7,10):
  fname='wrfout_d04_'+str(f)
  nc = netCDF4.Dataset(fname,'r+')        
  nc.SIMULATION_START_DATE=nc0.SIMULATION_START_DATE
  nc.START_DATE           =nc0.START_DATE
  nc.JULYR                =nc0.JULYR
  nc.JULDAY               =nc0.JULDAY
  nc.TITLE                =nc0.TITLE  
  for x in ['XTIME','ITIMESTEP']:
    x0=nc0.variables[x][:][-1]      
    nc.variables[x][:]+=x0
  for ac in acc:
    var=np.array(nc.variables[ac][:])
    nc.variables[ac][:,:,:]=var[:,:,:]+acmx[ac][None,:,:]
  nc.close()
65/49: history
65/50: !vi acc_wrf.py
65/51: !vi acc_wrf.py
66/1: run prep_linegridLL.py
66/2: !vi prep_linegridLL.py
66/3: run prep_linegridLL.py
66/4: i
66/5: XYRCk[i]
66/6: XYRCk[:3]
66/7: XYRCk[10]
66/8: XYRCk[11]
66/9: NREC
66/10: !vi prep_linegridLL.py
66/11: kin.head()
66/12: kin.tail()
66/13: kin.drop_duplicates(inplace=True).drop_index(True)
66/14: kin.drop_duplicates(inplace=True).reset_index(drop=True)
66/15: kin=kin.drop_duplicates(inplace=True).reset_index(drop=True)
66/16: kin=kin.drop_duplicates(reset_index=True)
66/17: kin=kin.drop_duplicates()
66/18: kin=kin.reset_index(drop=True)
66/19: kin.tail()
66/20: XYRCk=kin.X*10000*10*100+kin.Y*10*100+kin.R*100+kin.C
66/21:
kin=DataFrame(DD)
kin=kin.drop_duplicates(inplace=True)
kin=kin.reset_index(drop=True)
66/22:
kin=DataFrame(DD)
kin.drop_duplicates(inplace=True)
kin=kin.reset_index(drop=True)
66/23: XYRCk=kin.X*10000*10*100+kin.Y*10*100+kin.R*100+kin.C
66/24: XYRCk[11]
66/25: df.head()
66/26:
df['REC']=-1
for i in range(NREC):
  boo=df.XYRC==XYRCk[i]
  idx=df.loc[boo].index
  df.loc[idx,'REC']=i
df.set_index('UTME').to_csv('TEDS11_LINE_WGS84.csv')
66/27: !lst ..
66/28: df.head()
66/29: n=[len(df.loc[df.REC==i]) for i in range(NREC)]
66/30: len(set(n))
66/31: set(n)
66/32: !vi line.py
66/33: !vi prep_linegridLL.py
66/34: run prep_linegridLL.py
66/35: df.loc[n,'XYRC']
66/36: type(XYRCk)
66/37: n
66/38: XYRCk=list(kin.X*10000*10*100+kin.Y*10*100+kin.R*100+kin.C)
66/39: iVTYP=np.array([VTYPE.index(i) for i in df.NSC],dtype=int)
66/40: df['RECV']=np.array(df.REC,dtype=int)*100+iVTYP
66/41: df.tail()
66/42: VTYP.index('bldlpg')
66/43:
VTYPE=list(set(df.NSC))
VTYPE.sort()
NVTYP=len(VTYPE)
66/44: VTYPE.index('bldlpg')
66/45: APOL
66/46: pv=pivot_table(df,index='RECV',values=APOL,aggfunc=sum).reset_index()
66/47: len(pv)
66/48: pv.head()
66/49: pv['REC']=np.array(pv.RECV//10,dtype=int)
66/50: pv.head()
66/51: pv['iVT']=np.array(pv.RECV%10,dtype=int)
66/52: pv.head()
66/53: pv.tail()
66/54:
pv['REC']=np.array(pv.RECV//100,dtype=int)
pv['iVT']=np.array(pv.RECV%100,dtype=int)
66/55: pv.tail()
66/56: NREC
66/57: pv.head()
66/58: APOL
66/59: APOL.sort()
66/60: APOL
66/61: pv.columns
66/62: pv.columns[3:-2]
66/63: list(pv.columns[3:-2])==APOL
66/64: list(pv.columns[3:-2])
66/65: (list(pv.columns[3:-2])==APOL).all()
66/66: list(pv.columns[3:-2])==APOL[:]
66/67: a=list(pv.columns[3:-2])
66/68: a==POL
66/69: a==APOL
66/70:
for i in range(NPOL):
    print(a[i]==APOL[i])
66/71: len(a),len(APOL)
66/72: a
66/73: a=list(pv.columns[1:-2])
66/74: a[:5]
66/75: a==APOL
66/76: i=0;k=0
66/77: boo=(pv.REC==i)&(pv.iVT==k)
66/78: pv.iloc[boo,1:-2]
66/79: pv.iloc[boo,1:17]
66/80: pv.loc[boo,1:17]
66/81: pv.loc[boo,APOL]
66/82: pv.loc[boo,APOL][:]
66/83: list(pv.loc[boo,APOL])
66/84: np.array(pv.loc[boo,APOL])
66/85: np.array(pv.loc[boo,APOL]).shape
66/86: np.array(pv.loc[boo,APOL])[0]
66/87: np.array(pv.loc[boo,APOL])[0].shape
66/88:
EM=np.zeros(shape=(NREC,NPOL,NVTYP))

for i in range(NREC):
  for k in range(NVTYP):
    boo=(pv.REC==i)&(pv.iVT==k)
    EM[i,:,k]=np.array(pv.loc[boo,APOL])[0]
66/89: len(pv)
66/90: len(pv)/21
66/91: i
66/92: ar=np.array(pv.loc[boo,APOL])
66/93: ar.shape
66/94:
  boo=(pv.REC==i)
  ar=np.array(pv.loc[boo,APOL])[:]
66/95: ar.shape
66/96: ar[:5,:5]
66/97: ar.T[:5,:5]
66/98:
EM=np.zeros(shape=(NREC,NPOL,NVTYP))
for i in range(NREC):
  boo=(pv.REC==i)
  EM[i,:,k]=np.array(pv.loc[boo,APOL]).T
66/99:
EM=np.zeros(shape=(NREC,NPOL,NVTYP))
for i in range(NREC):
  boo=(pv.REC==i)
  EM[i,:,:]=np.array(pv.loc[boo,APOL]).T
66/100:
fname = 'cl08_'+'{:d}_{:d}_{:d}'.format(NREC,NPOL,NVTYP)+'.bin'
with FortranFile(fname, 'w') as f:
  f.write_record(EM)
66/101: !lst
66/102: pv.set_index('RECV').to_csv('TEDS11_LINE_WGS84_1Km.csv')
66/103: !lst
66/104: !head TEDS11_LINE_WGS84_1Km.csv
66/105: !tail TEDS11_LINE_WGS84_1Km.csv
66/106: APOL
66/107: [i.replace('EM_','') for i in APOL]
66/108: Aold='TSP PM25 SOX NOX CO EXHC EHC RHC NMHC PB'.split()
66/109: d_ON={i:APOL.index(Aold[i]) for i in range(NPOL)}
66/110: APOL=['CO', 'EHC', 'EXHC', 'NH3', 'NMHC', 'NOX', 'PB', 'PM', 'PM25', 'PM6', 'RHC', 'RST', 'SOX', 'THC', 'TSP']
66/111: d_ON={i:APOL.index(Aold[i]) for i in range(NPOL)}
66/112: NPOL
66/113: NPOL=len(Aold)
66/114: d_ON={i:APOL.index(Aold[i]) for i in range(NPOL)}
66/115: d_ON
66/116: run line.py
66/117: 15692040/21/15
66/118: NREC
66/119: EM.shape
66/120: !lst
66/121: !grep bin *py
66/122: !grep Fortr *py
66/123: !vi line.py
66/124: run line.py
66/125: !vi line.py
66/126: run line.py
66/127: !grep 105 *py
66/128: vi include2.py
66/129: !vi include2.py
66/130: ls *csv
66/131: run line.py
66/132: !rm *pyc
66/133: run line.py
66/134: !vi include2.py
66/135: run line
66/136: run line.py
67/1: run ptseE_ONS.py SOX
67/2: len(days),len(md3)
67/3: len(hrs)
67/4: dy1
67/5: len(pv2MD)
67/6: c_cems
67/7: cems.head()
67/8: cp_no=set(cems.CP_NO)
67/9:
for c in cp_no:
    n=len(cems.loc[cems.CP_NO==c])
    if n != 24*365:print(c)
67/10: len(pv_cems1)
67/11: pv_cems1.head()
67/12: cems_HROD.head()
67/13: len(cems_HROD.C_NO)
67/14: len(cems_HROD)
67/15: len(cp_no)
67/16: len(cems_DAOD)
67/17: len(pv2MD)
67/18: pv2MD[:5]
67/19: 8088/24
67/20: 7872/24
67/21: s365=set([i for i in range(id365)])
67/22: s365-set(pv2MD)
67/23: s365=set([i*24 for i in range(id365)])
67/24: s365-set(pv2MD)
67/25: run ptseE_ONS.py SOX
67/26: ons.shape
67/27: len(cems.loc[cems.CP_NO==p])
67/28: cems/head()
67/29: cems.head()
67/30: len(mdh)
67/31: mdh[:5]
67/32: md[:5]
67/33: ons.shape()
67/34: ons.shape
67/35: p
67/36: cemsp=cems.loc[cems.CP_NO==p]
67/37: len(cemsp)< ih8760
67/38: ih8760=id365*24
67/39: len(cemsp)< ih8760
67/40: c
67/41: c=list(cp_no)[0]
67/42: c
67/43:
  cemsp=cems.loc[cems.CP_NO==c]
  n=len(cemsp)
67/44: n
67/45: s=set(mdh)-set(cemsp.mdh)
67/46: s=set(mdh)-set(cemsp.MDH)
67/47: len(s)
67/48: s
67/49: cemsp=cems.loc[cems.CP_NO==c].reset_index(drop=True)
67/50: cemsp.loc[0]
67/51: val='SOX NOX FLOW PM'.split
67/52: cems.loc[0,val]
67/53: val='SOX NOX FLOW PM'.split()
67/54: cems.loc[0,val]
67/55:
    a=cems.loc[0]
    a.loc[0,val]=0
67/56:     a=cems.loc[0]
67/57: a.loc[0,val]
67/58: a
67/59: cems.loc[0,val]
67/60: a
67/61: a.loc[0]
67/62: a.index
67/63: a
67/64: cemsp.head()
67/65: a=cemsp.loc[0]
67/66: a
67/67: a.loc[0]
67/68: a=cemsp[0]
67/69: a.loc[0]
67/70: a
67/71: a[val]
67/72: a[val]=0
67/73: a[val][:]=0
67/74: a
67/75:
    sMDH=set(mdh)-set(cemsp.MDH)
    for s in sMDH:
      m,d,h=s//10000,s//100%100,s%100
      print(m,d,h)
67/76:
    sMDH=set(mdh)-set(cemsp.MDH)
    for s in sMDH:
      m,d,h=s//10000,s//100%100,s%100
      print(m,d,h)
      a.MONTH,a.DATE,a.HOUR=m,d,h
67/77: a
67/78:
    sMDH=set(mdh)-set(cemsp.MDH)
    for s in sMDH:
      m,d,h=s//10000,s//100%100,s%100
      print(m,d,h)
      a.MONTH[0],a.DATE[0],a.HOUR[0]=m,d,h
67/79:
    sMDH=set(mdh)-set(cemsp.MDH)
    for s in sMDH:
      m,d,h=s//10000,s//100%100,s%100
      print(m,d,h)
      a.MONTH,a.DATE,a.HOUR=m,d,h
67/80: !grep append ~/bin/*py|grep df
67/81:
cp_no=set(cems.CP_NO)
add_cems=DataFrame({})
for c in cp_no:
  cemsp=cems.loc[cems.CP_NO==c].reset_index(drop=True)
  n=len(cemsp)
  if n != ih8760:
    a=cemsp.loc[0]
    a[val][:]=0
    sMDH=set(mdh)-set(cemsp.MDH)
    for s in sMDH:
      m,d,h=s//10000,s//100%100,s%100
      a.MONTH,a.DATE,a.HOUR=m,d,h
      add_cems=add_cems.append(a,ignore_index=True)
67/82: len(add_cems)
67/83: len(cems)
67/84: a
67/85: s
67/86: ns=len(sMDH)
67/87: ns
67/88: sMDH
67/89:     sMDH=set(mdh)-set(cemsp.MDH)
67/90: b=cems.loc[:ns]
67/91: b
67/92: m
67/93: m,d,h,mdh_c=[s//10000 for s in sMDH],[s//100%100 for s in sMDH],[s%100 for s in sMDH],[s for s in sMDH]
67/94: m
67/95: sMDH
67/96:
fname=P+'point_cems.csv'
cems=read_csv(fname)
idx=cems[cems.C_NO=='C_NO'].index
cems=cems.drop(idx).reset_index(drop=True)
cems['PM']=[(i+j)/2 for i,j in zip(cems.SOX,cems.NOX)]
if max(cems.HOUR)>100:
  cems['MDH']=[int(i*10000+j*100+k/100) for i,j,k in zip(cems.MONTH,cems.DATE,cems.HOUR)]
else:
  cems['MDH']=[int(i*10000+j*100+k) for i,j,k in zip(cems.MONTH,cems.DATE,cems.HOUR)]
cems['CP_NO'] = [i + j for i, j in zip(list(cems['C_NO']), list(cems['NO_S']))]
cems=cems.sort_values('MDH',ascending=True).reset_index(drop=True)
67/97: c
67/98:
  cemsp=cems.loc[cems.CP_NO==c].reset_index(drop=True)
  n=len(cemsp)
  if n != ih8760:
    a=cemsp.loc[0]
    a[val][:]=0
    sMDH=set(mdh)-set(cemsp.MDH)
    ns=len(sMDH)
    m,d,h,mdh_c=[s//10000 for s in sMDH],[s//100%100 for s in sMDH],[s%100 for s in sMDH],[s for s in sMDH]
    b=cems.loc[:ns]
    for ci in idc:
      b[ci]=a[ci]
    b.MONTH,b.DATE,b.HOUR,b.MDH=m,d,h,mdh_i
67/99: ns
67/100: b
67/101: idc
67/102: idc='NO_S C_NO CP_NO'.split()
67/103:
    a=cemsp.loc[0]
    a[val][:]=0
    sMDH=set(mdh)-set(cemsp.MDH)
    ns=len(sMDH)
    m,d,h,mdh_c=[s//10000 for s in sMDH],[s//100%100 for s in sMDH],[s%100 for s in sMDH],[s for s in sMDH]
    b=cems.loc[:ns]
    for ci in idc:
      b[ci]=a[ci]
    b.MONTH,b.DATE,b.HOUR,b.MDH=m,d,h,mdh_i
67/104:
    a=cemsp.loc[0]
    a[val][:]=0
    sMDH=set(mdh)-set(cemsp.MDH)
    ns=len(sMDH)
    m,d,h,mdh_c=[s//10000 for s in sMDH],[s//100%100 for s in sMDH],[s%100 for s in sMDH],[s for s in sMDH]
    b=cems.loc[:ns]
    for ci in idc:
      b[ci]=a[ci]
    b.MONTH,b.DATE,b.HOUR,b.MDH=m,d,h,mdh_c
67/105: b
67/106: n
67/107: c
67/108: cems.head()
67/109: c=O1708678P001
67/110: c='O1708678P001'
67/111:
  cemsp=cems.loc[cems.CP_NO==c].reset_index(drop=True)
  n=len(cemsp)
  if n != ih8760:
    a=cemsp.loc[0]
    a[val][:]=0
    sMDH=set(mdh)-set(cemsp.MDH)
    ns=len(sMDH)
    m,d,h,mdh_c=[s//10000 for s in sMDH],[s//100%100 for s in sMDH],[s%100 for s in sMDH],[s for s in sMDH]
    b=cems.loc[:ns]
    for ci in idc:
      b[ci]=a[ci]
    b.MONTH,b.DATE,b.HOUR,b.MDH=m,d,h,mdh_c
67/112: b
67/113: ns
67/114:
    b=cems.loc[:ns-1]
    for ci in idc:
      b[ci]=a[ci]
    b.MONTH,b.DATE,b.HOUR,b.MDH=m,d,h,mdh_c
67/115: b
67/116:
    for ci in idc+val:
      b[ci]=a[ci]
67/117: b
67/118: a
67/119:
    a=cemsp.loc[0]
    a[val][:]=0
67/120:
    for ci in idc+val:
      b[ci]=a[ci]
67/121: b
67/122: a
67/123: a[val][:]=0
67/124: a
67/125: val
67/126: a[val][:]=0.
67/127: a
67/128: a[val]=0.
67/129: a
67/130: a
67/131: d=dict(a)
67/132: d
67/133: a=dict(cemsp.loc[0])
67/134: a[val]
67/135: cems.head()
67/136: run ptseE_ONS.py SOX
67/137: run ptseE_ONS.py SOX
67/138: len(cems.loc[cems.CP_NO==p])
67/139: len(b)
67/140: a
67/141: len(cems)
67/142: p
67/143: len(cems.loc[cems.CP_NO==p])
67/144:
fname=P+'point_cems.csv'
cems=read_csv(fname)
67/145: len(cems)
67/146:
idx=cems[cems.C_NO=='C_NO'].index
cems=cems.drop(idx).reset_index(drop=True)
cems['PM']=[(i+j)/2 for i,j in zip(cems.SOX,cems.NOX)]
if max(cems.HOUR)>100:
  cems['MDH']=[int(i*10000+j*100+k/100) for i,j,k in zip(cems.MONTH,cems.DATE,cems.HOUR)]
else:
  cems['MDH']=[int(i*10000+j*100+k) for i,j,k in zip(cems.MONTH,cems.DATE,cems.HOUR)]
cems['MD']=[int(i*100+j) for i,j in zip(cems.MONTH,cems.DATE)]
cems['CP_NO'] = [i + j for i, j in zip(list(cems['C_NO']), list(cems['NO_S']))]
cems=cems.sort_values('MDH',ascending=True).reset_index(drop=True)
#coverage test
id365=365
if len(cems.loc[(cems.MD==229)])>0:id365=366
s365=set([i*24 for i in range(id365)])
ih8760=id365*24
mdh=list(set(cems.MDH))
if len(mdh)!=ih8760:sys.exit('mdh coverage not enough!')
mdh.sort()
md=list(set(cems.MD))
md.sort()
#fill the hours
val='SOX NOX FLOW PM X_BLANK1 X_BLANK2'.split()
idc='NO_S C_NO CP_NO'.split()
cp_no=set(cems.CP_NO)
add_cems=DataFrame({})
67/147:
for c in cp_no:
  cemsp=cems.loc[cems.CP_NO==c].reset_index(drop=True)
  n=len(cemsp)
  if n != ih8760:
    a=cemsp.loc[0]
    a[val]=0.
    sMDH=set(mdh)-set(cemsp.MDH)
    ns=len(sMDH)
    m,d,h,md_c,mdh_c=[s//10000 for s in sMDH],[s//100%100 for s in sMDH],[s%100 for s in sMDH],[s//100 for s in sMDH],[s for s in sMDH]
    b=cems.loc[:ns-1]
    for ci in idc+val:
      b[ci]=a[ci]
    b.MONTH,b.DATE,b.HOUR,b.MD,b.MDH=m,d,h,md_c,mdh_c
    add_cems=add_cems.append(b,ignore_index=True)
67/148: len(add_cems)
67/149: len(add_cems)+len(cems)
67/150: p
67/151: len(cems.loc[cems.CP_NO==p])
67/152: len(add_cems.loc[add_cems.CP_NO==p])
67/153: 8571+180
67/154: c=p
67/155:
  cemsp=cems.loc[cems.CP_NO==c].reset_index(drop=True)
  n=len(cemsp)
67/156: n
67/157:
    sMDH=set(mdh)-set(cemsp.MDH)
    ns=len(sMDH)
67/158: nc
67/159: ns
67/160: ns+n
67/161:
    m,d,h,md_c,mdh_c=[s//10000 for s in sMDH],[s//100%100 for s in sMDH],[s%100 for s in sMDH],[s//100 for s in sMDH],[s for s in sMDH]
    b=cems.loc[:ns-1]
67/162: len(b)
67/163: len(m)
67/164: a
67/165: c
67/166:
    a=cemsp.loc[0]
    a[val]=0.
67/167:
    for ci in val:
      a[ci]=0.
67/168:
    for ci in val:
      a[ci][:]=0.
67/169:
    for ci in val:
      a[ci]=0.
67/170:
    for ci in val:
      a[ci]=[0.]
67/171: a[ci]
67/172: a
67/173:
    for ci in val:
      a[ci]=0.
67/174: a
67/175:
    for ci in val:
      a.loc[:,ci]=0.
67/176:
    for ci in val:
      a.iloc[0,ci]=0.
67/177:
    for ci in val:
      a.iloc[0,a.colunms[ci]]=0.
67/178: a
67/179: type(a)
67/180: a=dict(cemsp.loc[0])
67/181: a
67/182:
    for ci in val:
      a[ci]=0.
67/183: a
67/184:
    for ci in idc+val:
      b[ci]=a[ci]
67/185: b
67/186:     b=cems.loc[:ns-1]
67/187:     b=cems.loc[:ns-1].reset_index(drop=True)
67/188:
    for ci in idc+val:
      b.loc[:,ci]=a[ci]
67/189: b
67/190: c
67/191:
  cemsp=cems.loc[cems.CP_NO==c].reset_index(drop=True)
  n=len(cemsp)
  if n != ih8760:
    a=dict(cemsp.loc[0])
    for ci in val:
      a[ci]=0.
    sMDH=set(mdh)-set(cemsp.MDH)
    ns=len(sMDH)
    m,d,h,md_c,mdh_c=[s//10000 for s in sMDH],[s//100%100 for s in sMDH],[s%100 for s in sMDH],[s//100 for s in sMDH],[s for s in sMDH]
    b=cems.loc[:ns-1].reset_index(drop=True)
    for ci in idc+val:
      b.loc[:,ci]=a[ci]
    b.MONTH,b.DATE,b.HOUR,b.MD,b.MDH=m,d,h,md_c,mdh_c
    add_cems=add_cems.append(b,ignore_index=True)
67/192: len(add_cems)
67/193: len(b)
67/194: n
67/195: cems=cems.append(add_cems,ignore_index=True)
67/196: run ptseE_ONS.py SOX
68/1: from pandas import *
68/2:
try:
  df = read_csv('point.csv', encoding='utf8')
except:
  df = read_csv('point.csv')
68/3: df.loc[0]
68/4: df.loc[0,'UTME']
68/5: df.loc[0,'UTM_E']
68/6: df.loc[0,'UTM_N']
69/1: run ptseE_ONS.py PM
69/2: cp_no=set(cems.CP_NO)
69/3:
for c in cp_no:
    n=len(cems.loc[cems.CP_NO==c])
    if n != 24*365:print(c)
69/4: c=J5601064P002
69/5: c='J5601064P002'
69/6:
    n=len(cems.loc[cems.CP_NO==c])
    if n != 24*365:print(c)
69/7: n
69/8: a=cems.loc[cems.CP_NO==c]
69/9: a.head()
69/10: !gerp dupli ~/bin/*py
69/11: !grep dupli ~/bin/*py
69/12: a.drop_duplicates(inplace=True)
69/13: len(a)
69/14: a.head()
69/15:
aMDH=list(a.MDH)
for i in mdh:
    if aMDH.count(i)>1:print i
69/16:
aMDH=list(a.MDH)
for i in mdh:
    if aMDH.count(i)>1:print (i,aMDH.count(i))
69/17: a.loc[71400:71403]
69/18: a=a.reset_index(drop=True)
69/19:
aMDH=list(a.MDH)
for i in mdh:
    if aMDH.count(i)>1:print (i,aMDH.count(i))
69/20: a.loc[71400:71403]
69/21: a.loc[71400]
69/22: a.loc[91520]
69/23: a.loc[a.MDH==91520]
69/24:
fname=P+'point_cems.csv'
cems=read_csv(fname)
69/25:
for c in cp_no:
  a=cems.loc[cems.CP_NO==c]    
  n=len(a)
  if n != 24*365:
    aMDH=list(a.MDH)
    for i in mdh:
      if aMDH.count(i)>1:
        ii=a.loc[a.MDH==i].index
        for j in ii:
          if sum(cems.loc[j,'SOX']+cems.loc[j,'PM']+cems.loc[j,'PM'])==0:
            cems.drop(j, inplace=True)
69/26: j
69/27:
for c in cp_no:
  a=cems.loc[cems.CP_NO==c]    
  n=len(a)
  if n != 24*365:
    aMDH=list(a.MDH)
    for i in mdh:
      if aMDH.count(i)>1:
        ii=a.loc[a.MDH==i].index
        for j in ii:
          if (cems.loc[j,'SOX']+cems.loc[j,'PM']+cems.loc[j,'PM'])==0:
            cems.drop(j, inplace=True)
69/28:
fname=P+'point_cems.csv'
cems=read_csv(fname)
69/29:
js=[]
for c in cp_no:    
  a=cems.loc[cems.CP_NO==c]    
  n=len(a)
  if n != 24*365:
    aMDH=list(a.MDH)
    for i in mdh:
      if aMDH.count(i)>1:
        ii=a.loc[a.MDH==i].index
        for j in ii:
          if (cems.loc[j,'SOX']+cems.loc[j,'PM']+cems.loc[j,'PM'])==0:js.append(j)
69/30:
cs=[]
for c in cp_no:
    n=len(cems.loc[cems.CP_NO==c])
    if n != 24*365:
        print(c)
        cs.append(c)
69/31:
js=[]
for c in cs:    
  a=cems.loc[cems.CP_NO==c]    
  n=len(a)
  if n != 24*365:
    aMDH=list(a.MDH)
    for i in mdh:
      if aMDH.count(i)>1:
        ii=a.loc[a.MDH==i].index
        for j in ii:
          if (cems.loc[j,'SOX']+cems.loc[j,'PM']+cems.loc[j,'PM'])==0:js.append(j)
69/32: len(js)
69/33: cems.drop(js,inplace=True)
69/34:
cs=[]
for c in cp_no:
    n=len(cems.loc[cems.CP_NO==c])
    if n != 24*365:
        print(c)
        cs.append(c)
69/35: n
69/36:
for c in cs:
    n=len(cems.loc[cems.CP_NO==c])
    if n != 24*365:
        print(c)
69/37: n
69/38: a=cems.loc[cems.CP_NO==c]
69/39:
aMDH=list(a.MDH)
for i in mdh:
    if aMDH.count(i)>1:print (i,aMDH.count(i))
69/40: len(a)
69/41: a=cems.loc[cems.CP_NO==c]
69/42: cs
69/43: c
69/44: a.head()
69/45:
fname=P+'point_cems.csv'
cems=read_csv(fname)
69/46:
js=[]
for c in cs:    
  a=cems.loc[cems.CP_NO==c]    
  n=len(a)
  if n != 24*365:
    aMDH=list(a.MDH)
    for i in mdh:
      if aMDH.count(i)>1:
        ii=a.loc[a.MDH==i].index[1:]
        for j in ii:
          if (cems.loc[j,'SOX']+cems.loc[j,'PM']+cems.loc[j,'PM'])==0:js.append(j)
69/47: len(js)
69/48: cems.drop(js[:5],inplace=True)
69/49:
fname=P+'point_cems.csv'
cems=read_csv(fname)
69/50: cems.loc[js[:5]]
69/51: cems.loc[55:58]
69/52: js[:5]
69/53: c
69/54: a=cems.loc[cems.CP_NO==c]
69/55: n=len(a)
69/56: n
69/57: aMDH=list(a.MDH)
69/58: i=mdh[0]
69/59: i
69/60: aMDH.count(i)
69/61: a.loc[a.MDH==i].index[1:]
69/62: a.head()
69/63: js[:5]
69/64:
fname=P+'point_cems.csv'
cems=read_csv(fname)
69/65:
js=[]
for c in cs:    
  a=cems.loc[cems.CP_NO==c]    
  n=len(a)
  if n != 24*365:
    aMDH=list(a.MDH)
    for i in mdh:
      if aMDH.count(i)>1:
        ii=a.loc[a.MDH==i].index[1:]
        for j in ii:
          if (cems.loc[j,'SOX']+cems.loc[j,'PM']+cems.loc[j,'PM'])==0:js.append(j)
69/66: js[:5]
69/67: n
69/68: ii
69/69: js[-5:]
69/70: cems.loc[1732179:]
69/71: cems['sum']=cems.SOX+cems.NOX+cems.PM
69/72: cems.drop(cems.loc[cems['sum']==0].index,inplace=True)
69/73: len(cems)
69/74: del cems['CP_NO']
69/75: cems.set_index('CP_NO').to_csv(fname)
69/76: cems.set_index('C_NO').to_csv(fname)
69/77: pwd
69/78: cd ../rars
69/79: ls *csv
69/80: ls *dbf
69/81: run ../dbf2csv.py 108_point_cems.dbf
69/82: !pip install simpledbf
69/83: run ../dbf2csv.py 108_point_cems.dbf
69/84:
fname=P+'point_cems.csv'
cems=read_csv(fname)
69/85: cd ../ptse
69/86:
fname=P+'point_cems.csv'
cems=read_csv(fname)
69/87: cs
69/88:
  idx=cems[cems.C_NO=='C_NO'].index
  cems=cems.drop(idx).reset_index(drop=True)
  cems['PM']=[(i+j)/2 for i,j in zip(cems.SOX,cems.NOX)]
  if max(cems.HOUR)>100:
    cems['MDH']=[int(i*10000+j*100+k/100) for i,j,k in zip(cems.MONTH,cems.DATE,cems.HOUR)]
  else:
    cems['MDH']=[int(i*10000+j*100+k) for i,j,k in zip(cems.MONTH,cems.DATE,cems.HOUR)]
  cems['MD']=[int(i*100+j) for i,j in zip(cems.MONTH,cems.DATE)]
  cems['CP_NO'] = [i + j for i, j in zip(list(cems['C_NO']), list(cems['NO_S']))]
  cems=cems.sort_values('MDH',ascending=True).reset_index(drop=True)
  #coverage test
  id365=365
  if len(cems.loc[(cems.MD==229)])>0:id365=366
  s365=set([i*24 for i in range(id365)])
  ih8760=id365*24
  mdh=list(set(cems.MDH))
  if len(mdh)!=ih8760:sys.exit('mdh coverage not enough!')
  mdh.sort()
  md=list(set(cems.MD))
  md.sort()
  #fill the hours
  val='SOX NOX FLOW PM X_BLANK1 X_BLANK2'.split()
  idc='NO_S C_NO CP_NO'.split()
  cp_no=set(cems.CP_NO)
  add_cems=DataFrame({})
69/89:
js=[]
for c in cs:    
  a=cems.loc[cems.CP_NO==c]    
  n=len(a)
  if n != 24*365:
    aMDH=list(a.MDH)
    for i in mdh:
      if aMDH.count(i)>1:
        ii=a.loc[a.MDH==i].index[1:]
        for j in ii:
          if (cems.loc[j,'SOX']+cems.loc[j,'NOX']+cems.loc[j,'PM'])==0:js.append(j)
69/90: len(js)
69/91: len(a)
69/92: len(set(aMDH))
69/93: cems.head()
69/94:   cems['SNF']=[i+j+k for i,j in zip(cems.SOX,cems.NOX,cems.FLOW)]
69/95:   cems['SNF']=[i+j+k for i,j,k in zip(cems.SOX,cems.NOX,cems.FLOW)]
69/96: cems.drop(cems.loc[cems.SNF==0].index,inplace=True)
69/97: len(cems)
69/98: cems.tail()
69/99:
fname=P+'point_cems.csv'
cems=read_csv(fname)
69/100:
  idx=cems[cems.C_NO=='C_NO'].index
  cems=cems.drop(idx).reset_index(drop=True)
  cems['SNF']=[i+j+k for i,j,k in zip(cems.SOX,cems.NOX,cems.FLOW)]
  cems.drop(cems.loc[cems.SNF==0].index,inplace=True).reset_index(drop=True)
  cems['PM']=[(i+j)/2 for i,j in zip(cems.SOX,cems.NOX)]
  if max(cems.HOUR)>100:
    cems['MDH']=[int(i*10000+j*100+k/100) for i,j,k in zip(cems.MONTH,cems.DATE,cems.HOUR)]
  else:
    cems['MDH']=[int(i*10000+j*100+k) for i,j,k in zip(cems.MONTH,cems.DATE,cems.HOUR)]
  cems['MD']=[int(i*100+j) for i,j in zip(cems.MONTH,cems.DATE)]
  cems['CP_NO'] = [i + j for i, j in zip(list(cems['C_NO']), list(cems['NO_S']))]
  cems=cems.sort_values('MDH',ascending=True).reset_index(drop=True)
  #coverage test
  id365=365
  if len(cems.loc[(cems.MD==229)])>0:id365=366
  s365=set([i*24 for i in range(id365)])
  ih8760=id365*24
  mdh=list(set(cems.MDH))
  if len(mdh)!=ih8760:sys.exit('mdh coverage not enough!')
  mdh.sort()
  md=list(set(cems.MD))
  md.sort()
  #fill the hours
  val='SOX NOX FLOW PM X_BLANK1 X_BLANK2'.split()
  idc='NO_S C_NO CP_NO'.split()
  cp_no=set(cems.CP_NO)
  add_cems=DataFrame({})
69/101: cems.drop(cems.loc[cems.SNF==0].index,inplace=True)
69/102: cems=cems.reset_index(drop=True)
69/103:
  cems['PM']=[(i+j)/2 for i,j in zip(cems.SOX,cems.NOX)]
  if max(cems.HOUR)>100:
    cems['MDH']=[int(i*10000+j*100+k/100) for i,j,k in zip(cems.MONTH,cems.DATE,cems.HOUR)]
  else:
    cems['MDH']=[int(i*10000+j*100+k) for i,j,k in zip(cems.MONTH,cems.DATE,cems.HOUR)]
  cems['MD']=[int(i*100+j) for i,j in zip(cems.MONTH,cems.DATE)]
  cems['CP_NO'] = [i + j for i, j in zip(list(cems['C_NO']), list(cems['NO_S']))]
  cems=cems.sort_values('MDH',ascending=True).reset_index(drop=True)
  #coverage test
  id365=365
  if len(cems.loc[(cems.MD==229)])>0:id365=366
  s365=set([i*24 for i in range(id365)])
  ih8760=id365*24
  mdh=list(set(cems.MDH))
  if len(mdh)!=ih8760:sys.exit('mdh coverage not enough!')
  mdh.sort()
  md=list(set(cems.MD))
  md.sort()
  #fill the hours
  val='SOX NOX FLOW PM X_BLANK1 X_BLANK2'.split()
  idc='NO_S C_NO CP_NO'.split()
  cp_no=set(cems.CP_NO)
  add_cems=DataFrame({})
69/104:
js=[]
for c in cs:    
  a=cems.loc[cems.CP_NO==c]    
  n=len(a)
  if n != 24*365:
    aMDH=list(a.MDH)
    for i in mdh:
      if aMDH.count(i)>1:
        ii=a.loc[a.MDH==i].index[1:]
        for j in ii:
          if (cems.loc[j,'SOX']+cems.loc[j,'NOX']+cems.loc[j,'PM'])==0:js.append(j)
69/105: len(js)
69/106: js[:5]
69/107:
cs=[]
for c in cp_no:
    n=len(cems.loc[cems.CP_NO==c])
    if n > 24*365:
        print(c)
        cs.append(c)
69/108:
  cs,cp_no=[],set(cems.CP_NO)
  for c in cp_no:
    n=len(cems.loc[cems.CP_NO==c])
    if n != 24*365: cs.append(c)
69/109: len(cs)
69/110: len(cp_no)
69/111:   pv=pivot_table(cems,index=['CP_NO','MDH'],values=val,aggfunc=sum).reset_index()
69/112: len(pv)
69/113: pv.head()
69/114: run ptseE_ONS.py PM
69/115: run ptseE_ONS.py PM
69/116: run ptseE_ONS.py PM
69/117: cems.head()
69/118: len('A4000283')
69/119:
  if 'C_NO' not in cems.columns:
    cems['C_NO']=[i[:8] for i in cems.CP_NO]
69/120: run ptseE_ONS.py PM
70/1: run ptseG.py 1901
70/2: s
69/121: !lst
70/3: [i for i in sccV if i[:5]=='30117']
70/4: [i for i in sccV if i[:6]=='301174']
70/5: [i for i in gsrefV.SCC if i[:6]=='301174']
70/6: pwd
70/7: run ptseG.py 1901
70/8: s
70/9: [i for i in gsrefV.SCC if i[:6]=='301176']
70/10: [i for i in sccV if i[:6]=='301176']
70/11: run ptseG.py 1901
70/12: s
70/13: [i for i in sccV if i[:6]=='301211']
70/14: [i for i in gsrefV.SCC if i[:6]=='301211']
70/15: run ptseG.py 1901
70/16: a=set(sccV)-set(gsrefV.SCC)
70/17: len(a)
70/18: a
70/19: [i for i in gsrefV.SCC if i[:6]=='302011']
70/20: [i for i in sccV if i[:6]=='302011']
70/21: [i for i in sccV if i[:6]=='303005']
70/22: [i for i in gsrefV.SCC if i[:6]=='303005']
70/23: [i for i in gsrefV.SCC if i[:5]=='30300']
70/24: s
70/25: a
70/26: [i for i in gsrefV.SCC if i[:6]=='303010']
70/27: [i for i in sccV if i[:6]=='303010']
70/28: run ptseG.py 1901
70/29: vi ptseG.py
70/30: !vi ptseG.py
70/31: endp
70/32: endp=365*24-ibdate
70/33: endp
70/34: run ptseG.py 1901
70/35: run ptseG.py 1901
71/1: run ptseE.py 1901
71/2: [i for i in gsrefV.SCC if i[:6]=='301205']
71/3: [i for i in sccV if i[:6]=='301205']
71/4: run ptseE.py 1901
71/5: set(sccV)-set(gsrefV.SCC)
71/6: s
71/7: [i for i in sccV if i[:6]=='304002']
71/8: [i for i in gsrefV.SCC if i[:6]=='304002']
71/9: run ptseE.py 1901
71/10: !conda install pyarrow
72/1: run ECnEPA_timvar.py 1904
73/1: !vi paraSNC.py
73/2: from pyarrow.feather import write_feather as wf
73/3: from pandas import *
73/4: ls *csv
73/5: df=read_csv('areagrid11LL.csv')
73/6: wf(df0,'a',compression ='zstd',compression_level =2)
73/7: wf(df,'a',compression ='zstd',compression_level =2)
74/1: from pandas import *
74/2: from pyarrow.feather import write_feather as wf
74/3: df=read_csv('areagrid11LL.csv')
74/4: wf(df,'a',compression ='zstd',compression_level =2)
74/5: !pip uninstall pyarrow
75/1: from pyarrow.feather import write_feather as wf
75/2: from pandas import *
75/3: df=read_csv('areagrid11LL.csv')
75/4: wf(df,'a',compression ='zstd',compression_level =2)
76/1: from pandas import *
76/2: from pyarrow.feather import write_feather as wf
76/3: df=read_csv('areagrid11LL.csv')
76/4: wf(df,'a',compression ='zstd',compression_level =2)
77/1: from pandas import *
77/2: !head fnames01.txt
77/3: df=read_csv('csvs/201901/100b.csv')
77/4: jjjhh=list(df.loc[0,'JJJHH'])[0]
77/5: df.head()
77/6: df.loc[0,'JJJHH']
77/7: jjjhh=df.loc[0,'JJJHH']
77/8: df=df.loc[df.JJJHH=jjjhh]
77/9: df=df.loc[df.JJJHH==jjjhh]
77/10: len(df)
77/11: len(set(df.YX))
77/12: n
77/13: n='100b'
77/14: df['nsc2']=n
77/15: df.head()
77/16:
cold=['nsc2','YX']
for fname in fnames:
  try:
    df=read_csv(fname)
  except:
    continue
  if len(df)==0:continue
  jjjhh=df.loc[0,'JJJHH']
  df=df.loc[df.JJJHH==jjjhh].reset_index(drop=True)
  n=fname.split('/')[-1].split('.')[0]
  df['nsc2']=n
  df[cold].set_index('nsc2').to_csv('fnames'+mm+'.csv',mode='a')
77/17:
with open('fnames'+mm+'.txt','r') as f:
  fnames=[l.strip('\n') for l in f if '51' not in l] #the ship emission is processed separately
var1=np.zeros(shape=(ntm*nrow*ncol))
cold=['nsc2','YX']
for fname in fnames:
  try:
    df=read_csv(fname)
  except:
    continue
  if len(df)==0:continue
  jjjhh=df.loc[0,'JJJHH']
  df=df.loc[df.JJJHH==jjjhh].reset_index(drop=True)
  n=fname.split('/')[-1].split('.')[0]
  df['nsc2']=n
  df[cold].set_index('nsc2').to_csv('fnames'+mm+'.csv',mode='a')
78/1: run  ptseG.py 1901
78/2: ntm,NREC
78/3: len(dfi)
78/4: len(dfV2)
78/5: nc.dimensions
78/6: nc.dimensions['TSTEP']
78/7: nc.dimensions['TSTEP'].values
78/8: nc.dimensions['TSTEP'].value
78/9: nc.dimensions['TSTEP'].value()
78/10: nc.dimensions['TSTEP'].size
78/11: from ptse_sub import pv_nc
78/12: pv_nc(dfV2[col],VOC)
78/13: !lst
78/14: !rm -fr __pycache__/
78/15: from ptse_sub import pv_nc
78/16: pv_nc(dfV2[col],VOC)
79/1: run  ptseG.py 1901
80/1: run  ptseG.py 1901
80/2: idatetime[:5]
80/3: len(idatetime)
80/4: idatetime[-5:]
80/5: !rm -fr __pycache__/ ptse_sub.pyc
80/6: from ptse_sub import pv_nc
80/7: pv_nc(dfV2[col],nc,VOC)
81/1: run  ptseG.py 1901
81/2: len(sdt)
81/3: sdt,ix,iy=(np.zeros(shape=(ntm*NREC),dtype=int) for i in range(3))
81/4:
  for t in range(ntm):
    sdt[t*NREC:(t+1)*NREC]=t
81/5: sdt[:5]
81/6: sdt[100:105]
81/7: sdt[900:905]
81/8: sdt[-5:]
81/9: len(sdt)
81/10: nc.dimensions
81/11: ntm,nrow,ncol=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'])
81/12: ntm,nrow,ncol
82/1: run  ptseG.py 1901
82/2: nc.variables
82/3: ntm,nrow,ncol,V=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL']),[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
82/4: ntm,nrow,ncol,V=nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'],[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
83/1: run  ptseG.py 1901
84/1: run  ptseG.py 1901
84/2: run  ptseG.py 1901
86/1: run pt2em_d04.py fortBE.413_teds11.ptsE01.nc
86/2: run pt2em_d04.py fortBE.413_teds10.ptsE01.nc
86/3:
sint=[]
for v in set(Vt1)&set(V[3]):
  if np.sum(var[Vt1.index(v),:,:])==0.:continue
  sint.append(v)
86/4: len(sint)
86/5: Vt1
86/6: V[3]
86/7: !lst
86/8: fname='fortBE.413_teds10.ptsE01.nc'
86/9: nc = netCDF4.Dataset(fname,'r')
86/10: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
86/11: V[2]
86/12: V[1]
86/13: nc.NVARS
86/14: len(V[1])
86/15: max(nc['NO2'][:])
86/16: np.max(nc['NO2'][:])
86/17: np.max(nc['SO2'][:])
87/1: fname='fortBE.413_teds10.ptsE02.nc'
87/2: nc = netCDF4.Dataset(fname,'r')
87/3: import netCDF4
87/4: import numpy as np
87/5: nc = netCDF4.Dataset(fname,'r')
87/6: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
87/7: np.max(nc['SO2'][:])
88/1: import netCDF4
88/2: import numpy as np
88/3: fname='fortBE.413_teds10.ptsE02.nc'
88/4:
nct = netCDF4.Dataset(fname,'r')
Vt=[list(filter(lambda x:nct.variables[x].ndim==j, [i for i in nct.variables])) for j in [1,2,3,4]]
ntt,nvt,dt=nct.variables[Vt[2][0]].shape
try:
  nopts=nct.NOPTS
except:
  nopts=nct.dimensions['COL'].size

TFLAG=nct.variables['TFLAG'][:,0,:]
ETFLAG=nct.variables['ETFLAG'][:,0,:]
SDATE=nct.SDATE
STIME=nct.STIME
Vt1=[i for i in Vt[1] if i not in ['CP_NO','plumerise']]
88/5: MM='fortBE.413_teds10.ptsE02.nc'
88/6: fname=MM+'_d04.nc'
88/7:
nc = netCDF4.Dataset(fname,'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
88/8:
var=np.zeros(shape=(len(Vt1),ntt,nopts))
for v in Vt1:
  iv=Vt1.index(v)
  var[iv,:,:]=nct.variables[v][:,:]
88/9: len(Vt1),ntt,nopts
88/10: sint=[v for v in set(Vt1)&set(V[3]) if np.sum(var[Vt1.index(v),:,:])!=0.]
88/11: len(sint)
88/12: sint
88/13: s=''.join([c+',' for c in set(V[3])-set(sint)])
88/14: s
88/15: len(V[3])
88/16: run  ptseG.py 1901
88/17: run  ptseG.py 1901
88/18: pwd
88/19: VOC.shape
88/20: NREC
88/21: ntm
88/22: NREC*ntm
88/23: len(dfV2)
88/24: dfV2.columns
88/25: ons2.shape
88/26: VOC,a,b=[np.zeros(shape=(NREC,ntm,NC))]*3
88/27: a[:5,:5,:5]
88/28: b                 [:5,:5,:5]
88/29: VOC[:5,:5,:5]
88/30:
for c in cbm:
  ic=cbm.index(c)
  a[:,:,ic]=dfV2[c][:,None]
88/31: a[:5,:5,:5]
88/32: dfV2.head()
88/33: col[:5]
88/34:
for c in cbm:
  ic=cbm.index(c)
  a[:,:,ic]=np.array(dfV2[c])[:,None]
88/35: a[:5,0,0]
88/36: a[0,:5,0]
88/37:
b[:,:,:]=ons2[:,:,None]
VOC[:]=a[:]*b[:]
88/38: VOC[0,:5,0]
88/39: (a==b).all()
88/40: VOC,a,b=[np.zeros(shape=(NREC,ntm,NC))]*3
88/41:
for c in cbm:
  ic=cbm.index(c)
  a[:,:,ic]=np.array(dfV2[c])[:,None]
88/42: b[0,:5,0]
88/43:
for c in 'VOC,a,b'.split(','):
  exec(c+'=np.zeros(shape=(NREC,ntm,NC))')
88/44:
for c in cbm:
  ic=cbm.index(c)
  a[:,:,ic]=np.array(dfV2[c])[:,None]
88/45: b[0,:5,0]
88/46:
b[:,:,:]=ons2[:,:,None]
VOC[:]=a[:]*b[:]
88/47: VOC[:5,:5,:5]
88/48: b[0,:5,0]
88/49: b[:5,:5,:5]
88/50: VOC,a,b=(np.zeros(shape=(NREC,ntm,NC)) for i in range(3))
88/51:
for c in cbm:
  ic=cbm.index(c)
  a[:,:,ic]=np.array(dfV2[c])[:,None]
88/52: b[:5,:5,:5]
88/53:
b[:,:,:]=ons2[:,:,None]
VOC[:]=a[:]*b[:]
88/54: VOC[:5,:5,:5]
88/55: dfT=Mat2DF(VOC)
88/56:
def Mat2DF(a):
  import sys
  import numpy as np
  from pandas import DataFrame
  ndim=a.ndim
  if ndim<2:sys.exit('ndim too small, no need to convert')
  H,T,C,N='[', ']', ':,', 'None,'
  ranks=[]
  for n in range(ndim):
    s=H
    for i in range(ndim):
      m=N
      if i==n:m=C
      s+=m
    ranks.append(s.strip(',')+T)
  DD={}
  for i in range(ndim):
    var=np.zeros(shape=a.shape,dtype=int)
    var[:]=eval('np.array([j for j in range(a.shape[i])],dtype=int)'+ranks[i],locals())
    DD['col_'+str(i+1)]=var[:].flatten()
  DD['val']=a.flatten()
  return DataFrame(DD)
88/57: dfT=Mat2DF(VOC)
88/58: ranks
88/59: a=VOC[:]
88/60:
  ndim=a.ndim
  if ndim<2:sys.exit('ndim too small, no need to convert')
  H,T,C,N='[', ']', ':,', 'None,'
  ranks=[]
  for n in range(ndim):
    s=H
    for i in range(ndim):
      m=N
      if i==n:m=C
      s+=m
    ranks.append(s.strip(',')+T)
88/61: ranks
88/62: a.shape
88/63: DD
88/64:
  DD={}
  for i in range(ndim):
    var=np.zeros(shape=a.shape,dtype=int)
    var[:]=eval('np.array([j for j in range(a.shape[i])],dtype=int)'+ranks[i],locals())
    DD['col_'+str(i+1)]=var[:].flatten()
88/65:
  DD={}
  for i in range(ndim):
    var=np.zeros(shape=a.shape,dtype=int)
    exec('var[:]=np.array([j for j in range(a.shape[i])],dtype=int)'+ranks[i])
    DD['col_'+str(i+1)]=var[:].flatten()
88/66: len(DD)
88/67: len(DD['col_1'])
88/68: len(DD['col_2'])
88/69: len(DD['col_3'])
88/70:
def Mat2DF(a):
  import sys
  import numpy as np
  from pandas import DataFrame
  ndim=a.ndim
  if ndim<2:sys.exit('ndim too small, no need to convert')
  H,T,C,N='[', ']', ':,', 'None,'
  ranks=[]
  for n in range(ndim):
    s=H
    for i in range(ndim):
      m=N
      if i==n:m=C
      s+=m
    ranks.append(s.strip(',')+T)
  DD={}
  for i in range(ndim):
    var=np.zeros(shape=a.shape,dtype=int)
#mac    var[:]=eval('np.array([j for j in range(a.shape[i])],dtype=int)'+ranks[i],locals())
    exec('var[:]=np.array([j for j in range(a.shape[i])],dtype=int)'+ranks[i],locals())
    DD['col_'+str(i+1)]=var[:].flatten()
  DD['val']=a.flatten()
  return DataFrame(DD)
88/71: dfT=Mat2DF(VOC)
88/72: dfT.head()
88/73: df.tail()
88/74: dfT.tail()
88/75: VOC.shape
88/76: dfi=dfV2
88/77: dfi=dfV2[:]
88/78:
  dfT['IX']=[dfi.IX[i] for i in dfT.col_1]
  dfT['IY']=[dfi.IY[i] for i in dfT.col_1]
88/79: dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
88/80: len(dfT)
88/81: 125573135/57492448
88/82:
for i in set(dfT.col_1):
  idx=dfT.loc[dfT.col_1==i].index
  dfT.loc[idx,'IX']=dfi.IX[i]
88/83: dfT['IX'],dfT['IY']=0,0
88/84:
  for i in range(NREC):
    idx=dfT.loc[dfT.col_1==i].index
    dfT.loc[idx,'IX']=dfi.IX[i]
    dfT.loc[idx,'IY']=dfi.IY[i]
    if i//10000==0:print(i)
88/85:
  dfT['IX'],dfT['IY']=0,0
  for i in range(NREC):
    idx=dfT.loc[dfT.col_1==i].index
    dfT.loc[idx,'IX']=dfi.IX[i]
    dfT.loc[idx,'IY']=dfi.IY[i]
    if i%10000==0:print(i)
88/86:
  dfT['IX'],dfT['IY']=0,0
  for i in range(NREC):
    idx=dfT.loc[dfT.col_1==i].index
    dfT.loc[idx,'IX']=dfi.IX[i]
    dfT.loc[idx,'IY']=dfi.IY[i]
    if i%100==0:print(i)
88/87: IX,IY=(np.zeros(shape=len(dfT),dtype=int) for i in range(2))
88/88:
  col_1=np.array(dfT.col_1,dtype=int)
  for i in range(NREC):
    idx=np.where(col_1==i)[0] #dfT.loc[dfT.col_1==i].index
    IX[idx[:]]=dfi.IX[i]
    IY[idx[:]]=dfi.IY[i]
    if i%10000==0:print(i)
88/89: dfT=Mat2DF(VOC[:,:,0])
88/90: len(dfT)
88/91: dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
88/92: len(dfT)
88/93:
  dfT['IX'],dfT['IY']=0,0
  for i in range(NREC):
    idx=dfT.loc[dfT.col_1==i].index
    dfT.loc[idx,'IX']=dfi.IX[i]
    dfT.loc[idx,'IY']=dfi.IY[i]
    if i%5000==0:print(i)
88/94: NREC
88/95: dfV2.columns
88/96: col
88/97: dfi.columns
88/98: dfi=dfV2[col]
88/99: dfi.columns
88/100: col=dfi.columns
88/101: len(col)
88/102:
    boo=(dfT.IX>=0) & (dfT.IY>=0) & (dfT.IX<ncol) & (dfT.IY<nrow)
    dfT=dfT.loc[boo].reset_index(drop=True)
88/103: len(dfT)
88/104: dfT.head()
88/105: pv=pivot_table(dfT,index=['col_2','IX','IY'],values='val',aggfunc=sum).reset_index()
88/106: len(pv)
88/107: pv.head()
88/108: pv.tail()
88/109:
def DF2Mat(dd,idx_lst,vname):
  import sys
  import numpy as np
  from pandas import DataFrame
  ret_lst, num_lst=[],[]
  for c in idx_lst:
    lst=eval('list(set(dd.'+c+'))');lst.sort()
    n=len(lst)
    ret_lst.append(lst);num_lst.append(n)
    dct={lst[i]:i for i in range(n)}
    dd['i'+c]=[dct[i] for i in dd[c]]
  mat=np.zeros(shape=num_lst)
  s='mat['+''.join(['dd.i'+c+'[:],' for c in idx_lst]).strip(',')+']=dd.'+vname+'[:]'
  exec(s,locals())
  return mat, ret_lst
88/110:     var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
88/111:
def DF2Mat(dd,idx_lst,vname):
  import sys
  import numpy as np
  from pandas import DataFrame
  ret_lst, num_lst=[],[]
  for c in idx_lst:
#mac    lst=eval('list(set(dd.'+c+'))');lst.sort()
    exec('lst=list(set(dd.'+c+'))');lst.sort()
    n=len(lst)
    ret_lst.append(lst);num_lst.append(n)
    dct={lst[i]:i for i in range(n)}
    dd['i'+c]=[dct[i] for i in dd[c]]
  mat=np.zeros(shape=num_lst)
  s='mat['+''.join(['dd.i'+c+'[:],' for c in idx_lst]).strip(',')+']=dd.'+vname+'[:]'
  exec(s,locals())
  return mat, ret_lst
88/112:     var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
88/113: dd=pv[:]
88/114: idx_lst=['col_2','IY','IX']
88/115:
  ret_lst, num_lst=[],[]
  for c in idx_lst:
#mac    lst=eval('list(set(dd.'+c+'))');lst.sort()
    exec('lst=list(set(dd.'+c+'))');lst.sort()
    n=len(lst)
    ret_lst.append(lst);num_lst.append(n)
    dct={lst[i]:i for i in range(n)}
    dd['i'+c]=[dct[i] for i in dd[c]]
88/116: lst
88/117: len(lst)
88/118:
def DF2Mat(dd,idx_lst,vname):
  import sys
  import numpy as np
  from pandas import DataFrame
  ret_lst, num_lst=[],[]
  for c in idx_lst:
#mac    lst=eval('list(set(dd.'+c+'))');lst.sort()
    exec('lst=list(set(dd.'+c+'))',locals());lst.sort()
    n=len(lst)
    ret_lst.append(lst);num_lst.append(n)
    dct={lst[i]:i for i in range(n)}
    dd['i'+c]=[dct[i] for i in dd[c]]
  mat=np.zeros(shape=num_lst)
  s='mat['+''.join(['dd.i'+c+'[:],' for c in idx_lst]).strip(',')+']=dd.'+vname+'[:]'
  exec(s,locals())
  return mat, ret_lst
88/119: var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
88/120: pv.head()
88/121:
  ret_lst, num_lst=[],[]
  for c in idx_lst:
#mac    lst=eval('list(set(dd.'+c+'))');lst.sort()
    exec('lst=list(set(dd.'+c+'))',locals());lst.sort()
    n=len(lst)
    ret_lst.append(lst);num_lst.append(n)
    dct={lst[i]:i for i in range(n)}
    dd['i'+c]=[dct[i] for i in dd[c]]
88/122: dd.head()
88/123: len(ret_lst[0])
88/124: len(ret_lst[1])
88/125: len(ret_lst[2])
88/126:
def DF2Mat(dd,idx_lst,vname):
  import sys
  import numpy as np
  from pandas import DataFrame
  ret_lst, num_lst=[],[]
  for c in idx_lst:
#mac    lst=eval('list(set(dd.'+c+'))');lst.sort()
    lst=list(set(dd[c]));lst.sort()
    n=len(lst)
    ret_lst.append(lst);num_lst.append(n)
    dct={lst[i]:i for i in range(n)}
    dd['i'+c]=[dct[i] for i in dd[c]]
  mat=np.zeros(shape=num_lst)
  s='mat['+''.join(['dd.i'+c+'[:],' for c in idx_lst]).strip(',')+']=dd.'+vname+'[:]'
  exec(s,locals())
  return mat, ret_lst
88/127: var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
88/128: var.shape
88/129: col=[i for i in dfi.columns if i not in ['IX','IY']]
88/130: col
88/131:
def DF2Mat(dd,idx_lst,vname):
  import sys
  import numpy as np
  from pandas import DataFrame
  ret_lst, num_lst=[],[]
  for c in idx_lst:
#mac    lst=eval('list(set(dd.'+c+'))');lst.sort()
    lst=list(set(dd[c]));lst.sort()
    n=len(lst)
    ret_lst.append(np.array(lst));num_lst.append(n)
    dct={lst[i]:i for i in range(n)}
    dd['i'+c]=[dct[i] for i in dd[c]]
  mat=np.zeros(shape=num_lst)
  s='mat['+''.join(['dd.i'+c+'[:],' for c in idx_lst]).strip(',')+']=dd.'+vname+'[:]'
  exec(s,locals())
  return mat, ret_lst
88/132: var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
88/133: lst[0]
88/134:
    i0,i1,i2=(np.zeros(shape=var.shape,dtype=int) for i in range(3))
    i0[:]=lst[0][:,None,None]
    i1[:]=lst[1][None,:,None]
    i2[:]=lst[2][None,None,:]
88/135: ntm,nrow,ncol
88/136: var.shape
88/137:
def pv_nc(dfi,nc,spec):
  NREC=len(dfi)
  col=[i for i in dfi.columns if i not in ['IX','IY']]
  if col!=spec.shape[-1]:sys.exit('last dimension must be ic')
  ntm,nrow,ncol=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'])
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  for ic in range(len(col)):
    c=col[c]
    if c not in V[3]:continue
    if np.sum(spec[:,:,ic])==0. continue
    dfT=Mat2DF(spec[:,:,ic])
    dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
    dfT['IX'],dfT['IX']=0
    for i in range(NREC):
      idx=np.where(col_1==i)[0] #dfT.loc[dfT.col_1==i].index
      dfT.loc[idx,'IX']=dfi.IX[i]
      dfT.loc[idx,'IY']=dfi.IY[i]
    boo=(dfT.IX>=0) & (dfT.IY>=0) & (dfT.IX<ncol) & (dfT.IY<nrow)
    dfT=dfT.loc[boo].reset_index(drop=True)
    pv=pivot_table(dfT,index=['col_2','IY','IX'],values='val',aggfunc=sum).reset_index()
    var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
    i0,i1,i2=(np.zeros(shape=var.shape,dtype=int) for i in range(3))
    i0[:]=lst[0][:,None,None]
    i1[:]=lst[1][None,:,None]
    i2[:]=lst[2][None,None,:]
    #Note that negative indices are not bothersome and are only at the end of the axis.
    z=np.zeros(shape=(ntm,nrow,ncol))
    z[i0.flatten(),i1.flatten(),i2.flatten()]=var.flatten()
#also mapping whole matrix, NOT by parts
    nc.variables[c][:,0,:,:]=z[:,:,:]
  return
88/138:
def pv_nc(dfi,nc,spec):
  NREC=len(dfi)
  col=[i for i in dfi.columns if i not in ['IX','IY']]
  if col!=spec.shape[-1]:sys.exit('last dimension must be ic')
  ntm,nrow,ncol=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'])
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  for ic in range(len(col)):
    c=col[c]
    if c not in V[3]:continue
    if np.sum(spec[:,:,ic])==0.: continue
    dfT=Mat2DF(spec[:,:,ic])
    dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
    dfT['IX'],dfT['IX']=0
    for i in range(NREC):
      idx=np.where(col_1==i)[0] #dfT.loc[dfT.col_1==i].index
      dfT.loc[idx,'IX']=dfi.IX[i]
      dfT.loc[idx,'IY']=dfi.IY[i]
    boo=(dfT.IX>=0) & (dfT.IY>=0) & (dfT.IX<ncol) & (dfT.IY<nrow)
    dfT=dfT.loc[boo].reset_index(drop=True)
    pv=pivot_table(dfT,index=['col_2','IY','IX'],values='val',aggfunc=sum).reset_index()
    var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
    i0,i1,i2=(np.zeros(shape=var.shape,dtype=int) for i in range(3))
    i0[:]=lst[0][:,None,None]
    i1[:]=lst[1][None,:,None]
    i2[:]=lst[2][None,None,:]
    #Note that negative indices are not bothersome and are only at the end of the axis.
    z=np.zeros(shape=(ntm,nrow,ncol))
    z[i0.flatten(),i1.flatten(),i2.flatten()]=var.flatten()
#also mapping whole matrix, NOT by parts
    nc.variables[c][:,0,:,:]=z[:,:,:]
  return
88/139: pv_nc(dfV2[col],nc,VOC)
88/140: VOC.shape
88/141: len(col)
88/142: col=['IX','IY']+cbm
88/143: pv_nc(dfV2[col],nc,VOC)
88/144:
def pv_nc(dfi,nc,spec):
  NREC=len(dfi)
  col=[i for i in dfi.columns if i not in ['IX','IY']]
  if len(col)!=spec.shape[-1]:sys.exit('last dimension must be ic')
  ntm,nrow,ncol=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'])
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  for ic in range(len(col)):
    c=col[c]
    if c not in V[3]:continue
    if np.sum(spec[:,:,ic])==0.: continue
    dfT=Mat2DF(spec[:,:,ic])
    dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
    dfT['IX'],dfT['IX']=0
    for i in range(NREC):
      idx=np.where(col_1==i)[0] #dfT.loc[dfT.col_1==i].index
      dfT.loc[idx,'IX']=dfi.IX[i]
      dfT.loc[idx,'IY']=dfi.IY[i]
    boo=(dfT.IX>=0) & (dfT.IY>=0) & (dfT.IX<ncol) & (dfT.IY<nrow)
    dfT=dfT.loc[boo].reset_index(drop=True)
    pv=pivot_table(dfT,index=['col_2','IY','IX'],values='val',aggfunc=sum).reset_index()
    var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
    i0,i1,i2=(np.zeros(shape=var.shape,dtype=int) for i in range(3))
    i0[:]=lst[0][:,None,None]
    i1[:]=lst[1][None,:,None]
    i2[:]=lst[2][None,None,:]
    #Note that negative indices are not bothersome and are only at the end of the axis.
    z=np.zeros(shape=(ntm,nrow,ncol))
    z[i0.flatten(),i1.flatten(),i2.flatten()]=var.flatten()
#also mapping whole matrix, NOT by parts
    nc.variables[c][:,0,:,:]=z[:,:,:]
  return
88/145: pv_nc(dfV2[col],nc,VOC)
88/146:
def pv_nc(dfi,nc,spec):
  NREC=len(dfi)
  col=[i for i in dfi.columns if i not in ['IX','IY']]
  if len(col)!=spec.shape[-1]:sys.exit('last dimension must be ic')
  ntm,nrow,ncol=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'])
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  for ic in range(len(col)):
    c=col[ic]
    if c not in V[3]:continue
    if np.sum(spec[:,:,ic])==0.: continue
    dfT=Mat2DF(spec[:,:,ic])
    dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
    dfT['IX'],dfT['IX']=0
    for i in range(NREC):
      idx=np.where(col_1==i)[0] #dfT.loc[dfT.col_1==i].index
      dfT.loc[idx,'IX']=dfi.IX[i]
      dfT.loc[idx,'IY']=dfi.IY[i]
    boo=(dfT.IX>=0) & (dfT.IY>=0) & (dfT.IX<ncol) & (dfT.IY<nrow)
    dfT=dfT.loc[boo].reset_index(drop=True)
    pv=pivot_table(dfT,index=['col_2','IY','IX'],values='val',aggfunc=sum).reset_index()
    var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
    i0,i1,i2=(np.zeros(shape=var.shape,dtype=int) for i in range(3))
    i0[:]=lst[0][:,None,None]
    i1[:]=lst[1][None,:,None]
    i2[:]=lst[2][None,None,:]
    #Note that negative indices are not bothersome and are only at the end of the axis.
    z=np.zeros(shape=(ntm,nrow,ncol))
    z[i0.flatten(),i1.flatten(),i2.flatten()]=var.flatten()
#also mapping whole matrix, NOT by parts
    nc.variables[c][:,0,:,:]=z[:,:,:]
  return
88/147: pv_nc(dfV2[col],nc,VOC)
88/148: col
88/149: dfi=dfV2[col]
88/150:
  NREC=len(dfi)
  col=[i for i in dfi.columns if i not in ['IX','IY']]
  if len(col)!=spec.shape[-1]:sys.exit('last dimension must be ic')
  ntm,nrow,ncol=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'])
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  for ic in range(len(col)):
    c=col[ic]
    if c not in V[3]:continue
    if np.sum(spec[:,:,ic])==0.: continue
    dfT=Mat2DF(spec[:,:,ic])
    dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
    dfT['IX'],dfT['IX']=0
    for i in range(NREC):
      idx=np.where(col_1==i)[0] #dfT.loc[dfT.col_1==i].index
      dfT.loc[idx,'IX']=dfi.IX[i]
      dfT.loc[idx,'IY']=dfi.IY[i]
    boo=(dfT.IX>=0) & (dfT.IY>=0) & (dfT.IX<ncol) & (dfT.IY<nrow)
    dfT=dfT.loc[boo].reset_index(drop=True)
    pv=pivot_table(dfT,index=['col_2','IY','IX'],values='val',aggfunc=sum).reset_index()
    var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
    i0,i1,i2=(np.zeros(shape=var.shape,dtype=int) for i in range(3))
    i0[:]=lst[0][:,None,None]
    i1[:]=lst[1][None,:,None]
    i2[:]=lst[2][None,None,:]
    #Note that negative indices are not bothersome and are only at the end of the axis.
    z=np.zeros(shape=(ntm,nrow,ncol))
    z[i0.flatten(),i1.flatten(),i2.flatten()]=var.flatten()
#also mapping whole matrix, NOT by parts
    nc.variables[c][:,0,:,:]=z[:,:,:]
88/151: spec=VOC[:]
88/152:
  NREC=len(dfi)
  col=[i for i in dfi.columns if i not in ['IX','IY']]
  if len(col)!=spec.shape[-1]:sys.exit('last dimension must be ic')
  ntm,nrow,ncol=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'])
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  for ic in range(len(col)):
    c=col[ic]
    if c not in V[3]:continue
    if np.sum(spec[:,:,ic])==0.: continue
    dfT=Mat2DF(spec[:,:,ic])
    dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
    dfT['IX'],dfT['IX']=0
    for i in range(NREC):
      idx=np.where(col_1==i)[0] #dfT.loc[dfT.col_1==i].index
      dfT.loc[idx,'IX']=dfi.IX[i]
      dfT.loc[idx,'IY']=dfi.IY[i]
    boo=(dfT.IX>=0) & (dfT.IY>=0) & (dfT.IX<ncol) & (dfT.IY<nrow)
    dfT=dfT.loc[boo].reset_index(drop=True)
    pv=pivot_table(dfT,index=['col_2','IY','IX'],values='val',aggfunc=sum).reset_index()
    var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
    i0,i1,i2=(np.zeros(shape=var.shape,dtype=int) for i in range(3))
    i0[:]=lst[0][:,None,None]
    i1[:]=lst[1][None,:,None]
    i2[:]=lst[2][None,None,:]
    #Note that negative indices are not bothersome and are only at the end of the axis.
    z=np.zeros(shape=(ntm,nrow,ncol))
    z[i0.flatten(),i1.flatten(),i2.flatten()]=var.flatten()
#also mapping whole matrix, NOT by parts
    nc.variables[c][:,0,:,:]=z[:,:,:]
88/153: len(dfT)
88/154:
def pv_nc(dfi,nc,spec):
  NREC=len(dfi)
  col=[i for i in dfi.columns if i not in ['IX','IY']]
  if len(col)!=spec.shape[-1]:sys.exit('last dimension must be ic')
  ntm,nrow,ncol=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'])
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  for ic in range(len(col)):
    c=col[ic]
    if c not in V[3]:continue
    if np.sum(spec[:,:,ic])==0.: continue
    dfT=Mat2DF(spec[:,:,ic])
    dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
    dfT['IX'],dfT['IX']=0,0
    for i in range(NREC):
      idx=np.where(col_1==i)[0] #dfT.loc[dfT.col_1==i].index
      dfT.loc[idx,'IX']=dfi.IX[i]
      dfT.loc[idx,'IY']=dfi.IY[i]
    boo=(dfT.IX>=0) & (dfT.IY>=0) & (dfT.IX<ncol) & (dfT.IY<nrow)
    dfT=dfT.loc[boo].reset_index(drop=True)
    pv=pivot_table(dfT,index=['col_2','IY','IX'],values='val',aggfunc=sum).reset_index()
    var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
    i0,i1,i2=(np.zeros(shape=var.shape,dtype=int) for i in range(3))
    i0[:]=lst[0][:,None,None]
    i1[:]=lst[1][None,:,None]
    i2[:]=lst[2][None,None,:]
    #Note that negative indices are not bothersome and are only at the end of the axis.
    z=np.zeros(shape=(ntm,nrow,ncol))
    z[i0.flatten(),i1.flatten(),i2.flatten()]=var.flatten()
#also mapping whole matrix, NOT by parts
    nc.variables[c][:,0,:,:]=z[:,:,:]
  return
88/155: pv_nc(dfV2[col],nc,VOC)
88/156: col=['IX','IY']+cbm
88/157: pv_nc(dfV2[col],nc,VOC)
88/158: col
88/159: dfi=dfV2[col]
88/160: spec=VOC[:]
88/161:
  NREC=len(dfi)
  col=[i for i in dfi.columns if i not in ['IX','IY']]
  if len(col)!=spec.shape[-1]:sys.exit('last dimension must be ic')
  ntm,nrow,ncol=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'])
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
88/162: ic=0
88/163:     c=col[ic]
88/164: c not in V[3]
88/165:  np.sum(spec[:,:,ic])==0.
88/166:
    dfT=Mat2DF(spec[:,:,ic])
    dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
    dfT['IX'],dfT['IX']=0,0
    for i in range(NREC):
      idx=np.where(col_1==i)[0] #dfT.loc[dfT.col_1==i].index
      dfT.loc[idx,'IX']=dfi.IX[i]
      dfT.loc[idx,'IY']=dfi.IY[i]
88/167:
    dfT['IX'],dfT['IX']=0,0
    for i in range(NREC):
      idx=dfT.loc[dfT.col_1==i].index
      dfT.loc[idx,'IX']=dfi.IX[i]
      dfT.loc[idx,'IY']=dfi.IY[i]
88/168:
    boo=(dfT.IX>=0) & (dfT.IY>=0) & (dfT.IX<ncol) & (dfT.IY<nrow)
    dfT=dfT.loc[boo].reset_index(drop=True)
    pv=pivot_table(dfT,index=['col_2','IY','IX'],values='val',aggfunc=sum).reset_index()
    var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
    i0,i1,i2=(np.zeros(shape=var.shape,dtype=int) for i in range(3))
    i0[:]=lst[0][:,None,None]
    i1[:]=lst[1][None,:,None]
    i2[:]=lst[2][None,None,:]
    #Note that negative indices are not bothersome and are only at the end of the axis.
    z=np.zeros(shape=(ntm,nrow,ncol))
    z[i0.flatten(),i1.flatten(),i2.flatten()]=var.flatten()
88/169: col
88/170: col=['IX','IY']+cbm
88/171:
def pv_nc(dfi,nc,spec):
  NREC=len(dfi)
  col=[i for i in dfi.columns if i not in ['IX','IY']]
  if len(col)!=spec.shape[-1]:sys.exit('last dimension must be ic')
  ntm,nrow,ncol=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'])
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  for ic in range(len(col)):
    c=col[ic]
    if c not in V[3]:continue
    if np.sum(spec[:,:,ic])==0.: continue
    dfT=Mat2DF(spec[:,:,ic])
    dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
    dfT['IX'],dfT['IX']=0,0
    for i in range(NREC):
      idx=dfT.loc[dfT.col_1==i].index
      dfT.loc[idx,'IX']=dfi.IX[i]
      dfT.loc[idx,'IY']=dfi.IY[i]
    boo=(dfT.IX>=0) & (dfT.IY>=0) & (dfT.IX<ncol) & (dfT.IY<nrow)
    dfT=dfT.loc[boo].reset_index(drop=True)
    pv=pivot_table(dfT,index=['col_2','IY','IX'],values='val',aggfunc=sum).reset_index()
    var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
    i0,i1,i2=(np.zeros(shape=var.shape,dtype=int) for i in range(3))
    i0[:]=lst[0][:,None,None]
    i1[:]=lst[1][None,:,None]
    i2[:]=lst[2][None,None,:]
    #Note that negative indices are not bothersome and are only at the end of the axis.
    z=np.zeros(shape=(ntm,nrow,ncol))
    z[i0.flatten(),i1.flatten(),i2.flatten()]=var.flatten()
#also mapping whole matrix, NOT by parts
    nc.variables[c][:,0,:,:]=z[:,:,:]
    print(c)
  return
88/172: pv_nc(dfV2[col],nc,VOC)
88/173: dfT=Mat2DF(spec[:,:,ic])
88/174: dfT.head()
88/175: VOC.shape
88/176: VOC.tail()
88/177: dfT.tail()
88/178:
  z=np.zeros(shape=spec.shape[:-1],dtype=int)
  z[:,:]=np.array([i for i in range(NREC)])[:,None]
  z=z.flatten()
  ix,iy=[dfi.IX[i] for i in z],[dfi.IY[i] for i in z]
88/179: dIX={i:dfi.IX[i] for i in range(NREC)}
88/180: dIY={i:dfi.IY[i] for i in range(NREC)}
88/181:
  z=np.zeros(shape=spec.shape[:-1],dtype=int)
  z[:,:]=np.array([i for i in range(NREC)])[:,None]
  z=z.flatten()
  ix,iy=[dIX[i] for i in z],[dIY[i] for i in z]
88/182:
def pv_nc(dfi,nc,spec):
  NREC=len(dfi)
  col=[i for i in dfi.columns if i not in ['IX','IY']]
  if len(col)!=spec.shape[-1]:sys.exit('last dimension must be ic')
  ntm,nrow,ncol=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'])
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  z=np.zeros(shape=spec.shape[:-1],dtype=int)
  z[:,:]=np.array([i for i in range(NREC)])[:,None]
  z=z.flatten()
  dIX, dIY={i:dfi.IX[i] for i in range(NREC)}, {i:dfi.IY[i] for i in range(NREC)}
  ix,iy=[dIX[i] for i in z],[dIY[i] for i in z]
  for ic in range(len(col)):
    c=col[ic]
    if c not in V[3]:continue
    if np.sum(spec[:,:,ic])==0.: continue
    dfT=Mat2DF(spec[:,:,ic])
    dfT['IX'],dfT['IX']=ix,iy
    dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
    boo=(dfT.IX>=0) & (dfT.IY>=0) & (dfT.IX<ncol) & (dfT.IY<nrow)
    dfT=dfT.loc[boo].reset_index(drop=True)
    pv=pivot_table(dfT,index=['col_2','IY','IX'],values='val',aggfunc=sum).reset_index()
    var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
    i0,i1,i2=(np.zeros(shape=var.shape,dtype=int) for i in range(3))
    i0[:]=lst[0][:,None,None]
    i1[:]=lst[1][None,:,None]
    i2[:]=lst[2][None,None,:]
    #Note that negative indices are not bothersome and are only at the end of the axis.
    z=np.zeros(shape=(ntm,nrow,ncol))
    z[i0.flatten(),i1.flatten(),i2.flatten()]=var.flatten()
#also mapping whole matrix, NOT by parts
    nc.variables[c][:,0,:,:]=z[:,:,:]
    print(c)
  return
88/183: col
88/184: pv_nc(dfV2[col],nc,VOC)
88/185:
def pv_nc(dfi,nc,spec):
  NREC=len(dfi)
  col=[i for i in dfi.columns if i not in ['IX','IY']]
  if len(col)!=spec.shape[-1]:sys.exit('last dimension must be ic')
  ntm,nrow,ncol=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL'])
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  z=np.zeros(shape=spec.shape[:-1],dtype=int)
  z[:,:]=np.array([i for i in range(NREC)])[:,None]
  z=z.flatten()
  dIX, dIY={i:dfi.IX[i] for i in range(NREC)}, {i:dfi.IY[i] for i in range(NREC)}
  ix,iy=[dIX[i] for i in z],[dIY[i] for i in z]
  for ic in range(len(col)):
    c=col[ic]
    if c not in V[3]:continue
    if np.sum(spec[:,:,ic])==0.: continue
    dfT=Mat2DF(spec[:,:,ic])
    dfT['IX'],dfT['IY']=ix,iy
    dfT=dfT.loc[dfT.val>0].reset_index(drop=True)
    boo=(dfT.IX>=0) & (dfT.IY>=0) & (dfT.IX<ncol) & (dfT.IY<nrow)
    dfT=dfT.loc[boo].reset_index(drop=True)
    pv=pivot_table(dfT,index=['col_2','IY','IX'],values='val',aggfunc=sum).reset_index()
    var,lst=DF2Mat(pv,['col_2','IY','IX'],'val')
    i0,i1,i2=(np.zeros(shape=var.shape,dtype=int) for i in range(3))
    i0[:]=lst[0][:,None,None]
    i1[:]=lst[1][None,:,None]
    i2[:]=lst[2][None,None,:]
    #Note that negative indices are not bothersome and are only at the end of the axis.
    z=np.zeros(shape=(ntm,nrow,ncol))
    z[i0.flatten(),i1.flatten(),i2.flatten()]=var.flatten()
#also mapping whole matrix, NOT by parts
    nc.variables[c][:,0,:,:]=z[:,:,:]
    print(c)
  return
88/186: pv_nc(dfV2[col],nc,VOC)
88/187: spec.shape
88/188: c2m
88/189:
c2m={'SOX':64,'NOX':46,'CO':28,'PM':1}
colc=['CCRS','FCRS','CPRM','FPRM']
88/190:
for sp in ['SOX','NOX','CO','PM']:
  if sp=='PM':
    dfS2=add_PMS(dfS2)
  dfS2[c2s[sp]]=np.array(dfS2[sp+'_EMI'])/c2m[sp]
88/191:
c2s={'SOX':'SO2','NOX':'NO2','CO':'CO','PM':'PM'}
boo2=(df.SOX_EMI+df.NOX_EMI+df.CO_EMI+df.PM_EMI)>0
cole=[i+'_EMI' for i in c2s]+['PM25_EMI']
col=['C_NO','CP_NO','HD1','DY1','HY1','UTM_E','UTM_N']+cole
colT=['HD1','DY1','HY1']
dfS=df[col].loc[boo2].reset_index(drop=True)
if 'IY' not in dfS.columns:
  dfS=disc(dfS,nc)
for c in cole:
    dfS[c]=[i*1E6/j/k for i,j,k in zip(dfS[c],dfS.DY1,dfS.HD1)]
dfS_cp=pivot_table(dfS,index='CP_NO',values=cole,aggfunc=sum).reset_index()
dfS_xy=pivot_table(dfS,index='CP_NO',values=['IX','IY']+colT,aggfunc=np.mean).reset_index()
dfS2=merge(dfS_cp,dfS_xy,on='CP_NO')
dfS2=dfS2.sort_values('CP_NO').reset_index(drop=True)
NREC=len(dfS2)
88/192: NREC
88/193: spec.shape
88/194:
fnameO=fns['SNCP']
with FortranFile(fnameO, 'r') as f:
  cp = f.read_record(dtype=np.dtype('U12'))
  mdh = f.read_record(dtype=np.int)
  ons = f.read_record(dtype=np.int)
ons=ons.reshape(len(cp),len(mdh))
if ibdate>iedate:
  ons2=np.zeros(shape=(len(dfS2),ntm))
  endp=id365*24-ibdate
  ons2[:,:endp]=ons[:,ibdate:]
  ons2[:,endp:ntm]=ons[:,:iedate]
else:
  ons2=ons[:,ibdate:iedate]
88/195:
c2m={'SOX':64,'NOX':46,'CO':28,'PM':1}
colc=['CCRS','FCRS','CPRM','FPRM']
for sp in ['SOX','NOX','CO','PM']:
  if sp=='PM':
    dfS2=add_PMS(dfS2)
  dfS2[c2s[sp]]=np.array(dfS2[sp+'_EMI'])/c2m[sp]
88/196: dfS2.head()
88/197: cols
88/198: colc
88/199: dfS2.columns
88/200: sp
88/201: sp='SOX'
88/202: c2s[sp]
88/203: col
88/204: col=['IX','IY']+[c2s[sp] for sp in ['SOX','NOX','CO']]+colc
88/205: col
88/206: NREC,NC=len(dfS2),len(col[2:])
88/207: NREC,NC
88/208:
for c in col[2:]:
  ic=col[2:].index(c)
  print (c,ic)
88/209:
SPC,a,b=(np.zeros(shape=(NREC,ntm,NC)) for i in range(3))
for c in col[2:]:
  ic=col[2:].index(c)
  a[:,:,ic]=np.array(dfS2[c])[:,None]
b[:,:,:]=ons2[:,:,None]
SPC[:,:,:]=a[:,:,:]*b[:,:,:]
88/210: SPC.shape
88/211: spc=col[2:]
88/212: SPC.shape
88/213: len(spc)
88/214: pv_nc(dfS2[col],nc,SPC)
88/215: run  ptseG.py 1901
88/216: run  ptseG.py 1901
88/217: run  ptseG.py 1901
89/1:
from pandas import *
import numpy as np
import os, sys, subprocess
import netCDF4
import datetime
from calendar import monthrange

from ioapi_dates import dt2jul

def str2lst(A):
    return [bytes(i,encoding='utf-8') for i in A[1:-1].replace("b'","").replace("'","").replace(" ","").split(',')][:8]


#Main
#locate the programs and root directory
ncks=subprocess.check_output('which ncks',shell=True).decode('utf8').strip('\n')
P='./'

#time and space initiates
89/2: ym='1901'
89/3: ls -lh *fth
89/4:
mm=ym[2:4]
mo=int(mm)
yr=2000+int(sys.argv[1][:2])
ntm=(monthrange(yr,mo)[1]+2)*24+1
bdate=datetime.datetime(yr,mo,1)+datetime.timedelta(days=-1+8./24)
edate=bdate+datetime.timedelta(days=ntm/24)#monthrange(yr,mo)[1]+3)
89/5:
yr=2000+int(ym[:2])
ntm=(monthrange(yr,mo)[1]+2)*24+1
bdate=datetime.datetime(yr,mo,1)+datetime.timedelta(days=-1+8./24)
edate=bdate+datetime.timedelta(days=ntm/24)#monthrange(yr,mo)[1]+3)
#prepare the uamiv template
print('template applied')
NCfname='fortBE.413_teds10.ptsE'+mm+'.nc'
try:
  nc = netCDF4.Dataset(NCfname, 'r+')
except:
  os.system('cp '+P+'template_v7.nc '+NCfname)
  nc = netCDF4.Dataset(NCfname, 'r+')
89/6: ls -lh *nc
89/7: nc = netCDF4.Dataset(NCfname, 'r+')
89/8: nc.NCOLS
89/9: os.system('cp '+P+'template_v7.nc '+NCfname)
89/10: nc = netCDF4.Dataset(NCfname, 'r+')
89/11: nc.close()
89/12: fname='template_v7.nc'
89/13: nc = netCDF4.Dataset(NCfname, 'r')
89/14: !lst
89/15: rm fortBE.413_teds10.ptsE01.nc
89/16: os.system('cp '+P+'template_v7.nc '+NCfname)
89/17: P
89/18: nc = netCDF4.Dataset(NCfname, 'r+')
89/19:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nv,dt=nc.variables[V[2][0]].shape
nv=len([i for i in V[1] if i !='CP_NO'])
nc.SDATE,nc.STIME=dt2jul(bdate)
nc.EDATE,nc.ETIME=dt2jul(edate)
nc.NOTE='Point Emission'
nc.NOTE=nc.NOTE+(60-len(nc.NOTE))*' '
nc.NVARS=nv
#Name-names may encounter conflicts with newer versions of NCFs and PseudoNetCDFs.
nc.name='PTSOURCE  '
nc.NSTEPS=ntm
if 'ETFLAG' not in V[2]:
  zz=nc.createVariable('ETFLAG',"i4",("TSTEP","VAR","DATE-TIME"))
if nt!=ntm or (nc.variables['TFLAG'][0,0,0]!=nc.SDATE and nc.variables['TFLAG'][0,0,1]!=nc.STIME):
  for t in range(ntm):
    sdate,stime=dt2jul(bdate+datetime.timedelta(days=t/24.))
    nc.variables['TFLAG'][t,:,0]=[sdate for i in range(nv)]
    nc.variables['TFLAG'][t,:,1]=[stime for i in range(nv)]
    ndate,ntime=dt2jul(bdate+datetime.timedelta(days=(t+1)/24.))
    nc.variables['ETFLAG'][t,:,0]=[ndate for i in range(nv)]
    nc.variables['ETFLAG'][t,:,1]=[ntime for i in range(nv)]
for c in V[1]:
  nc.variables[c].set_auto_mask(False)
  if c=='CP_NO': continue
  nc.variables[c][:]=0.
nc.close()
89/20:

df=read_feather('df'+mm+'.fth')
pv=read_csv('pv'+mm+'.csv')
pv.CP_NOb=[str2lst(i) for i in pv.CP_NOb]
nopts=len(pv)
#item sets definitions
XYHDTV=['UTM_E','UTM_N','HEI','DIA','TEMP','VEL','ORI_QU1']

#determination of camx version
ver=7
if 'XSTK' in V[0]:ver=6
col_mn=['ORI_QU1','TEMP','VEL','UTM_E', 'UTM_N','HY1','HD1','DY1']
lspec=[i for i in df.columns if i not in col_mn+['index','C_NO']]

dimn={6:'NSTK',7:'COL'}
print(dimn[ver]+' expanding and reopening')
89/21: !pip install pandas=0.15.0
89/22: !pip install pandas==0.15.0
89/23:

df=read_feather('df'+mm+'.fth')
pv=read_csv('pv'+mm+'.csv')
pv.CP_NOb=[str2lst(i) for i in pv.CP_NOb]
nopts=len(pv)
#item sets definitions
XYHDTV=['UTM_E','UTM_N','HEI','DIA','TEMP','VEL','ORI_QU1']

#determination of camx version
ver=7
if 'XSTK' in V[0]:ver=6
col_mn=['ORI_QU1','TEMP','VEL','UTM_E', 'UTM_N','HY1','HD1','DY1']
lspec=[i for i in df.columns if i not in col_mn+['index','C_NO']]

dimn={6:'NSTK',7:'COL'}
print(dimn[ver]+' expanding and reopening')
90/1:
from pandas import *
import numpy as np
import os, sys, subprocess
import netCDF4
import datetime
from calendar import monthrange

from ioapi_dates import dt2jul

def str2lst(A):
    return [bytes(i,encoding='utf-8') for i in A[1:-1].replace("b'","").replace("'","").replace(" ","").split(',')][:8]


#Main
#locate the programs and root directory
ncks=subprocess.check_output('which ncks',shell=True).decode('utf8').strip('\n')
P='./'

#time and space initiates
90/2: from pandas import *
91/1: from pandas import *
92/1: from pandas import *
92/2: mm='01'
92/3: df=read_feather('df'+mm+'.fth')
92/4: !pip install pyarrow
92/5: ls -lh *fth
92/6: mm
92/7: !pip install pyarrow.lib
92/8: !pip uninstall pyarrow
92/9: !pip install pyarrow
92/10: df=read_feather('df'+mm+'.fth')
93/1: from pandas import *
93/2: mm
93/3: mm='01'
93/4: df=read_feather('df'+mm+'.fth')
93/5:
from pandas import *
import numpy as np
import os, sys, subprocess
import netCDF4
import datetime
from calendar import monthrange

from ioapi_dates import dt2jul

def str2lst(A):
    return [bytes(i,encoding='utf-8') for i in A[1:-1].replace("b'","").replace("'","").replace(" ","").split(',')][:8]


#Main
#locate the programs and root directory
ncks=subprocess.check_output('which ncks',shell=True).decode('utf8').strip('\n')
P='./'

#time and space initiates
93/6: ym='1901'
93/7:
mm=ym[2:4]
mo=int(mm)
yr=2000+int(ym[:2])
ntm=(monthrange(yr,mo)[1]+2)*24+1
bdate=datetime.datetime(yr,mo,1)+datetime.timedelta(days=-1+8./24)
edate=bdate+datetime.timedelta(days=ntm/24)#monthrange(yr,mo)[1]+3)
#prepare the uamiv template
print('template applied')
NCfname='fortBE.413_teds10.ptsE'+mm+'.nc'
try:
  nc = netCDF4.Dataset(NCfname, 'r+')
except:
  os.system('cp '+P+'template_v7.nc '+NCfname)
  nc = netCDF4.Dataset(NCfname, 'r+')
93/8:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nv,dt=nc.variables[V[2][0]].shape
nv=len([i for i in V[1] if i !='CP_NO'])
nc.SDATE,nc.STIME=dt2jul(bdate)
nc.EDATE,nc.ETIME=dt2jul(edate)
nc.NOTE='Point Emission'
nc.NOTE=nc.NOTE+(60-len(nc.NOTE))*' '
nc.NVARS=nv
#Name-names may encounter conflicts with newer versions of NCFs and PseudoNetCDFs.
nc.name='PTSOURCE  '
nc.NSTEPS=ntm
if 'ETFLAG' not in V[2]:
  zz=nc.createVariable('ETFLAG',"i4",("TSTEP","VAR","DATE-TIME"))
if nt!=ntm or (nc.variables['TFLAG'][0,0,0]!=nc.SDATE and nc.variables['TFLAG'][0,0,1]!=nc.STIME):
  for t in range(ntm):
    sdate,stime=dt2jul(bdate+datetime.timedelta(days=t/24.))
    nc.variables['TFLAG'][t,:,0]=[sdate for i in range(nv)]
    nc.variables['TFLAG'][t,:,1]=[stime for i in range(nv)]
    ndate,ntime=dt2jul(bdate+datetime.timedelta(days=(t+1)/24.))
    nc.variables['ETFLAG'][t,:,0]=[ndate for i in range(nv)]
    nc.variables['ETFLAG'][t,:,1]=[ntime for i in range(nv)]
for c in V[1]:
  nc.variables[c].set_auto_mask(False)
  if c=='CP_NO': continue
  nc.variables[c][:]=0.
nc.close()
93/9: !head ~/bin/tmNC
93/10:
if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")
93/11:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nv,dt=nc.variables[V[2][0]].shape
nv=len([i for i in V[1] if i !='CP_NO'])
nc.SDATE,nc.STIME=dt2jul(bdate)
nc.EDATE,nc.ETIME=dt2jul(edate)
nc.NOTE='Point Emission'
nc.NOTE=nc.NOTE+(60-len(nc.NOTE))*' '
nc.NVARS=nv
#Name-names may encounter conflicts with newer versions of NCFs and PseudoNetCDFs.
nc.name='PTSOURCE  '
nc.NSTEPS=ntm
if 'ETFLAG' not in V[2]:
  zz=nc.createVariable('ETFLAG',"i4",("TSTEP","VAR","DATE-TIME"))
if nt!=ntm or (nc.variables['TFLAG'][0,0,0]!=nc.SDATE and nc.variables['TFLAG'][0,0,1]!=nc.STIME):
  for t in range(ntm):
    sdate,stime=dt2jul(bdate+datetime.timedelta(days=t/24.))
    nc.variables['TFLAG'][t,:,0]=[sdate for i in range(nv)]
    nc.variables['TFLAG'][t,:,1]=[stime for i in range(nv)]
    ndate,ntime=dt2jul(bdate+datetime.timedelta(days=(t+1)/24.))
    nc.variables['ETFLAG'][t,:,0]=[ndate for i in range(nv)]
    nc.variables['ETFLAG'][t,:,1]=[ntime for i in range(nv)]
for c in V[1]:
  nc.variables[c].set_auto_mask(False)
  if c=='CP_NO': continue
  nc.variables[c][:]=0.
nc.close()
93/12:
pv=read_csv('pv'+mm+'.csv')
pv.CP_NOb=[str2lst(i) for i in pv.CP_NOb]
nopts=len(pv)
#item sets definitions
XYHDTV=['UTM_E','UTM_N','HEI','DIA','TEMP','VEL','ORI_QU1']

#determination of camx version
ver=7
if 'XSTK' in V[0]:ver=6
col_mn=['ORI_QU1','TEMP','VEL','UTM_E', 'UTM_N','HY1','HD1','DY1']
lspec=[i for i in df.columns if i not in col_mn+['index','C_NO']]

dimn={6:'NSTK',7:'COL'}
print(dimn[ver]+' expanding and reopening')
res=os.system(ncks+' -O --mk_rec_dmn '+dimn[ver]+' '+NCfname+' tmp'+mm)
if res!=0: sys.exit(ncks+' fail')
res=os.system('mv tmp'+mm+' '+NCfname)
if res!=0: sys.exit('mv fail')
#CP_NO in S1(byte) format
print('ncfile Enlargement')
#prepare the parameter dicts
PRM='XYHDTV'
v2n={PRM[i]:XYHDTV[i] for i in range(6)}
names={7:['xcoord','ycoord','stkheight','stkdiam','stktemp','stkspeed'],
       6:[v+'STK' for v in PRM]}
v2c={PRM[i]:names[ver][i] for i in range(6)}
a=DataFrame({'SN':df.SO2+df.NO2})
a=a.sort_values('SN',ascending=False)
93/13: len(a)
93/14: pig=a.index[:100]
93/15:
nc = netCDF4.Dataset(NCfname, 'r+')
#enlarge the record dimension (COL)
z=np.zeros(shape=ntm)
for c in V[1]:
  nc.variables[c].set_auto_mask(False)
  if c in ['CP_NO']:continue
  for i in range(nopts):
    nc.variables[c][:ntm,i]=z
if ver==7:nc.variables['pigflag'][:nopts]=0
nc.close()
93/16:
nc = netCDF4.Dataset(NCfname, 'r+')
for v in PRM:
  var=v2c[v]
  nc.variables[var].set_auto_mask(False)
  nc.variables[var][:nopts]=np.array(pv[v2n[v]])
nc.variables[v2c['V']][:nopts]=nc.variables[v2c['V']][:]*3600.
nc.variables[v2c['T']][:nopts]=nc.variables[v2c['T']][:]+273.
93/17:
if len(pig)>0:
  if ver==7:
    nc.variables['pigflag'][pig]=1
  else:
    nc.variables[v2c['D']][pig]=nc.variables[v2c['D']][pig]*-1.
93/18: V[1]
93/19: lspec
93/20: c='CO'
93/21:
  ic=lspec.index(c)
  nc.variables[c][:,:nopts]=np.array(df[c]).reshape(ntm,nopts)
94/1: run  wrtE.py 1901
95/1: run  wrtE.py 1901
96/1: run  wrtE.py 1901
96/2: c='CO'
96/3: CO=np.array(df[c]).reshape(ntm,nopts)
96/4: CO.shape
96/5: nc.variables['CO'][:]=CO[:]
96/6: nc.variables['NO'][:]=CO[:]
96/7: [i for i in np.array(V).flatten() if 'CO' in i]
96/8: [i for i in np.array(V[:]).flatten() if 'CO' in i]
96/9: [i for i in np.array(V[j]).flatten() if 'CO' in i for j in range(4)]
96/10: [i for i in np.array(V[j]).flatten() for j in range(4) if 'CO' in i]
96/11: [[i for i in np.array(V[j]).flatten() if 'CO' in i] for j in range(4)]
96/12: c='CO'
96/13: nc.variables['CO']
96/14: nc.variables['NO']
96/15: c
96/16: nc[c][:5,:5]
96/17: nc[c][:5,-5:]
96/18: np.max(nc[c][:])
97/1: fname='fortBE.413_teds10.ptsE01.nc'
97/2: import netCDF4
97/3: nc = netCDF4.Dataset(NCfname, 'r')
97/4: nc = netCDF4.Dataset(fname, 'r')
97/5: c='CO'
97/6: c='NO'
97/7: import numpy as np
97/8: np.max(nc[c][:])
97/9: a=np.array(nc['CO'])
98/1: import netCDF4
98/2: import numpy as np
98/3: ls temp*
98/4: fname=template_v7.nc
98/5: fname='template_v7.nc'
98/6: nc = netCDF4.Dataset(fname, 'r+')
98/7: c='NO'
98/8: np.max(nc[c][:])
98/9: c='CO'
98/10: np.max(nc[c][:])
98/11: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
98/12: V
98/13: np.max(nc[c][:])
98/14:
for c in V[1]:
    print(np.max(nc[c][:]))
98/15: c
98/16:
for c in V[1]:
    if c=='CP_NO':continue
    print(np.max(c,nc[c][:]))
98/17: c
98/18:
for c in V[1]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
98/19: nc['CO']==nc['NO']
98/20: nc['CO']=nc['NO']
98/21: nc.variables['CO']=nc.variables['NO']
98/22: nc.variables['CO']
98/23:
  for v in ['long_name','var_desc']:
    exec('nc.variables["CO"].'+v+'="CO              "')
98/24: nc.variables['CO']
98/25: nc.close()
98/26: nc = netCDF4.Dataset(fname, 'r+')
98/27:
for c in V[1]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
98/28: c
98/29: c='CO'
98/30: nc[c]
98/31: nc.variables[c]
98/32: nc.variables[c][:]
98/33: c='NO'
98/34: nc.variables[c][:]
98/35: nc.variables['CO']=nc.variables['NO']
98/36: c='CO'
98/37: nc.variables[c][:]
98/38:
for v in ['long_name','var_desc']:
  exec('nc.variables["CO"].'+v+'="CO              "')
98/39: nc.close()
98/40: nc = netCDF4.Dataset(fname, 'r')
98/41: nc.variables[c][:]
98/42: nc.variables['CO']=nc.variables['NO']
98/43: c
98/44:
for v in ['long_name','var_desc']:
  exec('nc.variables["CO"].'+v+'="CO              "')
98/45: nc.close()
98/46: nc = netCDF4.Dataset(fname, 'r+')
98/47: nc.variables['CO']=nc.variables['NO']
98/48:
for v in ['long_name','var_desc']:
  exec('nc.variables["CO"].'+v+'="CO              "')
98/49: nc.variables[c][:]
98/50: nc.variables[c]
98/51:
for c in V[1]:
    nc.variables[c].set_auto_mask(False)
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
98/52: nc.close()
98/53: nc = netCDF4.Dataset(fname, 'r')
98/54:
for c in V[1]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
98/55: c
98/56: nc.close()
98/57: nc = netCDF4.Dataset(fname, 'r+')
98/58: c='CO'
98/59: nc.variables[c][:]
98/60: nc.variables[c].mask
98/61: nc.variables[c].mask=False
98/62: nc.setncattr
98/63: nc.setncattr()
98/64: dir(nc.setncattr)
98/65: nc.variables[c].setncattr('mask', False)
98/66: nc.variables[c].setncattr('mask', 'False')
98/67: nc.variables[c][:]
98/68: nc.dimension
98/69: nc.dimensions
98/70: nc.mask
98/71: nc.ndim
98/72: nc.ndims
98/73: nc.variables.ndim
98/74: nc.variables[c].ndim
98/75: nc.variables[c].mask
98/76: nc.variables[c].always_mask
98/77:
for c in V[1]:
    nc.variables[c].set__mask(False)
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
98/78:
for c in V[1]:
    nc.variables[c].set_mask(False)
    if c=='CP_NO':contine
    print(c,np.max(nc[c][:]))
98/79: nc.variables['CO']=nc.variables['NO']
98/80:
for c in V[1]:
    nc.variables[c].set_auto_mask(False)
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
98/81: nc.set_auto_mask(False)
98/82:
for c in V[1]:
    nc.variables[c].set_auto_mask(False)
    nc.variables[c].set_always_mask(False)
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
98/83: nc.set_always_mask(False)
98/84: c
98/85: c='CO'
98/86: nc.variables[c][:]
98/87:
for v in ['long_name','var_desc']:
  exec('nc.variables["CO"].'+v+'="CO              "')
98/88: nc.close()
98/89: nc = netCDF4.Dataset(fname, 'r')
98/90:
for c in V[1]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
98/91: c
98/92: c='CO'
98/93: nc = netCDF4.Dataset(fname, 'r+')
98/94: nc.close()
98/95: nc = netCDF4.Dataset(fname, 'r+')
98/96: nc.variables[c].setncattr('mask', 'false')
98/97: nc.variables[c][:]
98/98: nc.variables[c]
98/99: c='NO'
98/100: nc.variables[c][:]
98/101: nc.variables['CO']=nc.variables['NO']
98/102: c='CO'
98/103: nc.variables[c][:]
98/104: nc.variables[c].set_auto_mask
98/105: nc.variables[c].set_auto_mask()
98/106: nc.variables[c].mask
98/107: c='NO'
98/108: nc.variables[c].mask
98/109: c='NO2'
98/110: nc.variables[c].mask
98/111: nc.variables[c]
98/112: nc.variables[c][:]
98/113: c='CO'
98/114: nc.variables[c][:]
98/115: c='TFLAG'
98/116: nc.variables[c][:]
98/117: nc.variables[c].shape
98/118: nc.NVARS
98/119: len(V[1])
98/120: V[1]
98/121: V[2]
98/122: V[3]
98/123: c
98/124:
for c in V[1]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:].data))
98/125:
for c in V[1]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
98/126: nc.close()
98/127: nc = netCDF4.Dataset(fname, 'r+')
98/128:
for c in V[1]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
98/129:
for c in V[1]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:].data))
98/130: !lst
98/131: fname='fortBE.413_teds10.ptsE01.nc'
98/132: nc.close()
98/133: nc = netCDF4.Dataset(fname, 'r+')
98/134:
for c in V[1]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:].data))
99/1: import netCDF4
99/2: import numpy as np
99/3: fname='fortBE.413_teds10.ptsE01.nc'
99/4: nc = netCDF4.Dataset(fname, 'r+')
99/5: c='CO'
99/6: a=nc.variables[c][:].data
100/1: import netCDF4
100/2: import numpy as np
100/3: fname='fortBE.413_teds10.ptsE01.nc'
100/4: nc = netCDF4.Dataset(fname, 'r+')
100/5: c='NO2'
100/6: a=nc.variables[c][:].data
100/7: c='CO'
100/8: c='NO2'
100/9: a=nc.variables[c][:].mask
100/10: a[:5,:5]
100/11: a
100/12: c='CO'
100/13: nc.variables[c][:].mask
101/1: import numpy as np
101/2: import netCDF4
101/3: fname='template_v7.nc'
101/4: nc = netCDF4.Dataset(fname, 'r+')
101/5: c='CO'
101/6: nc.variables[c][:].mask
101/7: nc.variables[c][:].mask=False
101/8: nc.variables[c][:].mask
101/9: nc.variables[c].set_auto_mask(False)
101/10: nc.variables[c].set_always_mask(False)
101/11: nc.variables[c][:].mask
101/12: nc.variables[c][:]
101/13: nc.variables[c][:]=0.5
101/14: nc.close()
101/15: nc = netCDF4.Dataset(fname, 'r+')
101/16:
for c in V[1]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:].data))
101/17: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
101/18:
for c in V[1]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:].data))
101/19: c
101/20: c='CO'
101/21: nc.variables[c][:]
101/22: nc.close()
101/23: nc = netCDF4.Dataset(fname, 'r+')
101/24:
for c in V[1]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
101/25: nc.close()
101/26: nc = netCDF4.Dataset(fname, 'r+')
101/27: c
101/28: c='CO'
101/29: nc.variables[c][:].mask
101/30: c='NO2'
101/31: nc.variables[c][:].mask
101/32: run  wrtE.py 1901
102/1: import numpy as np
102/2: import netCDF4
102/3: fname='template_d4.nc'
102/4: nc = netCDF4.Dataset(fname, 'r')
102/5:
for c in V[3]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
102/6: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
102/7:
for c in V[3]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
103/1: import netCDF4
103/2: fname='template_d4.nc'
103/3: nc = netCDF4.Dataset(fname, 'r')
103/4: c='NO2'
103/5: nc.variables[c]
103/6: nc.variables[c].set_auto_mask
103/7: nc.variables[c].set_auto_mask()
103/8: !psg python
103/9: !lst
103/10: cd ../ship
103/11: !lst
103/12: !lst
103/13: !ps
103/14: !kill -9 24318
103/15: !ps
103/16: !kill -9 24318
103/17: !ps
103/18: fname='template_d4.nc'
103/19: nc = netCDF4.Dataset(fname, 'r')
103/20: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/21:
for c in V[3]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
103/22: import numpy as np
103/23:
for c in V[3]:
    if c=='CP_NO':continue
    print(c,np.max(nc[c][:]))
103/24: len(V[3])
103/25: !top -p 35997
103/26: !top -p 36351
103/27: !top -p 36618
103/28: cd /nas1/cmaqruns/2019base/data/output_CCTM_v53_gcc_1901_run5/so4
103/29: import netCDF4
103/30: fname='/nas1/cmaqruns/2019base/data/wsites/1901/const.nc'
103/31: nc = netCDF4.Dataset(fname, 'r')
103/32: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/33: V
103/34: V='COL'
103/35: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/36: c='COL'
103/37: nc.variables[c][:]
103/38: nc = netCDF4.Dataset(fname, 'r')
103/39: nc = netCDF4.Dataset(fname, 'r+')
103/40: c='ROW'
103/41: nc.variables[c][:]
103/42: V
103/43: col=[(x-nc.XORIG)/nc.XCELL for x in nc.variables[XLOCA][0,0,0,:]]
103/44: c='XLOCA'
103/45: nc.variables[c].shape
103/46: col=[(x-nc.XORIG)/nc.XCELL for x in nc.variables['XLOCA'][0,0,:,0]]
103/47: col=[(x-nc.XORIG)/nc.XCELL for x in nc['XLOCA'][0,0,:,0]]
103/48: col
103/49: col=[int((x-nc.XORIG)/nc.XCELL) for x in nc['XLOCA'][0,0,:,0]]
103/50: col
103/51: row=[int((y-nc.YORIG)/nc.YCELL) for y in nc['YLOCA'][0,0,:,0]]
103/52: row
103/53: c='ROW'
103/54: nc.variables[c].shape
103/55: nc.variables[c][0,0,:,0]=row[:]
103/56: c='COL'
103/57: nc.variables[c][0,0,:,0]=col[:]
103/58: nc.close()
103/59: cd /nas1/cmaqruns/2019base/data/output_CCTM_v53_gcc_1901_run5/so4
103/60: V
103/61: fname
103/62: nc = netCDF4.Dataset(fname, 'r+')
103/63: c
103/64: nc.variables[c]
103/65: nc.variables[c][:]
103/66: !grep exist ~/bin/*py|M
103/67:
for v in ['COL', 'ROW']:
  exec(v+'=np.array(const.variables[v][:]).flatten()')
103/68: import datetime
103/69: datetime.datetime.strptime('18001', '%y%j')
103/70: datetime.datetime.strptime('18032', '%y%j')
103/71: datetime.datetime.strptime('18032', '%y%j').strftime('%y%m')
103/72: yrjulhh='1813018'
103/73:
j='20'+ yrjulhh[:-2]
yrmn=datetime.datetime.strptime(j, '%y%j').strftime('%y%m')
103/74: j
103/75: yrmn=datetime.datetime.strptime(yrjulhh[:-2], '%y%j').strftime('%y%m')
103/76: yrmn
103/77: pwd
103/78: cd /nas1/cmaqruns/2018base/data/bcon/m3
103/79: yrmn='1804'
103/80: fname='moz_41_20'+yrmn+'.nc'
103/81: fname
103/82:
ncM = netCDF4.Dataset(fname,'r')
v4M=list(filter(lambda x:ncM.variables[x].ndim==4, [i for i in ncM.variables]))
ntM,nlayM,nrowM,ncolM=(ncM.variables[v4M[0]].shape[i] for i in range(4))
lonM=[ncM.XORIG+ncM.XCELL*i for i in range(ncolM)]
latM=[ncM.YORIG+ncM.YCELL*i for i in range(nrowM)]
tflagM=[str(i*100+j/10000)[2:] for i,j in zip(nc.variables['TFLAG'][:,0,0],nc.variables['TFLAG'][:,0,1])]
tM=tflagM.index(yrjulhh)
103/83: yrjulhh='1812000'
103/84: yrjulhh in tflagM
103/85: tflagM[:5]
103/86: tflagM=[str(i*100+j//10000)[2:] for i,j in zip(nc.variables['TFLAG'][:,0,0],nc.variables['TFLAG'][:,0,1])]
103/87: tflagM=[str(i*100+j//10000)[2:] for i,j in zip(ncM.variables['TFLAG'][:,0,0],ncM.variables['TFLAG'][:,0,1])]
103/88: yrjulhh in tflagM
103/89:
tflagM=[str(i*100+j//10000)[2:] for i,j in zip(ncM.variables['TFLAG'][:,0,0],ncM.variables['TFLAG'][:,0,1])]
tM=tflagM.index(yrjulhh)
103/90: tM
103/91: !top
103/92: cd /nas1/cmaqruns/2018base/data/output_CCTM_v53_gcc_1804_run5
103/93: fname='CCTM_MEDIA_CONC_v53_gcc_1804_run5_20180404_EAsia_81K_11.nc'
103/94: nc = netCDF4.Dataset(fname, 'r+')
103/95: v='TFLAG'
103/96: nc.variables[v][:]
103/97: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/98: ntm,nrow,ncol,V=(nc.dimensions[c].size for c in ['TSTEP','ROW', 'COL']),[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/99: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
103/100: nt
103/101: len(V[3])
103/102: nc.SDATE=2018095
103/103: nc.variables[v][:,:,0]=nc.SDATE
103/104: nc.close()
103/105: nc = netCDF4.Dataset(fname, 'r+')
103/106: nc.SDATE=2018094
103/107: nc.variables[v][:,:,0]=nc.SDATE
103/108: nc.variables[v][-1,:,0]
103/109: nc.variables[v][-1,:,:]
103/110: nc.variables[v][-2:,:,:]
103/111: nc.variables[v][-1,:,0]=2018095
103/112: nc.variables[v][-2:,:,:]
103/113: nc.close()
103/114: fname='/home/cmaqruns/2018base/data/land/beld4.EAsia_81K.ncf'
103/115: nc = netCDF4.Dataset(fname, 'r')
103/116: nc.SDATE
103/117: nc.variables[v][-2:,:,:]
103/118: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/119: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
103/120: nt,nlay,nrow,ncol
103/121: fname=fname.replace('18','19')
103/122: fname
103/123: nc = netCDF4.Dataset(fname, 'r')
103/124: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
103/125: nt,nlay,nrow,ncol
103/126: !ncdump -h /home/cmaqruns/2018base/data/land/1804/2018_EAsia_81K_time20180404_bench.nc|H
103/127: !ncdump -h /home/cmaqruns/2018base/data/land/1804/beld4.EAsia_81K.ncf|H
103/128: !ncdump -h /home/cmaqruns/2018base/data/land/beld4.EAsia_81K.ncf|H
103/129: !ncdump -h /home/cmaqruns/2019base/data/land/beld4.EAsia.ncf|H
103/130: 'Hay' in V3[:]
103/131: 'Hay' in V[3][:]
103/132: v='Hay'
103/133: nc.variables[v]
103/134: np.min(nc.variables[v][:])
103/135: np.max(nc.variables[v][:])
103/136: !ncd
103/137: !ncdump -h /home/cmaqruns/2019base/data/land/beld4.EAsia.ncf|H
103/138: nc.close()
103/139: fname='/home/cmaqruns/2018base/data/land/beld4.EAsia_81K.ncf'
103/140: nc = netCDF4.Dataset(fname, 'r+')
103/141: nc.close()
103/142: nc = netCDF4.Dataset(fname, 'r+')
103/143: dir(nc)
103/144: nc.NCOLS
103/145: nc.NROWS
103/146: nc.NVARS
103/147: nc.close()
103/148: ls -lh /home/cmaqruns/201?base/lonlat.csv
103/149: pwd
103/150: ls
103/151: !ncdump -h CCTM_CGRID_v53_gcc_1804_run5_20180404_EAsia_81K_11.nc
103/152: !ncdump -h CCTM_CGRID_v53_gcc_1804_run5_20180404_EAsia_81K_11.nc|H
103/153: pwd
103/154: fname
103/155: fname='CCTM_CGRID_v53_gcc_1804_run5_20180404_EAsia_81K_11.nc'
103/156: nc = netCDF4.Dataset(fname, 'r+')
103/157: nc.SDATE=2018095
103/158: nc.STIME=0
103/159: v
103/160: v='TFLAG'
103/161: nc.variables[v][0,0,:]
103/162: nc.variables[v][:,:,0]=nc.SDATE
103/163: nc.close()
103/164: pwd
103/165: cd /nas1/cmaqruns/2018base/data/bcon/m3
103/166: fname='ICON_201809500.d1'
103/167: nc = netCDF4.Dataset(fname, 'r+')
103/168: nc.SDATE
103/169: nc.SDATE=2018095
103/170: v
103/171: nc.variables[v][:,:,0]=nc.SDATE
103/172: nc.close()
103/173: pwd
103/174: cd /home/cmaqruns/2018base/data/output_CCTM_v53_gcc_1804_run5
103/175: fname='/home/cmaqruns/2018base/data/bcon/ICON_d1_201804_run6.nc'
103/176: nc = netCDF4.Dataset(fname, 'r')
103/177: v
103/178: ncM = netCDF4.Dataset(fname, 'r')
103/179:
tflagM=[str(i*100+j//10000)[2:] for i,j in zip(ncM.variables['TFLAG'][:,0,0],ncM.variables['TFLAG'][:,0,1])]
tM=tflagM.index('1809300)
103/180:
tflagM=[str(i*100+j//10000)[2:] for i,j in zip(ncM.variables['TFLAG'][:,0,0],ncM.variables['TFLAG'][:,0,1])]
tM=tflagM.index('1809300')
103/181: tflagM[:5]
103/182:
tflagM=[str(i*100+j//10000)[2:] for i,j in zip(ncM.variables['TFLAG'][:,0,0],ncM.variables['TFLAG'][:,0,1])]
tM=tflagM.index('1809400')
103/183: tM
103/184: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/185: ntM,nlayM,nrowM,ncolM==(nc.variables[V[3][0]].shape[i] for i in range(4))
103/186: len(V[3])
103/187: V[3][0]
103/188: nc.variables[V[3][0]].shape
103/189: ntM,nlayM,nrowM,ncolM=(nc.variables[V[3][0]].shape[i] for i in range(4))
103/190: A5=np.zeros(shape=(len(v4M),nlayM,nrowM,ncolM))
103/191: fname='CCTM_MEDIA_CONC_v53_gcc_1804_run5_20180403_EAsia_81K_11.nc'
103/192: nc = netCDF4.Dataset(fname, 'r+')
103/193: nc.SDATE
103/194: nc.SDATE=2018093
103/195: v='TFLAG'
103/196: nc.variables[v][:,:,0]=nc.SDATE
103/197: nc.variables[v][-1,:,0]=nc.SDATE+1
103/198: nc.close()
103/199: fname='CCTM_CGRID_v53_gcc_1804_run5_20180403_EAsia_81K_11.nc'
103/200: nc.SDATE
103/201: nc = netCDF4.Dataset(fname, 'r+')
103/202: nc.SDATE
103/203: nc.SDATE=2018094
103/204: nc.STIME
103/205: nc.variables[v][:,:,0]=nc.SDATE
103/206: fname='/home/cmaqruns/2018base/data/bcon/m3/ICON_201809400.d1'
103/207: ncM = netCDF4.Dataset(fname, 'r')
103/208: fname='/nas1/cmaqruns/2018base/data/bcon/m3/ICON_201809400.d1'
103/209: ncM = netCDF4.Dataset(fname, 'r')
103/210: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/211:
for x in V[3]:
    nc.variables[x][:,:,:,:]=nc.variables[x][:,:,2:55,2:55]
103/212: nc.variables[x][:,:,2:55,2:55].shape
103/213: nc.variables[x][:,:,:,:].shape
103/214: ncM.variables[x][:,:,2:55,2:55].shape
103/215:
for x in V[3]:
    nc.variables[x][:,:,:,:]=ncM.variables[x][:,:,2:55,2:55]
103/216:
for x in V[3]:
    if x not in ncM.variables:
        print(x)
        continue
    nc.variables[x][:,:,:,:]=ncM.variables[x][:,:,2:55,2:55]
103/217: nc.close()
103/218:
SPECs =['carbon_monoxide', 'ethane', 'formaldehyde', 'isoprene', 'nitrogen_dioxide', 'nitrogen_monoxide', 'propane', 'sulphur_dioxide' ]
SPECs+=['ozone', 'ammonium', 'nitrate', 'olefins', 'organic_nitrates', 'paraffins']
PARTs =[
            'dust_aerosol_0.03-0.55um_mixing_ratio', 'dust_aerosol_0.55-0.9um_mixing_ratio', 'dust_aerosol_0.9-20um_mixing_ratio',
            'hydrophilic_black_carbon_aerosol_mixing_ratio', 'hydrophilic_organic_matter_aerosol_mixing_ratio', 'hydrophobic_black_carbon_aerosol_mixing_ratio',
            'hydrophobic_organic_matter_aerosol_mixing_ratio', 'nitric_acid', 'peroxyacetyl_nitrate',
            'sea_salt_aerosol_0.03-0.5um_mixing_ratio', 'sea_salt_aerosol_0.5-5um_mixing_ratio', 'sea_salt_aerosol_5-20um_mixing_ratio',
            'sulphate_aerosol_mixing_ratio',
        ]
103/219: len(SPECs)+len(PARTs)
103/220: cd /nas1/cmaqruns/2018base/data/output_CCTM_v53_gcc_1804
103/221: cd ../*6
103/222: cd /nas1/cmaqruns/2018base/data/output_CCTM_v53_gcc_1804_run6
103/223: fname='CCTM_SA_ACONC_v53_gcc_1804_run6_20180408_EAsia_81K_11_YZD.nc'
103/224: nc = netCDF4.Dataset(fname, 'r')
103/225: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/226: len(V[3])
103/227: V[3][:5]
103/228: v4=set([i.split('_')[0] for i in V[3]])
103/229: len(v4)
103/230: v4=list(set([i.split('_')[0] for i in V[3]]))
103/231: v4.sort()
103/232: Sulfate='ASO4I ANO3I ANH4I ACLI ASO4J  ANO3J  ANH4JASO4K ANO3K ANH4K'.split()
103/233: BC='AECI AECJ'.split()
103/234: OC='APOCI APNCOMI APOCJ AOTHRJ AXYL1J AXYL2J AXYL3J ATOL1J  ATOL2J  ATOL3J  ABNZ1J  ABNZ2J  ABNZ3J  AISO1J AISO2J  AISO3J  ATRP1J  ATRP2J  ASQTJ  AALK1J  AALK2J AORGCJ  AOLGBJ  AOLGAJ  APAH1J  APAH2J  APAH3J  APN-COMJ'.replace('_','').split
103/235: OC
103/236: OC='APOCI APNCOMI APOCJ AOTHRJ AXYL1J AXYL2J AXYL3J ATOL1J  ATOL2J  ATOL3J  ABNZ1J  ABNZ2J  ABNZ3J  AISO1J AISO2J  AISO3J  ATRP1J  ATRP2J  ASQTJ  AALK1J  AALK2J AORGCJ  AOLGBJ  AOLGAJ  APAH1J  APAH2J  APAH3J  APN-COMJ'.replace('_','').split()
103/237: OC
103/238: Dust='AFEJ AALJ ASIJ ACAJ AMGJ AKJ AMNJ ACORS ASOIL'.split()
103/239: SS='ANAJ ACLJ ACLK ASEACAT'
103/240: len(Sulfate+BC+OC+Dust+SS)
103/241: SS='ANAJ ACLJ ACLK ASEACAT'.split()
103/242: len(Sulfate+BC+OC+Dust+SS)
103/243: aer=Sulfate+BC+OC+Dust+SS
103/244: set(aer)-set(v4)
103/245: Sulfate='ASO4I ANO3I ANH4I ACLI ASO4J ANO3J ANH4J ASO4K ANO3K ANH4K'.split()
103/246: aer=Sulfate+BC+OC+Dust+SS
103/247: set(aer)-set(v4)
103/248: set(v4)-set(aer)
103/249: [i for i in set(v4)-set(aer) if i[0]=='A']
103/250: a=[i for i in set(v4)-set(aer) if i[0]=='A']
103/251: a.sort()
103/252: a
103/253: Semi='ASVPO1J ASVPO2J  ASVPO3J  ASVPO2I  ASVPO1I ALVPO1J   ALVPO1I  AIVPO1J'.split()
103/254: len(Semi)
103/255: SS='ANAI ANAJ ACLJ ACLK ASEACAT'.split()
103/256: OC='APOCI APNCOMI APOCJ AOTHRJ AXYL1J AXYL2J AXYL3J ATOL1J  ATOL2J  ATOL3J  ABNZ1J  ABNZ2J  ABNZ3J  AISO1J AISO2J  AISO3J  ATRP1J  ATRP2J  ASQTJ  AALK1J  AALK2J AORGCJ  AOLGBJ  AOLGAJ  APAH1J  APAH2J  APAH3J  APNCOMJ AOTHRI'.replace('_','').split()
103/257: aer=Sulfate+BC+OC+Dust+SS+Semi
103/258: len(aer)
103/259: set(v4)-set(aer)
103/260: Semi
103/261: Dust='AFEJ AALJ ASIJ ACAJ AMGJ AKJ AMNJ ACORS ASOIL ATIJ'.split()
103/262: aer=Sulfate+BC+OC+Dust+SS+Semi
103/263: len(set(v4)-set(aer))
103/264: set(v4)-set(aer)
103/265: grp=list(set([i.split('_')[1] for i in V[3]]))
103/266: grp.sort()
103/267: grp
103/268: fname
103/269:
intv=fname.split('_')[-1]
path=intv.strip('.nc')
103/270: path
103/271: aer=Sulfate+BC+OC+Dust+SS+Semi
103/272: 'ACORS' in aer
103/273:
OC='APOCI APNCOMI APOCJ AOTHRJ AXYL1J AXYL2J AXYL3J ATOL1J  ATOL2J  ATOL3J  ABNZ1J  ABNZ2J  ABNZ3J\
  AISO1J AISO2J  AISO3J  ATRP1J  ATRP2J  ASQTJ  AALK1J  AALK2J AORGCJ  AOLGBJ  AOLGAJ  APAH1J  APAH2J\
  APAH3J  APNCOMJ AOTHRI'.replace('_','').split()
103/274: aOC='APOCI APNCOMI APOCJ AOTHRJ AXYL1J AXYL2J AXYL3J ATOL1J  ATOL2J  ATOL3J  ABNZ1J  ABNZ2J  ABNZ3J  AISO1J AISO2J  AISO3J  ATRP1J  ATRP2J  ASQTJ  AALK1J  AALK2J AORGCJ  AOLGBJ  AOLGAJ  APAH1J  APAH2J  APAH3J  APNCOMJ AOTHRI'.replace('_','').split()
103/275: aOC==OC
103/276: set(aer)-set(v4)
103/277: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
103/278: g=grp[0]
103/279: g
103/280: fname = 'PM10'+path+'_'+g+'.nc'
103/281: fname
103/282: os.system('cp template_PM10.nc '+fname)
103/283: import os
103/284: os.system('cp template_PM10.nc '+fname)
103/285: nco= netCDF4.Dataset(fname, 'r+')
103/286:
  for t in range(nt):
    nco['TFLAG'][t,0,:]=nc['TFLAG'][t,0,:]
103/287: v
103/288: nco.variables[v][:,:,:]
103/289: set(aer) & set(v4)
103/290: len(set(aer) & set(v4))
103/291: len(aer)
103/292: V[3][0]
103/293: fname='CCTM_SA_ACONC_v53_gcc_1804_run6_20180408_EAsia_81K_11_YZD.nc'
103/294: ymdh=fname.split('_')[7]
103/295: ymdh
103/296: cd /nas1/cmaqruns/2018base/data/output_CCTM_v53_gcc_1804_run6/SA_ACONC
103/297: fname='PM10FWS_20180404_GR24.nc'
103/298: nc = netCDF4.Dataset(fname, 'r')
103/299: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/300: V
103/301: v='PM10'
103/302: nc.variables[v][:5,0,:5,:5]
103/303: cd ../
103/304: fname='CCTM_SA_ACONC_v53_gcc_1804_run6_20180408_EAsia_81K_11_YZD.nc'
103/305: a=set(aer) & set(v4)
103/306: len(a)
103/307: a
103/308: fname='CCTM_SA_ACONC_v53_gcc_1804_run6_20180408_EAsia_81K_11_YZD.nc'
103/309: nc = netCDF4.Dataset(fname, 'r')
103/310:
for v in a:
    print(v,np.max(nc[v][:]))
103/311:
g='GR13'
for v in a:
    vg=v+'_'+g
    print(v,np.max(nc[vg][:]))
103/312: v='ACLI'
103/313: vg=v+'_'+
103/314: vg=v+'_'+g
103/315: np.isnan(np.max(nc[vg][:]))
103/316: IX,IY=28-1,29-1
103/317: !lst
103/318: cd SA_ACONC
103/319: !lst
103/320: zs=['NWC','JJZ','YZD','FWS']
103/321: from pandas import *
103/322:
for z in zs:
    exec(zs+'=[]')
103/323:
for z in zs:
    exec(z+'=[]')
103/324:
for z in zs:
    fname='PM10'+z+'.nc'
    nc = netCDF4.Dataset(fname, 'r')
    V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
    nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
    exec(z+'=nc['+z+'][:,0,IY,IX]')
103/325:
for z in zs:
    fname='PM10'+z+'.nc'
    nc = netCDF4.Dataset(fname, 'r')
    V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
    nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
    exec(z+'=nc["PM10"][:,0,IY,IX]')
103/326:
DD={}
for z in zs:
    exec('DD.update({"'+z+'":'+z+'})')
103/327: list(DD)
103/328: df=DataFrame(DD)
103/329: df.head()
103/330: df.MWC.max
103/331: df.NWC.max
103/332: max(df.NWC)
103/333: max(df.JJZ)
103/334: max(df.YZD)
103/335: max(df.FWS)
103/336: IY,IX
103/337: nt,nlay,nrow,ncol
103/338: df.to_csv('ntw.csv')
103/339: pwd
103/340: history
103/341: !top
103/342: cd /nas1/cmaqruns/2018base/data/output_CCTM_v53_gcc_1803_run12
103/343: fname='CCTM_CGRID_v53_gcc_1803_run12_20180330_EAsia_81K_11.nc'
103/344: nc = netCDF4.Dataset(fname, 'r+')
103/345: nc.SDATE
103/346: nc.SDATE=2018090
103/347: v='TFLAG'
103/348: nc.variables[v][:,:,0]=nc.SDATE
103/349: nc.close()
103/350: fname='CCTM_MEDIA_CONC_v53_gcc_1803_run12_20180330_EAsia_81K_11.nc'
103/351: nc = netCDF4.Dataset(fname, 'r+')
103/352: nc.SDATE=2018089
103/353: nc.variables[v][:,:,0]=nc.SDATE
103/354: nc.variables[v][-1,:,0]=nc.SDATE+1
103/355: nc.close()
103/356: nc = netCDF4.Dataset(fname, 'r+')
103/357: tflag=[datetime.datetime.strptime(i,'%Y%j').strftime('%Y%m%d')+str(j//10000) for i,j in zip(nc['TFLAG'][:,0,0],nc['TFLAG'][:,0,1])]
103/358: tflag=[datetime.datetime.strptime(str(i),'%Y%j').strftime('%Y%m%d')+str(j//10000) for i,j in zip(nc['TFLAG'][:,0,0],nc['TFLAG'][:,0,1])]
103/359: tflag[:5]
103/360: i
103/361: i=nc['TFLAG'][0,0,0]
103/362: datetime.datetime.strptime(str(i),'%Y%j')
103/363: datetime.datetime.strptime(str(i),'%Y%j').strftime('%Y%m%d')
103/364: tflag=[datetime.datetime.strptime(str(i),'%Y%j').strftime('%Y%m%d')+{:02d}.format(str(j//10000)) for i,j in zip(nc['TFLAG'][:,0,0],nc['TFLAG'][:,0,1])]
103/365: tflag=[datetime.datetime.strptime(str(i),'%Y%j').strftime('%Y%m%d')+'{:02d}'.format(str(j//10000)) for i,j in zip(nc['TFLAG'][:,0,0],nc['TFLAG'][:,0,1])]
103/366: tflag=[datetime.datetime.strptime(str(i),'%Y%j').strftime('%Y%m%d')+'{:02d}'.format(j//10000) for i,j in zip(nc['TFLAG'][:,0,0],nc['TFLAG'][:,0,1])]
103/367: tflag[:5]
103/368: pwd
103/369: byr=subprocess.check_output('pwd',shell=True).decode('utf8').strip('\n')[-3:-1]
103/370: import subprocess
103/371: byr=subprocess.check_output('pwd',shell=True).decode('utf8').strip('\n')[-3:-1]
103/372: byr
103/373: byr=subprocess.check_output('pwd',shell=True).decode('utf8').strip('\n')[-2:]
103/374: byr
103/375: cd /nas1/ecmwf/reanalysis/gribs18
103/376: lst
103/377: !lsS
103/378: fname='1804.nc'
103/379: nc = netCDF4.Dataset(fname, 'r')
103/380: v='VAR_192_210_6_P0_L105_GLL0'
103/381: np.max(nc[v])
103/382: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/383: V
103/384:
dic={
"VAR_192_217_21_P0_L105_GLL0":"ammonium",
"VAR_192_210_123_P0_L105_GLL0":"carbon_monoxide",
"VAR_192_210_4_P0_L105_GLL0":"dust_aerosol_0.03-0.55um_mixing_ratio",
"VAR_192_210_5_P0_L105_GLL0":"dust_aerosol_0.55-0.9um_mixing_ratio",
"VAR_192_210_6_P0_L105_GLL0":"dust_aerosol_0.9-20um_mixing_ratio",
"VAR_192_217_45_P0_L105_GLL0":"ethane",
"VAR_192_210_124_P0_L105_GLL0":"formaldehyde",
"VAR_192_210_9_P0_L105_GLL0":"hydrophilic_black_carbon_aerosol_mixing_ratio",
"VAR_192_210_7_P0_L105_GLL0":"hydrophilic_organic_matter_aerosol_mixing_ratio",
"VAR_192_210_10_P0_L105_GLL0":"hydrophobic_black_carbon_aerosol_mixing_ratio",
"VAR_192_210_8_P0_L105_GLL0":"hydrophobic_organic_matter_aerosol_mixing_ratio",
"VAR_192_217_16_P0_L105_GLL0":"isoprene",
"VAR_192_217_51_P0_L105_GLL0":"nitrate",
"VAR_192_217_6_P0_L105_GLL0":"nitric_acid",
"VAR_192_210_121_P0_L105_GLL0":"nitrogen_dioxide",
"VAR_192_217_27_P0_L105_GLL0":"nitrogen_monoxide",
"VAR_192_217_11_P0_L105_GLL0":"olefins",
"VAR_192_217_15_P0_L105_GLL0":"organic_nitrates",
"VAR_192_210_203_P0_L105_GLL0":"ozone",
"VAR_192_217_9_P0_L105_GLL0":"paraffins",
"VAR_192_217_13_P0_L105_GLL0":"peroxyacetyl_nitrate",
"VAR_192_217_47_P0_L105_GLL0":"propane",
"VAR_192_210_1_P0_L105_GLL0":"sea_salt_aerosol_0.03-0.5um_mixing_ratio",
"VAR_192_210_2_P0_L105_GLL0":"sea_salt_aerosol_0.5-5um_mixing_ratio",
"VAR_192_210_3_P0_L105_GLL0":"sea_salt_aerosol_5-20um_mixing_ratio",
"VAR_192_210_11_P0_L105_GLL0":"sulphate_aerosol_mixing_ratio",
"VAR_192_210_122_P0_L105_GLL0":"sulphur_dioxide",
}
103/385:
for v in V[3]:
  print(dic[v],np.max(nc[v][:]))
103/386: len(V[3])
103/387: pwd
103/388: lsS
103/389: !lsS
103/390: fname
103/391: V
103/392: v='lat_0'
103/393: len(nc[v][:])
103/394: xlon, xlat = nc.variables['lon_0'][:].flatten(), np.flip(nc.variables['lat_0'][:].flatten())
103/395: xlon
103/396: xlat
103/397: dir(nc)
103/398: nc.close()
103/399: !lst
103/400: fname='templateD1.ncV49K34'
103/401: nc = netCDF4.Dataset(fname, 'r')
103/402: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/403: nt,nlay,nrow,ncol
103/404: 52*81/2.
103/405: nc.XORIG
103/406: nc.dimensions
103/407: nc.dimensions[0]
103/408: nc.dimensions()
103/409: nc.dimensions['TSTEP']
103/410: nc.dimensions['TSTEP'].size
103/411: 53/2.*81000
103/412: nc.XCELL
103/413: nc.YCELL
103/414: nc.NROWS
103/415: nc.NCOLS
103/416: nrow
103/417: ncol
103/418: pwd
103/419: lst
103/420: !lst
103/421: run grb2D1m3RHO.py 1804.nc
103/422: rm 1804D1.m3.nc
103/423: run grb2D1m3RHO.py 1804.nc
103/424: ntr
103/425: c
103/426: ncr.dimensions['TSTEP'].size
103/427: t1,t2
103/428: hrs
103/429: dens[t1:t2,:,:,:].shape
103/430: min(24,ntr)
103/431: t2-t1
103/432: dens.shape
103/433:   t2=min(ntA,t1+min(24,ntr))
103/434:   hrs=t2-t1
103/435:   dens[t1:t2,:,:,:]=ncr.variables['DENS'][:hrs,:,:,:] *1E9
103/436:
dens2=np.zeros(shape=(ntA,nlay1, nrow1, ncol1))
for k in range(nlay1):
  dens2[:,k,:,:]=dens[:,dlay[k],:,:]
var=np.zeros(shape=(nt, nlay, nrow, ncol))
zz=np.zeros(shape=(nt, nlay1, nrow1, ncol1))
var2=np.zeros(shape=(ntA,nlay1, nrow1, ncol1))
103/437:
for v in list(nms_gas)+list(nms_part):
  skip=0
  if v in nms_gas:
    nm=nms_gas[v]
    if nm not in V1[3]:skip=1
  else:
    nms=nms_part[v]
    for nm in nms:
      if nm not in V1[3]:skip=1
  if skip==1:continue
  var[:,:,:,:]=np.flip(nc.variables[v][:,:,:,:], [1,2])
  for t in range(nt):
    c = np.array([var[t,:,idx[0][i], idx[1][i]] for i in range(mp)])
    for k in range(nlay1):
      zz[t,k,:,: ] = griddata(xyc, c[:,k], (x1, y1), method='linear')
  for t in range(0,ntA,3):
    t3=int(t/3)
    var2[t+0,:,:,:]=zz[t3,:,:,:]
    var2[t+1,:,:,:]=zz[t3,:,:,:]*2/3+zz[t3+1,:,:,:]*1/3
    var2[t+2,:,:,:]=zz[t3,:,:,:]*1/3+zz[t3+1,:,:,:]*2/3
  if v in nms_gas:
    mw=mws[dic[v]]
    unit=28.E6/mw #mixing ratio to ppm
    nm=nms_gas[v]
    nc1.variables[nm][:]=var2[:]*rate[v][0]*unit
  else:
#    unit=1E9*dens[:] #28.E6/mvol #mixing ratio(kg/kg) to microgram/M3
    nms=nms_part[v]
    for nm in nms:
      im=nms.index(nm)
      nc1.variables[nm][:]+=var2[:]*rate[v][im]*dens2[:]
  print(v)
103/438: cd /home/cmaqruns/2018base/data/emis/REAS/1804
103/439: lst
103/440: fname='/nas1/cmaqruns/2018base/data/land/gridmask/AQFZones_EAsia_81K.nc'
103/441: nc = netCDF4.Dataset(fname, 'r')
103/442: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/443: V[3]
103/444: v='AQFZ2'
103/445: v='AQFZ4'
103/446: idx=np.where(nc[v][0,0,:,:]==0.)
103/447: len(idx[0])
103/448:

for d in range(4,10):
    fname='/home/cmaqruns/2018base/data/emis/REAS/1804/area_EAsia_81K.2018040'+str(d)+'.nc'
    nc = netCDF4.Dataset(fname, 'r+')
    fcrs=nc['FCRS'][0,0,:,:]
    ccrs=fcrs[:]
    ccrs[idx[0],idx[1]]=fcrs[idx[0],idx[1]]*100.
    fcrs[idx[0],idx[1]]=ccrs[idx[0],idx[1]]
    nc['FCRS'][0,0,:,:]=fcrs[:]
    nc['CCRS'][0,0,:,:]=ccrs[:]
    nc.close()
103/449: idx=np.where(nc[v][0,0,:,:]==1.)
103/450: fname='/nas1/cmaqruns/2018base/data/land/gridmask/AQFZones_EAsia_81K.nc'
103/451: nc = netCDF4.Dataset(fname, 'r')
103/452: idx=np.where(nc[v][0,0,:,:]==1.)
103/453: len(idx[0])
103/454:
for d in range(4,10):
    fname='/home/cmaqruns/2018base/data/emis/REAS/1804/area_EAsia_81K.2018040'+str(d)+'.nc'
    nc = netCDF4.Dataset(fname, 'r+')
    fcrs=nc['FCRS'][0,0,:,:]
    ccrs=fcrs[:]
    ccrs[:]=0.
    ccrs[idx[0],idx[1]]=fcrs[idx[0],idx[1]]*100.
    fcrs[idx[0],idx[1]]=fcrs[idx[0],idx[1]]*100.
    nc['FCRS'][0,0,:,:]=fcrs[:]
    nc['CCRS'][0,0,:,:]=ccrs[:]
    nc.close()
103/455: history
103/456: dic
103/457: cd /nas1/ecmwf/reanalysis/gribs18
103/458: fname='1804D1.m3.nc'
103/459: nc = netCDF4.Dataset(fname, 'r')
103/460: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/461:
for v in V[3]:
  print(dic[v],np.max(nc[v][:]))
103/462:
for v in V[3]:
  print(v,np.max(nc[v][:]))
103/463: nc.close()
103/464: cd /nas1/ecmwf/reanalysis/gribs18/POST
103/465: fname='1804D1.m3.nc'
103/466: nc = netCDF4.Dataset(fname, 'r')
103/467: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/468: V[3][0]
103/469: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
103/470: var=np.zeros(shape=(len(V[3]),nt,nlay,nrow,ncow))
103/471: var=np.zeros(shape=(len(V[3]),nt,nlay,nrow,ncol))
103/472:
for v in V[3]:
  iv=V[3].index(v)
  var[iv,:,:,:,:]=nc[v][:,:,:,:]
103/473: lst
103/474: !lst
103/475: tflag=[datetime.datetime.strptime(str(i),'%Y%j').strftime('%Y%m%d')+'{:02d}'.format(j//10000) for i,j in zip(nc['TFLAG'][:,0,0],nc['TFLAG'][:,0,1])]
103/476: tflag[:5]
103/477:
for d in range(4,9):
  fname='CCTM_ACONC_v53_gcc_1804_run6_2018040'+str(d)+'_EAsia_81K_11.nc'
  idx=tflag.index('2018040'+str(d)+'00')
  nc = netCDF4.Dataset(fname, 'r+')
  for v in V[3]:
    if v not in nc.variables:continue      
    iv=V[3].index(v)
    nc[v][:,:,:,:]=var[iv,idx:idx+24,:,:,:]
  nc.close()
103/478: nc[v][:,:,:,:].shape
103/479: var[iv,idx:idx+24,:,:,:].shape
103/480:
for d in range(4,9):
  fname='CCTM_ACONC_v53_gcc_1804_run6_2018040'+str(d)+'_EAsia_81K_11.nc'
  idx=tflag.index('2018040'+str(d)+'00')
  nc = netCDF4.Dataset(fname, 'r+')
  for v in V[3]:
    if v not in nc.variables:continue      
    iv=V[3].index(v)
    nc[v][:,:34,:,:]=var[iv,idx:idx+24,:,:,:]
  nc.close()
103/481: fname='1804D1.m3.nc'
103/482: nc.variables[3]
103/483: nc.variables
103/484: nc = netCDF4.Dataset(fname, 'r')
103/485: nc.variables
103/486: nc.variables[3]
103/487: [i for i in nc.variables]
103/488: [i for i in nc.variables][3]
103/489:
for d in range(4,9):
  fname='CCTM_ACONC_v53_gcc_1804_run6_2018040'+str(d)+'_EAsia_81K_11.nc'
  idx=tflag.index('2018040'+str(d)+'00')
  nc = netCDF4.Dataset(fname, 'r+')
  for v in set(nc.variables)-set(V[3]):      
    nc[v][:]=0
  nc.close()
103/490:
for d in range(4,9):
  fname='CCTM_ACONC_v53_gcc_1804_run6_2018040'+str(d)+'_EAsia_81K_11.nc'
  idx=tflag.index('2018040'+str(d)+'00')
  nc = netCDF4.Dataset(fname, 'r+')
  for v in V[3]:
    if v not in nc.variables:continue      
    iv=V[3].index(v)
    nc[v][:,:34,:,:]=var[iv,idx:idx+24,:,:,:]
  nc.close()
103/491: history
103/492: cd /nas1/Data/GIS/TWN_town
103/493: run rd_gml.py
103/494: df.head()
103/495: len(df)
103/496: pwd
103/497: cd /home/cmaqruns/2018base
103/498: cd data/*run6
103/499: cd data
103/500: cd *run6
103/501: ls
103/502: pwd
103/503: cd output_CCTM_v53_gcc_1804_run6
103/504: !lst
103/505: fname='CCTM_SA_CGRID_v53_gcc_1804_run6_20180408_EAsia_81K_11_NWC.nc'
103/506: nc = netCDF4.Dataset(fname, 'r')
103/507: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/508: [i for i in V[3] if i[0]=='P']
103/509: pwd
103/510: cd /nas1/Data/GIS
103/511: cd china_province/
103/512:
from pandas import *
from pykml import parser
from os import path
import numpy as np
import netCDF4
import os,sys, datetime, json
from pyproj import Proj
from shapely.geometry import Point, Polygon
103/513: !pip install pykml
103/514:
from pandas import *
from pykml import parser
from os import path
import numpy as np
import netCDF4
import os,sys, datetime, json
from pyproj import Proj
from shapely.geometry import Point, Polygon
103/515: !pip install shapely
103/516:
from pandas import *
from pykml import parser
from os import path
import numpy as np
import netCDF4
import os,sys, datetime, json
from pyproj import Proj
from shapely.geometry import Point, Polygon
103/517:
kml_file = path.join('doc.kml')
with open(kml_file) as f:
  doc = parser.parse(f).getroot()
plms=doc.findall('.//{http://www.opengis.net/kml/2.2}Placemark')
names=[i.name for i in plms]

mtgs=doc.findall('.//{http://www.opengis.net/kml/2.2}MultiGeometry')
mtg_tag=[str(i.xpath).split()[-1][:-2] for i in mtgs]

plgs=doc.findall('.//{http://www.opengis.net/kml/2.2}Polygon')
nplgs=len(plgs)
plg_prt=[str(i.getparent().values).split()[-1][:-2] for i in plgs]

lon,lat,num,nam=[],[],[],[]
n=0
#store the polygons
Dplg=[]
#name for the polygons
Nplg=[]
103/518:
for plg in plgs:
  iplg=plgs.index(plg)
  imtg=mtg_tag.index(plg_prt[iplg])
  name=names[imtg]
  Nplg.append(name)
  coord=plg.findall('.//{http://www.opengis.net/kml/2.2}coordinates')
  c=coord[0].pyval.split()
  long=[float(ln.split(',')[0]) for ln in c]
  lati=[float(ln.split(',')[1]) for ln in c]
  crd=[(i,j) for i,j in zip(lati,long)]
  Dplg.append(crd)
  for ln in c:
    if n%3==0:
      lon.append(ln.split(',')[0])
      lat.append(ln.split(',')[1])
      num.append('n='+str(n))
      nam.append(name)
    n+=1
#output the coordinates for checking
df=DataFrame({'lon':lon,'lat':lat,'num':num,'nam':nam})
df.set_index('lon').to_csv('doc.csv')
103/519:
df=read_csv('chn_admbnda_ocha.csv',encoding='big5')
nam2dis={i:j for i,j in zip(df.ADM1_EN,df.district)}
a=list(set(df.district));a.sort()
a=['NA']+a
dist={a[i]:i for i in range(len(a))}

#check the content of names
for i in names:
    if i not in nam2dis:print(i)

Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)

#reading the d1 template
fname='PM25_202001-05_d1.nc'
nc = netCDF4.Dataset(fname,'r+')
v4=list(filter(lambda x:nc.variables[x].ndim==4, [i for i in nc.variables]))
nt,nlay,nrow,ncol=(nc.variables[v4[0]].shape[i] for i in range(4))
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x_g, y_g = np.meshgrid(X, Y)
Plon, Plat= pnyc(x_g,y_g, inverse=True)
103/520: DIS=np.zeros(shape=(nrow,ncol),dtype=int)
103/521: Plat1d,Plon1d=Plat.flatten(),Plon.flatten()
103/522: p1=Point(Plat1d,Plon1d)
103/523: p1=Point(Plat1d[:],Plon1d[:])
103/524: p1=[Point(Plat1d[i],Plon1d[i]) for i in range(nrow*ncol)]
103/525: n=0
103/526: poly = Polygon(Dplg[n])
103/527: a=[i.within(poly) for i in p1]
103/528: a[:5]
103/529: a=np.array([i.within(poly) for i in p1])
103/530: idx=np.where(a==True)
103/531: len(idx[0])
103/532: len(a)
103/533: a=np.array([i.within(poly) for i in p1]).reshape(nrow,ncol)
103/534: idx=np.where(a==True)
103/535: [(idx[0][i],idx[1][i]) for i in range(12)]
103/536: nam2dis[Nplg[n]]
103/537: dist[nam2dis[Nplg[n]]]
103/538: grep YD *csv
103/539: !grep YD *csv
103/540: ls
103/541: DIS=np.zeros(shape=(nrow,ncol),dtype=int)
103/542: DIS[idx[0],idx[1]]=dist[nam2dis[Nplg[n]]]
103/543: DIS[30:40,20:30]
103/544: DIS[30:50,20:30]
103/545: nc.close()
103/546: cd /nas1/ecmwf/reanalysis/gribs18
103/547: import json
103/548:
with open('eac4_name.json', 'r', newline='') as jsonfile:
  dic=json.load(jsonfile)
103/549: !lst
103/550: !vi eac4_name.json
103/551:
with open('eac4_name.json', 'r', newline='') as jsonfile:
  dic=json.load(jsonfile)
103/552: !vi eac4_name.json
103/553:
with open('eac4_name.json', 'r', newline='') as jsonfile:
  dic=json.load(jsonfile)
103/554: dic
103/555:
dici={i:j for i,j in zip(dic.values(),dic.keys())}
gas=[i for i in list(dic.values()) if 'mix' not in i and i not in ['ammonium','nitrate']]
gas.sort()
ngas=len(gas)
par=[i for i in list(dic.values()) if i not in gas]
par.sort()
mws={
 'carbon_monoxide':28,
 'ethane':16,
 'formaldehyde':30,
 'isoprene':68.12,
 'nitric_acid':63,
 'nitrogen_dioxide':46,
 'nitrogen_monoxide':30,
 'olefins':42.1,
 'organic_nitrates':117.1,
 'ozone':48,
 'paraffins':72.1,
 "peroxyacetyl_nitrate":121,
 'propane':44,
 'sulphur_dioxide':64,
}
mws.update({p:0.0245 for p in [i for i in list(dic.values()) if i not in gas]})
gas_nm=['CO','ETH','FORM','ISOP','HNO3','NO2','NO','OLE','XPAR','O3','PAR','PAN','PRPA','SO2']
nms_gas={dici[i]:j for i,j in zip(gas,gas_nm)}
vlist=['APOCI','APNCOMI','APOCJ','AOTHRJ','AISO3J', 'ASQTJ', 'AORGCJ', 'AOLGBJ', 'AOLGAJ']
103/556:
nms_part={
 dici['ammonium']:['ANH4'+i for i in 'IJK'],
 dici['dust_aerosol_0.03-0.55um_mixing_ratio']:'AFEJ AALJ ASIJ ACAJ AMGJ AKJ AMNJ'.split(),
 dici['dust_aerosol_0.55-0.9um_mixing_ratio']:'AFEJ AALJ ASIJ ACAJ AMGJ AKJ AMNJ'.split(),
 dici['dust_aerosol_0.9-20um_mixing_ratio']:['ACORS','ASOIL'],
 dici['hydrophilic_black_carbon_aerosol_mixing_ratio']:['AEC'+i for i in 'IJ'],
 dici['hydrophilic_organic_matter_aerosol_mixing_ratio']:vlist,
 dici['hydrophobic_black_carbon_aerosol_mixing_ratio']:['AEC'+i for i in 'IJ'],
 dici['hydrophobic_organic_matter_aerosol_mixing_ratio']:vlist,
 dici['nitrate']:['ANO3'+i for i in 'IJK'],
 dici['sea_salt_aerosol_0.03-0.5um_mixing_ratio']:['ANAI', 'ACLI'],
 dici['sea_salt_aerosol_0.5-5um_mixing_ratio']:['ANAJ','ACLJ'],
 dici['sea_salt_aerosol_5-20um_mixing_ratio']:['ACLK','ASEACAT'],
 dici['sulphate_aerosol_mixing_ratio']:['ASO4'+i for i in 'IJK'],
}
par_nms=[]
for i in nms_part:
  par_nms+=nms_part[i]
npar=len(set(par_nms))
103/557: nms_part
103/558:
with open('nms_part.json', 'w', newline='') as jsonfile:
  json.dump(nms_part,jsonfile)
103/559: nms_gas
103/560:
with open('nms_gas.json', 'w', newline='') as jsonfile:
  json.dump(nms_gas,jsonfile)
103/561:
with open('mws.json', 'w', newline='') as jsonfile:
  json.dump(mws,jsonfile)
103/562: mws
103/563: !lst
103/564: ln -s eac4_name.json dic.json
103/565: !ln -s eac4_name.json dic.json
103/566:
for v in ['mws','dic','nms_gas','nms_part']
  with open(v+'.json', 'r') as jsonfile:
    exec(v+'=json.load(jsonfile)')
103/567:
for v in ['mws','dic','nms_gas','nms_part']:
  with open(v+'.json', 'r') as jsonfile:
    exec(v+'=json.load(jsonfile)')
103/568: !vi dtconverter.py
103/569: mv dtconverter.py dtconvertor.py
103/570: fname='templateD1.ncV49K34'
103/571: nc1 = netCDF4.Dataset(fname, 'r')
103/572: V1=[list(filter(lambda x:nc1.variables[x].ndim==j, [i for i in nc1.variables])) for j in [1,2,3,4]]
103/573: len(V1[3])
103/574: nc1 = netCDF4.Dataset(fname, 'r+')
103/575:
for v in V1[3]:
    nc1[v]=0.
103/576:
for v in V1[3]:
    nc1[v][:]=0.
103/577: nc1.close()
103/578: nms_gas.values()
103/579:
part=[]
for i in nms_part:
    part+=nms_part[i]
103/580: set(part)
103/581: set(list(nms_gas.values())+part) & set(V1[3])
103/582: nms_part.values()
103/583: c=set()
103/584: [c|set(i) for i in nms_part.values()][-1]
103/585: a=set(list(nms_gas.values())+part) & set(V1[3])
103/586: b=set(list(nms_gas.values())+part)
103/587: a-b
103/588: b-a
103/589: nms_gas
103/590:
for i in nms_gas:
    nms_gas.update(i:[nms_gas[i]])
103/591:
for i in nms_gas:
    nms_gas.update({i:[nms_gas[i]]})
103/592: nms_gas
103/593:
with open('nms_gas.json', 'w', newline='') as jsonfile:
  json.dump(nms_gas,jsonfile)
103/594:
for i in nms_gas:
    nms_gas.update({i:nms_gas[i][0]})
103/595: nms_gas
103/596:
with open('nms_gas.json', 'w', newline='') as jsonfile:
  json.dump(nms_gas,jsonfile)
103/597: len(a)
103/598: len(V1[3])
103/599: a='USGS_urban USGS_drycrop USGS_irrcrop USGS_cropgrass USGS_cropwdlnd USGS_grassland USGS_shrubland USGS_shrubgrass USGS_savanna USGS_decidforest USGS_evbrdleaf USGS_coniferfor USGS_mxforest USGS_water USGS_wetwoods USGS_sprsbarren USGS_woodtundr USGS_mxtundra USGS_snowice'.split()
103/600: len(a)
103/601: !psg mpirun
103/602: !psg wrf.exe
103/603: !topu
103/604: !topu
103/605: !vi /opt/intel_f/bin/compilervars.csh
103/606: pwd
103/607: cd /nas1/WRF4.0/WRF_chem
103/608: !findc '*.o'|wc
103/609: !topu
103/610: !topu
103/611: !topu
103/612: !topu
103/613: !vi namelist.input_fdda
103/614: pwd
103/615: cd 201804_run56
103/616: !vi namelist.input_fdda
103/617: !vi namelist.input_fdda
103/618: !lst
103/619: !tail rsl.error.0000
103/620: !tail rsl.error.0000
103/621: !lst
103/622: !lst
103/623: !lst
103/624: !lst
103/625: !ncdump -h wrffdda_d01|H
103/626: nc=wrfbdy_d01
103/627: !ncdump -h wrfbdy_d01|H
103/628: cd ./wrf4.3
103/629: ls
103/630: nc = netCDF4.Dataset('uv10', 'r')
103/631: nc = netCDF4.Dataset('uv10.nc', 'r')
103/632: u10=nc['U10'][:]
103/633: v10=nc['V10'][:]
103/634: nc = netCDF4.Dataset('ust.nc', 'r+')
103/635: nc.variables
103/636: v='UST'
103/637: nc.variables[v][:]=np.sqrt(u10*u10+v10*v10)
103/638: nc.close()
103/639: cd /nas1/WRF4.0/WRF_chem/201804_run56/WRFCHEM_FDDA
103/640: nc = netCDF4.Dataset('uv10.nc', 'r')
103/641: u10=nc['U10'][:]
103/642: v10=nc['V10'][:]
103/643: nc = netCDF4.Dataset('ust.nc', 'r+')
103/644: v
103/645: nc.variables[v][:]=np.sqrt(u10*u10+v10*v10)
103/646: nc.close()
103/647: !psg ncks
103/648: cd ..
103/649: !lst
103/650: !lst
103/651:
with open('a','r') as f:
  a=[i for i in f]
103/652:
with open('b','r') as f:
  b=[i for i in f]
103/653: len(a),len(b)
103/654: set(a)-set(b)
103/655: set(b)-set(a)
103/656:
with open('c','r') as f:
  c=[i for i in f]
103/657: set(c)-set(a)
103/658: set(a)-set(c)
103/659: pwd
103/660: fname='wrfout_d01_2018-03-30_00\:00\:00'
103/661: nc = netCDF4.Dataset(fname, 'r')
103/662: fname='wrfout_d01_2018-03-30_00:00:00'
103/663: nc = netCDF4.Dataset(fname, 'r')
103/664: a=[i.split()[1].split('(')[0] for i in a]
103/665: b=[i.split()[1].split('(')[0] for i in b]
103/666: set(b)-set(a)
103/667: set(a)-set(b)
103/668:
for v i in set(a)-set(b):
    print nc.variables[v]
103/669:
for v i in set(a)-set(b):
    print (nc.variables[v])
103/670:
for v in in set(a)-set(b):
    print (nc.variables[v])
103/671:
for v in set(a)-set(b):
    print (nc.variables[v])
103/672:
for v in set(a)-set(b):
    print (v,nc.variables[v].description)
103/673: !top
103/674: pwd
103/675: !lst
103/676: !lst
103/677: !lst
103/678: !ncrcat dust0*.nc dust.nc
103/679: history
103/680: fnames='dust03-30.nc dust03-31.nc dust04-01.nc dust04-02.nc dust04-03.nc dust04-04.nc dust04-05.nc dust04-06.nc dust04-07.nc dust04-08.nc dust04-09.nc'.split()
103/681:
nc = netCDF4.Dataset('dust.nc', 'r+')
it=0
for fname in fnames:
  nc_in = netCDF4.Dataset(fname, 'r')    
  dust=np.zeros(shape=nc_in['DUST1'].shape)
  for i in range(1,6):
    dust+=nc_in['DUST'+str(i)][:]
  nc['DUST1'][it:it+24,:,:]=dust[:,:,:]
  it+=24
103/682: fnames
103/683:
nc = netCDF4.Dataset('dust.nc', 'r+')
it=0
for fname in fnames:
  nc_in = netCDF4.Dataset(fname, 'r')    
  dust=np.zeros(shape=nc_in['DUST_1'].shape)
  for i in range(1,6):
    dust+=nc_in['DUST_'+str(i)][:]
  nc['DUST1_'][it:it+24,:,:]=dust[:,:,:]
  it+=24
103/684:
nc = netCDF4.Dataset('dust.nc', 'r+')
it=0
for fname in fnames:
  nc_in = netCDF4.Dataset(fname, 'r')    
  dust=np.zeros(shape=nc_in['DUST_1'].shape)
  for i in range(1,6):
    dust+=nc_in['DUST_'+str(i)][:]
  nc['DUST_1'][it:it+24,:,:]=dust[:,:,:]
  it+=24
103/685: fname
103/686:
nc = netCDF4.Dataset('dust.nc', 'r+')
it=0
for fname in fnames:
  nc_in = netCDF4.Dataset(fname, 'r')
  nt,nz,ny,nx=nc_in['DUST_1'].shape
  dust=np.zeros(shape=(nt,nz,ny,nx))
  for i in range(1,6):
    dust+=nc_in['DUST_'+str(i)][:]
  nc['DUST_1'][it:it+nt,:,:,:]=dust[:,:,:,:]
  it+=nt
103/687: nc.close()
103/688: history
103/689: !vi rd_dust.nc
103/690: v='Times'
103/691: nc.variables[v][0,:]
103/692: nc = netCDF4.Dataset('dust.nc', 'r')
103/693: nc.variables[v][0,:]
103/694: nc.variables[v][0,:].join()
103/695: np.array(nc.variables[v][0,:]).join()
103/696: a=[i.decode('utf-8') for i in nc.variables[v][0,:]]
103/697: a
103/698: a.join()
103/699: ''.join(a)
103/700: !grep nt *py
103/701: pwd
103/702: !lst
103/703: !grep nt *py
103/704: nt,nz,ny,nx=nc['DUST_1'].shape
103/705: nt
103/706: strT=[''.join([i.decode('utf-8') for i in nc.variables['Times'][t,:]]) for t in range(nt)]
103/707: Times=[datetime.datetime.strptime(a,'%Y-%m-%d_%H:00:00') for a in strT]
103/708: Times[:5]
103/709: tflag=[i.strftime('%Y%m%d%H') for i in Times]
103/710: tflag[:5]
103/711: tflag[-5:]
103/712:
IX,IY=335-1,212-1
nt,nz,ny,nx=nc['DUST_1'].shape
strT=[''.join([i.decode('utf-8') for i in nc.variables['Times'][t,:]]) for t in range(nt)]
Times=[datetime.datetime.strptime(a,'%Y-%m-%d_%H:00:00') for a in strT]
tflag=[i.strftime('%Y%m%d%H') for i in Times]
zs=['WRF-chem']
for z in zs:
  exec(z+'=nc["DUST_1"][:,0,IY,IX]')
DD={}
for z in zs+['tflag']:
  exec('DD.update({"'+z+'":'+z+'})')
df=DataFrame(DD)
df.set_index('tflag').to_csv('ntw.csv')
103/713: z
103/714:
zs=['WRFchem']
for z in zs:
  exec(z+'=nc["DUST_1"][:,0,IY,IX]')
DD={}
for z in zs+['tflag']:
  exec('DD.update({"'+z+'":'+z+'})')
df=DataFrame(DD)
df.set_index('tflag').to_csv('ntw.csv')
103/715: !lst
103/716: pwd
103/717: set(a)-set(b)
103/718: history
103/719:
with open('a','r') as f:
  a=[i for i in f]
103/720:
with open('b','r') as f:
  b=[i for i in f]
103/721: len(set(a)-set(b))
103/722: set(a)-set(b)
103/723:
for v in set(a)-set(b):
    vv=v.strip('\n')
    print (vv,nc.variables[vv].description)
103/724: fname='wrfout_d01_2018-03-30_00:00:00'
103/725: nc = netCDF4.Dataset(fname, 'r')
103/726:
for v in set(a)-set(b):
    vv=v.strip('\n')
    print (vv,nc.variables[vv].description)
103/727:
for v in set(a)-set(b):
    vv=v.strip('\n')
    print ('|'vv,'|',nc.variables[vv].description,'|')
103/728:
for v in set(a)-set(b):
    vv=v.strip('\n')
    print ('|',vv,'|',nc.variables[vv].description,'|')
103/729: l=list(set(a)-set(b))
103/730: l.sort()
103/731:
for v in l:
    vv=v.strip('\n')
    print ('|',vv,'|',nc.variables[vv].description,'|')
103/732:
for v in l:
    vv=v.strip('\n')
    print ('|',vv,'|',nc.variables[vv].description,'|',nc.variables[vv].unit,'|')
103/733:
for v in l:
    vv=v.strip('\n')
    print ('|',vv,'|',nc.variables[vv].description,'|',nc.variables[vv].units,'|')
103/734:
with open('d','r') as f:
  d=[i.strip('\n').split() for i in f]
103/735: d
103/736:
for i in d:
    if len(i)==0:continue
    print('|',i[0],'|','|',i[2],'|','|')
103/737: len(d)
103/738: cd /nas1/ecmwf/reanalysis/gribs18
103/739: fname=a.nc
103/740: fname='a.nc'
103/741: nc = netCDF4.Dataset(fname, 'r')
103/742: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/743: V
103/744: nc[lat_0][:]
103/745: nc['lat_0'][:]
103/746: nc['lon_0'][:]
103/747: nc.dimensions
103/748: v='VAR_192_210_123_P0_L105_GLL0'
103/749: nc[v].shape
103/750: nc[v][0,0,0,:]
103/751: len(nc[v][0,0,0,:])
103/752: nc = netCDF4.Dataset(fname, 'r+')
103/753: a=nc[v][:]
103/754: xlat=np.flip(nc.variables['lat_0'][:].flatten())
103/755: xlat[:5]
103/756: nc[v][:]=np.flip(a[:,:,:,:], [1,2])
103/757: nc.close()
103/758: Latitude_Pole, Longitude_Pole = 20,120
103/759: 20. in xlat
103/760: 120. in xlon
103/761: xlon
103/762: len(xlon)
103/763: V
103/764: v='lon_0'
103/765: nc[v][:]
103/766: nc = netCDF4.Dataset(fname, 'r')
103/767: nc[v][:]
103/768: xlon= nc.variables['lon_0'][:]
103/769: bdate
103/770: bdate.hours
103/771: bdate.hour
103/772: !grep pwd ~/bin/*py
103/773: pwd
103/774: fnames=subprocess.check_output('ls dust??-??.nc',shell=True).decode('utf8').strip('\n')
103/775: pwd
103/776: cd /nas1/WRF4.0/WRF_chem/201910_run11_12
103/777: fnames=subprocess.check_output('ls dust??-??.nc',shell=True).decode('utf8').strip('\n')
103/778: fnames
103/779: fnames=subprocess.check_output('ls dust??-??.nc',shell=True).decode('utf8').split('\n')
103/780: fnames
103/781: fnames=subprocess.check_output('ls dust??-??.nc',shell=True).decode('utf8').strip('\n').split('\n')
103/782: fnames
103/783:
nc = netCDF4.Dataset('dust.nc', 'r+')
it=0
for fname in fnames:
  nc_in = netCDF4.Dataset(fname, 'r')
  nt,nz,ny,nx=nc_in['DUST_1'].shape
  dust=np.zeros(shape=(nt,nz,ny,nx))
  for i in range(1,6):
    dust+=nc_in['DUST_'+str(i)][:]
  nc['DUST_1'][it:it+nt,:,:,:]=dust[:,:,:,:]
  it+=nt
103/784:
IX,IY=335-1,212-1
nt,nz,ny,nx=nc['DUST_1'].shape
strT=[''.join([i.decode('utf-8') for i in nc.variables['Times'][t,:]]) for t in range(nt)]
Times=[datetime.datetime.strptime(a,'%Y-%m-%d_%H:00:00') for a in strT]
tflag=[i.strftime('%Y%m%d%H') for i in Times]
zs=['WRFchem']
for z in zs:
  exec(z+'=nc["DUST_1"][:,0,IY,IX]*1.1839') #dens of air https://en.wikipedia.org/wiki/Density_of_air
DD={}
for z in zs+['tflag']:
  exec('DD.update({"'+z+'":'+z+'})')
df=DataFrame(DD)
df.set_index('tflag').to_csv('ntw.csv')

nc.close()
103/785: !lst
103/786: pwd
103/787: cd /nas1/ecmwf/reanalysis/gribs18
103/788: fname='templateD0.ncV49K34'
103/789: nc = netCDF4.Dataset(fname, 'r+')
103/790: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/791: nrow,nt,nlay,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
103/792: nrow,nt,nlay,ncol
103/793:
for v in V[3]:
    nc[v][:]=0
    for j in range(52,395):
        nc[v][j,:,:,:]=0.
103/794: nc.close()
103/795: nc = netCDF4.Dataset(fname, 'r+')
103/796: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/797: ncol,nt,nlay,nrow=(nc.variables[V[3][0]].shape[i] for i in range(4))
103/798: ncol,nt,nlay,nrow
103/799:
for v in V[3]:   
    for i in range(52,671):
        nc[v][i,:,:,:]=0.
103/800: nc.close()
103/801: 81*60
103/802: 1160*3
103/803: 673*3
103/804: 1168*3
103/805: 683*3
103/806: 148*3
103/807: 94*3
103/808: 70*27
103/809: 76*9
103/810: 58*9
103/811: cd /nas1/WRF4.0/WPS
103/812: fname='geo_em.d03_3Km.nc'
103/813: nc = netCDF4.Dataset(fname, 'r+')
103/814: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
103/815: V
103/816: v='XLAT_M'
103/817: np.min(nc.[v][:])
103/818: np.min(nc[v][:])
103/819: v='XLONG_M'
103/820: np.min(nc[v][:])
103/821: fname='geo_em.d01_15Km.nc'
103/822: nc = netCDF4.Dataset(fname, 'r')
103/823: nc[v].shape
103/824: x,y=101.86,13.18
103/825: dist=(nc['XLONG_M'][:]-x)**2+(nc['XLAT_M'][:]-y)**2
103/826: mndist=np.min(dist)
103/827: np.where(dist==mndist)
106/1: !lst
106/2: run reas2cmaqD1.py
106/3: run reas2cmaqD1.py
106/4: binf={i:1 for i in range(1,4),4:0.87,5:0}
106/5: binf={i:1 for i in range(1,4)}.update({4:0.87,5:0})
106/6: binf
106/7: binf={i:1 for i in range(1,4)}
106/8: binf
106/9: binf={i:1 for i in range(1,4)};binf.update({4:0.87,5:0})
106/10: binf
106/11: !which mpirun
106/12: pwd
106/13: cd /nas1/cmaqruns/2018base/data/land
106/14: fname='forest_EAsia_81K.nc'
106/15: nc = netCDF4.Dataset(fname, 'r+')
106/16: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
106/17: V
106/18: v='FOREST'
106/19: nc[v].long_name='FOREST'
106/20: nc[v]
106/21: nc[v].var_desc='MODIS1~5 summation from WRF geo_em'
106/22: fname='MODIS_1-5_EAsia_81K.nc'
106/23: nc0 = netCDF4.Dataset(fname, 'r')
106/24: var=np.zeros(shape=nc[v].shape)
106/25: var.shape
106/26:
for i in range(1,6):
    var[:]+=nc0['MODIS_'+str(i)][:]
106/27: np.max(var[:])
106/28: i
106/29: nc0['MODIS_'+str(i)]
106/30: np.max(nc0['MODIS_'+str(i)][:])
106/31: i=4
106/32: np.max(nc0['MODIS_'+str(i)][:])
106/33: i=1
106/34: np.max(nc0['MODIS_'+str(i)][:])
106/35: v='FOREST'
106/36: nc[v][:]=var[:]*100.
106/37: nc.NVARS
106/38: nc.NVARS=1
106/39: nc.TSTEPS
106/40: nc.TSTEP
106/41: nc.close()
106/42: history
106/43: fname='beld4.EAsia_81K.ncf'
106/44: nc0 = netCDF4.Dataset(fname, 'r')
106/45: V=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
106/46:
modis_usgs={
1:[14],
2:[13],
3:[12],
4:[11],
5:[15],
6:[9],
7:[8],
8:[0],
9:[10],
10:[7],
11:[17,18],
12:[2,3,4],
13:[1],
14:[5,6],
15:[24],
16:[19],
17:[16],
18:[21],
19:[22,23],
20:[20]
}
106/47: v=[]
106/48:
for i in modis_usgs:
    v+=modis_usgs[i]
106/49: v=list(set(v))
106/50: len(v)
106/51: v.sort()
106/52: a=[i.split('_')[1] for i in V[3] if 'MODIS' in i]
106/53: len(a)
106/54: a
106/55: np.max(nc0['MODIS_Res1'][:])
106/56: np.max(nc0['MODIS_Res2'][:])
106/57: np.max(nc0['MODIS_Res3'][:])
106/58: v='MODIS_Res3'
106/59: nc[v]
106/60: nc0[v]
106/61: modis_usgs2={str(i):modis_usgs[i] for i in modis_usgs}
106/62: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
106/63: nt,nlay,nrow,ncol=(nc0.variables[V[3][0]].shape[i] for i in range(4))
106/64: nt,nlay,nrow,ncol
106/65: v
106/66: var=np.zeros(shape=(25,nrow,ncol))
106/67:
c=0
for i in modis_usgs2:
    v='MODIS_'+i
    var[c,:,:]=nc0[v][0,0,:,:]
106/68: v
106/69: i=0
106/70: i=1
106/71: i='1'
106/72: v='MODIS_'+i
106/73: nc0[v]
106/74: i=2
106/75: i=16
106/76: i='16
106/77: i='16'
106/78: v='MODIS_'+i
106/79: nc0[v]
106/80: i='17'
106/81: v='MODIS_'+i;nc0[v]
106/82: var=np.zeros(shape=(25,nrow,ncol))
106/83:
c=0
for i in modis_usgs2:
    v='MODIS_'+i
    if i=='17':v=v='MODIS_0'
    var[c,:,:]=nc0[v][0,0,:,:]
    c+=1
106/84: modis_usgs2
106/85:
c=0
for i in modis_usgs2:
    v='MODIS_'+i
    if i=='17':v=v='MODIS_0'
    if v not in v4:continue
    var[c,:,:]=nc0[v][0,0,:,:]
    c+=1
106/86: var=np.zeros(shape=(25,nrow,ncol))
106/87:
c=0
for i in modis_usgs2:
    v='MODIS_'+i
    if i=='17':v=v='MODIS_0'
    if v not in V[3]:continue
    var[c,:,:]=nc0[v][0,0,:,:]
    c+=1
106/88: list(modis_usgs2}[:5]
106/89: list(modis_usgs2)[:5]
106/90: !lst
106/91: history
106/92: !lst
106/93: fname='USGS_EAsia_81K.nc'
106/94: nc = netCDF4.Dataset(fname, 'r+')
106/95: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
106/96: V
106/97:
for v in V[3]:
  nc[v][:]=0.
106/98: V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
106/99:
c=0
for i in modis_usgs2:
    v='MODIS_'+i
    if i=='17':v=v='MODIS_0'
    if v not in V0[3]:continue
    if np.sum(var[c,:,:])==0:continue
    for u in modis_usgs2[i]:
        vv=usgs_nam[u]
        if vv not in V[3]:continue        
        nc[vv][0,0,:,:]+=var[c,:,:]   
    c+=1
106/100:
usgs_nam={
1:'USGS_urban',
2:'USGS_drycrop',
3:'USGS_irrcrop',
4:'USGS_cropgrass',
5:'USGS_cropwdlnd',
6:'USGS_shrubgrass',
7:'USGS_grassland',
8:'USGS_shrubland',
9:'USGS_shrubland',
10:'USGS_savanna',
11:'USGS_decidforest',
12:'USGS_decidforest',
13:'USGS_evbrdleaf',
14:'USGS_coniferfor',
15:'USGS_mxforest',
16:'USGS_water',
17:'USGS_wetwoods',
18:'USGS_wetwoods',
19:'USGS_sprsbarren',
20:'USGS_mxtundra',
21:'USGS_woodtundr',
22:'USGS_mxtundra',
24:'USGS_snowice',
}
106/101:
c=0
for i in modis_usgs2:
    v='MODIS_'+i
    if i=='17':v=v='MODIS_0'
    if v not in V0[3]:continue
    if np.sum(var[c,:,:])==0:continue
    for u in modis_usgs2[i]:
        vv=usgs_nam[u]
        if vv not in V[3]:continue        
        nc[vv][0,0,:,:]+=var[c,:,:]   
    c+=1
106/102: u
106/103: i
106/104:
c=0
for i in modis_usgs2:
    v='MODIS_'+i
    if i=='17':v=v='MODIS_0'
    if v not in V0[3]:continue
    if np.sum(var[c,:,:])==0:continue
    for u in modis_usgs2[i]:
        if u==0:continue
        vv=usgs_nam[u]
        if vv not in V[3]:continue        
        nc[vv][0,0,:,:]+=var[c,:,:]   
    c+=1
106/105:
for v in V[3]:
  nc[v][:]=0.
106/106:
c=0
for i in modis_usgs2:
    v='MODIS_'+i
    if i=='17':v=v='MODIS_0'
    if v not in V0[3]:continue
    if np.sum(var[c,:,:])==0:continue
    for u in modis_usgs2[i]:
        if u==0:continue
        vv=usgs_nam[u]
        if vv not in V[3]:continue        
        nc[vv][0,0,:,:]+=var[c,:,:]   
    c+=1
106/107: var=np.zeros(shape=(nrow,ncol))
106/108:
for v in V[3]:
    var[:,:]+=nc[v][0,0,:,:]
106/109: np.max(var[:])
106/110: history
106/111:
fname='beld4.EAsia_81K.ncf'
nc = netCDF4.Dataset(fname, 'r')
V0=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc0.variables['MODIS_0'].shape[i] for i in range(4))
var=np.zeros(shape=(25,nrow,ncol))
c=0
for i in modis_usgs2:
    v='MODIS_'+i
    if i=='17':v=v='MODIS_0'
    if v not in V0[3]:continue
    print(v,np.max(nc[v][:]))
106/112:
for v in V[3]:
  if np.sum(nc[v][:])==0:continue
  mxnc=np.max(nc[v][:])
  if mxnc>1:
    nc[v][:]/=mxnc
  nc[v][:]*=100.
nc.close()
106/113: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
106/114: V
106/115: run modis_usgs.py
106/116: run modis_usgs.py
106/117: !lst
106/118: fname='USGS_EAsia_81K.nc'
106/119: nc = netCDF4.Dataset(fname, 'r+')
106/120: dir(nc)
106/121: a=dir(nc)
106/122: a[:5]
106/123: a[5:15]
106/124: a[15:25]
106/125: a
106/126: a[:20]
106/127: a[:40]
106/128:
fname='beld4.EAsia_81K.ncf'
nc0 = netCDF4.Dataset(fname, 'r')
106/129:
fname='USGS_EAsia_81K.nc'
nc = netCDF4.Dataset(fname, 'r+')
106/130:
atts=['CDATE',  'CTIME', 'EXEC_ID', 'FILEDESC', 'FTYPE', 'GDNAM', 'GDTYP', 'HISTORY', 'IOAPI_VERSION', 'NCO', 'NCOLS', 'NLAYS', 'NROWS',
 'NTHIK', 'NVARS', 'P_ALP', 'P_BET', 'P_GAM', 'SDATE', 'STIME', 'TSTEP', 'UPNAM', 'VAR-LIST', 'VGLVLS', 'VGTOP', 'VGTYP', 'WDATE', 
 'WTIME', 'XCELL', 'XCENT', 'XORIG', 'YCELL', 'YCENT', 'YORIG']

for i in atts:
  exec('nc.'+i+'=nc0.'+i)
106/131:
atts=['CDATE',  'CTIME', 'EXEC_ID', 'FILEDESC', 'FTYPE', 'GDNAM', 'GDTYP', 'HISTORY', 'IOAPI_VERSION', 'NCO', 'NCOLS', 'NLAYS', 'NROWS',
 'NTHIK', 'NVARS', 'P_ALP', 'P_BET', 'P_GAM', 'SDATE', 'STIME', 'TSTEP', 'UPNAM', 'VGLVLS', 'VGTOP', 'VGTYP', 'WDATE', 
 'WTIME', 'XCELL', 'XCENT', 'XORIG', 'YCELL', 'YCENT', 'YORIG']

for i in atts:
  exec('nc.'+i+'=nc0.'+i)
106/132: nc.close()
106/133: np.sum(var[0,:])
106/134: !lst
106/135: pwd
106/136: run modis_usgs.py
106/137: !lst
106/138: run modis_usgs.py
106/139: run modis_usgs.py
106/140: len(idx[0])
106/141: sumv.shape
106/142: run modis_usgs.py
106/143: a='ACET            '
106/144: len(a)
106/145: a="USGS_coniferfor USGS_cropgrass  USGS_cropwdlnd  USGS_decidforestUSGS_drycrop    USGS_evbrdleaf  USGS_grassland  USGS_irrcrop    USGS_mxforest   USGS_mxtundra   USGS_savanna    USGS_shrubgrass USGS_shrubland  USGS_snowice    USGS_sprsbarren USGS_urban      USGS_water      USGS_wetwoods USGS_woodtundr  ".split()
106/146: len(a)
106/147: len(s)
106/148: nc.NVARS
106/149: fname='USGS_EAsia_81K.nc'
106/150: nc = netCDF4.Dataset(fname, 'r+')
106/151: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
106/152: nc.close()
106/153: nc = netCDF4.Dataset(fname, 'r+')
106/154: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
106/155: nc.close()
106/156: nc = netCDF4.Dataset(fname, 'r+')
106/157: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
106/158: len(V)
106/159: len(V[3])
106/160: nc.NVARS
106/161: nc.NVARS=295
106/162: s=''
106/163:
for i in s:
    s+=i+' '*(16-len(i))
106/164: s
106/165: s=''
106/166:
for i in V[3]:
    s+=i+' '*(16-len(i))
106/167: s
106/168: nc.close()
106/169: fname
106/170: nc = netCDF4.Dataset(fname, 'r+')
106/171: nc.SDATE
106/172: nc['TFLAG'][0,:,0]=nc.SDATE
106/173: nc.STIME
106/174: nc['TFLAG'][0,:,1]=nc.STIME
106/175: nc.NVARS
106/176: nc.NVARS=len(V[3])
106/177: nc.NROWS
106/178: run modis_usgs.py
106/179: nc.close()
106/180: run modis_usgs.py
106/181: fname
106/182: nc = netCDF4.Dataset(fname, 'r+')
106/183: s='Corn';nc.createVariable(s,"f4",("TSTEP","LAY","ROW","COL"))
106/184: V[3][0]
106/185: v=Acacia
106/186: v='Acacia'
106/187: nc[v]
106/188: for i in 'var_desc units'.split()
106/189:
for i in 'var_desc units'.split():
    exec('nc[s].'+i+'=nc[V[3][0]].'+i')
106/190:
for i in 'var_desc units'.split():
    exec('nc[s].'+i+'=nc[V[3][0]].'+i)
106/191: nc.NVARS= nc.NVARS+1
106/192: s=''
106/193:
for i in V[3]+'Corn':
    s+=i+' '*(16-len(i))
106/194: s='Corn'
106/195: a=''
106/196:
for i in V[3]+['Corn']:
    a+=i+' '*(16-len(i))
106/197: print(a)
106/198: a
106/199: nc['TFLAG'].shape
106/200: nc.close()
106/201: nc = netCDF4.Dataset(fname, 'r')
106/202: v='USGS_shrubland'
106/203: nc[v]
106/204: nc[v][0,0,:5,:5]
106/205: np.max(nc[v][:])
106/206: np.min(nc[v][:])
106/207: lst
106/208: !ls
106/209: len(V[3])
106/210: v3=V[3]
106/211: fname='beld4.EAsia_81K.ncf'
106/212: nc = netCDF4.Dataset(fname, 'r')
106/213: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
106/214: a=set(V[3])+set(v3)
106/215: a=set(V[3]+v3)
106/216: len(a)
106/217: fname='/home/kuang/mac/cmaqruns/2016base/Spatial-Allocator/data/beld3/b3_a.tile10.nzero.ncf'
106/218: nc = netCDF4.Dataset(fname, 'r')
106/219: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
106/220: (v3==V[3]).all
106/221: (v3==V[3]).all()
106/222: all(v3==V[3])
106/223: v3==V[3]
106/224: set(V[3])=set(v3)
106/225: set(V[3])-set(v3)
106/226: ss=set(V[3]+v3)
106/227: len(ss)
106/228: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
106/229: ss=set(V[3]+list(ss))
106/230: len(ss)
106/231: ls
106/232: fname='beld4.CWBWRF_15k.ncf'
106/233: nc = netCDF4.Dataset(fname, 'r+')
106/234: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
106/235: nrow,nt,nlay,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
106/236: nrow,nt,nlay,ncol
106/237: for j in range(nrow,389)
106/238:
for j in range(nrow,389):
  for v in V[3]:
    nc[v][j,0,0,:]=0.
106/239: nc.NROWS
106/240: nc.NROWS=389
106/241: nc.close()
106/242: nc = netCDF4.Dataset(fname, 'r+')
106/243:
for i in range(ncol,665):
  for v in V[3]:
    nc[v][i,0,0,:]=0.
106/244: nc.NCOLS
106/245: nc.NCOLS=665
106/246: nc.close()
106/247: nc = netCDF4.Dataset(fname, 'r+')
106/248: fname='../mcip/1804_run5/CWBWRF_15k/METCRO2D_1804_run5.nc'
106/249: nc0 = netCDF4.Dataset(fname, 'r')
106/250:
atts=['CDATE',  'CTIME', 'EXEC_ID', 'FILEDESC', 'FTYPE', 'GDNAM', 'GDTYP', 'HISTORY', 'IOAPI_VERSION', 'NCO', 'NCOLS',  'NROWS',
 'NTHIK', 'P_ALP', 'P_BET', 'P_GAM', 'SDATE', 'STIME', 'UPNAM', 'WDATE', 
 'WTIME', 'XCELL', 'XCENT', 'XORIG', 'YCELL', 'YCENT', 'YORIG']
106/251:
atts=['CDATE',  'CTIME', 'EXEC_ID', 'FILEDESC', 'FTYPE', 'GDNAM', 'GDTYP', 'HISTORY', 'IOAPI_VERSION', 'NCO', 'NCOLS',  'NROWS',
 'NTHIK', 'P_ALP', 'P_BET', 'P_GAM', 'UPNAM', 'WDATE', 
 'WTIME', 'XCELL', 'XCENT', 'XORIG', 'YCELL', 'YCENT', 'YORIG']
106/252:
  for i in atts:
    exec('nc.'+i+'=nc0.'+i)
106/253: i
106/254: 'NCO' in dir(nc0)
106/255:
  for i in atts:
    if i not in dir(nc0):continue
    exec('nc.'+i+'=nc0.'+i)
106/256: nc.ALPS
106/257: nc.P_ALP
106/258: nc.XORIG
106/259: nc.YORIG
106/260: nc.YCELL
106/261: nc.close()
106/262: nc0.close()
106/263:
fname='beld4.CWBWRF_15k.ncf'
nc = netCDF4.Dataset(fname, 'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
modis=["MODIS_"+str((i+1)%17) for i in range(17)]
for v in V[3]:
  nc[v][:]=0.
  if v[:6]=="MODIS_":
    try:
      ii=int(v[6:8])-1
    except:
      ii=17+int(v[9:10])
    else:
      if ii<0:ii=ii+17
        nc[v][0,0,:,:]=nc0["LUFRAC"][0,ii,:,:]
      else:
        nc[v][:]=0
106/264: modis
106/265:
fname='beld4.CWBWRF_15k.ncf'
nc = netCDF4.Dataset(fname, 'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
modis=["MODIS_"+str((i+1)%17) for i in range(17)]
for v in V[3]:
  nc[v][:]=0.
  if v[:6]=="MODIS_":
    try:
      ii=int(v[6:8])-1
    except:
      ii=17+int(v[9:10])
    else:
      if ii<0:ii=ii+17
    nc[v][0,0,:,:]=nc0["LUFRAC"][0,ii,:,:]
106/266: fname='../mcip/1804_run5/CWBWRF_15k/LUFRAC_CRO_1804_run5.nc'
106/267: nc0 = netCDF4.Dataset(fname, 'r')
106/268:
fname='beld4.CWBWRF_15k.ncf'
nc = netCDF4.Dataset(fname, 'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
modis=["MODIS_"+str((i+1)%17) for i in range(17)]
for v in V[3]:
  nc[v][:]=0.
  if v[:6]=="MODIS_":
    try:
      ii=int(v[6:8])-1
    except:
      ii=17+int(v[9:10])
    else:
      if ii<0:ii=ii+17
    nc[v][0,0,:,:]=nc0["LUFRAC"][0,ii,:,:]
106/269: nc.close()
106/270: fname='beld4.CWBWRF_15k.ncf'
106/271: nc = netCDF4.Dataset(fname, 'r+')
106/272: modis
106/273:
for v in V[3]:
    if 'MODIS' in v:
        nc[v].units='Fraction'
106/274: nc.close()
106/275: nc = netCDF4.Dataset(fname, 'r+')
106/276: len([i for i in V[3] if 'MODIS' in i])
106/277: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
106/278: var=np.zeros(shape=(20,nrow,ncol))
106/279: modis=[i for i in V[3] if 'MODIS' in i]
106/280:
for v in modis:
    iv=modis.index(v)
    var[iv,:,:]=nc[v][0,0,:,:]
106/281: sumv=np.sum(var[:,:,:],axis=0)
106/282: np.max(sumv)
106/283: var=np.zeros(shape=(20,nrow,ncol))
106/284: modis=[i for i in V[3] if 'MODIS' in i and 'Res' not in i]
106/285: modis
106/286:
for v in modis:
    iv=modis.index(v)
    var[iv,:,:]=nc[v][0,0,:,:]
106/287: sumv=np.sum(var[:,:,:],axis=0)
106/288: np.max(sumv)
106/289: cd /nas1/cmaqruns/2018base/data/mcip/RHO/d00
106/290: fname='../../1804_run5/CWBWRF_15k/METCRO3D_1804_run5.nc'
106/291: nc = netCDF4.Dataset(fname, 'r')
106/292: nc.SDATE
106/293: nc.STIME
106/294: cd /nas1/ecmwf/reanalysis/gribs18
106/295: fname='templateD0.ncV49K34'
106/296: nc = netCDF4.Dataset(fname, 'r+')
106/297: fname='../../1804_run5/CWBWRF_15k/METCRO3D_1804_run5.nc'
106/298: nc0 = netCDF4.Dataset(fname, 'r')
106/299: fname='/nas1/cmaqruns/2018base/data/mcip/1804_run5/CWBWRF_15k/METCRO3D_1804_run5.nc'
106/300: nc0 = netCDF4.Dataset(fname, 'r')
106/301:
atts=['CDATE',  'CTIME', 'EXEC_ID', 'FILEDESC', 'FTYPE', 'GDNAM', 'GDTYP', 'HISTORY', 'IOAPI_VERSION', 'NCO', 'NCOLS',  'NROWS',
 'NTHIK', 'P_ALP', 'P_BET', 'P_GAM', 'UPNAM', 'WDATE', 
 'WTIME', 'XCELL', 'XCENT', 'XORIG', 'YCELL', 'YCENT', 'YORIG']
106/302:
  for i in atts:
    if i not in dir(nc0):continue
    exec('nc.'+i+'=nc0.'+i)
106/303: nc.close()
106/304: !lst /nas1/cmaqruns/2018base/data/mcip/1804_run5/CWBWRF_15k
106/305: pwd
106/306: cd /nas1/cmaqruns/2018base/data/mcip/RHO/d00/1804
106/307: bdate=datetime.datetime(2018,3,31)
106/308: import datetime
106/309: bdate=datetime.datetime(2018,3,31)
106/310: bdate+datetime.timedelta(day=9)
106/311: bdate+datetime.timedelta(days=9)
106/312:
for i in range(9,32):
    d1=bdate+datetime.timedelta(days=i//9)
    fname1='RHO.'+d1.strftime('%Y%m%d')+'.nc'    
    d2=bdate+datetime.timedelta(days=i)
    fname2='RHO.'+d2.strftime('%Y%m%d')+'.nc'
    os.system('cp '+fname1+' '+fname2)
    nc=netCDF4.Dataset(fname2, 'r+')
    nc.SDATE=d2.strftime('%Y%j')
    for t in range(24):
        nc['TFLAG'][t,:,0]=nc.SDATE
    nc['TFLAG'][24,:,0]=nc.SDATE+1
    nc.close()
106/313: nc.SDATE
106/314:
for i in range(9,32):
    d1=bdate+datetime.timedelta(days=i//9)
    fname1='RHO.'+d1.strftime('%Y%m%d')+'.nc'    
    d2=bdate+datetime.timedelta(days=i)
    fname2='RHO.'+d2.strftime('%Y%m%d')+'.nc'
    os.system('cp '+fname1+' '+fname2)
    nc=netCDF4.Dataset(fname2, 'r+')
    nc.SDATE=int(d2.strftime('%Y%j'))
    for t in range(24):
        nc['TFLAG'][t,:,0]=nc.SDATE
    nc['TFLAG'][24,:,0]=nc.SDATE+1
    nc.close()
106/315: cd ../1803
106/316: bdate
106/317:
for i in range(32):
    d1=bdate+datetime.timedelta(days=i)
    fname1='../1804/RHO.'+d1.strftime('%Y%m%d')+'.nc'    
    d2=bdate+datetime.timedelta(days=i-31)
    fname2='RHO.'+d2.strftime('%Y%m%d')+'.nc'
    os.system('cp '+fname1+' '+fname2)
    nc=netCDF4.Dataset(fname2, 'r+')
    nc.SDATE=int(d2.strftime('%Y%j'))
    for t in range(24):
        nc['TFLAG'][t,:,0]=nc.SDATE
    nc['TFLAG'][24,:,0]=nc.SDATE+1
    nc.close()
106/318: pwd
106/319: cd /nas1/ecmwf/reanalysis/gribs18
106/320: fname='templateD0.ncV49K34'
106/321: nc = netCDF4.Dataset(fname, 'r+')
106/322: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
106/323: nrow,nt,nlay,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
106/324: nrow,nt,nlay,ncol
106/325:
for j in range(nrow,394):
  for v in V[3]:
    nc[v][j,0,0,:]=0.
106/326: nc.close()
106/327: nc = netCDF4.Dataset(fname, 'r+')
106/328: ncol
106/329:
nc.NROWS=394
nc.NCOLS=670
for j in range(ncol,nc.NCOLS):
  for v in V[3]:
    nc[v][j,0,0,:]=0.
106/330: nc.close()
107/1: import datetime
107/2: import json
107/3: import subprocess
107/4: import os
107/5: import netCDF4
107/6: import numpy as np
107/7: fname='templateD0.ncV49K34'
107/8: nc = netCDF4.Dataset(fname, 'r+')
107/9: nc.close()
107/10: nc = netCDF4.Dataset(fname, 'r+')
107/11: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
107/12: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
107/13: nt,nlay,nrow,ncol
107/14: nc.XCELL
107/15: nc.XORIG
107/16: nc.XORIG=-(nc.XCELL*ncol)
107/17: nc.XORIG=-(nc.XCELL*ncol)/2.
107/18: nc.XORIG
107/19: nc.YCELL
107/20: nc.YORIG
107/21: nc.YORIG=-(nc.YCELL*nrow)/2.
107/22: nc.NROWS==nrow
107/23: nc.NCOLS==ncol
107/24: nc.NVARS
107/25: nc.close()
107/26: 1.4/13*1500
107/27: from pandas import *
107/28: df=read_csv('pms2001.csv')
107/29: df=read_csv('pms2001.csv',coding='utf8')
107/30: df=read_csv('pms2001.csv',encoding='utf8')
107/31: df=read_csv('pms2001.csv',encoding='utf7')
107/32: df=read_csv('pms2001.csv',encoding='big5')
107/33: df
108/1: from pandas import *
108/2: df=read_csv('pms2001.csv',encoding='big5')
108/3: df
108/4: df.columns
108/5: stn=list(set([i.split('_')[0] for i in df.columns if '_' in i]))
108/6: stn
108/7: stn_m=[i+'_M' for i in stn]
108/8: df[stn_m]
108/9: df_M=DataFrame({'spec':df.spec[:-1]})
108/10: dd_M={'spec':df.spec[:-1]}
108/11:
for s in stn_m:
    dd_M.update({s:list(df[s])[:-1]})
108/12: df_M=DataFrame(dd_M)
108/13: df_M
108/14: dd_M={'spec':df.spec[:-1]}
108/15:
for s in stn_m:
    dd_M.update({s:[float(i.split('(')[0]) for i in list(df[s])[:-3]]})
108/16: df_M=DataFrame(dd_M)
108/17: dd_M={'spec':df.spec[:-3]}
108/18:
for s in stn_m:
    dd_M.update({s:[float(i.split('(')[0]) for i in list(df[s])[:-3]]})
108/19: df_M=DataFrame(dd_M)
108/20: df_M
108/21: sum([float(i) for i in '7   12  6   3   3   0.1 24  0.5'.split()])
108/22: sum([float(i) for i in s.split()])
108/23: s='7    7   4   2   2   0.1 30  0.5'
108/24: sum([float(i) for i in s.split()])
108/25: s='7    8   4   3   1   0.1 29  0.5'
108/26: sum([float(i) for i in s.split()])
108/27: mw=[float(i) for i in '26.98|40.078|55.845|39.0983|24.305|54.938044|28.0855|47.867'.split('|')]
108/28: mw
108/29: sp='26.98|40.078|55.845|39.0983|24.305|54.938044|28.0855|47.867'.split('|')
108/30: sp
108/31: sp='Al|Ca|Fe|K|Mg|Mn|Si|Ti'.split('|')
108/32: sp
108/33: oxn=[3/2,1,3/2,0.5,1,7/2,2,2]
108/34: oxn
108/35: fac=[i+i/m*o*16 for i ,m, o in zip()]
108/36: WesternHDS=[float(i) for i in s.split()]
108/37: LoessArea=[float(i) for i in s.split()]
108/38: s='7    12  6   3   3   0.1 24  0.5'
108/39: WesternHDS=[float(i) for i in s.split()]
108/40: s='7    7   4   2   2   0.1 30  0.5'
108/41: NorthernHDS=[float(i) for i in s.split()]
108/42: LoessArea
108/43: WesternHDS
108/44: NorthernHDS
108/45: L=[i+i/m*o*16 for i ,m, o in zip(LoessArea,mw,oxn)]
108/46: L
108/47: sum(L)
108/48: N=[i+i/m*o*16 for i ,m, o in zip(NorthernHDS,mw,oxn)]
108/49: sum(N)
108/50: W=[i+i/m*o*16 for i ,m, o in zip(WesternHDS,mw,oxn)]
108/51: sum(W)
108/52: df=DataFrame({'spec':sp,'LoessArea':L,'NorthernHDS':N,'WesternHDS':W})
108/53: df
108/54: df.set_index('spec').to_csv('Table3.csv')
108/55: !mv Table3.csv Table3withOxy.csv
108/56: fname='/nas1/cmaqruns/2019base/data/output_CCTM_v53_gcc_1901/CCTM_ACONC_v53_gcc_1909_run9_20190916_TWN_3X3_11E.nc'
108/57: nc = netCDF4.Dataset(fname, 'r')
108/58: import netCDF4
108/59: nc = netCDF4.Dataset(fname, 'r')
108/60: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/61:
for m in sp:
    print([i for i in V[3] if 'A'+m in i])
108/62: sp
108/63: sp=[i.upper() for i in sp]
108/64: sp
108/65:
for m in sp:
    print([i for i in V[3] if 'A'+m in i])
108/66: [i for i in V[3] if i[0]=='A' and i[-1]=='I']
108/67: [1+1/m*o*16 for m, o in zip(mw,oxn)]
108/68: sp
108/69: WesternHDS
108/70: 24.0/7.0
108/71: NorthernHDS
108/72: 30.0/7
108/73: LoessArea
108/74: 29/7
108/75: bdate=datetime.datetime(2018,4,8)
108/76: import datetime
108/77: bdate=datetime.datetime(2018,4,8)
108/78:
for i in range(9,32):
    d1=bdate+datetime.timedelta(days=i//8)
    fname1='RHO.'+d1.strftime('%Y%m%d')+'.nc'    
    d2=bdate+datetime.timedelta(days=i)
    fname2='RHO.'+d2.strftime('%Y%m%d')+'.nc'
    os.system('cp '+fname1+' '+fname2)
    nc=netCDF4.Dataset(fname2, 'r+')
    nc.SDATE=int(d2.strftime('%Y%j'))
    for t in range(24):
        nc['TFLAG'][t,:,0]=nc.SDATE
    nc['TFLAG'][24,:,0]=nc.SDATE+1
    nc.close()
108/79: import os
108/80:
for i in range(9,32):
    d1=bdate+datetime.timedelta(days=i//8)
    fname1='RHO.'+d1.strftime('%Y%m%d')+'.nc'    
    d2=bdate+datetime.timedelta(days=i)
    fname2='RHO.'+d2.strftime('%Y%m%d')+'.nc'
    os.system('cp '+fname1+' '+fname2)
    nc=netCDF4.Dataset(fname2, 'r+')
    nc.SDATE=int(d2.strftime('%Y%j'))
    for t in range(24):
        nc['TFLAG'][t,:,0]=nc.SDATE
    nc['TFLAG'][24,:,0]=nc.SDATE+1
    nc.close()
108/81: i
108/82: 9//8
108/83: d1
108/84:
for i in range(8,32):
    d1=bdate+datetime.timedelta(days=i//8)
    fname1='RHO.'+d1.strftime('%Y%m%d')+'.nc'    
    d2=bdate+datetime.timedelta(days=i)
    fname2='RHO.'+d2.strftime('%Y%m%d')+'.nc'
    os.system('cp '+fname1+' '+fname2)
    nc=netCDF4.Dataset(fname2, 'r+')
    nc.SDATE=int(d2.strftime('%Y%j'))
    for t in range(24):
        nc['TFLAG'][t,:,0]=nc.SDATE
    nc['TFLAG'][24,:,0]=nc.SDATE+1
    nc.close()
108/85: bdate
108/86: bdate=datetime.datetime(2018,3,31)
108/87:
for i in range(8,32):
    d1=bdate+datetime.timedelta(days=i//8)
    fname1='RHO.'+d1.strftime('%Y%m%d')+'.nc'    
    d2=bdate+datetime.timedelta(days=i)
    fname2='RHO.'+d2.strftime('%Y%m%d')+'.nc'
    os.system('cp '+fname1+' '+fname2)
    nc=netCDF4.Dataset(fname2, 'r+')
    nc.SDATE=int(d2.strftime('%Y%j'))
    for t in range(24):
        nc['TFLAG'][t,:,0]=nc.SDATE
    nc['TFLAG'][24,:,0]=nc.SDATE+1
    nc.close()
108/88: pwd
108/89: cd /nas1/cmaqruns/2018base/data/mcip/RHO/sChina_81k/1804
108/90:
for i in range(8,32):
    d1=bdate+datetime.timedelta(days=i//8)
    fname1='RHO.'+d1.strftime('%Y%m%d')+'.nc'    
    d2=bdate+datetime.timedelta(days=i)
    fname2='RHO.'+d2.strftime('%Y%m%d')+'.nc'
    os.system('cp '+fname1+' '+fname2)
    nc=netCDF4.Dataset(fname2, 'r+')
    nc.SDATE=int(d2.strftime('%Y%j'))
    for t in range(24):
        nc['TFLAG'][t,:,0]=nc.SDATE
    nc['TFLAG'][24,:,0]=nc.SDATE+1
    nc.close()
108/91: cd /nas1/ecmwf/reanalysis/gribs18
108/92: run grb2m3.py 1804.nc D1
108/93: naly1
108/94: nlay1
108/95:
 l40=['21', '21', '22', '22', '23', '24', '24', '25', '25', '26', '27', '28', '28', '29', '30', '31', '32', '32', '33', '34',
     '35', '36', '37', '38', '39', '40', '42', '43', '44', '46', '47', '48', '49', '50', '51', '53', '54', '56', '57', '59']
d40_23={39-k:l34.index(l40[k]) for k in range(40)}
108/96: d40_23
108/97:
 l40=['21', '21', '22', '22', '23', '24', '24', '25', '25', '26', '27', '28', '28', '29', '30', '31', '32', '32', '33', '34',
     '35', '36', '37', '38', '39', '40', '42', '43', '44', '46', '47', '48', '49', '50', '51', '53', '54', '56', '57', '59']
d40_23={k:l34.index(l40[k]) for k in range(40)}
108/98:
l34=['21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37',
     '38', '39', '40', '42', '43', '44', '46', '47', '48', '49', '50', '51', '53', '54', '56', '57', '59']
108/99:
 l40=['21', '21', '22', '22', '23', '24', '24', '25', '25', '26', '27', '28', '28', '29', '30', '31', '32', '32', '33', '34',
     '35', '36', '37', '38', '39', '40', '42', '43', '44', '46', '47', '48', '49', '50', '51', '53', '54', '56', '57', '59']
d40_23={39-k:l34.index(l40[k]) for k in range(40)}
108/100: d40_23
108/101:
 l40=['21', '21', '22', '22', '23', '24', '24', '25', '25', '26', '27', '28', '28', '29', '30', '31', '32', '32', '33', '34',
     '35', '36', '37', '38', '39', '40', '42', '43', '44', '46', '47', '48', '49', '50', '51', '53', '54', '56', '57', '59']
d40_23={k:l34.index(l40[k]) for k in range(40)}
108/102: d40_23
108/103:
import netCDF4
import numpy as np
import datetime
from scipy.interpolate import griddata
import json

from pyproj import Proj
import sys,os,subprocess
from dtconvertor import dt2jul, jul2dt
108/104:
for v in ['mws','dic','nms_gas','nms_part']:
  with open(v+'.json', 'r') as jsonfile:
    exec(v+'=json.load(jsonfile)')
uts=['PPM',"ug m-3          "]
l34=['21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37',
     '38', '39', '40', '42', '43', '44', '46', '47', '48', '49', '50', '51', '53', '54', '56', '57', '59']
l40=['21', '21', '22', '22', '23', '24', '24', '25', '25', '26', '27', '28', '28', '29', '30', '31', '32', '32', '33', '34',
     '35', '36', '37', '38', '39', '40', '42', '43', '44', '46', '47', '48', '49', '50', '51', '53', '54', '56', '57', '59']
d40_34={k:l34.index(l40[k]) for k in range(40)}
byr=subprocess.check_output('pwd',shell=True).decode('utf8').strip('\n')[-2:]
#read a BC file as rate base
fname='/nas1/cmaqruns/2019base/data/bcon/BCON_v53_1912_run5_regrid_20191201_TWN_3X3'
nc = netCDF4.Dataset(fname,'r')
Vb=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/105:
rate={}
for v in nms_part:
  nms=nms_part[v]
  for nm in nms:
    if nm not in Vb[2]:sys.exit(v+' not in BCON file')
  avg=[np.mean(nc.variables[nm][:]) for nm in nms]
  sum_avg=sum(avg)
  if sum_avg==0:sys.exit('sum_avg==0')
  ratev=[avg[i]/sum_avg for i in range(len(avg))]
  rate.update({v:ratev})
for v in nms_gas:
  rate.update({v:[1.]})
108/106: nms
108/107: 'APOCI' in rate
108/108: rate
108/109: fname='1804.nc'
108/110:
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
#read the timestamp in nc and store at /expand the nc1
SDATE=[datetime.datetime.strptime(''.join([str(i, encoding='utf-8') for i in list(nc.variables[V[1][0]][t, :])]),\
 '%m/%d/%Y (%H:%M)') for t in range(nt)]
bdate,edate=SDATE[0],SDATE[-1]
delt=edate-bdate
ntA=int(delt.total_seconds()/3600.)
JuliHr=[int((bdate+datetime.timedelta(hours=t)).strftime("%Y%j%H")) for t in range(ntA)]
108/111: N='D0''
108/112: N='D0'
108/113:
tmps={'D'+i:'templateD'+i+'.ncV49K34' for i in '012'}
path='./'
fnameO=fname.replace('.nc',N+'.m3.nc')
if not os.path.exists(fnameO):os.system('cp '+path+tmps[N]+' '+fnameO)
nc1= netCDF4.Dataset(fnameO,'r+')
V1=[list(filter(lambda x:nc1.variables[x].ndim==j, [i for i in nc1.variables])) for j in [1,2,3,4]]
nv1=len(V1[3])
nt1,nlay1,nrow1,ncol1=nc1.variables[V1[3][0]].shape
108/114:  nt1<ntA
108/115:
  for t in range(ntA):
    nc1.variables['TFLAG'][t,0,0]=0
108/116: N='D1'
108/117:
tmps={'D'+i:'templateD'+i+'.ncV49K34' for i in '012'}
path='./'
fnameO=fname.replace('.nc',N+'.m3.nc')
if not os.path.exists(fnameO):os.system('cp '+path+tmps[N]+' '+fnameO)
nc1= netCDF4.Dataset(fnameO,'r+')
V1=[list(filter(lambda x:nc1.variables[x].ndim==j, [i for i in nc1.variables])) for j in [1,2,3,4]]
nv1=len(V1[3])
nt1,nlay1,nrow1,ncol1=nc1.variables[V1[3][0]].shape
108/118: ntA
108/119: edate
108/120: SDATE
108/121: len(SDATE)
108/122:
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
#read the timestamp in nc and store at /expand the nc1
SDATE=[datetime.datetime.strptime(''.join([str(i, encoding='utf-8') for i in list(nc.variables[V[1][0]][t, :])]),\
 '%m/%d/%Y (%H:%M)') for t in range(nt)]
bdate,edate=SDATE[0],SDATE[-1]
delt=edate-bdate
ntA=int(delt.total_seconds()/3600.)
JuliHr=[int((bdate+datetime.timedelta(hours=t)).strftime("%Y%j%H")) for t in range(ntA)]
108/123: fname
108/124: len(SDATE)
108/125: nt1<ntA
108/126:
  for t in range(ntA):
    nc1.variables['TFLAG'][t,0,0]=0
108/127:
nc1.SDATE=JuliHr[0]//100
nc1.STIME=JuliHr[0]%100*10000
var=np.zeros(shape=(ntA,nc1.NVARS,2))
var[:,:,0]=np.array([i//100 for i in JuliHr])[:,None]
var[:,:,1]=np.array([i%100  for i in JuliHr])[:,None]*10000
nc1.variables['TFLAG'][:,:,:]=var[:,:,:]
Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)

xlon, xlat = nc.variables['lon_0'][:].flatten(), np.flip(nc.variables['lat_0'][:].flatten())
lonm, latm = np.meshgrid(xlon, xlat)
x,y=pnyc(lonm,latm, inverse=False)

#interpolation indexing
x1d=[nc1.XORIG+nc1.XCELL*i for i in range(ncol1)]
y1d=[nc1.YORIG+nc1.YCELL*i for i in range(nrow1)]
x1,y1=np.meshgrid(x1d, y1d)
maxx,maxy=x1[-1,-1],y1[-1,-1]
minx,miny=x1[0,0],y1[0,0]
boo=(abs(x) <= (maxx - minx) /2+nc1.XCELL*10) & (abs(y) <= (maxy - miny) /2+nc1.YCELL*10)
idx = np.where(boo)
mp=len(idx[0])
xyc= [(x[idx[0][i],idx[1][i]],y[idx[0][i],idx[1][i]]) for i in range(mp)]

print('read the density of air')
dlay=np.array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
       17, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 32, 34, 35, 37, 39])
dens=np.zeros(shape=(ntA,40, nrow1, ncol1))
caldat=list(set([int((bdate+datetime.timedelta(hours=t)).strftime("%Y%m%d")) for t in range(ntA)]))
caldat.sort()
108/128:
for c in caldat:
  iday=caldat.index(c)
  fname='/nas1/cmaqruns/20'+byr+'base/data/mcip/RHO/RHO.'+str(c)+'.nc'
  ncr = netCDF4.Dataset(fname,'r')
  ntr=min(24,ncr.dimensions['TSTEP'].size)
  t1=iday*24
  t2=min(ntA,t1+min(24,ntr))
  hrs=t2-t1
  dens[t1:t2,:,:,:]=ncr.variables['DENS'][:hrs,:,:,:] *1E9 #(kg to microgram)
dens2=np.zeros(shape=(ntA,nlay1, nrow1, ncol1))
if nlay1==40:
  dens2[:]=dens[:]
  dd=d40_34
else:
  for k in range(nlay1):
    dens2[:,k,:,:]=dens[:,dlay[k],:,:]
  dd=[k for k in range(nlay1)]
var=np.zeros(shape=(nt, nlay, nrow, ncol))
zz=np.zeros(shape=(nt, nlay1, nrow1, ncol1))
var2=np.zeros(shape=(ntA,nlay1, nrow1, ncol1))
108/129: v="VAR_192_210_7_P0_L105_GLL0"
108/130:
  var[:,:,:,:]=np.flip(nc.variables[v][:,:,:,:], [1,2])
  for t in range(nt):
    c = np.array([var[t,:,idx[0][i], idx[1][i]] for i in range(mp)])
    for k in range(nlay1):
      zz[t,k,:,: ] = griddata(xyc, c[:,dd[k]], (x1, y1), method='linear')
108/131: t
108/132: nt
108/133:
  for t in range(0,ntA,3):
    t3=int(t/3)
    var2[t+0,:,:,:]=zz[t3,:,:,:]
    var2[t+1,:,:,:]=zz[t3,:,:,:]*2/3+zz[t3+1,:,:,:]*1/3
    var2[t+2,:,:,:]=zz[t3,:,:,:]*1/3+zz[t3+1,:,:,:]*2/3
108/134: v in nms_gas
108/135:     nms=nms_part[v]
108/136: nms
108/137: nm='APOCI'
108/138: rate[v][nms.index(nm)]
108/139:
    for nm in nms:
      nc1.variables[nm][:]+=var2[:] * rate[v][nms.index(nm)] * dens2[:]
108/140: bn
108/141: nm
108/142: nm in nc1
108/143: nm in nc1[:]
108/144: nm in V1[3]
108/145: len(V1[3])
108/146: ls
108/147: ls temp*
108/148: fname='templateD2.ncV49K34'
108/149: nc = netCDF4.Dataset(fname, 'r')
108/150: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/151: a=set(V1[3])-set(V[3])
108/152: len(a)
108/153: a=list(set(V1[3])-set(V[3]))
108/154: a.sort()
108/155: a[:5]
108/156: a
108/157: s=''
108/158:
for i in a:
    s+=i+','
108/159: s
108/160: a=list(set(V[3])-set(V1[3]))
108/161: a
108/162: len(V[3])
108/163: s=''
108/164:
for i in V[3]:
    s+='{:16s}'.format(i)
108/165: s
108/166: fname
108/167: fname='templateD1.ncV49K34'
108/168: run grb2m3.py 1804.nc D1
108/169: run grb2m3.py 1804.nc D1
108/170:  nc1.variables['TFLAG'].shape
108/171: var.shape
108/172: run grb2m3.py 1804.nc D1
108/173:
for v in ['mws','dic','nms_gas','nms_part']:
  with open(v+'.json', 'r') as jsonfile:
    exec(v+'=json.load(jsonfile)')
108/174: s=set()
108/175:
for i in nms_part.values():
    s+=set(i)
108/176:
for i in nms_part.values():
    s|=set(i)
108/177: len(s)
108/178: s
108/179:
for i in set(nms_part.values())|set(nms_gas.values()):
    s|=set(i)
108/180: set(nms_part.values())
108/181: list(nms_part.values())
108/182:
for i in nms_part.values()+nms_gas.values():
    s|=set(i)
108/183:
for i in nms_gas.values():
    s|=set(i)
108/184: len(s)
108/185: s
108/186: s=set()
108/187:
for i in nms_part.values():
    s|=set(i)
108/188:
for i in nms_gas.values():
    s|=set(i)
108/189: len(s)
108/190: s
108/191: nms_gas.values()
108/192: s=set()
108/193:
for i in nms_part.values():
    s|=set(i)
108/194:
for i in nms_gas.values():
    s|=i
108/195:
for i in nms_gas.values():
    s|=set([i])
108/196: s
108/197: len(s)
108/198: pwd
108/199: fname='templatesChina_81kV49.nc'
108/200: nc = netCDF4.Dataset(fname, 'r')
108/201: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/202: len(V[3])
108/203: set(V[3])-set(V1[3])
108/204: set(V1[3])-set(V[3])
108/205: len(V1[3])
108/206: fname='templateD2V49K34.nc'
108/207: nc = netCDF4.Dataset(fname, 'r')
108/208: fname='templateD2_V49K34.nc'
108/209: nc = netCDF4.Dataset(fname, 'r')
108/210: fname='templateD2.ncV49K1'
108/211: nc = netCDF4.Dataset(fname, 'r')
108/212: V1=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/213: len(V1[3])
108/214: set(V1[3])-set(V[3])
108/215: nc.close()
108/216: nc1.close()
108/217: run grb2m3.py 1804.nc D1
108/218: run grb2m3.py 1804.nc D1
109/1:
import netCDF4
import numpy as np
import datetime
from scipy.interpolate import griddata
import json

from pyproj import Proj
import sys,os,subprocess
from dtconvertor import dt2jul, jul2dt
110/1:
import netCDF4
import numpy as np
import datetime
from scipy.interpolate import griddata
import json

from pyproj import Proj
import sys,os,subprocess
from dtconvertor import dt2jul, jul2dt
110/2: bdate=datetime.datetime(2018,3,31)
110/3: sdate=[bdate+datetime.deltime(days=i) for i in range(10)]
110/4: sdate=[bdate+datetime.timedelta(days=i) for i in range(10)]
110/5: sdaet
110/6: sdate
110/7: !grep strf ~/bin/*py
110/8: !grep strp ~/bin/*py
110/9: bdate.strptime(%Y%m%d)
110/10: bdate.strftime(%Y%m%d)
110/11: bdate.strftime('%Y%m%d')
110/12:
for t in range(10):
    ymd=sdate[t].strftime(%Y%m%d)
    fname='1804/REAS_CWBWRF_15k.'+ymd+'.nc'
110/13:
for t in range(10):
    ymd=sdate[t].strftime('%Y%m%d')
    fname='1804/REAS_CWBWRF_15k.'+ymd+'.nc'
    nc = netCDF4.dataset(fname,'r+')
    nc.SDATE,nc.STIME=dt2jul(sdate[t])
    for h in range(25):
        nc['TFLAG'][h,:,0],nc['TFLAG'][h,:,0]=dt2jul(sdate[t]+datatime.timedelta(hours=h))
        for v in V[3]:
            nc[v][h,0,:,:]=nc[v][3,0,:,:]
    nc.close()
110/14: fname
110/15: nc = netCDF4.Dataset(fname, 'r')
110/16: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
110/17: len(V[3])
110/18:
for t in range(10):
    ymd=sdate[t].strftime('%Y%m%d')
    fname='1804/REAS_CWBWRF_15k.'+ymd+'.nc'
    nc = netCDF4.Dataset(fname,'r+')
    nc.SDATE,nc.STIME=dt2jul(sdate[t])
    for h in range(25):
        nc['TFLAG'][h,:,0],nc['TFLAG'][h,:,0]=dt2jul(sdate[t]+datatime.timedelta(hours=h))
        for v in V[3]:
            nc[v][h,0,:,:]=nc[v][3,0,:,:]
    nc.close()
110/19:
for t in range(10):
    ymd=sdate[t].strftime('%Y%m%d')
    fname='1804/REAS_CWBWRF_15k.'+ymd+'.nc'
    nc = netCDF4.Dataset(fname,'r+')
    nc.SDATE,nc.STIME=dt2jul(sdate[t])
    for h in range(25):
        nc['TFLAG'][h,:,0],nc['TFLAG'][h,:,0]=dt2jul(sdate[t]+datetime.timedelta(hours=h))
        for v in V[3]:
            nc[v][h,0,:,:]=nc[v][3,0,:,:]
    nc.close()
110/20: pwd
110/21:
for t in range(10):
    ymd=sdate[t].strftime('%Y%m%d')
    fname='1804/REAS_CWBWRF_15k.'+ymd+'.nc'
    nc = netCDF4.Dataset(fname,'r+')
    nc.SDATE,nc.STIME=dt2jul(sdate[t])
    for h in range(25):
        nc['TFLAG'][h,:,0],nc['TFLAG'][h,:,1]=dt2jul(sdate[t]+datetime.timedelta(hours=h))
        for v in V[3]:
            nc[v][h,0,:,:]=nc[v][3,0,:,:]
    nc.close()
110/22: history
108/219: pwd
108/220: !lst
108/221: fname='1804D1.m3.nc'
108/222: nc = netCDF4.Dataset(fname, 'r')
108/223: nc = netCDF4.Dataset(fname, 'r+')
108/224: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/225: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
108/226:
for v in ['APOCJ','APNCOMJ']:
    nc[v][:]=1.E-7
108/227: [i for i in V[3] if i[:2]='AP']
108/228: [i for i in V[3] if i[:2]=='AP']
108/229:
for v in ['APOCJ','APNCOMI','APOCI']:
    nc[v][:]=1.E-7
108/230: nc.close()
108/231: fname='/nas1/cmaqruns/2018base/data/bcon/BC_from_EAC4/BCON_v53_1804_run5_regrid_20180331_EAsia_81K'
108/232: nc = netCDF4.Dataset(fname, 'r')
108/233: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/234: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
108/235: [len(i) for i in V]
108/236: var=np.zeros(shape=nc[V[2][0]].shape)
108/237:
for iv in range(51):
    var[iv,:,:,:]=nc[V[2][iv]][:,:,:]
108/238: var.shape
108/239: var=np.zeros(shape=[51]+nc[V[2][0]].shape)
108/240: var=np.zeros(shape=[51]+list(nc[V[2][0]].shape))
108/241: var.shape
108/242:
for iv in range(51):
    var[iv,:,:,:]=nc[V[2][iv]][:,:,:]
108/243: nc[V[2][iv]].shape
108/244: nt,nlay,nbnd=nc[V[2][iv]].shape
108/245: var=np.zeros(shape=(51,nt,nlay,nbnd))
108/246:
for iv in range(51):
    var[iv,:,:,:]=nc[V[2][iv]][:,:,:]
108/247: var.shape
108/248:
for iv in range(51):
    v=V[2][iv]
    if v=='TFLAG':continue
    var[iv,:,:,:]=nc[V[2][iv]][:,:,:]
108/249: V0=V[2][:]
108/250: ls
108/251: cd /home/cmaqruns/2018base/data/bcon
108/252: ls
108/253: fname='BCON_v53_1804_run5_regrid_20180331_EAsia_81K'
108/254: nc = netCDF4.Dataset(fname, 'r+')
108/255:
for iv in range(51):
    v=V0[iv]
    if v=='TFLAG':continue
    nc[v][:,:,:]=var[iv,:,:,:]
108/256: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/257:
for iv in range(51):
    v=V0[iv]
    if v=='TFLAG':continue
    if v not in V[2]:continue
    nc[v][:,:,:]=var[iv,:,:,:]
108/258: nc.close()
108/259: fname='/nas1/cmaqruns/2018base/data/bcon/BC_from_EAC4/BCON_v53_1804_run6_regrid_20180331_EAsia_81K'
108/260: nc = netCDF4.Dataset(fname, 'r')
108/261: fname='/nas1/cmaqruns/2018base/data/bcon/BC_from_EAC4/BCON_v53_1804_run6_regrid_20180404_EAsia_81K'
108/262: nc = netCDF4.Dataset(fname, 'r')
108/263:
for iv in range(51):
    v=V0[iv]
    if v=='TFLAG':continue
    var[iv,:,:,:]=nc[v][:,:,:]
108/264: fname='BCON_v53_1804_run6_regrid_20180404_EAsia_81K'
108/265: nc = netCDF4.Dataset(fname, 'r+')
108/266:
for iv in range(51):
    v=V0[iv]
    if v=='TFLAG':continue
    if v not in V[2]:continue
    nc[v][:,:,:]=var[iv,:,:,:]
108/267: nc.close()
108/268: hidyoty
108/269: history
108/270: cd /nas1/ecmwf/reanalysis/gribs18
108/271: fname='a'
108/272: nc = netCDF4.Dataset(fname, 'r+')
108/273: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/274: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
108/275: nt,nlay,nrow,ncol
108/276: var=np.zeros(shape=(nt,nlay,nrow,ncol))
108/277: var[:]=nc[V[3][0]][:]
108/278: idx=np.where(np.isnan(var))
108/279: len(idx[0])
108/280: [len(i) for i in idx]
108/281: np.max(var)
108/282: a=np.max(var)
108/283: idx=np.where(var==a)
108/284: [len(i) for i in idx]
108/285: len(var.flatten())
108/286: b=np.where(var==a,0,var)
108/287: np.max(b)
108/288: nc[V[3][0]][:]=b[:]
108/289: nc.close()
108/290: !top
108/291: cd /nas1/ecmwf/reanalysis/gribs18/POST
108/292: ls
108/293: fname='1804D1.m3.nc'
108/294: nc = netCDF4.Dataset(fname, 'r')
108/295: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/296:
nc0 = netCDF4.Dataset(fname, 'r')
V0=[list(filter(lambda x:nc0[x].ndim==j, [i for i in nc0])) for j in [1,2,3,4]]
108/297:
nc0 = netCDF4.Dataset(fname, 'r')
V0=[list(filter(lambda x:nc0[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
108/298: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
108/299: nt0,nlay0,nrow0,ncol0=(nc0[V0[3][0]].shape[i] for i in range(4))
108/300: from dtconvertor import dt2jul, jul2dt
108/301: sdatetime=[jul2dt(nc0['TFLAG'][t,0,:]) for t in range(nt0)]
108/302: sdate=list(set([i.strftime("%Y%m%d") for i in sdatetime]))
108/303: sdate.sort()
108/304: sdate
108/305: fnameT='CCTM_ACONC_v53_gcc_1804_run5_YMD_EAsia_81K_11.nc'
108/306: ymd=sdate[0]
108/307: fnameT.replace('YMD',ymd)
108/308:  os.path.exists(fnameT.replace('YMD',ymd))
108/309: sdates=[i for i in sdatetime if i.hours==0]
108/310: sdates=[i for i in sdatetime if i.hour==0]
108/311: sdates
108/312: tmd
108/313: ymd
108/314: sdatec=[i.strftime("%Y%m%d") for i in sdates]
108/315: sdatetime.index(sdates[sdatec.index(ymd)])
108/316: ymd='20180401'
108/317: sdatetime.index(sdates[sdatec.index(ymd)])
108/318: v
108/319:
  nc = netCDF4.Dataset(fname, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
  rs,cs=(nrow0-nrow)/2,(ncol0-ncol)/2
  t0=sdatetime.index(sdates[sdatec.index(ymd)])
108/320:     nc[v][:,:,:,:]=nc0[t0:t0+nt,:,rs:rs+nrow,cs:cs+ncol]
108/321: nc0 = netCDF4.Dataset(fname, 'r')
108/322:
V0=[list(filter(lambda x:nc0[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nrow0,ncol0=(nc0[V0[3][0]].shape[i] for i in range(4))
108/323: rs,cs
108/324: fname
108/325:
fnameT='CCTM_ACONC_v53_gcc_1804_run5_YMD_EAsia_81K_11.nc'
fname=fnameT.replace('YMD',ymd)
108/326:
  nc = netCDF4.Dataset(fname, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
  rs,cs=(nrow0-nrow)/2,(ncol0-ncol)/2
  t0=sdatetime.index(sdates[sdatec.index(ymd)])
108/327: rs,cs
108/328:   rs,cs=(nrow0-nrow)//2,(ncol0-ncol)//2
108/329: rs,cs
108/330:     nc[v][:,:,:,:]=nc0[t0:t0+nt,:,rs:rs+nrow,cs:cs+ncol]
108/331:     nc[v][:,:,:,:]=nc0[v][t0:t0+nt,:,rs:rs+nrow,cs:cs+ncol]
108/332: run fil_aconc.py
108/333: run fil_aconc.py
108/334: run fil_aconc.py
108/335: cd /nas1/cmaqruns/2018base/data/land
108/336: import libtiff
108/337: !pip install libtiff
108/338: import libtiff
108/339:
fname='HarvestedAreaYield175Crops_Geotiff/HarvestedAreaYield175Crops_Geotiff/yautia_HarvAreaYield_Geotiff/yautia_Production.tif'
from libtiff import TIFF
tif = TIFF.open('fname')
108/340:
fname='HarvestedAreaYield175Crops_Geotiff/HarvestedAreaYield175Crops_Geotiff/yautia_HarvAreaYield_Geotiff/yautia_Production.tif'
from libtiff import TIFF
tif = TIFF.open(fname)
108/341: tif.info()
108/342: tif.info().ImageWidth
108/343: tif.info.ImageWidth
108/344: tif.info('ImageWidth')
108/345: info=tif.info().split('\n')
108/346: len(info)
108/347: info
108/348: infoD={i.split()[0]:i.split()[1] for i in info}
108/349: infoD['ImageWidth']
108/350: infoD={i.split(': ')[0]:i.split()[1] for i in info}
108/351: infoD['ImageWidth']
108/352: ls -r *tif>fnames.txt
108/353: pwd
108/354: lsd
108/355: !lsd
108/356: !lsr "*tif" >fnames.txt
108/357: !head fnames.txt
108/358: !lsf "*tif" >fnames.txt
108/359: !head fnames.txt
108/360: !lsf "*tif"|awkk 9 >fnames.txt
108/361: !head fnames.txt
108/362: !wc fnames.txt
108/363:
with open('fnames.txt','r') as f:
  d=[i.strip('\n').split('/')[-1] for i in f]
108/364: spec=[i.split('_')[0] for i in d]
108/365: spec=set([i.split('_')[0] for i in d])
108/366: len(spec)
108/367: len(d)
108/368: set([len([i for i in d if i.split('_')==j]) for j in spec])
108/369: s=set()
108/370:
for i in spec:
    s|=set(len([j for j in d if i in j]))
108/371: i
108/372: j
108/373: s
108/374:
for i in spec:
    s|=set([len([j for j in d if i in j])])
108/375: len(s)
108/376: s
108/377:
i6=[]
for i in spec:
    if len([j for j in d if i in j])==6:i6.append(i)
108/378: i6[:5]
108/379: i='popcorn'
108/380: [j for j in d if i in j]
108/381: i12=[]
108/382:
i12=[]
for i in spec:
    if len([j for j in d if i in j])==12:i12.append(i)
108/383: i12[:5]
108/384: i='chestnut'
108/385: [j for j in d if i in j]
108/386: !findc chestnut_Production.tif
108/387: !lsr chestnut_Production.tif
108/388: !tiffinfo ./HarvestedAreaYield175Crops_Geotiff/GeoTiff/chestnut/chestnut_Production.tif
108/389: tiffinfo ./HarvestedAreaYield175Crops_Geotiff/HarvestedAreaYield175Crops_Geotiff/chestnut_HarvAreaYield_Geotiff/chestnut_Production.tif
108/390: !tiffinfo ./HarvestedAreaYield175Crops_Geotiff/HarvestedAreaYield175Crops_Geotiff/chestnut_HarvAreaYield_Geotiff/chestnut_Production.tif
108/391: !tiffinfo ./HarvestedAreaYield175Crops_Geotiff/HarvestedAreaYield175Crops_Geotiff/chestnut_HarvAreaYield_Geotiff/chestnut_Production.tif >b
108/392: !tiffinfo ./HarvestedAreaYield175Crops_Geotiff/HarvestedAreaYield175Crops_Geotiff/chestnut_HarvAreaYield_Geotiff/chestnut_Production.tif >&b
108/393: !tiffinfo ./HarvestedAreaYield175Crops_Geotiff/GeoTiff/chestnut/chestnut_Production.tif >&a
108/394: !diff a b
108/395: s
108/396:
i24=[]
for i in spec:
    if len([j for j in d if i in j])==24:i24.append(i)
108/397: i24[:5]
108/398: i24[-5:]
108/399: i='grape'
108/400: [j for j in d if i in j]
108/401: s=set()
108/402:
for i in spec:
    s|=set([len([j for j in d if i+'_' in j])])
108/403: s
108/404:
i12=[]
for i in spec:
    if len([j for j in d if i+'_' in j])==12:i12.append(i)
108/405: i12[:5]
108/406: i='chestnut'
108/407: [j for j in d if i+'_' in j]
108/408: *
108/409: 175*6
108/410: pwd
108/411: ls
108/412: fname='beld4.EAsia_81K.ncf'
108/413: nc = netCDF4.Dataset(fname, 'r')
108/414: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/415: v3=[i.lowwer() for i in V[3]]
108/416: v3=[i.lower() for i in V[3]]
108/417: v3[:5]
108/418: set(v3)-set(spec)
108/419: type(spec)
108/420: len(set(v3)-spec)
108/421: len(v3)
108/422: len(spec)
108/423: v3=[i for i in v3 if 'modis' not in v3]
108/424: len(v3)
108/425: [i for i in v3 if 'modis' in v3]
108/426: [i for i in v3 if 'usgs' in v3]
108/427: ls
108/428: v3[:5]
108/429: v3=list(set([i.replace('_ir') for i in v3]))
108/430: v3=list(set([i.replace('_ir','') for i in v3]))
108/431: len(v3)
108/432: v3.sort()
108/433: v3[:5]
108/434: spec[:5]
108/435: spec=list(spec)
108/436: spec.sort()
108/437: spec[:5]
108/438: set(v3)-set(spec)
108/439: len(set(v3)-set(spec))
108/440: [i for i in spec if 'potatoe' in i]
108/441: [i for i in spec if 'pota' in i]
108/442: [i for i in spec if 'spru' in i]
108/443: !df -h
108/444: s='        float L1_Bulk_D(TSTEP, LAY, ROW, COL) ;        float L1_Cation(TSTEP, LAY, ROW, COL) ;        float L1_Field_C(TSTEP, LAY, ROW, COL) ;        float L1_PH(TSTEP, LAY, ROW, COL) ;        float L1_Porosity(TSTEP, LAY, ROW, COL) ;        float L1_SoilNum(TSTEP, LAY, ROW, COL) ;        float L1_Wilt_P(TSTEP, LAY, ROW, COL) ;        float L2_Bulk_D(TSTEP, LAY, ROW, COL) ;        float L2_Cation(TSTEP, LAY, ROW, COL) ;        float L2_Field_C(TSTEP, LAY, ROW, COL) ;        float L2_PH(TSTEP, LAY, ROW, COL) ;        float L2_Porosity(TSTEP, LAY, ROW, COL) ;        float L2_Wilt_P(TSTEP, LAY, ROW, COL) ;'
108/445: s=s.split(';')
108/446: s=[i.replace('float','').replace('(TSTEP, LAY, ROW, COL)','') for i in s]
108/447: s
108/448: s=[i.replace(' ','') for i in s]
108/449: s
108/450: s.sort()
108/451: s
108/452: set([i[4:] for i in s])
108/453: set([i[3:] for i in s])
108/454: s=s[1:]
108/455: set([i[3:] for i in s])
108/456: len(s)
108/457: a=[i[3:] for i in s]
108/458: a.sort()
108/459: a
108/460: df=read_csv('/nas1/cmaqruns/2016base/data/land/most_freq.L1_SoilNum.csv')
108/461: df.head()
108/462: np.min(df.L1_SoilNum>0)
108/463: np.min([i for i in df.L1_SoilNum if i>0])
108/464: np.max([i for i in df.L1_SoilNum if i>0])
108/465: fname
108/466: ls
108/467: pwd
108/468: cd /nas1/cmaqruns/2016base/data/land
108/469: ls
108/470: pwd
108/471: cd ../../2016base
108/472: cd ../../../2016base
108/473: cd data/land
108/474: fname='epic_festc1.4_20180516/2016_US1_soil_bench.nc'
108/475: nc = netCDF4.Dataset(fname, 'r')
108/476: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/477: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
108/478:
for v in V[3]:
    a=nc[v][:]
    print(v,np.max(a),np.min(a))
108/479:
for v in V[3]:
    a=nc[v][:]>0
    print(v,np.max(a),np.min(a))
108/480:
for v in V[3]:
    a=[i for i in nc[v][:].flatten() if i>0]
    print(v,np.max(a),np.min(a))
108/481: !vi aaa
108/482:
with open('aaa','r') as f:
  d=[i.strip('\n') for i in f]
108/483: d
108/484:
with open('aaa','r') as f:
  d=[i.strip('\n').split(',')[0] for i in f]
108/485: d[:5]
108/486: d=[i for i in d if '_ir' not in i]
108/487: d
108/488: d=[i for i in d if len(i)>0]
108/489: d=[i.replace("'",'').replace(' ','') for i in d]
108/490: d
108/491: spec
108/492: d=[i.lower() for i in d]
108/493: set(d)-set(spec)
108/494: [for i in spec if 'pea' in i]
108/495: [i for i in spec if 'pea' in i]
108/496: 'goober' in spec
108/497: [i for i in spec if 'nut' in i]
108/498: [i for i in spec if 'grass' in i]
108/499: [i for i in spec if 'str' in i]
108/500: set(d)-set(spec)
108/501: [i for i in spec if 'bean' in i]
108/502: [i for i in spec if 'col' in i]
108/503: [i for i in spec if 'rape' in i]
108/504: [i for i in spec if 'corn' in i]
108/505: [i for i in d if 'corn' in i]
108/506: [i for i in d if 'pota' in i]
108/507: [i for i in spec if 'pota' in i]
108/508: [i for i in spec if 'sorgh' in i]
108/509: [i for i in spec if 'wheat' in i]
108/510: [i for i in spec if 'grass' in i]
108/511: [i for i in spec if 'ha' in i]
108/512: [i for i in spec if 'str' in i]
108/513: [i for i in spec if 'fod' in i]
108/514: [i for i in spec if 'for' in i]
108/515: [i for i in d if 'gras' in i]
108/516: [i for i in d if 'crop' in i]
108/517: [i for i in spec if 'crop' in i]
108/518: [i for i in spec if 'oth' in i]
108/519: pwd
108/520: lsd
108/521: cd ../../../2018base/data/land
108/522: !lsd
108/523: cd HarvestedAreaYield175Crops_Geotiff/
108/524: ls
108/525: cd HarvestedAreaYield175Crops_Geotiff/
108/526: hs
108/527: ls
108/528: [i for i in d if 'oat' in i]
108/529: [i for i in spec if 'oat' in i]
108/530: [i for i in spec if 'mill' in i]
108/531: [i for i in d if 'millet' in i]
108/532: !vi aaa
108/533: pwd
108/534: d21_175={}
108/535:
with open('aaa','r') as f:
  for i in f:
    ss=i.split('|')      
    d21_175.update({ss[1]:ss[2]})
108/536: d21_175
108/537: d21_175.update({j:j for j in [i for i in d if i in spec]})
108/538: d21_175
108/539: len(d21_175)
108/540: d21_175.update({'beansedible':['broadbean', 'greenbean', 'greenbroadbean', 'stringbean']})
108/541: d21_175.update({i:'wheat' for i in ['broadbean', 'wheat_winter']})
108/542: len(d21_175)
108/543: fnameO='d21_175.json'
108/544:
with open(fnameO,'w', newline='') as jsonfile:
    json.dump(d21_175, jsonfile)
108/545: d21_175
108/546: !vi d21_175.json
108/547: cd /nas1/cmaqruns/2016base/data/land/epic_festc1.4_20180516
108/548: fname='2016_US1_soil_bench.nc2'
108/549: nc = netCDF4.Dataset(fname, 'r')
108/550: v='SoilNum'
108/551: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/552: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
108/553: nt,nlay,nrow,ncol
108/554: nc[v][:,:,33,33]
108/555: V
108/556: v='L1_SoilNum'
108/557: nc[v][:,:,33,33]
108/558: v='L2_PH'
108/559: nc[v][:,:,33,33]
108/560: cd /nas1/cmaqruns/2018base/data/land/InternationlWaterManagementInstitute
108/561: ls
108/562: history
108/563: ls
108/564: fname='giam_28_classes_global.tif'
108/565: tif = TIFF.open(fname,mode='r')
108/566: image = tif.read_image()
108/567: !pip install rasterio
108/568: import rasterio
108/569: img = rasterio.open(fname)
108/570: nx,ny=img.width,img.height
108/571: nx,ny
108/572: img.crs
108/573: dir(img)
108/574: dir(img) >& a
108/575: dir(img)|more
108/576: dir(img)
108/577: a=dir(img)
108/578: a[:5]
108/579: a[5:25]
108/580: a[25:45]
108/581: a[45:75]
108/582: a[75:105]
108/583: dd=img.read()
108/584: dd.shape
108/585: dd[0,:5,:5]
108/586: img.count
108/587: s=set(dd.flatten())
108/588: len(s)
108/589: ls
108/590: s
108/591: nx,ny,nz=img.width,img.height,img.count
108/592: nx,ny,nz
108/593: data.shape
108/594: dd.shape
108/595: ls
108/596: cat giam_28_classes_global.tfw
108/597:
from pyproj import Proj
Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
Xcent, Ycent = twd97.fromwgs84(Latitude_Pole, Longitude_Pole)
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
108/598: import twd97
108/599:
from pyproj import Proj
Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
Xcent, Ycent = twd97.fromwgs84(Latitude_Pole, Longitude_Pole)
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
108/600: fname='temp.nc'
108/601: nc = netCDF4.Dataset(fname, 'r+')
108/602: 40008/nx
108/603: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/604: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
108/605: var=np.zeros(shape=(29,nrow,ncol))
108/606: lat_1d=[90-(180./(ny-1))*i for i in range(ny)]
108/607: lon_1d=[-180+(360./(nx-1))*i for i in range(nx)]
108/608: lonm, latm = np.meshgrid(lon_1d, lat_1d)
108/609: lonm.shape
108/610: lonm[:5,:5]
108/611: x,y=pnyc(lonm,latm, inverse=False)
108/612: np.searchsorted(lonm,60)
108/613: np.searchsorted(lon_1d,60)
108/614: i=np.searchsorted(lon_1d,60)
108/615: lon_1d[i,i+1]
108/616: lon_1d[i],lon_1d[i+1]
108/617: i=np.searchsorted(lon_1d,60)-1
108/618: lon_1d[i],lon_1d[i+1]
108/619: j=np.searchsorted(lat_1d,-10)-1
108/620: lat_1d[j],lat_1d[j+1]
108/621: j
108/622: lat=lat_1d.sort()
108/623: j=np.searchsorted(lat,-10)-1
108/624: lat=lat_1d[:]
108/625: lat.sort()
108/626: j=np.searchsorted(lat,-10)-1
108/627: j
108/628: ny
108/629: jj=ny-j
108/630: lat_1d[jj],lat_1d[jj+1]
108/631: jj
108/632: jj=jj+1
108/633: lat_1d[jj],lat_1d[jj+1]
108/634: jj=jj-2
108/635: lat_1d[jj],lat_1d[jj+1]
108/636: jj=jj-1
108/637: lat_1d[jj],lat_1d[jj+1]
108/638: lat_1d=[90-(180./(ny-1))*i for i in range(ny)]
108/639: lat_1d[jj],lat_1d[jj+1]
108/640: lat=lat_1d[:]
108/641: lat.sort()
108/642: j=np.searchsorted(lat,-10)-1
108/643: jj=ny-j
108/644: lat_1d[jj],lat_1d[jj+1]
108/645: jj=jj-1
108/646: lat_1d[jj],lat_1d[jj+1]
108/647: jj=jj-1
108/648: lat_1d[jj],lat_1d[jj+1]
108/649: jj
108/650: j=np.searchsorted(lat,50)-1
108/651: j2=ny-j
108/652: lat_1d[j2],lat_1d[j2+1]
108/653: j2=j2-1
108/654: lat_1d[j2],lat_1d[j2+1]
108/655: j2=j2-1
108/656: lat_1d[j2],lat_1d[j2+1]
108/657: j2
108/658: dd.shape
108/659: i
108/660: data=dd[0,26952:,4479+1:11199+1]
108/661: data.shape
108/662: data=dd[0,4479+1:11199+1,26952:]
108/663: data.shape
108/664:
lon_1d=lon_1d[26952:]
lat_1d=lat_1d[4479+1:11199+1]
108/665:
lonm, latm = np.meshgrid(lon_1d, lat_1d)
x,y=pnyc(lonm,latm, inverse=False)
108/666: nc1=nc
108/667: V1=[list(filter(lambda x:nc1.variables[x].ndim==j, [i for i in nc1.variables])) for j in [1,2,3,4]]
108/668: nt1,nlay1,nrow1,ncol1=nc1.variables[V1[3][0]].shape
108/669: x1d=[nc1.XORIG+nc1.XCELL*i for i in range(ncol1)]
108/670: y1d=[nc1.YORIG+nc1.YCELL*i for i in range(nrow1)]
108/671: x1,y1=np.meshgrid(x1d, y1d)
108/672: maxx,maxy=x1[-1,-1],y1[-1,-1]
108/673: minx,miny=x1[0,0],y1[0,0]
108/674: boo=(abs(x) <= (maxx - minx) /2+nc1.XCELL*10) & (abs(y) <= (maxy - miny) /2+nc1.YCELL*10)
108/675: idx = np.where(boo)
108/676: mp=len(idx[0])
108/677: xyc= [(x[idx[0][i],idx[1][i]],y[idx[0][i],idx[1][i]]) for i in range(mp)]
108/678: from scipy.interpolate import griddata
108/679: c=np.array([data[idx[0][i], idx[1][i]] for i in range(mp)])
108/680: c.shape
108/681: mp
108/682: c=data[idx[0][:], idx[1][:]]
108/683: c.shape
108/684: z = griddata(xyc, c, (x1, y1), method='linear')
108/685: z.shape
108/686: i,j=z.shape//2
108/687: (i,j)=z.shape//2
108/688: z.shape//2
108/689: a=np.array(z.shape)
108/690: i,j=a//2
108/691: i,j
108/692: z[i,j]
108/693: nc[V[3][0]][0,0,:,:]=z
108/694: nc.close()
108/695: pwd
108/696: %history -f his.txt
108/697:
fname='giam_28_classes_global.tif'
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
lat_1d=[90-(180./(ny-1))*i for i in range(ny)]
lon_1d=[-180+(360./(nx-1))*i for i in range(nx)]
108/698: data=img.read()
108/699: lonm, latm = np.meshgrid(lon_1d, lat_1d)
108/700: idx=np.where(data>0)
108/701: DD={'lon':lonm[idx[0],idx[1]],'lat':latm[idx[0],idx[1]],'irr':data[0,idx[0],idx[1]]}
108/702: df=DataFrame(DD)
108/703: df.set_index('lon').to_csv('irr.csv')
108/704: len(df)
108/705: len(data.flatten())
108/706: len(idx)
108/707: DD={'lon':lonm[idx[1],idx[2]],'lat':latm[idx[1],idx[2]],'irr':data[0,idx[1],idx[2]]}
108/708: df=DataFrame(DD)
108/709: df.set_index('lon').to_csv('irr.csv')
108/710: boo=(df.lon>=60&df.lon<=180)&(df.lon>=-10&df.lon<=50)
108/711: boo=(df.lon>=60&df.lon<=180)&(df.lat>=-10&df.lat<=50)
108/712: boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
108/713: df1=df.loc[boo].reset_index(drop=True)
108/714: len(df1)
108/715: dx,dy=360./(nx-1),180./(ny-1)
108/716: dx,dy
108/717: df1['lonn']=df.lon-dx/2
108/718: df1['lonx']=df.lon+dx/2
108/719: df1['latx']=df.lat+dy/2
108/720: df1['latn']=df.lat-dy/2
108/721: x,y=pnyc(df1.lonn,df1.latn, inverse=False)
108/722: x,y=pnyc(list(df1.lonn),list(df1.latn), inverse=False)
108/723: xn,yn=pnyc(list(df1.lonn),list(df1.latn), inverse=False)
108/724: xx,yn=pnyc(list(df1.lonx),list(df1.latn), inverse=False)
108/725: xn,yx=pnyc(list(df1.lonn),list(df1.latx), inverse=False)
108/726: xx,yx=pnyc(list(df1.lonx),list(df1.latx), inverse=False)
108/727: his
108/728: history
108/729: xn[:5]
108/730:
for i in 'xy':
    for j in 'nx':
        exec(i+j+'=np.array('+i+j+',dtype=int)')
108/731: xn[:5]
108/732: xx[:5]
108/733:
for i in 'xy':
    for j in 'nx':
        exec(i+j+'=np.array('+i+j+'/1000,dtype=int)*1000')
108/734: xx[:5]
108/735: xn[:5]
108/736: yn[:5]
108/737: yx[:5]
108/738: df1.head()
108/739: df1['latn']=df1.lat-dy/2
108/740: df1['lonn']=df1.lon-dx/2
108/741: df1['latx']=df1.lat+dy/2
108/742: df1['lonx']=df1.lon+dx/2
108/743: xn,yn=pnyc(list(df1.lonn),list(df1.latn), inverse=False)
108/744: xn,yx=pnyc(list(df1.lonn),list(df1.latx), inverse=False)
108/745: xx,yn=pnyc(list(df1.lonx),list(df1.latn), inverse=False)
108/746: xx,yx=pnyc(list(df1.lonx),list(df1.latx), inverse=False)
108/747:
for i in 'xy':
    for j in 'nx':
        exec(i+j+'=np.array('+i+j+'/1000,dtype=int)*1000')
108/748: type(xx)
108/749:
for i in 'xy':
    for j in 'nx':
        exec('a=np.array('+i+j+')')
        exec(i+j+'=np.array(a/1000,dtype=int)*1000')
108/750: xn[:5]
108/751: xx[:5]
108/752: yn[:5]
108/753: yx[:5]
108/754:
for i in 'xy':
    for j in 'nx':
        ij=i+j
        exec('a=np.array('+ij+')')
        exec('df1["'+ij+'"]=np.array(a/1000,dtype=int)*1000')
108/755: df1.head()
108/756:
for i in 'xy':
    for j in 'nx':
        ij=i+j
        exec('a=np.array('+ij+')')
        exec('df1["'+ij+'"]=np.array(a/1000,dtype=int)')
108/757: df1.head()
108/758: nc.XORIG
108/759: fname
108/760: fname='temp.nc'
108/761: nc = netCDF4.Dataset(fname, 'r')
108/762: x,y=pnyc(list(df1.lon),list(df1.lat), inverse=False)
108/763: df1['ix']=[(i-nc.XORIG)/nc.XCELL for i in x]
108/764: x=np.array(x)
108/765: y=np.array(y)
108/766: df1['ix']=(x-nc.XORIG)/nc.XCELL
108/767: df1['iy']=(y-nc.YORIG)/nc.YCELL
108/768: df1.head()
108/769: df1['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
108/770: df1['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
108/771: df1.head()
108/772: min(df1.ix)
108/773: min(df1.iy)
108/774: boo=(df1.ix==37)&(df1.iy==458)
108/775: df1.loc[boo]
108/776: df1['ixy']=[str(i)+'_'+str(j) for i,j in zip(df1.ix,df1,iy)]
108/777: df1['ixy']=[str(i)+'_'+str(j) for i,j in zip(df1.ix,df1.iy)]
108/778: var.shape()
108/779: nrow,ncol
108/780: var=np.zeros(shape=(29,nrow,ncol))
108/781: len(set(df1.ixy))
108/782: df2=df1.loc[df1.ix>=0&df1.ix<ncol&df1.iy>=0&df1.iy<nrow].reset_index(drop=True)
108/783: df2=df1.loc[(df1.ix>=0)&(df1.ix<ncol)&(df1.iy>=0)&(df1.iy<nrow)].reset_index(drop=True)
108/784: len(df1),len(df2)
108/785:
for ixy in set(df1.ixy):
  a=df2.loc[df1.ixy==ixy]  
  ix,iy=(int(i) for i in ixy.split('_'))
  for i in set(a.irr):
    var[i,iy,ix]=len(a.loc[a.irr==i])
108/786: ixy
108/787: ncol
108/788: nrow
108/789: df1.head()
108/790: df2.head()
108/791: a.head()
108/792: var=np.zeros(shape=(29,nrow,ncol))
108/793:
for ixy in set(df2.ixy):
  a=df2.loc[df2.ixy==ixy]  
  ix,iy=(int(i) for i in ixy.split('_'))
  for i in set(a.irr):
    var[i,iy,ix]=len(a.loc[a.irr==i])
108/794: fname='gmlulca_10classes_global.tif'
108/795: img = rasterio.open(fname)
108/796: data=img.read()
108/797: data.shape
108/798: nx,ny,nz=img.width,img.height,img.count
108/799: dx,dy=360./(nx-1),180./(ny-1)
108/800: dx,dy
108/801:
for ixy in set(df2.ixy):
  a=df2.loc[df2.ixy==ixy]  
  ix,iy=(int(i) for i in ixy.split('_'))
  for i in set(a.irr):
    var[i,iy,ix]=len(a.loc[a.irr==i])
111/1:
import netCDF4
import numpy as np
import datetime
from scipy.interpolate import griddata
import json

from pyproj import Proj
import sys,os,subprocess
from dtconvertor import dt2jul, jul2dt
111/2: import netCDF4
111/3: fname='temp.nc'
111/4: nc = netCDF4.Dataset(fname, 'r+')
111/5: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
111/6:
for v in V[3][0]:
    for k in range(29):
        nc[v][k,:,:,:]=0
111/7: v
111/8: V[3]
111/9:
for v in V[3]:
    for k in range(29):
        nc[v][k,:,:,:]=0
111/10: nc.close()
108/802: pv=pivot_table(df2,index='ixy',values='irr',aggfunc=count).reset_index()
108/803: pv=pivot_table(df2,index='ixy',values='irr',aggfunc='count').reset_index()
108/804: pv.head()
108/805: df2['ixyr']=[i+'_'+str(j) for i in zip(df2.ixy,df2.irr)]
108/806: df2['ixyr']=[i+'_'+str(j) for i,j in zip(df2.ixy,df2.irr)]
108/807: pv=pivot_table(df2,index='ixyr',values='irr',aggfunc='count').reset_index()
108/808: pv.head()
108/809: len(pv)
108/810: var=np.zeros(shape=(29,nrow,ncol))
108/811:
for n in range(len(pv)):
  ixy=pv.loc[n,'ixyr']  
  ix,iy,ir=(int(i) for i in ixy.split('_')) 
  var[ir,iy,ix]=pv.loc[n,'irr']
108/812: ix,iy,ir
108/813: var[:,iy,ix]
108/814: fname
108/815: fname='temp.nc'
108/816: nc = netCDF4.Dataset(fname, 'r+')
108/817: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/818: V
108/819: v='GMIA_28'
108/820: nc[v][0,:,:,:]=var[:,:,:]
108/821: nc[v][0,:,:,:]=var[:,:,:]/15/15
108/822: nc.close()
108/823: np.max(var)
108/824: 15*15
108/825: idx=np.where(var==554.0)
108/826: idx
108/827: var[:,294,0]
108/828: nc = netCDF4.Dataset(fname, 'r+')
108/829: svar=np.sum(var,axis=0)
108/830: a=np.where(svar<255,svar,255)
108/831: np.max(a)
108/832: svar[:5,:5]
108/833: idx=(svar==np.max(svar))
108/834: idx
108/835: idx=np.where(svar==np.max(svar))
108/836: idx
108/837: svar[292,0]
108/838: var[:,292,0]
108/839: a[292,0]
108/840: a.shape
108/841: svar[291,0]
108/842: a[291,0]
108/843: nc[v][0,:,:,:]=var[:,:,:]/svar[None,:,:]
108/844: nc.close()
108/845: %history -f his.txt
108/846:
fname='temp.nc'
nc = netCDF4.Dataset(fname, 'r+')
108/847: nc[v][0,:,:,:]=var[:,:,:]/a[None,:,:]
108/848: nc['GMIA'][0,:,:,:]=var[:,:,:]/a[None,:,:]
108/849: nc.close()
108/850: fname='gmlulca_10classes_global.tif'
108/851: img = rasterio.open(fname)
108/852: data.shape
108/853: nx,ny
108/854: lon_1d=[-180+(360./(nx-1))*i for i in range(nx)]
108/855: lat_1d=[90-(180./(ny-1))*i for i in range(ny)]
108/856: lonm.shape
108/857: nx,ny
108/858: lonm, latm = np.meshgrid(lon_1d, lat_1d)
108/859:
idx=np.where(data>0)
DD={'lon':lonm[idx[1],idx[2]],'lat':latm[idx[1],idx[2]],'irr':data[0,idx[1],idx[2]]}
df=DataFrame(DD)
108/860: df.set_index('lon').to_csv('lu.csv')
108/861: df.head
108/862: df.head()
108/863: pv=pivot_table(df,index='irr',values='irr',aggfunc='count').reset_index()
108/864: df.head()
108/865: df['lu']=df.irr
108/866: pv=pivot_table(df,index='lu',values='irr',aggfunc='count').reset_index()
108/867: pv
108/868:
idx=np.where(data<9)
DD={'lon':lonm[idx[1],idx[2]],'lat':latm[idx[1],idx[2]],'irr':data[0,idx[1],idx[2]]}
df=DataFrame(DD)
108/869: len(df)
108/870: len(data.flatten())
108/871: boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
108/872:
idx=np.where(data>=0)
DD={'lon':lonm[idx[1],idx[2]],'lat':latm[idx[1],idx[2]],'irr':data[0,idx[1],idx[2]]}
df=DataFrame(DD)
108/873: len(df)
108/874: boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
108/875: df1=df.loc[boo].reset_index(drop=True)
108/876: x,y=pnyc(list(df1.lon),list(df1.lat), inverse=False)
108/877: x,y=np.array(x),np.array(y)
108/878: nc.XORIG
108/879: fname='lu.nc'
108/880: nc = netCDF4.Dataset(fname, 'r+')
108/881:
df1['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df1['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
df2=df1.loc[(df1.ix>=0)&(df1.ix<ncol)&(df1.iy>=0)&(df1.iy<nrow)].reset_index(drop=True)
df2['ixy']=[str(i)+'_'+str(j) for i,j in zip(df2.ix,df2.iy)]
108/882: df2['ixyr']=[i+'_'+str(j) for i,j in zip(df2.ixy,df2.irr)]
108/883: pv=pivot_table(df2,index='ixyr',values='irr',aggfunc='count').reset_index()
108/884: var=np.zeros(shape=(11,nrow,ncol))
108/885: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
108/886: nt,nlay,nrow,ncol=(nc.variables[v].shape[i] for i in range(4))
108/887: v='GMIA'
108/888: nt,nlay,nrow,ncol=(nc.variables[v].shape[i] for i in range(4))
108/889: nt,nlay,nrow,ncol
108/890: nc = netCDF4.Dataset(fname, 'r+')
108/891: nt,nlay,nrow,ncol=(nc.variables[v].shape[i] for i in range(4))
108/892: nt,nlay,nrow,ncol
108/893: pv.head()
108/894: var=np.zeros(shape=(11,nrow,ncol))
108/895:
for n in range(len(pv)):
  ixy=pv.loc[n,'ixyr']
  ix,iy,ir=(int(i) for i in ixy.split('_'))
  var[ir,iy,ix]=pv.loc[n,'irr']
108/896: np.max(var)
108/897: idx=np.where(var==706.0)
108/898: idx
108/899: var[:,0,0]
108/900:
svar=np.sum(var,axis=0)
a=np.where(svar<255,svar,255) #255=15*15
nc[V[3]][0,:,:,:]=var[:,:,:]/a[None,:,:]
108/901: svar=np.sum(var,axis=0)
108/902: a=np.where(svar<255,svar,255)
108/903: nc[V[3]][0,:,:,:]=var[:,:,:]/a[None,:,:]
108/904: nc[v][0,:,:,:]=var[:,:,:]/a[None,:,:]
108/905: nc.close()
108/906: nc = netCDF4.Dataset(fname, 'r+')
108/907: nc['GMIA'][0,:,:,:]=var[:,:,:]/svar[None,:,:]
108/908: nc.close()
108/909: pv.head()
108/910: len(pv)
108/911: var=np.zeros(shape=(11,nrow,ncol))
108/912: pv['ix']=[int(i.split('_')[0]) for i in df.ixyr]
108/913: pv['ix']=[int(i.split('_')[0]) for i in pv.ixyr]
108/914: pv['iy']=[int(i.split('_')[1]) for i in pv.ixyr]
108/915: pv['ir']=[int(i.split('_')[2]) for i in pv.ixyr]
108/916: var[pv.ir,pv.iy,pv.ix]=pv.irr
108/917: nc = netCDF4.Dataset(fname, 'r+')
108/918: svar=np.sum(var,axis=0)
108/919: nc['GMIA'][0,:,:,:]=var[:,:,:]/svar[None,:,:]
108/920: nc.close()
108/921: pwd
108/922: ls
108/923: cd ../HarvestedAreaYield175Crops_Geotiff/
108/924: ls
108/925: fname
108/926:
fname='temp.nc'
nc = netCDF4.Dataset(fname, 'r+')
108/927: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/928: V
108/929: nlay,nt,nrow,ncol=(nc.variables[v].shape[i] for i in range(4))
108/930: nlay,nt,nrow,ncol
108/931:
for k in range(nlay,42):
    nc[v][k,:,:,:]=0.
108/932: nc.NLAYS=42
108/933: nc.close()
108/934: fname='HarvestedAreaYield175Crops_Geotiff/rapeseed_HarvAreaYield_Geotiff/rapeseed_HarvestedAreaHectares.tif'
108/935:
fname='temp.nc'
nc = netCDF4.Dataset(fname, 'r+')
108/936: fname='HarvestedAreaYield175Crops_Geotiff/rapeseed_HarvAreaYield_Geotiff/rapeseed_HarvestedAreaHectares.tif'
108/937: img = rasterio.open(fname)
108/938: nx,ny,nz=img.width,img.height,img.count
108/939: nx,ny,nz
108/940: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/941: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
108/942: V
108/943:
def tif2nc(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df=df.loc[boo].reset_index(drop=True)

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixyr']=[i+'_'+str(j) for i,j in zip(df.ixy,df.val)]
  pv=pivot_table(df,index='ixyr',values='val',aggfunc='count').reset_index()
  var=np.zeros(shape=(nlay,nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixyr']
    ix,iy,ir=(int(i) for i in ixy.split('_'))
    var[ir,iy,ix]=pv.loc[n,'val']
  svar=np.sum(var,axis=0)
  a=np.where(svar<255,svar,255) #255=15*15
  nc[V[3]][0,:,:,:]=var[:,:,:]/a[None,:,:]
  nc.close()
  return 0
108/944: tif_name
108/945: tif_name='HarvestedAreaYield175Crops_Geotiff/rapeseed_HarvAreaYield_Geotiff/rapeseed_HarvestedAreaHectares.tif'
108/946: nc_name='temp.nc'
108/947: i=tif2nc(tif_name,nc_name)
108/948: spec
108/949: dd
108/950: d21_175.json
108/951: d21_175
108/952: len(d21_175)
108/953: crop21=list(d21_175)
108/954: crop21.sort()
108/955: tif_name
108/956: s=set()
108/957:
for i in crop21:
    c175=d21_175[i]
    if type(c175)==list or c175==wheat:continue
    s|=c175
108/958:
for i in crop21:
    c175=d21_175[i]
    if type(c175)==list or c175=='wheat':continue
    s|=c175
108/959:
for i in crop21:
    c175=d21_175[i]
    if type(c175)==list or c175=='wheat':continue
    s|=set(c175)
108/960: s
108/961: s=set()
108/962:
for i in crop21:
    c175=d21_175[i]
    if type(c175)==list or c175=='wheat':continue
    s|=set([c175])
108/963: s
108/964: ls
108/965:
with open(fnameO,'r') as jsonfile:
    d21_175=json.load(jsonfile)
108/966: fnameO
108/967: s=set()
108/968:
for i in crop21:
    c175=d21_175[i]
    if type(c175)==list or c175=='wheat':continue
    s|=set([c175])
108/969: s
108/970: tif_name
108/971: d175_21={j:i for i,j in zip(d21_175.keys,d21_175.values)}
108/972: d175_21={j:i for i,j in zip(d21_175.keys(),d21_175.values())}
108/973: d175_21={d21_175[i]:i for i in crop21}
108/974: d175_21={d21_175[i]:i for i in crop21 if i not in ['wheat_sprint','wheat_winter']}
108/975: i
108/976: crop21
108/977: len(crop21)
108/978: d21_175
108/979: !vi
108/980: !vi d21_175.json
108/981:
with open(fnameO,'r') as jsonfile:
    d21_175=json.load(jsonfile)
108/982: crop21=list(d21_175)
108/983: crop21.sort()
108/984: d175_21={d21_175[i]:i for i in crop21 if i not in ['wheat_spring','wheat_winter','beansedible']}
108/985: d175_21
108/986: len(d175_21)
108/987: len(s)
108/988: set(s)-set(d175_21)
108/989: d175_21={d21_175[i]:crop21.index(i) for i in crop21 if i not in ['wheat_spring','wheat_winter','beansedible']}
108/990: d175_21
108/991:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df=df.loc[boo].reset_index(drop=True)

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  nc[V[3]][0,lev,:,:]=var[:,:,:]
  nc.close()
  return 0
108/992: tif_name
108/993: d175_21['rapeseed']
108/994: i=tif2nc(tif_name,nc_name,4)
108/995:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df=df.loc[boo].reset_index(drop=True)

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  nc[V[3]][0,lev,:,:]=var[:,:]
  nc.close()
  return 0
108/996: i=tif2nc(tif_name,nc_name,4)
108/997: nc.close()
108/998: i=tif2nc(tif_name,nc_name,4)
108/999: V[3]
108/1000: V[3][0]
108/1001: var.shape
108/1002: var=var[0,:,:]
108/1003: var.shape
108/1004: lev=4
108/1005: nc[V[3]][0,lev,:,:]=var[:,:]
108/1006:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df=df.loc[boo].reset_index(drop=True)

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  nc[V[3][0]][0,lev,:,:]=var[:,:]
  nc.close()
  return 0
108/1007: i=tif2nc(tif_name,nc_name,4)
108/1008: i
108/1009: tif_name
108/1010: c175[:5]
108/1011: c175
108/1012: s
108/1013:
for c in s:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,d175_21[c])
  print(c,i)
108/1014: set(crop21)-s
108/1015: fnameO
108/1016: crop21.index('beansedible')
108/1017: fname
108/1018:
fname='temp.nc'
nc = netCDF4.Dataset(fname, 'r+')
108/1019: nc[V[3][0]][0,3,:,:]=0
108/1020: nc.close()
108/1021: crop21[4]
108/1022: tif_name
108/1023: d175_21['mixedgrass']
108/1024: s
108/1025: c='alfalfa'
108/1026:   tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
108/1027: tif_name
108/1028: ls -lh HarvestedAreaYield175Crops_Geotiff/alfalfa_HarvAreaYield_Geotiff
108/1029:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df1=df.loc[boo].reset_index(drop=True)
  df=df1

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df1=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df=df1
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  if lev<nlay:
    nc[V[3]][0,lev,:,:]=var[:,:]
  else:    
    nc[V[3]][0,lev,:,:]+=var[:,:]
  nc.close()
  return 0
108/1030: c
108/1031:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,d175_21[c])
108/1032:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df1=df.loc[boo].reset_index(drop=True)
  df=df1

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df1=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df=df1
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  if lev<nlay:
    nc[V[3][0]][0,lev,:,:]=var[:,:]
  else:    
    nc[V[3][0]][0,lev,:,:]+=var[:,:]
  nc.close()
  return 0
108/1033:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,d175_21[c])
108/1034: d175_21[c]
108/1035:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df1=df.loc[boo].reset_index(drop=True)
  df=df1

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df1=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df=df1
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  if lev<nlay:      
    nc[V[3][0]][0,lev,:,:]=var[:,:]
  else:    
    nc[V[3][0]][0,lev,:,:]+=var[:,:]
  nc.close()
  return 0
108/1036:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,d175_21[c])
108/1037:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*(i+5) for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df1=df.loc[boo].reset_index(drop=True)
  df=df1

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df1=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df=df1
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  if lev<nlay:
    nc[V[3][0]][0,lev,:,:]=var[:,:]
  else:    
    nc[V[3][0]][0,lev,:,:]+=var[:,:]
  nc.close()
  return 0
108/1038:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,d175_21[c])
108/1039: fname
108/1040: nc = netCDF4.Dataset(fname, 'r+')
108/1041: nc[V[3][0]][0,0,:,:]=0
108/1042: nc.close()
108/1043:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,d175_21[c])
108/1044: nc.XORIG
108/1045: nc = netCDF4.Dataset(fname, 'r+')
108/1046: nc.XORIG
108/1047:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df1=df.loc[boo].reset_index(drop=True)
  df=df1

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)-5
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df1=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df=df1
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  if lev<nlay:
    nc[V[3][0]][0,lev,:,:]=var[:,:]
  else:    
    nc[V[3][0]][0,lev,:,:]+=var[:,:]
  nc.close()
  return 0
108/1048:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,d175_21[c])
108/1049: pv.head()
108/1050:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df1=df.loc[boo].reset_index(drop=True)
  df=df1

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)-5
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df1=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df=df1
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  if lev<nlay:
    nc[V[3][0]][0,lev,:,:]=var[:,:]
  else:    
    nc[V[3][0]][0,lev,:,:]+=var[:,:]
  nc.close()
  return pv
108/1051:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,d175_21[c])
108/1052: i.head()
108/1053: i.tail()
108/1054: tif_name
108/1055: (120.9900+121.73)/2
108/1056:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df1=df.loc[boo].reset_index(drop=True)
  df=df1

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df1=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df=df1
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  if lev<nlay:
    nc[V[3][0]][0,lev,:,:]=var[:,:]
  else:    
    nc[V[3][0]][0,lev,:,:]+=var[:,:]
  nc.close()
  return 0
108/1057:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,d175_21[c])
108/1058: crop21[4]
108/1059: crop21
108/1060: c='beansedible'
108/1061: d21_175[c]
108/1062: nc = netCDF4.Dataset(fname, 'r+')
108/1063: nc[V[3][0]][0,3,:,:]=0
108/1064: crop21.index(c)
108/1065:
for c in ['broadbean', 'greenbean', 'greenbroadbean', 'stringbean']:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,3)
108/1066: c='wheat_spring'
108/1067: crop21.index(c)
108/1068: c='wheat_winter'
108/1069: crop21.index(c)
108/1070:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df1=df.loc[boo].reset_index(drop=True)
  df=df1

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df1=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df=df1
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  if lev==3: #beansedible
    nc[V[3][0]][0,lev,:,:]+=var[:,:]
  else
    nc[V[3][0]][0,lev,:,:]=0.
    if lev==19: #wheat_spring
      nc[V[3][0]][0,lev,310:,:]=var[310:,:]
    elif lev==20: #wheat_winter
      nc[V[3][0]][0,lev,:310,:]=var[:310,:]
    else:
      nc[V[3][0]][0,lev,:,:]+=var[:,:]
  nc.close()
  return 0
108/1071:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df1=df.loc[boo].reset_index(drop=True)
  df=df1

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df1=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df=df1
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  if lev==3: #beansedible
    nc[V[3][0]][0,lev,:,:]+=var[:,:]
  else:
    nc[V[3][0]][0,lev,:,:]=0.
    if lev==19: #wheat_spring
      nc[V[3][0]][0,lev,310:,:]=var[310:,:]
    elif lev==20: #wheat_winter
      nc[V[3][0]][0,lev,:310,:]=var[:310,:]
    else:
      nc[V[3][0]][0,lev,:,:]+=var[:,:]
  nc.close()
  return 0
108/1072: c='wheat'
108/1073:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,19)
108/1074:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,20)
108/1075:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df1=df.loc[boo].reset_index(drop=True)
  df=df1

  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df1=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df=df1
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  var=np.zeros(shape=(nrow,ncol))
  for n in range(len(pv)):
    ixy=pv.loc[n,'ixy']
    ix,iy=(int(i) for i in ixy.split('_'))
    var[iy,ix]=pv.loc[n,'val']
  if lev==3: #beansedible
    nc[V[3][0]][0,lev,:,:]+=var[:,:]
  else:
    nc[V[3][0]][0,lev,:,:]=0.
    if lev==19: #wheat_spring
      nc[V[3][0]][0,lev,320:,:]=var[320:,:]
    elif lev==20: #wheat_winter
      nc[V[3][0]][0,lev,:320,:]=var[:320,:]
    else:
      nc[V[3][0]][0,lev,:,:]+=var[:,:]
  nc.close()
  return 0
108/1076:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,19)
108/1077:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,20)
108/1078: lst ../InternationlWaterManagementInstitute/
108/1079: !lst ../InternationlWaterManagementInstitute/
108/1080: fname='../InternationlWaterManagementInstitute/irr28.nc'
108/1081: nc = netCDF4.Dataset(fname, 'r')
108/1082: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/1083: V
108/1084: irr=np.sum(nc[V[3][0]][0,:,:,:],axis=0)
108/1085: np.max(irr)
108/1086: V[3][0]
108/1087: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
108/1088: nt,nlay,nrow,ncol
108/1089: irr=np.sum(nc[V[3][0]][0,1:,:,:],axis=0)
108/1090: np.max(irr)
108/1091: cd ../InternationlWaterManagementInstitute/
108/1092:
fname='giam_28_classes_global.tif'
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
dx,dy=360./(nx-1),180./(ny-1)
lon_1d=[-180+dx*i for i in range(nx)]
lat_1d=[90-dy*i for i in range(ny)]
data=img.read()
lonm, latm = np.meshgrid(lon_1d, lat_1d)
idx=np.where(data>0)
DD={'lon':lonm[idx[1],idx[2]],'lat':latm[idx[1],idx[2]],'irr':data[0,idx[1],idx[2]]}
df=DataFrame(DD)
108/1093: fname
108/1094: fname='irr28.nc'
108/1095: nc = netCDF4.Dataset(fname, 'r+')
108/1096: ls *nc
108/1097: nc.close()
108/1098: nc = netCDF4.Dataset(fname, 'r+')
108/1099: nlay
108/1100:
boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
df1=df.loc[boo].reset_index(drop=True)
x,y=pnyc(list(df1.lon),list(df1.lat), inverse=False)
x,y=np.array(x),np.array(y)
df1['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df1['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
df2=df1.loc[(df1.ix>=0)&(df1.ix<ncol)&(df1.iy>=0)&(df1.iy<nrow)].reset_index(drop=True)
df2['ixy']=[str(i)+'_'+str(j) for i,j in zip(df2.ix,df2.iy)]
df2['ixyr']=[i+'_'+str(j) for i,j in zip(df2.ixy,df2.irr)]
pv=pivot_table(df2,index='ixyr',values='irr',aggfunc='count').reset_index()
var=np.zeros(shape=(nlay,nrow,ncol))
108/1101: nc[V[3][0]][0,:,:,:]=var[:,:,:]
108/1102:
for n in range(len(pv)):
  ixy=pv.loc[n,'ixyr']
  ix,iy,ir=(int(i) for i in ixy.split('_'))
  var[ir,iy,ix]=pv.loc[n,'irr']
108/1103:
svar=np.sum(var,axis=0)
a=np.where(svar<255,svar,255)
108/1104: np.min(a)
108/1105: a=np.where(svar<255,255,svar)
108/1106: np.min(a)
108/1107: np.max(a)
108/1108: nc[V[3]][0,:,:,:]=var[:,:,:]/a[None,:,:]
108/1109: nc[V[3][0]][0,:,:,:]=var[:,:,:]/a[None,:,:]
108/1110: np.max(nc[V[3][0]][0,0,:,:])
108/1111: np.min(nc[V[3][0]][0,0,:,:])
108/1112: nc[V[3][0]][0,0,:,:]=a[:,:]
108/1113: nc.close()
108/1114: nc = netCDF4.Dataset(fname, 'r')
108/1115: irr=np.sum(nc[V[3][0]][0,1:,:,:],axis=0)
108/1116: np.max(irr)
108/1117: np.min(irr)
108/1118: cd ../HarvestedAreaYield175Crops_Geotiff/
108/1119:
fname='temp.nc'
nc = netCDF4.Dataset(fname, 'r+')
108/1120: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/1121: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
108/1122: tif_name
108/1123: km2=nc[V[3][0]][0,:21,:,:]*0.01
108/1124:
for lev in range(21,42):
    nc[V[3][0]][0,lev,:,:]=irr[:,:]*km2[lev-21,:,:]
108/1125:
for lev in range(21):
    nc[V[3][0]][0,lev,:,:]=(1-irr[:,:])*km2[lev,:,:]
108/1126:
svar=np.sum(nc[V[3][0]][0,:,:,:],axis=0)
a=np.where(svar<255,svar,255)
108/1127: np.max(svar)
108/1128: np.min(svar)
108/1129: a=np.where(svar<255,255,svar)
108/1130: nc[V[3][0]][0,:,:,:]=nc[V[3][0]][0,:,:,:]/a[None,:,:]
108/1131: nc.close()
108/1132: %history -f his.txt
108/1133: pwd
108/1134: fname
108/1135: nc.close()
108/1136: nc = netCDF4.Dataset(fname, 'r+')
108/1137: svar=np.sum(nc[V[3][0]][0,22,:,:],axis=0)
108/1138: np.max(svar)
108/1139: svar=np.sum(nc[V[3][0]][0,40,:,:],axis=0)
108/1140: np.max(svar)
108/1141: svar=np.sum(nc[V[3][0]][0,41,:,:],axis=0)
108/1142: np.max(svar)
108/1143: nc.NLAYS
108/1144: nc.close()
108/1145: pwd
108/1146: %history -f his.txt
108/1147: lst
108/1148: !lst
108/1149: cp temp.nc a.nc
108/1150: nc_name
108/1151: nc_name='a.nc'
108/1152: tif_name
108/1153: lev
108/1154:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df=df.loc[boo].reset_index(drop=True)
  
  Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
  nc = netCDF4.Dataset(nc_name, 'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
  var=np.zeros(shape=(nrow,ncol))
  var[pv.iy,pv.ix]=pv.val
  if lev==3: #beansedible
    nc[V[3][0]][0,lev,:,:]+=var[:,:]
  else:
    nc[V[3][0]][0,lev,:,:]=0.
    if lev==19: #wheat_spring
      nc[V[3][0]][0,lev,320:,:]=var[320:,:]
    elif lev==20: #wheat_winter
      nc[V[3][0]][0,lev,:320,:]=var[:320,:]
    else:
      nc[V[3][0]][0,lev,:,:]+=var[:,:]
  nc.close()
108/1155: c
108/1156:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,20)
108/1157: pwd
108/1158:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,dy=360./(nx-1),180./(ny-1)
  lon_1d=[-180+dx*i for i in range(nx)]
  lat_1d=[90-dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)
  df=df.loc[boo].reset_index(drop=True)
  
  nc = netCDF4.Dataset(nc_name, 'r+')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
  var=np.zeros(shape=(nrow,ncol))
  var[pv.iy,pv.ix]=pv.val
  if lev==3: #beansedible
    nc[V[3][0]][0,lev,:,:]+=var[:,:]
  else:
    nc[V[3][0]][0,lev,:,:]=0.
    if lev==19: #wheat_spring
      nc[V[3][0]][0,lev,320:,:]=var[320:,:]
    elif lev==20: #wheat_winter
      nc[V[3][0]][0,lev,:320,:]=var[:320,:]
    else:
      nc[V[3][0]][0,lev,:,:]+=var[:,:]
  nc.close()
  return 0
108/1159: len(s)
108/1160: nc_name='temp.nc'
108/1161:
for c in s:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,d175_21[c])
  print(c,d175_21[c],i)
108/1162:
for c in ['broadbean', 'greenbean', 'greenbroadbean', 'stringbean']:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,3)
108/1163: c='wheat'
108/1164:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,19)
108/1165:
  tif_name='HarvestedAreaYield175Crops_Geotiff/'+c+'_HarvAreaYield_Geotiff/'+c+'_HarvestedAreaHectares.tif'
  i=tif2nc(tif_name,nc_name,20)
108/1166: s=s0
108/1167: s0=s
108/1168: s=set([d21_175[i] for i in crop21 if i not in ['wheat_spring','wheat_winter','beansedible']])
108/1169: s==s0
108/1170: s1=set([d21_175[i] for i in crop21 if i not in ['wheat_spring','wheat_winter','beansedible']])
108/1171: s1==s0
108/1172: !vi his.txt
108/1173:
fname='temp.nc'
nc = netCDF4.Dataset(fname, 'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[3][0]
nt,nlay,nrow,ncol=(nc.variables[v].shape[i] for i in range(4))
km2=nc[v][0,:21,:,:]*0.01
for lev in range(21,42):
  nc[v][0,lev,:,:]=irr[:,:]*km2[lev-21,:,:]
for lev in range(21):
  nc[v][0,lev,:,:]=(1-irr[:,:])*km2[lev,:,:]
svar=np.sum(nc[v][0,:,:,:],axis=0)
a=np.where(svar<255,255,svar)
nc[v][0,:,:,:]=nc[v][0,:,:,:]/a[None,:,:]
nc.close()
108/1174: pwd
108/1175: crop21
108/1176: c='rice'
108/1177: d175_21[c]
108/1178: np.max(irr)
108/1179: fname='../InternationlWaterManagementInstitute/irr28.nc'
108/1180: nc = netCDF4.Dataset(fname, 'r+')
108/1181: pwd
108/1182: nc.close()
108/1183: cd ../InternationlWaterManagementInstitute/
108/1184: fname
108/1185: fname='irr28.nc'
108/1186: nc = netCDF4.Dataset(fname, 'r')
108/1187: nc = netCDF4.Dataset(fname, 'r+')
108/1188: fname='irr.nc'
108/1189: nc = netCDF4.Dataset(fname, 'r+')
108/1190: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/1191: V
108/1192: v='GMIA'
108/1193: nc[v][0,0,:,:]=irr[:,:]
108/1194: nc.close()
108/1195: 360*3600/30
108/1196: cd /nas1/cmaqruns/2018base/data/land/isric/cec
108/1197:
with open('f','r') as f:
  fnames=[i.strip('\n') for i in f]
108/1198: len(fnames)
108/1199: fnames[:5]
108/1200: set([i.split('-')[0] for i in fnames])
108/1201: set([i.split('-')[1] for i in fnames])
108/1202: len(set([i.split('-')[1] for i in fnames]))
108/1203: len(set([i.split('-')[2] for i in fnames]))
108/1204: l=list(set([i.split('-')[2] for i in fnames]))
108/1205: l.sort()
108/1206: l[:5]
108/1207: l[-5:]
108/1208: set(['{3d}'.format(i) for i in range(89)])-set(l)
108/1209: set(['{:3d}'.format(i) for i in range(89)])-set(l)
108/1210: set(['{:03d}'.format(i) for i in range(89)])-set(l)
108/1211: len(fnames)
108/1212: fname[-5]
108/1213: fnames[-5]
108/1214: pwd
108/1215: fname='tileSG-027-080_1-1.tif'
108/1216: img = rasterio.open(fname)
108/1217: nx,ny,nz=img.width,img.height,img.count
108/1218: nx,ny,nz
108/1219: a=dir(img)
108/1220: a[:30]
108/1221: a[30:60]
108/1222: img.offets
108/1223: img._offets
108/1224: a[60:90]
108/1225: img.block_shapes
108/1226: img.get_transform
108/1227: img.get_transform()
108/1228: img.descriptions
108/1229: img.descriptions()
108/1230: img.lnglat
108/1231: img.lnglat()
108/1232: cd ../../HarvestedAreaYield175Crops_Geotiff/
108/1233: tif_name
108/1234: fname=tif_name
108/1235: img = rasterio.open(fname)
108/1236: img.lnglat()
108/1237: cd ../InternationlWaterManagementInstitute/
108/1238:
fname='giam_28_classes_global.tif'
img = rasterio.open(fname)
108/1239: img.lnglat()
108/1240: cd /nas1/cmaqruns/2018base/data/land/isric/cec
108/1241: fname='tileSG-027-080_1-1.tif'
108/1242: img = rasterio.open(fname);img.lnglat()
108/1243: fname='tileSG-027-080_4-4.tif'
108/1244: img = rasterio.open(fname);img.lnglat()
108/1245: fname='tileSG-027-080_1-2.tif'
108/1246: img = rasterio.open(fname);img.lnglat()
108/1247: fname='tileSG-027-080_2-1.tif'
108/1248: img = rasterio.open(fname);img.lnglat()
108/1249: -32.38875756892934+-33.39936226356381
108/1250: -32.38875756892934+33.39936226356381
108/1251: 145.8139414158076-144.61715654664926
108/1252: fname='tileSG-027-080_1-1.tif'
108/1253: img = rasterio.open(fname);img.lnglat()
108/1254: fname='tileSG-028-080_1-1.tif'
108/1255: img = rasterio.open(fname);img.lnglat()
108/1256: 450*30/3600
108/1257: 0.0106046946344662*3600
108/1258: 1.0106046946344662/4.
108/1259: 1.0106046946344662/4.*3600
108/1260: 1.0106046946344662/4.*3600/450
108/1261: -32.38875756892934+36.431176347467186
108/1262: 4.042418778537844/4/450*3600
108/1263: a[60:90]
108/1264: img.count
108/1265: img.bounds
108/1266: fname='tileSG-027-080_1-1.tif'
108/1267: img = rasterio.open(fname);img.lnglat()
108/1268: !lst
108/1269: fname='tileSG-011-068_4-4.tif';
108/1270: img = rasterio.open(fname);img.bounds()
108/1271: img = rasterio.open(fname);img.bounds
108/1272: fname='tileSG-012-068_4-4.tif';
108/1273: img = rasterio.open(fname);img.bounds
108/1274: ls
108/1275: fname='tileSG-012-062_2-1.tif'
108/1276: img = rasterio.open(fname);img.bounds
108/1277: fname='tileSG-012-062_1-1.tif'
108/1278: img = rasterio.open(fname);img.bounds
108/1279: 3200750.0-3088250.0
108/1280: (3200750.0-3088250.0)/450
108/1281: fname='tileSG-012-062_1-2.tif'
108/1282: img = rasterio.open(fname);img.bounds
108/1283: (7975000.0-7862500.0)/450
108/1284: a[60:90]
108/1285: img.crs
108/1286: 6378137./360/3600*30
108/1287: 6378137./360/3600*60
108/1288: 6378137./298.257223563
108/1289: 0.0174532925199433*3600
108/1290: 6378137./360/3600*62.83185307179588
108/1291: 6378137./360/3600*62.
108/1292: img.indexes
108/1293: img.index
108/1294: img.index()
108/1295: a[60:90]
108/1296: img.files
108/1297: img.get_gcps
108/1298: img.get_gcps()
108/1299: a[90:]
108/1300: img.xy
108/1301: img.xy()
108/1302: img.xy(0,0)
108/1303: img.bounds
108/1304: img.units
108/1305: a[90:]
108/1306: img.profile
108/1307: img.tags
108/1308: img.tags()
108/1309: img.scales
108/1310: img.start
108/1311: img.start()
108/1312: img.window_bounds
108/1313: img.window_bounds()
108/1314: img.window_transfor
108/1315: img.window_transform
108/1316: img.window_transform()
108/1317: img.info
108/1318: data=img.read()
108/1319: data.shape
108/1320: np.max(data)
108/1321: np.min(data)
108/1322: np.min(data!=-32768)
108/1323: np.min(data>-32768)
108/1324: a=data>-32768
108/1325: np.min(a)
108/1326: a=np.where(data>-32768,data,np.isnan)
108/1327: np.min(a)
108/1328: a=np.where(data>-32768,data,1/0)
108/1329: idx=np.where(data!=-32768)
108/1330: len(idx)
108/1331: len(idx[0])
108/1332: len(data.flatten())
108/1333: np.min(data[idx[0],idx[1],idx[2]])
108/1334: nc.XORIG*2
108/1335: fname
108/1336: pwd
108/1337: cd ../../HarvestedAreaYield175Crops_Geotiff/
108/1338:
fname='temp.nc'
nc = netCDF4.Dataset(fname, 'r')
108/1339: nc.XORIG*2
108/1340: ls *nc
108/1341:
fname='temp.nc'
nc = netCDF4.Dataset(fname, 'r+')
108/1342: nc.close()
108/1343:
fname='temp.nc'
nc = netCDF4.Dataset(fname, 'r+')
108/1344: nc.XORIG*2
108/1345: ls *nc
108/1346: fname='a0.nc'
108/1347: nc = netCDF4.Dataset(fname, 'r')
108/1348: nc.XORIG*2
108/1349: x0,y0=pnyc(img.lnglat[0],img.lnglat[0], inverse=False)
108/1350: img.lnglat
108/1351: img.lnglat()
108/1352: x0,y0=pnyc(img.lnglat()[0],img.lnglat()[0], inverse=False)
108/1353: x0,y0
108/1354: np.min(a)
108/1355: df.head()
108/1356: a=df.lon//2
108/1357: a.head()
108/1358: pv.head()
108/1359: pv2=pivot_table(pv,index='ixyr',values='ixyr',aggfunc='count').reset_index()
108/1360: pv2.head()
108/1361: a=pv2.sort_values(on='ixyr')
108/1362: a=pv2.sort_values('ixyr')
108/1363: a==pv2
108/1364: (a==pv2).all()
108/1365:
def tif2nc(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat[0],img.lnglat[0], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy',aggfunc='count').reset_index()
  pv1['N']=pv2['ixy']
  pv1['X']=[int(i.split('_')[0])*1000 for i in pv1.ixy]
  pv1['Y']=[int(i.split('_')[1])*1000 for i in pv1.ixy]
  col=['X','Y','N','val']
  pv1[col].set_index('X').to_csv(tif_name.replace('.tif','.csv'))
  return 0
108/1366: nc_name
108/1367: nc_name='a0.nc'
108/1368: fname
108/1369: fname='tileSG-012-062_1-2.tif'
108/1370: tif_name='tileSG-012-062_1-2.tif'
108/1371: i=tif2nc(tif_name,nc_name)
108/1372: pwd
108/1373: cd ../isric/cec/
108/1374: i=tif2nc(tif_name,nc_name)
108/1375: !ln -s /nas1/cmaqruns/2018base/data/land/HarvestedAreaYield175Crops_Geotiff/a0.nc .
108/1376: i=tif2nc(tif_name,nc_name)
108/1377:
def tif2df(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[0], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy',aggfunc='count').reset_index()
  pv1['N']=pv2['ixy']
  pv1['X']=[int(i.split('_')[0])*1000 for i in pv1.ixy]
  pv1['Y']=[int(i.split('_')[1])*1000 for i in pv1.ixy]
  col=['X','Y','N','val']
  pv1[col].set_index('X').to_csv(tif_name.replace('.tif','.csv'))
  return 0
108/1378: i=tif2df(tif_name,nc_name)
108/1379: pv1.head()
108/1380:
def tif2df(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[0], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixy2']=df.ixy
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy',aggfunc='count').reset_index()
  pv1['N']=pv2['ixy']
  pv1['X']=[int(i.split('_')[0])*1000 for i in pv1.ixy]
  pv1['Y']=[int(i.split('_')[1])*1000 for i in pv1.ixy]
  col=['X','Y','N','val']
  pv1[col].set_index('X').to_csv(tif_name.replace('.tif','.csv'))
  return 0
108/1381: i=tif2df(tif_name,nc_name)
108/1382:
def tif2df(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[0], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixy2']=df.ixy
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy2',aggfunc='count').reset_index()
  pv1['N']=pv2['ixy']
  pv1['X']=[int(i.split('_')[0])*1000 for i in pv1.ixy]
  pv1['Y']=[int(i.split('_')[1])*1000 for i in pv1.ixy]
  col=['X','Y','N','val']
  pv1[col].set_index('X').to_csv(tif_name.replace('.tif','.csv'))
  return 0
108/1383: i=tif2df(tif_name,nc_name)
108/1384:
def tif2df(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[0], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixy2']=df.ixy
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy2',aggfunc='count').reset_index()
  pv1['N']=pv2['ixy']
  pv1['X']=[float(i.split('_')[0])*1000 for i in pv1.ixy]
  pv1['Y']=[float(i.split('_')[1])*1000 for i in pv1.ixy]
  col=['X','Y','N','val']
  pv1[col].set_index('X').to_csv(tif_name.replace('.tif','.csv'))
  return 0
108/1385: i=tif2df(tif_name,nc_name)
108/1386: !lst
108/1387: df=read_csv('tileSG-012-062_1-2.csv')
108/1388: len(df)
108/1389: df.head()
108/1390:
def tif2df(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[0], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixy2']=df.ixy
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy2',aggfunc='count').reset_index()
  pv1['N']=pv2.ixy2
  pv1['X']=[float(i.split('_')[0])*1000 for i in pv1.ixy]
  pv1['Y']=[float(i.split('_')[1])*1000 for i in pv1.ixy]
  col=['X','Y','N','val']
  pv1[col].set_index('X').to_csv(tif_name.replace('.tif','.csv'),header=None)
  return 0
108/1391: i=tif2df(tif_name,nc_name)
108/1392: df=read_csv('tileSG-012-062_1-2.csv',header=None)
108/1393: df.head()
108/1394: cd cec_L1
108/1395:
with open('fnames.txt','r') as f:
  fnames=[i.strip('\n') for i in f]
108/1396:
for tif_name in fnames:
    print(tif_name,tif2df(tif_name,nc_name))
108/1397: !ln -s /nas1/cmaqruns/2018base/data/land/HarvestedAreaYield175Crops_Geotiff/a0.nc .
108/1398:
for tif_name in fnames:
    print(tif_name,tif2df(tif_name,nc_name))
108/1399: ls -lh tileSG-010-063_4-4.tif
108/1400: i=fnames.index('tileSG-010-063_4-4.tif')
108/1401: fnames[i-1:i+3]
108/1402: ls -lh tileSG-010-064_1-1.tif
108/1403: tif_name
108/1404:
  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[0], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixy2']=df.ixy
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy2',aggfunc='count').reset_index()
108/1405:
  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[0], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixy2']=df.ixy
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy2',aggfunc='count').reset_index()
108/1406: plen(pv2)
108/1407: len(pv2)
108/1408: len(pv1)
108/1409: len(df)
108/1410: df.to_csv(tif_name.replace('.tif','.csv'),header=None)
108/1411: !lst
108/1412:
def tif2df(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[0], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  if len(df)==0:
    df.to_csv(tif_name.replace('.tif','.csv'),header=None)
    return 1
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixy2']=df.ixy
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy2',aggfunc='count').reset_index()
  pv1['N']=pv2.ixy2
  pv1['X']=[float(i.split('_')[0])*1000 for i in pv1.ixy]
  pv1['Y']=[float(i.split('_')[1])*1000 for i in pv1.ixy]
  col=['X','Y','N','val']
  pv1[col].set_index('X').to_csv(tif_name.replace('.tif','.csv'),header=None)
  return 0
108/1413: import os
108/1414:
for tif_name in fnames:
    if os.path.exists(tif_name.replace('.tif','.csv')):continue
    print(tif_name,tif2df(tif_name,nc_name))
108/1415: col
108/1416: echo 'X,Y,N,val'>../header.txt
108/1417: !echo 'X,Y,N,val'>../header.txt
108/1418: !cat ../header.txt *.csv > all.txt
108/1419: df=read_csv('all.txt')
108/1420: len(df)
108/1421: df.head()
108/1422: rm a0.nc
108/1423: cp /nas1/cmaqruns/2018base/data/land/HarvestedAreaYield175Crops_Geotiff/a0.nc .
108/1424: fname='a0.nc'
108/1425: nc = netCDF4.Dataset(fname, 'r+')
108/1426: x,y=df.X,df.Y
108/1427:
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
108/1428: df.head()
108/1429: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
108/1430: v=V[3][0]
108/1431: nc[v][:]=0
108/1432:
  pv=pivot_table(df,index='ixy',values=['N','val'],aggfunc=np.sum).reset_index()
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
  pv['cec']=pv.val/pv.N
108/1433: pv.head()
108/1434: df['vn']=df.val*df.N
108/1435:
  pv=pivot_table(df,index='ixy',values=['N','vn'],aggfunc=np.sum).reset_index()
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
  pv['cec']=pv.val/pv.N
108/1436:
  pv=pivot_table(df,index='ixy',values=['N','vn'],aggfunc=np.sum).reset_index()
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
  pv['cec']=pv.vn/pv.N
108/1437: pv.head()
108/1438: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
108/1439: var=np.zeros(shape=(nrow,ncol))
108/1440: var[pv.iy,pv.ix]=pv.cec
108/1441: nc[v][0,0,:,:]=var[:,:]
108/1442: nc.close()
108/1443: pwd
108/1444:
def tif2df(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[1], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  if len(df)==0:
    df.to_csv(tif_name.replace('.tif','.csv'),header=None)
    return 1
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixy2']=df.ixy
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy2',aggfunc='count').reset_index()
  pv1['N']=pv2.ixy2
  pv1['X']=[float(i.split('_')[0])*1000 for i in pv1.ixy]
  pv1['Y']=[float(i.split('_')[1])*1000 for i in pv1.ixy]
  col=['X','Y','N','val']
  pv1[col].set_index('X').to_csv(tif_name.replace('.tif','.csv'),header=None)
  return 0
108/1445: rm *csv
108/1446:
for tif_name in fnames:
    if os.path.exists(tif_name.replace('.tif','.csv')):continue
    print(tif_name,tif2df(tif_name,nc_name))
112/1:
def tif2df(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[1], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  if len(df)==0:
    df.to_csv(tif_name.replace('.tif','.csv'),header=None)
    return 1
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixy2']=df.ixy
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy2',aggfunc='count').reset_index()
  pv1['N']=pv2.ixy2
  pv1['X']=[float(i.split('_')[0])*1000 for i in pv1.ixy]
  pv1['Y']=[float(i.split('_')[1])*1000 for i in pv1.ixy]
  col=['X','Y','N','val']
  pv1[col].set_index('X').to_csv(tif_name.replace('.tif','.csv'),header=None)
  return 0
112/2: nc_name='a0.nc'
112/3:
with open('fnames.txt','r') as f:
  fnames=[i.strip('\n') for i in f]
112/4:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
112/5: import os
112/6:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
112/7: import rasterio
112/8:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
112/9: pwd
112/10: !lsd
112/11: cd cec_L1
112/12:
with open('fnames.txt','r') as f:
  fnames=[i.strip('\n') for i in f]
112/13:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
112/14: from pandas import *
112/15:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
112/16: fnames[:5]
112/17: s='tileSG-010-060_1'
112/18: len(s)
112/19: fnames[0][16]
112/20: fnames[0][15]
112/21: fnames[1][17]
112/22:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    i,j=[int(tif_name[k]) for k in [15,17])
    if (i+j)%2==0:continue     
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
112/23:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    i,j=[int(tif_name[k]) for k in [15,17]]
    if (i+j)%2==0:continue     
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
113/1: from pandas import *
113/2:
def tif2df(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[1], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  if len(df)==0:
    df.to_csv(tif_name.replace('.tif','.csv'),header=None)
    return 1
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixy2']=df.ixy
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy2',aggfunc='count').reset_index()
  pv1['N']=pv2.ixy2
  pv1['X']=[float(i.split('_')[0])*1000 for i in pv1.ixy]
  pv1['Y']=[float(i.split('_')[1])*1000 for i in pv1.ixy]
  col=['X','Y','N','val']
  pv1[col].set_index('X').to_csv(tif_name.replace('.tif','.csv'),header=None)
  return 0
113/3: nc_name='a0.nc'
113/4:
with open('fnames.txt','r') as f:
  fnames=[i.strip('\n') for i in f]
113/5:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    i,j=[int(tif_name[k]) for k in [15,17]]
    if (i+j)%2==1:continue     
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
113/6: import os
113/7:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    i,j=[int(tif_name[k]) for k in [15,17]]
    if (i+j)%2==1:continue     
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
113/8: tif_name
113/9: pwd
113/10: cd csc_L1
113/11:
with open('fnames.txt','r') as f:
  fnames=[i.strip('\n') for i in f]
113/12:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    i,j=[int(tif_name[k]) for k in [15,17]]
    if (i+j)%2==1:continue     
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
113/13: pwd
113/14: !lsd
113/15: cd cec_L1
113/16:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    i,j=[int(tif_name[k]) for k in [15,17]]
    if (i+j)%2==1:continue     
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
113/17:
with open('fnames.txt','r') as f:
  fnames=[i.strip('\n') for i in f]
113/18:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    i,j=[int(tif_name[k]) for k in [15,17]]
    if (i+j)%2==1:continue     
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
114/1: cd cec_L1
114/2:
with open('fnames.txt','r') as f:
  fnames=[i.strip('\n') for i in f]
114/3: import os
114/4: from pandas import *
114/5: nc_name='a0.nc'
114/6:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    i,j=[int(tif_name[k]) for k in [15,17]]
    if (i+j)%3!=0:continue     
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
114/7:
def tif2df(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[1], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  if len(df)==0:
    df.to_csv(tif_name.replace('.tif','.csv'),header=None)
    return 1
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixy2']=df.ixy
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy2',aggfunc='count').reset_index()
  pv1['N']=pv2.ixy2
  pv1['X']=[float(i.split('_')[0])*1000 for i in pv1.ixy]
  pv1['Y']=[float(i.split('_')[1])*1000 for i in pv1.ixy]
  col=['X','Y','N','val']
  pv1[col].set_index('X').to_csv(tif_name.replace('.tif','.csv'),header=None)
  return 0
114/8:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    i,j=[int(tif_name[k]) for k in [15,17]]
    if (i+j)%3!=0:continue     
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
115/1:
def tif2df(tif_name,nc_name):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  data=img.read()
  dx,dy=250.,250.
  if nz!=1:return -1

  nc = netCDF4.Dataset(nc_name, 'r')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  x0,y0=pnyc(img.lnglat()[0],img.lnglat()[1], inverse=False)
  x0,y0=x0-dx*(nx//2),x0-dy*(ny//2)
  x_1d=[x0+dx*i for i in range(nx)]
  y_1d=[y0-dy*i for i in range(ny)] 
  xm, ym = np.meshgrid(x_1d, y_1d)
  x,y=xm.flatten(),ym.flatten()
  lon, lat = pnyc(x, y, inverse=True)
  DD={'lon':lon,'lat':lat,'X':x,'Y':y,'val':data.flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(df.val!=-32768)
  df=df.loc[boo].reset_index(drop=True)
  if len(df)==0:
    df.to_csv(tif_name.replace('.tif','.csv'),header=None)
    return 1
  df['ix'],df['iy']=df.X//1000,df.X//1000
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  df['ixy2']=df.ixy
  pv1=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv2=pivot_table(df,index='ixy',values='ixy2',aggfunc='count').reset_index()
  pv1['N']=pv2.ixy2
  pv1['X']=[float(i.split('_')[0])*1000 for i in pv1.ixy]
  pv1['Y']=[float(i.split('_')[1])*1000 for i in pv1.ixy]
  col=['X','Y','N','val']
  pv1[col].set_index('X').to_csv(tif_name.replace('.tif','.csv'),header=None)
  return 0
115/2: import os
115/3: from pandas import *
115/4: nc_name='a0.nc'
115/5:
with open('fnames.txt','r') as f:
  fnames=[i.strip('\n') for i in f]
115/6:
for tif_name in fnames:
    csv=tif_name.replace('.tif','.csv')
    if os.path.exists(csv):continue
    i,j=[int(tif_name[k]) for k in [15,17]]
    if (i+j)%5!=0:continue     
    os.system('touch '+csv)
    print(tif_name,tif2df(tif_name,nc_name))
115/7: tif_name
108/1447: ls *csv|wc
108/1448: len(fnames)
108/1449: !cat ../header.txt *.csv > all.txt
108/1450: df=read_csv('all.txt')
108/1451: %history -f his.txt
108/1452: !vi his.txt
108/1453: pwd
108/1454:
fname='a0.nc'
nc = netCDF4.Dataset(fname, 'r+')
108/1455: pwd
108/1456: nc.close()
108/1457:
fname='a0.nc'
nc = netCDF4.Dataset(fname, 'r+')
112/24: nc.close()
108/1458:
fname='a0.nc'
nc = netCDF4.Dataset(fname, 'r+')
108/1459: !cp a0.nc a1.nc
108/1460:
fname='a1.nc'
nc = netCDF4.Dataset(fname, 'r+')
108/1461: x,y=df.X,df.Y
108/1462: v
108/1463: nc[v][:]=0
108/1464: df['vn']=df.val*df.N
108/1465:
  pv=pivot_table(df,index='ixy',values=['N','vn'],aggfunc=np.sum).reset_index()
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
  pv['cec']=pv.vn/pv.N
108/1466:
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
108/1467:
  pv=pivot_table(df,index='ixy',values=['N','vn'],aggfunc=np.sum).reset_index()
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
  pv['cec']=pv.vn/pv.N
108/1468:
nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
var=np.zeros(shape=(nrow,ncol))
var[pv.iy,pv.ix]=pv.cec
nc[v][0,0,:,:]=var[:,:]
nc.close()
108/1469: !cat ../header.txt *.csv > all.txt
108/1470: df=read_csv('all.txt')
108/1471:
x,y=df.X,df.Y
df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
108/1472:
fname='a0.nc'
nc = netCDF4.Dataset(fname, 'r+')
108/1473:
fname='a1.nc'
nc = netCDF4.Dataset(fname, 'r+')
108/1474:
x,y=df.X,df.Y
df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
108/1475:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[3][0]
nc[v][:]=0
108/1476:
pv=pivot_table(df,index='ixy',values=['N','vn'],aggfunc=np.sum).reset_index()
pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
pv['cec']=pv.vn/pv.N
108/1477: df['vn']=df.val*df.N
108/1478:
pv=pivot_table(df,index='ixy',values=['N','vn'],aggfunc=np.sum).reset_index()
pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
pv['cec']=pv.vn/pv.N
108/1479:
nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
var=np.zeros(shape=(nrow,ncol))
var[pv.iy,pv.ix]=pv.cec
nc[v][0,0,:,:]=var[:,:]
nc.close()
108/1480: pv.head()
108/1481: !vi ../tif2df.py
108/1482: !cat ../header.txt *.csv > all.txt
108/1483:
fname='a1.nc'
nc = netCDF4.Dataset(fname, 'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[3][0]
nc[v][:]=0

!cat ../header.txt *.csv > all.txt
df=read_csv('all.txt')
x,y=df.X,df.Y
df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
df['vn']=df.val*df.N
pv=pivot_table(df,index='ixy',values=['N','vn'],aggfunc=np.sum).reset_index()
pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
pv['cec']=pv.vn/pv.N
nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
var=np.zeros(shape=(nrow,ncol))
var[pv.iy,pv.ix]=pv.cec
nc[v][0,0,:,:]=var[:,:]
nc.close()
108/1484: pwd
108/1485: cd ../cec_0-5cm_mean/
108/1486: !cat ../header.txt *.csv > all.txt
108/1487: ls *nc
108/1488: cp /nas1/cmaqruns/2018base/data/land/isric/cec/cec_L1/a1.nc .
108/1489:
fname='a1.nc'
nc = netCDF4.Dataset(fname, 'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[3][0]
nc[v][:]=0

df=read_csv('all.txt')
x,y=df.X,df.Y
df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
df['vn']=df.val*df.N
pv=pivot_table(df,index='ixy',values=['N','vn'],aggfunc=np.sum).reset_index()
pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
pv['cec']=pv.vn/pv.N
nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
var=np.zeros(shape=(nrow,ncol))
var[pv.iy,pv.ix]=pv.cec
nc[v][0,0,:,:]=var[:,:]
nc.close()
108/1490: img.bounds
108/1491: img.bounds()
108/1492: img.bounds[left]
108/1493: img.bounds['left']
108/1494: type(img.bounds)
108/1495: img.bounds.left
108/1496: nx,ny,nz=img.width,img.height,img.count
108/1497: dx=(img.bounds.right-img.bounds.left)/nx
108/1498: dx
108/1499: dx,dy=(img.bounds.right-img.bounds.left)/nx,(img.bounds.top-img.bounds.bottom)/ny
108/1500: dx,dy
108/1501: 6954-4483
108/1502: 6954-4218
108/1503:
fname='a1.nc'
nc = netCDF4.Dataset(fname, 'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[3][0]
nc[v][:]=0

!cat ../header.txt *.csv > all.txt
df=read_csv('all.txt')
x,y=df.X,df.Y
df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
df['vn']=df.val*df.N
pv=pivot_table(df,index='ixy',values=['N','vn'],aggfunc=np.sum).reset_index()
pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
pv['cec']=pv.vn/pv.N
nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
var=np.zeros(shape=(nrow,ncol))
var[pv.iy,pv.ix]=pv.cec
nc[v][0,0,:,:]=var[:,:]
nc.close()
108/1504: fname='tileSG-020-076_2-3'
108/1505: fname='tileSG-020-076_2-3.tif'
108/1506: img = rasterio.open(fname);img.bounds
108/1507: dx,dy=(img.bounds.right-img.bounds.left)/nx,(img.bounds.top-img.bounds.bottom)/ny
108/1508: dx,dy
108/1509: dx,dy=(img.bounds.right-img.bounds.left)/img.width,(img.bounds.top-img.bounds.bottom)/img.height
108/1510: dx,dy
108/1511: nx,ny,nz=img.width,img.height,img.count
108/1512: nx,ny,nz
108/1513: img.lnglat()
108/1514: fname='tileSG-020-076_1-3.tif'
108/1515: img = rasterio.open(fname);img.lnglat()
108/1516: fname='tileSG-020-076_2-2'
108/1517: img = rasterio.open(fname);img.lnglat()
108/1518: data.shape
108/1519: cd /nas1/cmaqruns/2018base/data/land/isric/wise/wise_30min_v3/DBF
108/1520: fname='yTPOR.DBF'
108/1521: from simpledbf import Dbf5
108/1522: dbf = Dbf5(fname, codec='utf-8')
108/1523: df = dbf.to_dataframe()
108/1524: df.head()
108/1525: df.columns
108/1526: [i for i in df.columns if 'SOIL' in i]
108/1527: [i for i in df.columns if 'AREA' in i]
108/1528: [i for i in df.columns if 'TPOR' in i]
108/1529: !psg wget
108/1530: df.columns
108/1531: len(df.columns)
108/1532: min(df.SNUM)
108/1533: max(df.SNUM)
108/1534: ls
108/1535: ls -lh *atx
108/1536: more PHH2O_S.SNUM.atx
108/1537: pwd
108/1538: len(df)
108/1539: !pip install glob
108/1540: !pip install arcpy
116/1:
import pandas as pd
import geopandas as gp
116/2:
fname='./Layers/Wisesnum.lyr'
a=gp.read_file(fname)
116/3:
fname='./Wisesnum/dblbnd.adf'
a=gp.read_file(fname)
116/4: a=dir(gp.read_file
116/5: a=dir(gp.read_file)
116/6: a
116/7: a=dir(gp.read_file.format)
116/8: a=dir(gp.read_file__format)
116/9: a=dir(gp.read_file._format)
116/10: import gdal
108/1541: !which ipython
117/1: import gdal
108/1542: pwd
108/1543: cd /nas1/cmaqruns/2018base/data/land/isric/cec/globe
108/1544: ls
108/1545: fname='CECSOL_M_sl1_250m_ll.tif'
108/1546: img = rasterio.open(fname);img.lnglat()
108/1547: nx,ny,nz=img.width,img.height,img.count
108/1548: nx,ny,nz
108/1549: dx,dy=(img.bounds.right-img.bounds.left)/img.width,(img.bounds.top-img.bounds.bottom)/img.height
108/1550: dx,dy
108/1551: img.bounds()
108/1552: img.bounds
108/1553: !tifinfo CECSOL_M_sl1_250m_ll.tif
108/1554: pwd
108/1555: img.profile
108/1556: img.profile['nodata']
108/1557: img.profile['transform']
108/1558: img.profile['transform'].Affine
108/1559: img.profile['transform'][:]
108/1560: dx,x0,dy,y0=[img.profile['transform'][i] for i in [0,2,4,5]]
108/1561: dx,x0,dy,y0
108/1562: nz
108/1563: !ln -s /nas1/cmaqruns/2018base/data/land/HarvestedAreaYield175Crops_Geotiff/a1.nc .
108/1564: nc_name
108/1565: nc_name='a1.nc'
108/1566:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,x0,dy,y0=[img.profile['transform'][i] for i in [0,2,4,5]]
  lon_1d=[x0+dx*i for i in range(nx)]
  lat_1d=[y0+dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[0,:,:].flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(data != img.profile['nodata'])
  df=df.loc[boo].reset_index(drop=True)
  
  nc = netCDF4.Dataset(nc_name, 'r+')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.sum).reset_index()
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
  var=np.zeros(shape=(nrow,ncol))
  var[pv.iy,pv.ix]=pv.val
  nc[V[3][0]][0,lev,:,:]=var[:,:]
  nc.close()
  return 0
108/1567: tif_name=fname
108/1568: lev=0
108/1569: i=tif2nc(tif_name,nc_name,lev)
108/1570:
def tif2nc(tif_name,nc_name,lev):
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
  if nz!=1:return -1
  dx,x0,dy,y0=[img.profile['transform'][i] for i in [0,2,4,5]]
  lon_1d=[x0+dx*i for i in range(nx)]
  lat_1d=[y0+dy*i for i in range(ny)]
  data=img.read()
  lonm, latm = np.meshgrid(lon_1d, lat_1d)
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[0,:,:].flatten()}
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(data != img.profile['nodata'])
  df=df.loc[boo].reset_index(drop=True)
  
  nc = netCDF4.Dataset(nc_name, 'r+')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
  var=np.zeros(shape=(nrow,ncol))
  var[pv.iy,pv.ix]=pv.val
  nc[V[3][0]][0,lev,:,:]=var[:,:]
  nc.close()
  return 0
108/1571: i=tif2nc(tif_name,nc_name,lev)
108/1572:
  lon_1d=np.array([x0+dx*i for i in range(nx)])
  lat_1d=np.array([y0+dy*i for i in range(ny)])
  idx0,idx1=np.where((lat_1d>=10)&(lat_1d<=50)),np.where((lon_1d>=60)&(lon_1d<=180))
108/1573: idx0
108/1574: len(idx0)
108/1575: idx0,idx1=np.where((lat_1d>=10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
108/1576: len(idx0)
108/1577: i0,i1=np.meshgrid(idx0[:], idx1[:])
108/1578: lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
108/1579: len(lonm.flatten())
108/1580: len(i0.flatten())
108/1581: data.shape
108/1582: data=img.read()
108/1583: data.shape
108/1584: DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[0,i0.flatten(),i1.flatten()]}
108/1585: a=data[0,:,:]
108/1586: DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':a[0,i0.flatten(),i1.flatten()]}
108/1587: DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':a[i0.flatten(),i1.flatten()]}
108/1588:
  df=DataFrame(DD)
  boo=(df.lon>=60)&(df.lon<=180)&(df.lat>=-10)&(df.lat<=50)&(data != img.profile['nodata'])
  df=df.loc[boo].reset_index(drop=True)
118/1: nc_name='a0.nc'
118/2: fname='CECSOL_M_sl1_250m_ll.tif'
118/3: tif_name=fname
118/4:
  from pandas import *
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
118/5: pwd
118/6: cd /nas1/cmaqruns/2018base/data/land/isric/cec/globe
118/7:
  from pandas import *
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz=img.width,img.height,img.count
118/8:
  dx,x0,dy,y0=[img.profile['transform'][i] for i in [0,2,4,5]]
  lon_1d=np.array([x0+dx*i for i in range(nx)])
  lat_1d=np.array([y0+dy*i for i in range(ny)])
  idx0,idx1=np.where((lat_1d>=10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
  data=img.read()[0,:,:]
  lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
  i1,i0=np.meshgrid(idx1[:], idx0[:])
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[i0.flatten(),i1.flatten()]}
  df=DataFrame(DD)
118/9:   img,lonm,latm,data=0,0,0,0
118/10:   df=DataFrame(DD)
118/11: img = rasterio.open(tif_name)
118/12: img.profile['nodata']
118/13: nodata=img.profile['nodata']
118/14:   img=0
118/15:   boo=df.val != nodata
118/16:   df=df.loc[boo].reset_index(drop=True)
118/17: 200~  nc = netCDF4.Dataset(nc_name, 'r+')
118/18:   pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
118/19:   V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
118/20:   nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
118/21:   #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
118/22:   x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
118/23:   x,y=np.array(x),np.array(y)
118/24:   df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
118/25:
  nc = netCDF4.Dataset(nc_name, 'r+')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
118/26: nc_name='a1.nc'
118/27:
  nc = netCDF4.Dataset(nc_name, 'r+')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
118/28:
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False)
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
118/29:   x,y=0,0 #clean_up the memory
118/30:   pv=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
118/31:
  df=0 #clean_up the memory
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
118/32: len(pv)
118/33:
  var=np.zeros(shape=(nrow,ncol))
  var[pv.iy,pv.ix]=pv.val
  nc[V[3][0]][0,lev,:,:]=var[:,:]
  nc.close()
118/34: lev=0
118/35:
  var=np.zeros(shape=(nrow,ncol))
  var[pv.iy,pv.ix]=pv.val
  nc[V[3][0]][0,lev,:,:]=var[:,:]
  nc.close()
118/36: !to
118/37: !top
118/38: !top
118/39: len(df)
118/40: len(pv)
118/41: pv.head()
118/42: df=pv
118/43:   val=np.exp10(-df.val)
118/44:   val=10^(-df.val)
118/45:   val=10**(-df.val)
118/46: val[:5]
118/47: var.shape()
118/48: var.shape
118/49: var[:5,:5]
118/50: a=-np.log10(val)
118/51: a[:5]
118/52: !top
118/53: !top
118/54: !top
118/55: !top
118/56: nc_name
118/57: ls
118/58: nc='nCECSOL_M_sl1_250m_ll.nc'
118/59: nc = netCDF4.Dataset(fname, 'r')
118/60: fname='CECSOL_M_sl5_250m_ll.nc'
118/61: nc = netCDF4.Dataset(fname, 'r')
118/62: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
118/63: v
118/64: v=V[3][0]
118/65: nc[v]
118/66: nc_FillValue
118/67: nc._FillValue
118/68: nc.FillValue
118/69: dir(nc)
118/70: a=dir(nc)
118/71: a[:30]
118/72: a[30:70]
118/73: nc[v]
118/74: nc.default
118/75: var[:5,:5]
118/76: var=np.where(var==0,9.969209968386869e+36,var)
118/77: var[:5,:5]
118/78: !top
118/79: fname='PHIHOX_M_sl5_250m_ll.tif'
118/80: img = rasterio.open(tif_name)
118/81: img = rasterio.open(fname)
118/82: data=img.read()[0,:,:]
118/83: np.max(data)
118/84: pwd
118/85: img.profile
118/86: data[:5,:5]
118/87: img.nodata
118/88: idx=np.where(data != img.nodata)
118/89: len(idx[0])
118/90: a=data[idx[0],idx[1]]
118/91: max(a)
118/92: min(a)
118/93: 255.0==255
119/1:
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np
119/2: tif_name='PHIHOX_M_sl5_250m_ll.tif'
119/3:
  img = rasterio.open(tif_name)
  nx,ny,nz,nodata=img.width,img.height,img.count,img.profile['nodata']
  if nz!=1:return -1
  dx,x0,dy,y0=[img.profile['transform'][i] for i in [0,2,4,5]]
  lon_1d=np.array([x0+dx*i for i in range(nx)])
  lat_1d=np.array([y0+dy*i for i in range(ny)])
  idx0,idx1=np.where((lat_1d>=-10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
  data=img.read()[0,:,:]
  img=0 #clean_up the memory
  lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
  i1,i0=np.meshgrid(idx1[:], idx0[:])
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[i0.flatten(),i1.flatten()]}
  img,lonm,latm,data=0,0,0,0 #clean_up the memory
  df=DataFrame(DD)
  df=df.loc[df.val != nodata].reset_index(drop=True)
119/4: pwd
120/1:
  img = rasterio.open(tif_name)
  nx,ny,nz,nodata=img.width,img.height,img.count,img.profile['nodata']
  if nz!=1:return -1
  dx,x0,dy,y0=[img.profile['transform'][i] for i in [0,2,4,5]]
  lon_1d=np.array([x0+dx*i for i in range(nx)])
  lat_1d=np.array([y0+dy*i for i in range(ny)])
  idx0,idx1=np.where((lat_1d>=-10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
  data=img.read()[0,:,:]
  img=0 #clean_up the memory
  lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
  i1,i0=np.meshgrid(idx1[:], idx0[:])
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[i0.flatten(),i1.flatten()]}
  img,lonm,latm,data=0,0,0,0 #clean_up the memory
  df=DataFrame(DD)
  df=df.loc[df.val != nodata].reset_index(drop=True)
120/2:
  import numpy as np
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np
120/3: tif_name='PHIHOX_M_sl5_250m_ll.tif'
120/4:
  img = rasterio.open(tif_name)
  nx,ny,nz,nodata=img.width,img.height,img.count,img.profile['nodata']
  if nz!=1:return -1
  dx,x0,dy,y0=[img.profile['transform'][i] for i in [0,2,4,5]]
  lon_1d=np.array([x0+dx*i for i in range(nx)])
  lat_1d=np.array([y0+dy*i for i in range(ny)])
  idx0,idx1=np.where((lat_1d>=-10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
  data=img.read()[0,:,:]
  img=0 #clean_up the memory
  lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
  i1,i0=np.meshgrid(idx1[:], idx0[:])
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[i0.flatten(),i1.flatten()]}
  img,lonm,latm,data=0,0,0,0 #clean_up the memory
  df=DataFrame(DD)
  df=df.loc[df.val != nodata].reset_index(drop=True)
120/5:
  img = rasterio.open(tif_name)
  nx,ny,nz,nodata=img.width,img.height,img.count,img.profile['nodata']  
  dx,x0,dy,y0=[img.profile['transform'][i] for i in [0,2,4,5]]
  lon_1d=np.array([x0+dx*i for i in range(nx)])
  lat_1d=np.array([y0+dy*i for i in range(ny)])
  idx0,idx1=np.where((lat_1d>=-10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
  data=img.read()[0,:,:]
  img=0 #clean_up the memory
  lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
  i1,i0=np.meshgrid(idx1[:], idx0[:])
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[i0.flatten(),i1.flatten()]}
  img,lonm,latm,data=0,0,0,0 #clean_up the memory
  df=DataFrame(DD)
  df=df.loc[df.val != nodata].reset_index(drop=True)
120/6: from pandas import *
120/7: pwd
120/8:
  df=DataFrame(DD)
  df=df.loc[df.val != nodata].reset_index(drop=True)
120/9:
  nc = netCDF4.Dataset(nc_name, 'r+')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False) #taking time, can not parallelize
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  x,y=0,0 #clean_up the memory
120/10: nc_name='a1.nc'
120/11: ls
120/12:
  nc = netCDF4.Dataset(nc_name, 'r+')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

  #d00範圍：北緯-10~50、東經60~180。'area': [50, 60, -10, 180,],
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False) #taking time, can not parallelize
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  x,y=0,0 #clean_up the memory
120/13:
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
  val=10**(-df.val/10.)
  df.val=val
  pv=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
  df=0 #clean_up the memory
120/14:
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
  var=np.zeros(shape=(nrow,ncol))
  var[pv.iy,pv.ix]=pv.val
  idx=np.where(var>0)
  a=-np.log10(var[idx[0][:],idx[1][:]])
  var[idx[0][:],idx[1][:]]=a[:]
  var=np.where(var<=0,9.969209968386869e+36,var)
  nc[V[3][0]][0,lev,:,:]=var[:,:]
  nc.close()
120/15: lev=0
120/16:
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
  var=np.zeros(shape=(nrow,ncol))
  var[pv.iy,pv.ix]=pv.val
  idx=np.where(var>0)
  a=-np.log10(var[idx[0][:],idx[1][:]])
  var[idx[0][:],idx[1][:]]=a[:]
  var=np.where(var<=0,9.969209968386869e+36,var)
  nc[V[3][0]][0,lev,:,:]=var[:,:]
  nc.close()
120/17: pv.head()
120/18:   img = rasterio.open(tif_name)
120/19:   data=img.read()[0,:,:]
120/20:
  lon_1d=np.array([x0+dx*i for i in range(nx)])
  lat_1d=np.array([y0+dy*i for i in range(ny)])
  idx0,idx1=np.where((lat_1d>=-10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
120/21:
  lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
  i1,i0=np.meshgrid(idx1[:], idx0[:])
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[i0.flatten(),i1.flatten()]}
120/22:   img,lonm,latm,data=0,0,0,0 #clean_up the memory
120/23:   df=DataFrame(DD)
120/24: DD=0
120/25: nodata
120/26: len(df)
120/27:   df=df.loc[df.val != nodata].reset_index(drop=True)
120/28: len(df)
120/29: nc = netCDF4.Dataset(nc_name, 'r+')
120/30: x[:5]
120/31: df.head()
120/32: min(df.val)
120/33: max(df.val)
120/34:
  x,y=pnyc(list(df.lon),list(df.lat), inverse=False) #taking time, can not parallelize
  x,y=np.array(x),np.array(y)
  df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
  df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
  x,y=0,0 #clean_up the memory
  df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
  df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
120/35: df.head()
120/36: val=10**(-df.val/10.)
120/37: val[:5]
120/38: val=-df.val/10.
120/39: val[:5]
120/40: df.tail()
120/41: val[-5:]
120/42: val=10**(-np.array(df.val)/10.)
120/43: val[-5:]
120/44: a=np.array(df.val)
120/45: val=10**(-a/10.)
120/46: val[-5:]
120/47: a[-5:]
120/48: val=np.power(10,-a/10.)
120/49: val[-5:]
120/50: a[-5:]
120/51: a=-np.array(df.val)/10.
120/52: a[-5:]
120/53: a=-np.array(list(df.val))/10.
120/54: a[-5:]
120/55: df.tail()
120/56: a=np.array(df.val)
120/57: a[-5:]
120/58: a=np.array(df.val)/10.
120/59: a[-5:]
120/60: val=np.power(10,-a)
120/61: val[-5:]
120/62: val=10**(-a)
120/63: val[-5:]
120/64: val=np.power(10,-np.array(df.val)/10.)
120/65: val[-5:]
120/66:
  a=np.array(df.val)/10.
  df.val=10**(-a)
120/67: df.tail()
120/68:   pv=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
120/69: df=0
120/70:
  pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
  pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
  var=np.zeros(shape=(nrow,ncol))
  var[pv.iy,pv.ix]=pv.val
  idx=np.where(var>0)
120/71:   a=-np.log10(var[idx[0][:],idx[1][:]])
120/72: a[:5]
120/73:   var[idx[0][:],idx[1][:]]=a[:]
120/74:   var=np.where(var<=0,9.969209968386869e+36,var)
120/75:
  nc[V[3][0]][0,lev,:,:]=var[:,:]
  nc.close()
120/76:   var=np.zeros(shape=(nrow,ncol))
120/77:   var[idx[0][:],idx[1][:]]=a[:]
120/78:   nc = netCDF4.Dataset(nc_name, 'r+')
120/79:
  nc[V[3][0]][0,lev,:,:]=var[:,:]
  nc.close()
120/80: import multiprocessing as mp
120/81: pool = mp.Pool(mp.cpu_count())
120/82: df=0;pv=0
120/83: pv.head()
120/84: df.head()
121/1: tif_name='PHIHOX_M_sl5_250m_ll.tif'
121/2:
  import numpy as np
  from pandas import DataFrame, pivot_table
  import netCDF4
  from pyproj import Proj
  import rasterio
  import numpy as np

  img = rasterio.open(tif_name)
  nx,ny,nz,nodata=img.width,img.height,img.count,img.profile['nodata']
121/3:
  dx,x0,dy,y0=[img.profile['transform'][i] for i in [0,2,4,5]]
  lon_1d=np.array([x0+dx*i for i in range(nx)])
  lat_1d=np.array([y0+dy*i for i in range(ny)])
  idx0,idx1=np.where((lat_1d>=-10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
  data=img.read()[0,:,:]
  img=0 #clean_up the memory
  lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
  i1,i0=np.meshgrid(idx1[:], idx0[:])
  DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[i0.flatten(),i1.flatten()]}
  img,lonm,latm,data=0,0,0,0 #clean_up the memory
  df=DataFrame(DD)
  df=df.loc[df.val != nodata].reset_index(drop=True)
121/4:
  nc = netCDF4.Dataset(nc_name, 'r+')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
121/5: nc_name='PHIHOX_M_sl5_250m_ll.nc'
121/6:
  nc = netCDF4.Dataset(nc_name, 'r+')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
121/7: nc_name='PHIHOX_M_sl1_250m_ll.nc'
121/8:
  nc = netCDF4.Dataset(nc_name, 'r+')
  pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
121/9: import multiprocessing as mp
121/10: pool = mp.Pool(mp.cpu_count())
121/11: pool.apply(pnyc,args=(df.lon[0],df.lat[0],inverse=False))
121/12: pool.apply(pnyc,args=(df.lon[0],df.lat[0],'inverse=False'))
121/13: pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0,inverse=False)
121/14: pool.apply(pnyc,args=(df.lon[0],df.lat[0]))
121/15: results=[pool.apply(pnyc,args=(df.lon[i],df.lat[i])) for i in range(len(df))]
121/16: pool = mp.Pool(10)
121/17: results=[pool.apply(pnyc,args=(df.lon[i],df.lat[i])) for i in range(100)]
121/18: results
121/19: results=[pool.apply(pnyc,args=(df.lon[i],df.lat[i])) for i in range(len(df))]
121/20: pool = mp.Pool(5)
121/21: results=[pool.apply(pnyc,args=(df.lon[i],df.lat[i])) for i in range(500)]
121/22: results=[pool.apply(pnyc,args=(df.lon[i],df.lat[i])) for i in range(5000)]
121/23: results=[pool.apply(pnyc,args=(df.lon[i],df.lat[i])) for i in range(len(df))]
122/1: !topu
122/2: cd /nas1/cmaqruns/2018base/data/land/isric/por.GLDAS.NASA
122/3: import netCDF4
122/4: ls
122/5: fname='GLDASp5_porosity_025d.nc4'
122/6: nc = netCDF4.Dataset(nc_name, 'r')
122/7: nc = netCDF4.Dataset(fname, 'r')
122/8: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/9: V
122/10: v='GLDAS_porosity'
122/11: nc[v]
122/12: nc[v].missing_value
122/13: !ncks -d LAY,0 /nas1/cmaqruns/2018base/data/land/HarvestedAreaYield175Crops_Geotiff/a1.nc a1.nc
122/14: nt,nrow,ncol=(nc[v].shape[i] for i in range(3))
122/15: nrow,ncol
122/16:
lon_1d=nc['lon']
lat_1d=nc['lat']
122/17:
data=nc[v][0,:,:]
nodata=nc[v].missing_value
lon_1d=nc['lon']
lat_1d=nc['lat']
idx0,idx1=np.where((lat_1d>=-10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
i1,i0=np.meshgrid(idx1[:], idx0[:])
DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[i0.flatten(),i1.flatten()]}
img,lonm,latm,data=0,0,0,0 #clean_up the memory
df=DataFrame(DD)
df=df.loc[f.val != nodata].reset_index(drop=True)
122/18: import numpy as np
122/19:
from pandas import DataFrame, pivot_table
import numpy as np
import netCDF4
from pyproj import Proj
122/20:
data=nc[v][0,:,:]
nodata=nc[v].missing_value
lon_1d=nc['lon']
lat_1d=nc['lat']
idx0,idx1=np.where((lat_1d>=-10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
i1,i0=np.meshgrid(idx1[:], idx0[:])
DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[i0.flatten(),i1.flatten()]}
img,lonm,latm,data=0,0,0,0 #clean_up the memory
df=DataFrame(DD)
df=df.loc[f.val != nodata].reset_index(drop=True)
122/21:
lon_1d=nc['lon'][:]
lat_1d=nc['lat'][:]
idx0,idx1=np.where((lat_1d>=-10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
i1,i0=np.meshgrid(idx1[:], idx0[:])
DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[i0.flatten(),i1.flatten()]}
img,lonm,latm,data=0,0,0,0 #clean_up the memory
df=DataFrame(DD)
df=df.loc[f.val != nodata].reset_index(drop=True)
122/22: df=df.loc[df.val != nodata].reset_index(drop=True)
122/23: len(df)
122/24:
fname='a1.nc'
nc = netCDF4.Dataset(nc_name, 'r+')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
122/25:
nc = netCDF4.Dataset(fname, 'r+')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
122/26:
x,y=pnyc(list(df.lon),list(df.lat), inverse=False) #taking time, can not parallelize
x,y=np.array(x),np.array(y)
df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
x,y=0,0 #clean_up the memory
df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
pv=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
df=0 #clean_up the memory
pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
var=np.zeros(shape=(nrow,ncol))
var[pv.iy,pv.ix]=pv.val
nc[V[3][0]][0,lev,:,:]=var[:,:]
nc.close()
122/27: lev=0
122/28:
x,y=pnyc(list(df.lon),list(df.lat), inverse=False) #taking time, can not parallelize
x,y=np.array(x),np.array(y)
df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
x,y=0,0 #clean_up the memory
df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
pv=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
df=0 #clean_up the memory
pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
var=np.zeros(shape=(nrow,ncol))
var[pv.iy,pv.ix]=pv.val
nc[V[3][0]][0,lev,:,:]=var[:,:]
nc.close()
122/29: df.head()
122/30:
df=DataFrame(DD)
df=df.loc[df.val != nodata].reset_index(drop=True)
122/31: df.head()
122/32: nodata
122/33: df=df.loc[df.val >0].reset_index(drop=True)
122/34: df.head()
122/35: df.tail()
122/36:
fname='GLDASp5_porosity_025d.nc4'
nc = netCDF4.Dataset(nc_name, 'r')
v='GLDAS_porosity'
data=nc[v][0,:,:]
lon_1d=nc['lon'][:]
lat_1d=nc['lat'][:]
idx0,idx1=np.where((lat_1d>=-10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
i1,i0=np.meshgrid(idx1[:], idx0[:])
DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[i0.flatten(),i1.flatten()]}
img,lonm,latm,data=0,0,0,0 #clean_up the memory
df=DataFrame(DD)
df=df.loc[df.val >0 ].reset_index(drop=True)
122/37:
fname='GLDASp5_porosity_025d.nc4'
nc = netCDF4.Dataset(fname, 'r')
v='GLDAS_porosity'
data=nc[v][0,:,:]
lon_1d=nc['lon'][:]
lat_1d=nc['lat'][:]
idx0,idx1=np.where((lat_1d>=-10)&(lat_1d<=50))[0],np.where((lon_1d>=60)&(lon_1d<=180))[0]
lonm, latm = np.meshgrid(lon_1d[idx1[:]], lat_1d[idx0[:]])
i1,i0=np.meshgrid(idx1[:], idx0[:])
DD={'lon':lonm.flatten(),'lat':latm.flatten(),'val':data[i0.flatten(),i1.flatten()]}
img,lonm,latm,data=0,0,0,0 #clean_up the memory
df=DataFrame(DD)
df=df.loc[df.val >0 ].reset_index(drop=True)
122/38: df.tail()
122/39: df.head()
122/40:
x,y=pnyc(list(df.lon),list(df.lat), inverse=False) #taking time, can not parallelize
x,y=np.array(x),np.array(y)
df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
122/41:
fname='a1.nc'
nc = netCDF4.Dataset(fname, 'r+')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))

x,y=pnyc(list(df.lon),list(df.lat), inverse=False) #taking time, can not parallelize
x,y=np.array(x),np.array(y)
df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
122/42:
df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
df['ixy']=[str(i)+'_'+str(j) for i,j in zip(df.ix,df.iy)]
pv=pivot_table(df,index='ixy',values='val',aggfunc=np.mean).reset_index()
df=0 #clean_up the memory
pv['ix']=[int(i.split('_')[0]) for i in pv.ixy]
pv['iy']=[int(i.split('_')[1]) for i in pv.ixy]
var=np.zeros(shape=(nrow,ncol))
var[pv.iy,pv.ix]=pv.val
nc[V[3][0]][0,lev,:,:]=var[:,:]
nc.close()
122/43: from scipy.interpolate import griddata
122/44: df['x'],df['y']=x,y
122/45: x[:5]
122/46: y[:5]
122/47:
df=DataFrame(DD)
df=df.loc[df.val >0 ].reset_index(drop=True)
122/48:
x,y=pnyc(list(df.lon),list(df.lat), inverse=False) #taking time, can not parallelize
x,y=np.array(x),np.array(y)
df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
df['x'],df['y']=x,y
122/49:
fname='a1.nc'
nc = netCDF4.Dataset(fname, 'r+')
122/50:
x,y=pnyc(list(df.lon),list(df.lat), inverse=False) #taking time, can not parallelize
x,y=np.array(x),np.array(y)
df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
df['x'],df['y']=x,y
122/51:
x,y=pnyc(list(df.lon),list(df.lat), inverse=False) #taking time, can not parallelize
x,y=np.array(x),np.array(y)
df['ix']=np.array((x-nc.XORIG)/nc.XCELL,dtype=int)
df['iy']=np.array((y-nc.YORIG)/nc.YCELL,dtype=int)
df['x'],df['y']=x,y
122/52: df.head()
122/53: df=df.loc[(df.ix>=0)&(df.ix<ncol)&(df.iy>=0)&(df.iy<nrow)].reset_index(drop=True)
122/54: len(df)
122/55: mp=len(df)
122/56: xyc= [(df.x[i],df.y[i]) for i in range(mp)]
122/57:
zz=np.zeros(shape=(nrow,ncol))
c = np.array(df.val)
zz[:,: ] = griddata(xyc, c[:], (x1, y1), method='linear')
122/58:
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
122/59:
zz=np.zeros(shape=(nrow,ncol))
c = np.array(df.val)
zz[:,: ] = griddata(xyc, c[:], (x1, y1), method='linear')
122/60: var=np.zeros(shape=(nrow,ncol))
122/61: var[:,: ] = griddata(xyc, c[:], (x1, y1), method='linear')
122/62:
nc[V[3][0]][0,lev,:,:]=var[:,:]
nc.close()
122/63: data=np.where(data>0,data,0)
122/64:
lon_1d=nc['lon'][:]
lat_1d=nc['lat'][:]
lonm, latm = np.meshgrid(lon_1d, lat_1d)
122/65:
fname='GLDASp5_porosity_025d.nc4'
nc = netCDF4.Dataset(fname, 'r')
v='GLDAS_porosity'
data=nc[v][0,:,:]
data=np.where(data>0,data,0)
lon_1d=nc['lon'][:]
lat_1d=nc['lat'][:]
lonm, latm = np.meshgrid(lon_1d, lat_1d)
122/66: x,y=pnyc(lonm,latm, inverse=False)
122/67: type(x)
122/68: x.shape
122/69:
mp=len(x.flatten())
xyc= [(x.flatten()[i],y.flatten()[i]) for i in range(mp)]
122/70: x,y=pnyc(lonm,latm, inverse=False)
122/71:
maxx,maxy=x1[-1,-1],y1[-1,-1]
minx,miny=x1[0,0],y1[0,0]
boo=(abs(x) <= (maxx - minx) /2+nc.XCELL*10) & (abs(y) <= (maxy - miny) /2+nc.YCELL*10)
idx = np.where(boo)
mp=len(idx[0])
xyc= [(x[idx[0][i],idx[1][i]],y[idx[0][i],idx[1][i]]) for i in range(mp)]
122/72:
fname='a1.nc'
nc = netCDF4.Dataset(fname, 'r+')
122/73:
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
maxx,maxy=x1[-1,-1],y1[-1,-1]
minx,miny=x1[0,0],y1[0,0]
boo=(abs(x) <= (maxx - minx) /2+nc.XCELL*10) & (abs(y) <= (maxy - miny) /2+nc.YCELL*10)
idx = np.where(boo)
mp=len(idx[0])
xyc= [(x[idx[0][i],idx[1][i]],y[idx[0][i],idx[1][i]]) for i in range(mp)]
122/74: c = data[idx[0][:],idx[1][:]]
122/75: len(c)
122/76: mp
122/77: var[:,: ] = griddata(xyc, c[:], (x1, y1), method='linear')
122/78:
nc[V[3][0]][0,lev,:,:]=var[:,:]
nc.close()
122/79: cp a1.nc GLDASp5_porosityCWBWRF_15Km.nc
122/80:
fname='GLDASp5_porosity_025d.nc4'
nc = netCDF4.Dataset(fname, 'r')
v='GLDAS_porosity'
data=nc[v][0,:,:]
data=np.where(data>0,data,0)
lon_1d=nc['lon'][:]
lat_1d=nc['lat'][:]
lonm, latm = np.meshgrid(lon_1d, lat_1d)
x,y=pnyc(lonm,latm, inverse=False)
mp=len(x.flatten())
xyc= [(x.flatten()[i],y.flatten()[i]) for i in range(mp)]

fname='GLDASp5_porosityCWBWRF_15Km.nc'
nc = netCDF4.Dataset(fname, 'r+')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
maxx,maxy=x1[-1,-1],y1[-1,-1]
minx,miny=x1[0,0],y1[0,0]
boo=(abs(x) <= (maxx - minx) /2+nc.XCELL*10) & (abs(y) <= (maxy - miny) /2+nc.YCELL*10)
idx = np.where(boo)
mp=len(idx[0])
xyc= [(x[idx[0][i],idx[1][i]],y[idx[0][i],idx[1][i]]) for i in range(mp)]

var=np.zeros(shape=(nrow,ncol))
c = data[idx[0][:],idx[1][:]]
var[:,: ] = griddata(xyc, c[:], (x1, y1), method='linear')
nc[V[3][0]][0,lev,:,:]=var[:,:]
nc.close()
122/81: xyc=zip(x.flatten(),y.flatten())
122/82: xyc[:5]
122/83: a=np.array(xyc)
122/84: a[:5]
122/85: xyc=tuple(zip(x.flatten(),y.flatten()))
122/86: xyc[:5]
122/87:
fname='GLDASp5_porosity_025d.nc4'
nc = netCDF4.Dataset(fname, 'r')
v='GLDAS_porosity'
data=nc[v][0,:,:]
data=np.where(data>0,data,0)
lon_1d=nc['lon'][:]
lat_1d=nc['lat'][:]
lonm, latm = np.meshgrid(lon_1d, lat_1d)
x,y=pnyc(lonm,latm, inverse=False)

fname='GLDASp5_porosityCWBWRF_15Km.nc'
nc = netCDF4.Dataset(fname, 'r+')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
maxx,maxy=x1[-1,-1],y1[-1,-1]
minx,miny=x1[0,0],y1[0,0]
boo=(abs(x) <= (maxx - minx) /2+nc.XCELL*10) & (abs(y) <= (maxy - miny) /2+nc.YCELL*10)
idx = np.where(boo)
mp=len(idx[0])
xyc= [(x[idx[0][i],idx[1][i]],y[idx[0][i],idx[1][i]]) for i in range(mp)]

var=np.zeros(shape=(nrow,ncol))
c = data[idx[0][:],idx[1][:]]
var[:,: ] = griddata(xyc, c[:], (x1, y1), method='linear')
nc[V[3][0]][0,lev,:,:]=var[:,:]
nc.close()
122/88: fname='GLDASp5_porosityCWBWRF_15Km.nc'
122/89: nc = netCDF4.Dataset(fname, 'r+')
122/90: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/91: nc[V[3][0]].units='Vv/Vs(fraction)'
122/92: nc[V[3][0]]
122/93: nc[V[3][0]].var_desc='GLDASp5_porosity in CWBWRF_15Km system'
122/94: nc[V[3][0]].long_name='GLDASp5_porosity'
122/95: nc.close()
122/96: !ncrename -v HARV_AREA,POROSITY GLDASp5_porosityCWBWRF_15Km.nc
122/97: xyc= [(x[idx[0],idx[1]],y[idx[0],idx[1]])]
122/98: xyc[:5]
122/99: len(xyc)
122/100: xyc[0]
122/101: len(xyc[0])
122/102: len(xyc[0][0])
122/103: !topu
122/104: ls
122/105: pwd
122/106: cd ../wilt/
122/107: ls
122/108: run rdWWP.py WWP_M_sl1_250m_ll.tif WWP_M_sl1_250m_ll.nc 0
122/109: cd /nas1/cmaqruns/2016base/data/land/epic_festc1.4_20180516
122/110: fname='2016_US1_time20160702_bench.nc2'
122/111: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/112: V
122/113: nc = netCDF4.Dataset(fname, 'r')
122/114: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/115: V
122/116: cd /home/cmaqruns/2018base/data/bcon
122/117: !lst
122/118: fname=BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/119: fname='BCON_v53_1804_run5_regrid_20180331_CWBWRF_15'
122/120: nc = netCDF4.Dataset(fname, 'r+')
122/121: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/122: nbnd,nt,nlay=(nc[v].shape[i] for i in range(3))
122/123: nbnd,nt,nlay=(nc[v].shape[i] for i in range(2))
122/124: nc = netCDF4.Dataset(fname, 'r+')
122/125: nbnd,nt,nlay=(nc[V[2]].shape[i] for i in range(2))
122/126: nbnd,nt,nlay=(nc[V[2][3]].shape[i] for i in range(2))
122/127: V
122/128: V[2][3]
122/129: nbnd,nt,nlay=nc[V[2][3]].shape
122/130: nbnd,nt,nlay
122/131:
for v in V[2]:
    if v=='TFLAG':continue
    for b in range(nb,2212):
        nc[v][b,:,:]=0.
122/132:
for v in V[2]:
    if v=='TFLAG':continue
    for b in range(nbnd,2212):
        nc[v][b,:,:]=0.
122/133: nc.NBNDS
122/134: a=dir(nc)
122/135: a[:40]
122/136:
atts=['CDATE',  'CTIME', 'EXEC_ID', 'FILEDESC', 'FTYPE', 'GDNAM', 'GDTYP', 'HISTORY', 'IOAPI_VERSION', 'NCO', 'NCOLS',  'NROWS',
     'NTHIK', 'P_ALP', 'P_BET', 'P_GAM', 'UPNAM', 'WDATE',
     'WTIME', 'XCELL', 'XCENT', 'XORIG', 'YCELL', 'YCENT', 'YORIG']
122/137: pwd
122/138: !lst ../mcip/1804_run5/CWBWRF_15k/
122/139: fname='METDOT3D_1804_run5.nc'
122/140: nc0 = netCDF4.Dataset(fname, 'r')
122/141: fname='../mcip/1804_run5/CWBWRF_15k/METDOT3D_1804_run5.nc'
122/142: nc0 = netCDF4.Dataset(fname, 'r')
122/143:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/144: nc.close()
122/145: cd ../icon
122/146: ls
122/147: fname='CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc'
122/148: nc = netCDF4.Dataset(fname, 'r+')
122/149:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/150: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/151: ncol,nt,nlay,nrow=(nc.variables[V[3][0]].shape[i] for i in range(4))
122/152: ncol,nt,nlay,nrow
122/153: nc.NCOL
122/154: nc.NCOLS
122/155:
for v in V[3]:
  for c in range(ncol,nc.NCOLS):
    nc[v][c,:,:,:]=0.
122/156: nc.close()
122/157: fname
122/158: nc = netCDF4.Dataset(fname, 'r+')
122/159:
for v in V[3]:
  for r in range(nrow,nc.NROWS):
    nc[v][r,:,:,:]=0.
122/160: nc.close()
122/161: cd /nas1/ecmwf/reanalysis/gribs18
122/162: fname='1804.nc'
122/163: nc = netCDF4.Dataset(fname, 'r')
122/164: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/165: V
122/166: v='initial_time0'
122/167: nc[v]
122/168:
SDATE=[datetime.datetime.strptime(''.join([str(i, encoding='utf-8') for i in list(nc.variables[V[1][0]][t, :])]),\
 '%m/%d/%Y (%H:%M)') for t in range(nt)]
122/169: import datetime
122/170:
SDATE=[datetime.datetime.strptime(''.join([str(i, encoding='utf-8') for i in list(nc.variables[V[1][0]][t, :])]),\
 '%m/%d/%Y (%H:%M)') for t in range(nt)]
122/171: SDATE[:5]
122/172: run grb2m3.py 18033100.nc D0
122/173: ntA
122/174: delt
122/175: bdate,edate
122/176: ntA=max([1,int(delt.total_seconds()/3600.)])
122/177: ntA
122/178: vi grb2m3.py
122/179: !vi grb2m3.py
122/180: run grb2m3.py 18033100.nc D0
122/181: !vi grb2m3.py
122/182: !vi grb2m3.py
122/183: !vi grb2m3.py
122/184: fname
122/185: cd /nas1/ecmwf/reanalysis/gribs18
122/186: !lst
122/187: fname='templateD0.ncV49K34'
122/188: nc = netCDF4.Dataset(fname, 'r')
122/189: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/190: len(V[3])
122/191: nc.NVARS
122/192: fname='templateCWBWRF_15k.ncV50K40'
122/193: nc = netCDF4.Dataset(fname, 'r+')
122/194:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/195: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/196:
for v in V[3]:
  for c in range(ncol,nc.NCOLS):
    nc[v][c,:,:,:]=0.
122/197: nc.close()
122/198:
for v in V[3]:
  for r in range(nrow,nc.NROWS):
    nc[v][r,:,:,:]=0.
122/199: nc = netCDF4.Dataset(fname, 'r+')
122/200:
for v in V[3]:
  for r in range(nrow,nc.NROWS):
    nc[v][r,:,:,:]=0.
122/201: nc.close()
122/202: nc = netCDF4.Dataset(fname, 'r+')
122/203:
for v in V[3]:
  for c in range(ncol,nc.NCOLS):
    nc[v][c,:,:,:]=0.
122/204: nc.close()
122/205: fname='templateCWBWRF_15k.ncV50K40'
122/206: nc = netCDF4.Dataset(fname, 'r+')
122/207:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/208: nc.close()
122/209: fname='/nas1/cmaqruns/2018base/data/mcip/1804_run5/CWBWRF_15k/METCRO3D_1804_run5.nc'
122/210: nc0 = netCDF4.Dataset(fname, 'r')
122/211:
for i in atts:
  if i not in dir(nc0):continue
    exec('print(i,nc0.'+i)
122/212:
for i in atts:
  if i not in dir(nc0):continue
  exec('print(i,nc0.'+i)
122/213:
for i in atts:
  if i not in dir(nc0):continue
  exec('print(i,nc0.'+i+')')
122/214: fname='templateCWBWRF_15k.ncV50K40'
122/215:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/216: nc.close()
122/217: fname
122/218: nc = netCDF4.Dataset(fname, 'r+')
122/219:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/220: nc.close()
122/221: nc0.NROWS
122/222: fname='/nas1/cmaqruns/2018base/data/mcip/1804_run5/CWBWRF_15k/METCRO3D_1804_run5.nc'
122/223: nc0 = netCDF4.Dataset(fname, 'r')
122/224: nc0.NROWS
122/225: fname='templateCWBWRF_15k.ncV50K40'
122/226: pwd
122/227: nc = netCDF4.Dataset(fname, 'r+')
122/228:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/229: nc0.NROWS
122/230: nc.NROWS
122/231: nc.close()
122/232: 15*670/2
122/233: 15*665/2
122/234: 15*664/2
122/235: 15*388/2
122/236: pwd
122/237: cd /home/cmaqruns/2018base/data/bcon
122/238: lst
122/239: !lst
122/240: fnameO='BCON_v53_1804_run5_regrid_20180331_CWBWRF_15'
122/241: nc1= netCDF4.Dataset(fnameO,'r')
122/242:
V1=[list(filter(lambda x:nc1.variables[x].ndim==j, [i for i in nc1.variables])) for j in [1,2,3,4]]
nv1=len(V1[2])
nt1,nlay1,nbnd1=nc1.variables[V1[2][0]].shape
122/243: pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc1.P_ALP, lat_2=nc1.P_BET,lat_0=nc1.YCENT, lon_0=nc1.XCENT, x_0=0, y_0=0.0)
122/244:
nrow1,ncol1=nc1.NROWS,nc1.NCOLS
nrow0,ncol0=nc1.NROWS+5,nc1.NCOLS+5
x1d=[nc1.XORIG+nc0.XCELL*(i-2) for i in range(ncol0)]
y1d=[nc1.YORIG+nc0.YCELL*(i-2) for i in range(nrow0)]
x1,y1=np.meshgrid(x1d, y1d)
122/245:
i0,j0=1,1
i1,j1=i0+ncol1,j0+nrow1
idx=[(i,j0) for i in range(i0+1,i1+1)]  +   [(i1,j) for j in range(j0+1,j1+1)] + \
    [(i,j1) for i in range(i1-1,i0-1,-1)] + [(i0,j) for j in range(j1-1,j0-1,-1)]
122/246: len(idx)
122/247:
idx=np.array(idx)
x1=x1[idx[0],idx[1]]
122/248: len(x1)
122/249: idx[:5]
122/250:
idx=[(j0,i) for i in range(i0+1,i1+1)]  +   [(i,i1) for j in range(j0+1,j1+1)] + \
    [(j1,i) for i in range(i1-1,i0-1,-1)] + [(j,i0) for j in range(j1-1,j0-1,-1)]
idx=np.array(idx).flatten()
122/251: idx[:5]
122/252:
idx=[(j0,i) for i in range(i0+1,i1+1)]  +   [(i,i1) for j in range(j0+1,j1+1)] + \
    [(j1,i) for i in range(i1-1,i0-1,-1)] + [(j,i0) for j in range(j1-1,j0-1,-1)]
122/253: idx[:5]
122/254:
idx=[(j0,i) for i in range(i0+1,i1+1)]  +   [(i,i1) for j in range(j0+1,j1+1)] + \
    [(j1,i) for i in range(i1-1,i0-1,-1)] + [(j,i0) for j in range(j1-1,j0-1,-1)]
idx=np.array(idx,dtype=int).flatten().reshape(2,2112)
122/255: i0,j0
122/256:
i0,j0=1,1
i1,j1=i0+ncol1,j0+nrow1
122/257: i1,j1
122/258:
idx=[(j0,i) for i in range(i0+1,i1+1)]  +   [(i,i1) for j in range(j0+1,j1+1)] + \
    [(j1,i) for i in range(i1-1,i0-1,-1)] + [(j,i0) for j in range(j1-1,j0-1,-1)]
122/259: ii=np.array(idx,dtype=int)
122/260: idx[-5:]
122/261: i0,j0
122/262: i1,j1
122/263:
idx=[(j0,i) for i in range(i0+1,i1+1)]  +   [(j,i1) for j in range(j0+1,j1+1)] + \
    [(j1,i) for i in range(i1-1,i0-1,-1)] + [(j,i0) for j in range(j1-1,j0-1,-1)]
122/264: ii=np.array(idx,dtype=int)
122/265: ii[:5]
122/266: ii[:5][0]
122/267: ii=np.array(idx,dtype=int).flatten()
122/268: ii[:10]
122/269: a=ii.reshape(2112,2)
122/270: a[:5,:]
122/271: a=ii.reshape(2112,2).T
122/272: a[0][:5], a[1][:5]
122/273: nbnd1
122/274: ii=np.array(idx,dtype=int).flatten()
122/275: len(ii)
122/276: (nrow1+ncol1)*2+4
122/277:
i0,j0=1,1
i1,j1=i0+ncol1+1,j0+nrow1+1
idx=[(j0,i) for i in range(i0+1,i1+1)]  +   [(j,i1) for j in range(j0+1,j1+1)] + \
    [(j1,i) for i in range(i1-1,i0-1,-1)] + [(j,i0) for j in range(j1-1,j0-1,-1)]
122/278: len(idx)
122/279: nbnd1
122/280: nc1.NCOLS
122/281: pwd
122/282: !lst
122/283: fname='/nas1/cmaqruns/2018base/data/mcip/1804_run5/CWBWRF_15k/METCRO3D_1804_run5.nc'
122/284: nc0 = netCDF4.Dataset(fname, 'r')
122/285: nc0.NROWS
122/286: nc0.NCOLS
122/287: fname='BCON_v53_1804_run5_regrid_20180331_CWBWRF_15'
122/288: nc = netCDF4.Dataset(fname, 'r+')
122/289:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/290: nc.close()
122/291: fnameO
122/292:
nc1= netCDF4.Dataset(fnameO,'r+')
V1=[list(filter(lambda x:nc1.variables[x].ndim==j, [i for i in nc1.variables])) for j in [1,2,3,4]]
nv1=len(V1[2])
nt1,nlay1,nbnd1=nc1.variables[V1[2][0]].shape
122/293:
nrow1,ncol1=nc1.NROWS,nc1.NCOLS
nrow0,ncol0=nc1.NROWS+5,nc1.NCOLS+5
x1d=[nc1.XORIG+nc0.XCELL*(i-2) for i in range(ncol0)]
y1d=[nc1.YORIG+nc0.YCELL*(i-2) for i in range(nrow0)]
x1,y1=np.meshgrid(x1d, y1d)
122/294:
i0,j0=1,1
i1,j1=i0+ncol1+1,j0+nrow1+1
idx=[(j0,i) for i in range(i0+1,i1+1)]  +   [(j,i1) for j in range(j0+1,j1+1)] + \
    [(j1,i) for i in range(i1-1,i0-1,-1)] + [(j,i0) for j in range(j1-1,j0-1,-1)]
122/295: len(idx)
122/296: nbnd1
122/297: (nrow1+ncol1)*2+4
122/298: !lst
122/299: pwd
122/300:
nc1= netCDF4.Dataset(fnameO,'r+')
V1=[list(filter(lambda x:nc1.variables[x].ndim==j, [i for i in nc1.variables])) for j in [1,2,3,4]]
nv1=len(V1[2])
nt1,nlay1,nbnd1=nc1.variables[V1[2][0]].shape
122/301:
nrow1,ncol1=nc1.NROWS,nc1.NCOLS
nrow0,ncol0=nc1.NROWS+5,nc1.NCOLS+5
x1d=[nc1.XORIG+nc0.XCELL*(i-2) for i in range(ncol0)]
y1d=[nc1.YORIG+nc0.YCELL*(i-2) for i in range(nrow0)]
x1,y1=np.meshgrid(x1d, y1d)
122/302: nbnd1
122/303: fname
122/304: nc = netCDF4.Dataset(fname, 'r+')
122/305: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/306:
for v in V[3]:
  for c in range(2112):
    nc[v][c,:,:]=0.
122/307: nc.close()
122/308: fnameO
122/309:
nc1= netCDF4.Dataset(fnameO,'r+')
V1=[list(filter(lambda x:nc1.variables[x].ndim==j, [i for i in nc1.variables])) for j in [1,2,3,4]]
nv1=len(V1[2])
nt1,nlay1,nbnd1=nc1.variables[V1[2][0]].shape
122/310: nbnd1
122/311: nc = netCDF4.Dataset(fname, 'r+')
122/312:
for v in V[2]:
  for c in range(2112):
    nc[v][c,:,:]=0.
122/313: nc.close()
122/314: nc = netCDF4.Dataset(fname, 'r+')
122/315: fname
122/316:
for v in V[2]:
  if v=='TFLAG':continue
  for c in range(2112):      
    nc[v][c,:,:]=0.
122/317: nc.close()
122/318:
nc1= netCDF4.Dataset(fnameO,'r+')
V1=[list(filter(lambda x:nc1.variables[x].ndim==j, [i for i in nc1.variables])) for j in [1,2,3,4]]
nv1=len(V1[2])
nt1,nlay1,nbnd1=nc1.variables[V1[2][0]].shape
122/319: nbnd1
122/320: len(idx)
122/321: idx=np.array(idx,dtype=int).flatten().reshape(nbnd,2).T
122/322: nbnd1
122/323: idx=np.array(idx,dtype=int).flatten().reshape(nbnd1,2).T
122/324: idx[0][:5],idx[1][:5]
122/325: idx[0][-5:],idx[1][-5:]
122/326: pwd
122/327: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/328: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/329: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/330: len(idx[0])
122/331: x1.shape
122/332: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/333: y1.shape
122/334: a=y1[idx[0][:],idx[1][:]]
122/335: len(a)
122/336: a=y1[idx[0],idx[1]]
122/337: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/338:
maxx,maxy=max(x1),max(y1)
minx,miny=min(x1),min(y1)
122/339: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/340: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/341: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/342: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/343: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/344: ntA
122/345: dens2.shape
122/346: dens.shape
122/347: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/348: np.max(idxb[0])
122/349: np.min(idxb[0])
122/350:
idx=[(j0,i) for i in range(ncol1)] +[(j0,i1)] +   [(j,i1) for j in range(nrow1)] +[(j1,i1)] + \
    [(j1,i) for i in range(i1,i0-1,-1)] +[(j1,i0)] + [(j,i0) for j in range(j1,j0-1,-1)]+[(j0,i0)]
idxb=np.array(idx,dtype=int).flatten().reshape(nbnd1,2).T
122/351: ncol1
122/352: nrow1
122/353:
i0,j0=0,0
i1,j1=ncol1-1,nrow1-1
idx=[(j0,i) for i in range(ncol1)] +[(j0,i1)] +   [(j,i1) for j in range(nrow1)] +[(j1,i1)] + \
    [(j1,i) for i in range(i1,i0-1,-1)] +[(j1,i0)] + [(j,i0) for j in range(j1,j0-1,-1)]+[(j0,i0)]
idxb=np.array(idx,dtype=int).flatten().reshape(nbnd1,2).T
122/354: len(idxb)
122/355: len(idxb[0])
122/356: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/357: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15
122/358:  !lst
122/359: cd /home/cmaqruns/2018base/data/land/1804
122/360: fname='2018_CWBWRF_15k_time20180331_bench.nc'
122/361: nc = netCDF4.Dataset(fname, 'r+')
122/362: ncol
122/363: ncol,nt,nlay,nrow=(nc.variables[V[3][0]].shape[i] for i in range(4))
122/364: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/365: ncol,nt,nlay,nrow=(nc.variables[V[3][0]].shape[i] for i in range(4))
122/366:
for v in V[3]:
  if v=='TFLAG':continue
  for c in range(ncol,nc0.NCOLS):      
    nc[v][c,:,:,:]=0.
122/367:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/368: nc.NCOLS
122/369: nc.NROWS
122/370: nc.NVARS
122/371: 'NVARS' in atts
122/372: nc.close()
122/373: nc = netCDF4.Dataset(fname, 'r+')
122/374:
for v in V[3]:
  if v=='TFLAG':continue
  for r in range(nrow,nc0.NROWS):      
    nc[v][r,:,:,:]=0.
122/375: nc.close()
122/376: !lst
122/377: !lst
122/378: !vi ../../../CTM_LOG_000.v53_gcc_1804_run5_20180331_CWBWRF_15k_11
122/379: pwd
122/380: fname
122/381: pwd
122/382: cd ../../emis/REAS/1804
122/383: ls
122/384: fname='area_CWBWRF_15k.20180331.nc'
122/385: ls
122/386: nc = netCDF4.Dataset(fname, 'r')
122/387: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/388: V[3]
122/389: s=''
122/390:
for v in V[3]:
    s+='{:16s}'.format(v)
122/391: s
122/392: !lst
122/393: pwd
122/394: !ls
122/395: cd /home/cmaqruns/2018base/data/land/1804
122/396: fname='2018_CWBWRF_15k_time20180331_bench.nc'
122/397: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/398: s=''
122/399:
for v in V[3]:
    s+='{:16s}'.format(v)
122/400: s
122/401: nc = netCDF4.Dataset(fname, 'r')
122/402: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/403: s=''
122/404:
for v in V[3]:
    s+='{:16s}'.format(v)
122/405: s
122/406: fname='/home/cmaqruns/2018base/data/icon/ICON_v53_1804_run5_CWBWRF_15k_regrid_20180331'
122/407: nc = netCDF4.Dataset(fname, 'r+')
122/408: nc.XCENTX
122/409: nc.XCENT
122/410:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/411: nc.XCENT
122/412: nc.close()
122/413: cd /home/cmaqruns/2018base/data/land/1804
122/414: fname='
122/415: fname='2018_CWBWRF_15k_soil_bench1804.nc'
122/416: nc = netCDF4.Dataset(fname, 'r+')
122/417:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/418: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/419: ncol,nt,nlay,nrow=(nc.variables[V[3][0]].shape[i] for i in range(4))
122/420:
for v in V[3]:
  if v=='TFLAG':continue
  for c in range(ncol,nc0.NCOLS):      
    nc[v][c,:,:,:]=0.
122/421: nc.close()
122/422: nc = netCDF4.Dataset(fname, 'r+')
122/423:
for v in V[3]:
  if v=='TFLAG':continue
  for r in range(nrow,nc0.NROWS):      
    nc[v][r,:,:,:]=0.
122/424: nc.close()
122/425: !lst
122/426: cd ../../../icon
122/427: cd ../../..
122/428: cd data/icon
122/429: ls
122/430: pwd
122/431: cd /home/cmaqruns/2018base/data/icon
122/432: ls
122/433: fname='CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc'
122/434: nc = netCDF4.Dataset(fname, 'r+')
122/435: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/436: len(V[3])
122/437: 'AACD' in V[3]
122/438: np.mean(nc['AACD'][:])
122/439: ls -lhrt
122/440: ls /nas1/cmaqruns/2018base/data/output_CCTM_v53_gcc_1804_run5/../output_CCTM_v53_gcc_1803_run12/
122/441: mv CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc /nas1/cmaqruns/2018base/data/output_CCTM_v53_gcc_1804_run5/../output_CCTM_v53_gcc_1803_run12/CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc
122/442: pwd
122/443: cd ../land
122/444: ls
122/445: fname='beld4.CWBWRF_15k.ncf'
122/446: nc = netCDF4.Dataset(fname, 'r+')
122/447: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/448: 'Hay' in V[3]
122/449: v='Hay'
122/450: nc[v]
122/451: np.min(nc[v][:])
122/452: np.max(nc[v][:])
122/453: a=dir(nc)
122/454: 'bound' in a
122/455: [i.lower() for i in a if 'bound' in i.lower()]
122/456: [i.lower() for i in a if 'bou' in i.lower()]
122/457: [i.lower() for i in a if 'b' in i.lower()]
122/458: [i.lower() for i in a if 'col' in i.lower()]
122/459: nc.NCOLS
122/460: nc[v]
122/461: c=v
122/462: nc.variables[c][:].data
122/463: (nc.variables[c][:].data==0).all()
122/464: ls
122/465: fname='beld4.EAsia_81K.ncf'
122/466: nc = netCDF4.Dataset(fname, 'r')
122/467: (nc.variables[c][:].data==0).all()
122/468: fname='beld4.CWBWRF_15k.ncf'
122/469: nc = netCDF4.Dataset(fname, 'r')
122/470: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/471:
for v in V[3]:
    print(v,np.max(nc[v][:]))
122/472:
for v in V[3]:
    mx=np.max(nc[v][:])
    if mx !=0.: print(v,np.max(nc[v][:]))
122/473: fname='beld4.EAsia_81K.ncf'
122/474: nc = netCDF4.Dataset(fname, 'r')
122/475:
for v in V[3]:
    mx=np.max(nc[v][:])
    if mx !=0.: print(v,np.max(nc[v][:]))
122/476: nc.close()
122/477: pwd
122/478: ls -lh
122/479: s='ALD2            '
122/480: len(s)
122/481: pwd
122/482: cd ../emis
122/483: cd 1804
122/484: cd REAS/1804
122/485: ls
122/486: fname='area_CWBWRF_15k.20180331.nc'
122/487: nc = netCDF4.Dataset(fname, 'r')
122/488: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/489: v=V[3][0]
122/490: nt,nrow,ncol=(nc[v].shape[i] for i in range(3))
122/491: nt,nlay,nrow,ncol=(nc[v].shape[i] for i in range(4))
122/492: nt,nlay,nrow,ncol
122/493: nc.NROWS
122/494:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/495: nc = netCDF4.Dataset(fname, 'r+')
122/496:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/497: nc.close()
122/498: fname
122/499: nc = netCDF4.Dataset(fname, 'r+')
122/500: nc.GDNAM
122/501: nc.GDNAM=''
122/502: nc.close()
122/503: nc = netCDF4.Dataset(fname, 'r+')
122/504: nc.GDNAM='CWBWRF_15k'
122/505: nc.close()
122/506: nc = netCDF4.Dataset(fname, 'r+')
122/507: nc.close()
122/508: nc = netCDF4.Dataset(fname, 'r+')
122/509: ncol,nt,nlay,nrow=(nc.variables[V[3][0]].shape[i] for i in range(4))
122/510: ncol,nt,nlay,nrow
122/511:
for v in V[3]:
  if v=='TFLAG':continue
  for c in range(ncol,666):      
    nc[v][c,:,:,:]=0.
122/512: nc.NCOLS
122/513: nc.NCOLS=666
122/514: nc.close()
122/515: nc = netCDF4.Dataset(fname, 'r+')
122/516: nc.NROWS
122/517: nc.NROWS=390
122/518: nrow
122/519:
for v in V[3]:
  if v=='TFLAG':continue
  for r in range(nrow,nc.NROWS):      
    nc[v][r,:,:,:]=0.
122/520: nc.close()
122/521: fname="/home/cmaqruns/2018base/data/bcon/BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k"
122/522: nc = netCDF4.Dataset(fname, 'r+')
122/523: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/524: v=V[2][0]
122/525: nt,nrow,ncol=(nc[v].shape[i] for i in range(3))
122/526: nt
122/527: bdate=datetime.datetime(2018,3,31)
122/528: pwd
122/529:
def dt2jul(dt):
  yr=dt.year
  deltaT=dt-datetime.datetime(yr,1,1)
  deltaH=int((deltaT.total_seconds()-deltaT.days*24*3600)/3600.)
  return (yr*1000+deltaT.days+1,deltaH*10000)

def jul2dt(jultm):
  jul,tm=jultm[:]
  yr=int(jul/1000)
  ih=int(tm/10000.)
  return datetime.datetime(yr,1,1)+datetime.timedelta(days=int(jul-yr*1000-1))+datetime.timedelta(hours=ih)
122/530: sdate=[bdate+datetime.timedelta(hours=t) for t in range(nt)]
122/531: sdate[:5]
122/532: sdate[-5:]
122/533: dt2jul[sdate[0]]
122/534: dt2jul(sdate[0])
122/535:
for t in range(nt):
  nc['TFLAG'][t,:,0]=dt2jul(sdate[t])[0]
  nc['TFLAG'][t,:,1]=dt2jul(sdate[t])[1]
122/536: nc.close()
122/537: fname
122/538: nc = netCDF4.Dataset(fname, 'r+')
122/539: nc.TSTEP
122/540: nc.SDATE
122/541: nc.SDATE=2018090
122/542: nc.STIME
122/543: nc.close()
122/544: nc = netCDF4.Dataset(fname, 'r+')
122/545: v='AALJ'
122/546: np.max(nc[v][:])
122/547: np.min(nc[v][:])
122/548: cd /nas1/cmaqruns/2018base/data/bcon
122/549: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k
122/550: run grb2bc.py 1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k
122/551: pwd
122/552: ls -l 1804.nc
122/553: run grb2bc.py /nas1/ecmwf/reanalysis/gribs18/1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k
122/554: mv /nas1/cmaqruns/2018base/data/bcon/*15k .
122/555: mv /home/cmaqruns/2018base/data/bcon/*15k_11 .
122/556: mv /home/cmaqruns/2018base/data/bcon/*15k .
122/557: run grb2bc.py /nas1/ecmwf/reanalysis/gribs18/1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k
122/558: np.max(c)
122/559: np.min(c)
122/560: v
122/561: fname
122/562: pwd
122/563: ls
122/564: fname=BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k
122/565: fname='BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k'
122/566: v='O3'
122/567: nc = netCDF4.Dataset(fname, 'r')
122/568: np.min(nc[v][:])
122/569: np.max(nc[v][:])
122/570: v='NO2'
122/571: np.max(nc[v][:])
122/572: np.min(nc[v][:])
122/573: v='AALJ'
122/574: np.max(nc[v][:])
122/575: np.min(nc[v][:])
122/576: nt
122/577: v
122/578: v=list(nms_part)[0]
122/579: v
122/580: t
122/581: np.max( zz[:t,:,:])
122/582: len(x1),len(y1)
122/583: len(xyc)
122/584: mp
122/585: abs(x1) <= (maxx - minx) /2+nc1.XCELL*10
122/586: (abs(x1) <= (maxx - minx) /2+nc1.XCELL*10).all()
122/587: (abs(y1) <= (maxy - miny) /2+nc1.YCELL*10).all()
122/588: c.shape
122/589: len(dd)
122/590: max(dd)
122/591: dd=d40_34
122/592: max(dd.values())
122/593: a=griddata(xyc, c[:,dd[k]], (x1, y1), method='linear')
122/594: a.shape
122/595: np.max(a)
122/596: a[:5]
122/597: (abs(y1) <= (maxy - miny) /2+nc1.YCELL*0).all()
122/598: maxy,miny
122/599: y1[0]
122/600: max(y1)
122/601: maxx,minx
122/602: x1[0]
122/603: xyc=np.array(xyc)
122/604: xyc.shape
122/605: xyc=np.array(xyc).T
122/606: xyc.shape
122/607: xyc[0][:5],xyc[1][:5]
122/608: xyc= [(x[idx[0][i],idx[1][i]],y[idx[0][i],idx[1][i]]) for i in range(mp)]
122/609: xyc[:5]
122/610: p=0
122/611: dist=(xyc[0]-x1[p])**2+(xyc[1]-y1[p])**2
122/612: dist.shape
122/613: dist[0].shape
122/614: xyc=np.array(xyc).T
122/615: dist=(xyc[0]-x1[p])**2+(xyc[1]-y1[p])**2
122/616: dist.shape
122/617: idxd=[[],[]]
122/618: i=np.where(dist==np.min(dist))
122/619: i
122/620: i[0]
122/621: i[0][0]
122/622: i=np.where(dist==np.min(dist))[0][0]
122/623: i
122/624:
idxd=[[],[]]
for p in range(nbnd1):
  dist=(xyc[0]-x1[p])**2+(xyc[1]-y1[p])**2
  i=np.where(dist==np.min(dist))[0][0]
  idxd[0].append(idx[0][i])
  idxd[1].append(idx[1][i])
122/625: idx[0][:5]
122/626: idx[0][100]
122/627: idx[1][100]
122/628: idx[1][nbnd1//2]
122/629: idx[0][nbnd1//2]
122/630: x[idx[0][nbnd1//2],idx[1][nbnd1//2]]
122/631: y[idx[0][nbnd1//2],idx[1][nbnd1//2]]
122/632: y1[nbnd1//2]
122/633: x1[nbnd1//2]
122/634: x[idxd[0][nbnd1//2],idxd[1][nbnd1//2]]
122/635: y[idxd[0][nbnd1//2],idxd[1][nbnd1//2]]
122/636:
def trans4_3(tt,ll,mm,ii):
#4-d transform to 3-d
  N=[np.zeros(shape=(tt,ll,mm),dtype=int) for i in range(4)]
  N[0][:,:,:]=np.array([t for t in range(tt)])[:,None,None]
  N[1][:,:,:]=np.array([k for k in range(ll)])[None,:,None]
  N[2][:,:,:]=ii[0][None,None,:]
  N[3][:,:,:]=ii[1][None,None,:]
  for n in range(4):
    N[n]=N[n].flatten()
  return N
122/637: N=trans4_3(nt,nlay,mp,idx)
122/638: c=np.zeros(shape=(nt,naly,mp))
122/639: c=np.zeros(shape=(nt,nlay,mp))
122/640: c[:,:,:] = var[N[0],N[1],N[2],N[3]].reshape(nt,nlay,mp)
122/641: N=trans4_3(nt,nlay,mp,idxd)
122/642: idxd=np.zeros(shape=(2,nbnd1),dtype=int)
122/643: N=trans4_3(nt,nlay,mp,idxd)
122/644: idxd=np.zeros(shape=(2,nbnd1),dtype=int)
122/645: p,i
122/646: p=0
122/647: idxd[:][p]=idx[:][i]
122/648: idx[:][i].shape
122/649: idxd[0][p]=idx[0][i]
122/650:
idxd=np.zeros(shape=(2,nbnd1),dtype=int)
for p in range(nbnd1):
  dist=(xyc[0]-x1[p])**2+(xyc[1]-y1[p])**2
  i=np.where(dist==np.min(dist))[0][0]
  idxd[0][p]=idx[0][i]
  idxd[1][p]=idx[1][i]
122/651: N=trans4_3(nt,nlay,mp,idx)
122/652: c[:,:,:] = var[N[0],N[1],N[2],N[3]].reshape(nt,nlay,mp)
122/653: M=trans4_3(nt,0,nbnd1,idxd)
122/654: len(M)
122/655: len(M[0])
122/656: nbnd1
122/657: len(idxd)
122/658:
M=[np.zeros(shape=(nt,nbnd1),dtype=int) for i in range(2)]
M[0][:,:]=(np.array([t for t in range(nt)])[:,None]).flatten()
M[1][:,:]=(idxd[None,:]).flatten()
122/659: M[0][:,:]=np.array([t for t in range(nt)])[:,None]
122/660: M[1][:,:]=idxd[None,:]
122/661:
idxd=np.zeros(shape=(nbnd1),dtype=int)
for p in range(nbnd1):
  dist=(xyc[0]-x1[p])**2+(xyc[1]-y1[p])**2
  idxd[p]=np.where(dist==np.min(dist))[0][0]
122/662: M[1][:,:]=idxd[None,:]
122/663: M[0],M[1]=M[0].flatten(),M[1].flatten()
122/664: c.shape
122/665: k=0
122/666: zz[:,k,:] = c[M[0],dd[k],M[1]].reshape(nt,nbnd1)
122/667: np.max( zz[:,0,:])
122/668: np.min( zz[:,0,:])
122/669:
  for k in range(nlay1):
    zz[:,k,:] = c[M[0],dd[k],M[1]].reshape(nt,nbnd1)
122/670: np.min( zz[:,:,:])
122/671: np.max( zz[:,:,:])
122/672: run grb2bc.py /nas1/ecmwf/reanalysis/gribs18/1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k
122/673: run grb2bc.py /nas1/ecmwf/reanalysis/gribs18/1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k
122/674: run grb2bc.py /nas1/ecmwf/reanalysis/gribs18/1804.nc BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k
122/675: nc1['TFLAG'][:,0,:]
122/676: ls
122/677: fnameO
122/678: nc = netCDF4.Dataset(fnameO, 'r')
122/679: nc['TFLAG'][:,0,:]
122/680: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/681: nt,nrow,ncol=(nc[v].shape[i] for i in range(3))
122/682: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
122/683: V[3][0]
122/684: nt,nlay,nbnd=(nc[V[2][0]].shape[i] for i in range(3))
122/685: bdate
122/686: nt
122/687: len(sdate)
122/688:
for t in range(nt):
  nc['TFLAG'][t,:,0]=dt2jul(sdate[t])[0]
  nc['TFLAG'][t,:,1]=dt2jul(sdate[t])[1]
122/689: nc = netCDF4.Dataset(fnameO, 'r+')
122/690:
for t in range(nt):
  nc['TFLAG'][t,:,0]=dt2jul(sdate[t])[0]
  nc['TFLAG'][t,:,1]=dt2jul(sdate[t])[1]
122/691: nc.SDATE
122/692: nc.STIME
122/693: nc.close()
122/694: nc = netCDF4.Dataset(fnameO, 'r')
122/695: v='AALJ'
122/696: np.min(nc[v][:])
122/697: np.max(nc[v][:])
122/698: fname
122/699: fnameO
122/700:
for v in V[2]:
    if v=='TFLAG':continue
    mx=np.max(nc[v][:])
    if mx !=0.: print(v,np.max(nc[v][:]))
122/701: v
122/702: v='AALJ'
122/703: np.max(nc[v][:])
122/704: np.min(nc[v][:])
122/705: nc.NVARS
122/706:
for i in atts:
  if i not in dir(nc0):continue
  exec('print (nc.'+i+')')
122/707:
for i in atts:
  if i not in dir(nc0):continue
  exec('print ('+i+',nc.'+i+')')
122/708:
for i in atts:
  if i not in dir(nc0):continue
  exec('print ("'+i+'",nc.'+i+')')
122/709: nc.close()
122/710: nc1.close()
122/711: nc = netCDF4.Dataset(fnameO, 'r')
122/712: v='TFLAG'
122/713: nc[v][:,0,:]
122/714: nc[v][:,1,:]
122/715: nc[v][:,49,:]
122/716: nc[v][:,49,-5:]
122/717: nc[v][-5:,49,:]
122/718: nc[v][-5:,48,:]
122/719: nc[v][-5:,0,:]
122/720: np.max(nc[v][-1,:,0])
122/721: np.max(nc[v][0,:,0])
122/722: np.max(nc[v][0,:,:])
122/723: np.max(nc[v][0,:,1])
122/724: nc.SDATE
122/725: nc.SDATE=2018090
122/726: nc = netCDF4.Dataset(fnameO, 'r+')
122/727: nc.SDATE=2018090
122/728: nc.STIME
122/729: nc.close()
122/730: fnameO
122/731: pwd
122/732: nc = netCDF4.Dataset(fnameO, 'r')
122/733: v='AALJ'
122/734: np.max(nc[v][:])
122/735: a=np.where(np.isnan(nc[v][:]))
122/736: len(a[0])
122/737: c=v
122/738: nc.variables[c][:].mask
122/739: nt,nlay,nbnd=
122/740: nt,nlay,nbnd
122/741: v='TFLAG'
122/742: nc[v].shape
122/743: fnameO
122/744: pwd
122/745: cd /home/cmaqruns/2018base/data/bcon
122/746: nc = netCDF4.Dataset(fnameO, 'r+')
122/747: len(V[2])
122/748:
for v in V[2]:
    if v=='TFLAG':continue
    nc[v].long_name=v
    nc[v].var_desc=v
122/749: nc.close()
122/750: pwd
122/751: nc = netCDF4.Dataset(fnameO, 'r+')
122/752: nc.GDNAM='CWBWRF_15k'
122/753: nc.close()
122/754: pwd
122/755: nc = netCDF4.Dataset(fnameO, 'r')
122/756: v
122/757: v='AALJ'
122/758: nc[v][0,:,5]
122/759: nc.NROWS
122/760: nc.NCOLS
122/761: (nc.NROWS+nc.NCOLS)*2+4==nbnd
122/762: nbnd
122/763: nc[v][0,0,:5]
122/764: np.max(nc[v][:,:,:],axis=0)
122/765: pwd
122/766: fname='bc'
122/767: nc = netCDF4.Dataset(fnameO, 'r')
122/768: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/769: nb,nt,nlay=(nc[V[2][0]].shape[i] for i in range(3))
122/770: nb,nt,nlay
122/771: V[2][0]
122/772: nc = netCDF4.Dataset(fname, 'r+')
122/773: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/774: nb,nt,nlay=(nc[V[2][0]].shape[i] for i in range(3))
122/775: nb,nt,nlay
122/776:
for v in V[2]:
    if v=='TFLAG':continue
    for p in range(nb,2112):
      nc[v][p,:,:]=0
122/777:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/778: nc.close()
122/779: nc = netCDF4.Dataset(fnameO, 'r')
122/780: var=np.zeros(shape=(50,270,2112))
122/781:
iv=0
for v in V[2]:
  if v=='TFLAG':continue
  var[iv,:,:]=nc[v][:,:,:]
  iv+=1
122/782: var=np.zeros(shape=(50,270,40,2112))
122/783:
iv=0
for v in V[2]:
  if v=='TFLAG':continue
  var[iv,:,:]=nc[v][:,:,:]
  iv+=1
122/784: nc = netCDF4.Dataset(fname, 'r+')
122/785: nt,nlay,nb=(nc[V[2][-1]].shape[i] for i in range(3))
122/786: nt,nlay,nb
122/787: nc.close()
122/788: nc = netCDF4.Dataset(fnameO, 'r')
122/789: var=np.zeros(shape=(50,270,40,2112))
122/790:
iv=0
for v in V[2]:
  if v=='TFLAG':continue
  var[iv,:,:,:]=nc[v][:,:,:]
  iv+=1
122/791: nc = netCDF4.Dataset(fname, 'r+')
122/792: nt,nlay,nb
122/793:
iv=0
for v in V[2]:
  if v=='TFLAG':continue
  nc[v][:,:,:]=var[iv,:nt,:,:]
  iv+=1
122/794: nc.close()
122/795: fnameO
122/796: nc = netCDF4.Dataset(fnameO, 'r+')
122/797: fname="/nas1/cmaqruns/2018base/data/bcon/BC_from_EAC4/BCON_v53_1804_run5_regrid_20180331_EAsia_81K"
122/798: nc = netCDF4.Dataset(fname, 'r')
122/799: FILEDESC=nc.FILEDESC
122/800: UPNAM=nc.UPNAM
122/801: GDNAM=nc.GDNAM
122/802: GDNAM
122/803: GDNAM='CWBWRF_15k      '
122/804: len(GDNAM)
122/805: nc = netCDF4.Dataset(fnameO, 'r+')
122/806: nc.UPNAM=UPNAM
122/807: nc.GDNAM=GDNAM
122/808: nc.FILEDESC=FILEDESC
122/809: nc.close()
122/810: fnameO
122/811: nc = netCDF4.Dataset(fname, 'r')
122/812: nc.EXEC_ID
122/813: EXEC_ID=nc.EXEC_ID
122/814: nc = netCDF4.Dataset(fnameO, 'r+')
122/815: nc.EXEC_ID=EXEC_ID
122/816: nc.close()
122/817: fname
122/818: !ncdump -h /nas1/cmaqruns/2018base/data/bcon/BC_from_EAC4/BCON_v53_1804_run5_regrid_20180331_EAsia_81K|M
122/819: nc = netCDF4.Dataset(fnameO, 'r+')
122/820: nc.FTYPE
122/821: nc.FTYPE=2
122/822: nc.close()
122/823: %history -f his.txt
122/824: pwd
122/825: !vi his.txt
122/826: bdate=datetime.datetime(2018,4,4)
122/827: sdate=[bdate+datetime.timedelta(hours=t) for t in range(121)]
122/828: ls
122/829: fname=BCON_v53_1804_run6_regrid_20180404_CWBWRF_15k
122/830: fname='BCON_v53_1804_run6_regrid_20180404_CWBWRF_15k'
122/831: nc = netCDF4.Dataset(fname, 'r+')
122/832:
for t in range(121):
  nc['TFLAG'][t,:,0]=dt2jul(sdate[t])[0]
  nc['TFLAG'][t,:,1]=dt2jul(sdate[t])[1]
122/833: var.shape
122/834: !vi his.txt
122/835: sdate[24]
122/836:
iv=0
for v in V[2]:
  if v=='TFLAG':continue
  nc[v][:,:,:]=var[iv,120:120+121,:,:]
  iv+=1
122/837: nc.SDATE,nc.STIME=dt2jul(sdate[0])
122/838: nc.close()
122/839: !psg tar
122/840: cd /home/cmaqruns/2018base/data/emis/REAS/1804
122/841: ls
122/842: fname='area_CWBWRF_15k.20180331.nc'
122/843: nc = netCDF4.Dataset(fname, 'r+')
122/844: nc.EXEC_ID="????????????????                                                                "
122/845: nc.IOAPI_VERSION="$Id:: init3.F 178 2015-03-02 16:35:15Z coats                $                   "
122/846: nc.UPNAM="CAMx2IOAPI      "
122/847: nc.GRIDNAM
122/848: nc.GDNAM
122/849: nc.GDNAM='{:16s}'.format('CWBWRF_15k')
122/850: nc.GDNAM
122/851: nc.FILEDESC="I/O API formatted CAMx EMIS output                                              "
122/852: nc.close()
122/853: fname='/nas1/ecmwf/reanalysis/gribs18/18033100D0.m3.nc'
122/854: nc = netCDF4.Dataset(fname, 'r')
122/855: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/856: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
122/857: var=np.zeros(shape=(nt,nlay,nrow,ncol))
122/858: var=np.zeros(shape=(len(V[3]),nt,nlay,nrow,ncol))
122/859:
iv=0
for v in V[3]:
  if v=='TFLAG':continue
  var[iv,:,:,:,:]=nc[v][:,:,:,:]
  iv+=1
122/860: fname='/home/cmaqruns/2018base/data/output_CCTM_v53_gcc_1803_run12/CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc'
122/861: nc = netCDF4.Dataset(fname, 'r+')
122/862:
iv=0
for v in V[3]:
  mn=np.min(nc[v][:,:,:,:],axis=(0,2,3))
  nc[v][:,:,:,:]=mn[None,:,None,None]  
  if v not in nc.variables:
    iv+=1
    continue
  nc[v][:,:,:,:]=var[iv,:,:,:,:]
  iv+=1
122/863: V1=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/864:
z=np.zeros(shape=(nt,nlay,nrow,ncol))
for v in V1[3]:
  mn=np.min(nc[v][:,:,:,:],axis=(0,2,3))
  z[:,:,:,:]=mn[None,:,None,None]  
  nc[v][:]=z[:]
  if v not in V[3]:continue
  iv=V[3].index(v)  
  nc[v][:,:,:,:]=var[iv,:,:,:,:]
122/865: mn.shape
122/866: z.shape
122/867: type(mn)
122/868: mn
122/869:
z=np.zeros(shape=(nt,nlay,nrow,ncol))
for v in V1[3]:
  mn=np.array(np.min(nc[v][:,:,:,:],axis=(0,2,3)))
  z[:,:,:,:]=mn[None,:,None,None]  
  nc[v][:]=z[:]
  if v not in V[3]:continue
  iv=V[3].index(v)  
  nc[v][:,:,:,:]=var[iv,:,:,:,:]
122/870: z.shape
122/871: nc[v][:].shape
122/872:
z=np.zeros(shape=(nt,nlay,nrow,ncol))
for v in V1[3]:
  mn=np.array(np.min(nc[v][:,:,:,:],axis=(0,2,3)))
  z[:,:,:,:]=mn[None,:,None,None]  
  nc[v][:,:,:,:]=z[:,:,:,:]
  if v not in V[3]:continue
  iv=V[3].index(v)  
  nc[v][:,:,:,:]=var[iv,:,:,:,:]
122/873: nc.NCOLS
122/874: nc.NCOLS=665
122/875: nc.NROWS=389
122/876: nc.close()
122/877: !ncks -d COL,0,664 -d ROW,0,388 CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc a
122/878: !ncks -O -d COL,0,664 -d ROW,0,388 CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc a
122/879: ls -lh CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc
122/880: pwd
122/881: cd /home/cmaqruns/2018base/data/output_CCTM_v53_gcc_1803_run12
122/882: !ncks -O -d COL,0,664 -d ROW,0,388 CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc a
122/883: mv a CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc
122/884: fname
122/885: nc = netCDF4.Dataset(fname, 'r+')
122/886:
z=np.zeros(shape=(nt,nlay,nrow,ncol))
for v in V1[3]:
  mn=np.array(np.min(nc[v][:,:,:,:],axis=(0,2,3)))
  z[:,:,:,:]=mn[None,:,None,None]  
  nc[v][:,:,:,:]=z[:,:,:,:]
  if v not in V[3]:continue
  iv=V[3].index(v)  
  nc[v][:,:,:,:]=var[iv,:,:,:,:]
122/887: nc.NVARS
122/888: len(V1[3])
122/889: nc.close()
122/890: nc = netCDF4.Dataset(fname, 'r+')
122/891: nc.SDATE
122/892: fname
122/893: nc = netCDF4.Dataset(fname, 'r+')
122/894:
z=np.zeros(shape=(nt,nlay,nrow,ncol))
for v in V1[3]:
  mn=np.array(np.min(nc[v][:,:,:,:],axis=(0,2,3)))
  z[:,:,:,:]=mn[None,:,None,None]  
  nc[v][:,:,:-1,:-1]=z[:,:,:,:]
  if v not in V[3]:continue
  iv=V[3].index(v)  
  nc[v][:,:,:-1,:-1]=var[iv,:,:,:,:]
122/895: nc.close()
122/896: 665/389
122/897: 99/2.7
122/898: 36*1.7
122/899: 1.7*6
122/900: 99/1.7
122/901: np.sqrt(99/1.7)
122/902: 99/7
122/903: 14*7
122/904: pwd
122/905: !ncdump -h CCTM_CGRID_v53_gcc_1803_run12_20180330_EAsia_81K_11.nc|M
122/906: !vi his.txt
122/907: %history -f his.txt
122/908: !vi his.txt
122/909: pwd
122/910: !lst
122/911: mv ic.tgz ic_DOT.tgz
122/912: fname='CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc'
122/913: nc = netCDF4.Dataset(fname, 'r+')
122/914: pwd
122/915:
z=np.zeros(shape=(nt,nlay,nrow,ncol))
for v in V1[3]:
  mn=np.array(np.min(nc[v][:,:,:,:],axis=(0,2,3)))
  z[:,:,:,:]=mn[None,:,None,None]  
  nc[v][:,:,:,:]=z[:,:,:,:]
  if v not in V[3]:continue
  iv=V[3].index(v)  
  nc[v][:,:,:,:]=var[iv,:,:,:,:]
122/916: nc.close()
122/917: fname='tmp/CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc'
122/918: fname='CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc'
122/919: nc = netCDF4.Dataset(fname, 'r+')
122/920:
z=np.zeros(shape=(nt,nlay,nrow,ncol))
for v in V1[3]:
  mn=np.array(np.min(nc[v][:,:,:,:],axis=(0,2,3)))
  z[:,:,:,:]=mn[None,:,None,None]  
  nc[v][:,:,:-1,:-1]=z[:,:,:,:]
  if v not in V[3]:continue
  iv=V[3].index(v)  
  nc[v][:,:,:-1,:-1]=var[iv,:,:,:,:]
122/921: nc.close()
122/922: cd /nas1/TEDS/REAS3.2/origins
122/923: fname='templateD0.nc'
122/924: nc = netCDF4.Dataset(fname, 'r+')
122/925:
for i in 'XORIG YORIG P_GAM XCENT'.split():
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/926: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/927: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
122/928: nrow,ncol
122/929: nc.NROWS=389
122/930: nc.NCOLS=665
122/931: nc.NVARS
122/932: nc.NLAYS
122/933: nc.close()
122/934: nc = netCDF4.Dataset(fname, 'r')
122/935: nc.YCENT
122/936: nc.XCENT
122/937: cd /nas1/cmaqruns/2018base/data/emis/REAS/1804
122/938: fname='area_CWBWRF_15k.20180331.nc'
122/939: nc = netCDF4.Dataset(fname, 'r')
122/940: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/941: v=V[3][0]
122/942: np.max(nc[v][:])
122/943: v
122/944: np.min(nc[v][:])
122/945: v='CO'
122/946: np.min(nc[v][:])
122/947: np.max(nc[v][:])
122/948: nc[v][:][0,0,-5:,-5:]
122/949: a= nc[v][:]
122/950: idx=np.where(np.isnan(a))
122/951: len(idx[0])
122/952: cd /nas1/cmaqruns/2018base/data/output_CCTM_v53_gcc_1804_run5
122/953: fname='CO.nc'
122/954: nc = netCDF4.Dataset(fname, 'r')
122/955: a= nc[v][:]
122/956: idx=np.where(np.isnan(a))
122/957: len(idx[0])
122/958: idx[0]
122/959: idx[1]
122/960: idx[2]
122/961: idx[3]
122/962: cd /home/cmaqruns/2018base/data/bcon
122/963: fname='BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k'
122/964: nc = netCDF4.Dataset(fname, 'r')
122/965: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/966: v
122/967: a= nc[v][:]
122/968: idx=np.where(np.isnan(a))
122/969: len(idx[0])
122/970:
for v in V[2]:
  a= nc[v][:]
  idx=np.where(np.isnan(a))
  print(v,len(idx[0]))
122/971: cd ../icon
122/972: fname='ICON_v53_1804_run5_CWBWRF_15k_regrid_20180331'
122/973: nc = netCDF4.Dataset(fname, 'r')
122/974: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/975:
for v in V[3]:
  a=nc[v][:]
  idx=np.where(np.isnan(a))
  ll=len(idx[0])
  if ll==0:continue
  print(v,len(idx[0]))
122/976: pwd
122/977: cd ../*run12
122/978: fname='/nas1/cmaqruns/2018base/data/output_CCTM_v53_gcc_1804_run5/../output_CCTM_v53_gcc_1803_run12/CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc'
122/979: nc = netCDF4.Dataset(fname, 'r')
122/980: idx[2][:5]
122/981: v
122/982: fname='ICON_v53_1804_run5_CWBWRF_15k_regrid_20180331'
122/983: nc = netCDF4.Dataset(fname, 'r+')
122/984: v='CO'
122/985: np.min(nc[v][:])
122/986: np.max(nc[v][:])
122/987: np.max(nc[v][:]>0)
122/988: idx=np.where(np.isnotnan(a))
122/989: idx=np.where(!np.isnan(a))
122/990: idx=np.where(np.isnan(a)!=True)
122/991: np.min(a[idx[0],idx[1],idx[2],idx[3]])
122/992:
for v in V[3]:
  a=nc[v][:]
  a=np.where(np.isnan(a),0,a)  
  nc[v][:,:,:,:]=a[:,:,:,:]
122/993: nc.close()
122/994: nc = netCDF4.Dataset(fname, 'r+')
122/995:
for v in V[3]:
  a=nc[v][:]
  a=np.where(a>1.E-10,1.E-10,a)  
  nc[v][:,:,:,:]=a[:,:,:,:]
122/996: cd /home/cmaqruns/2018base/data/emis/REAS/1804
122/997: fname='area_CWBWRF_15k.20180331.nc_665X389'
122/998: nc = netCDF4.Dataset(fname, 'r+')
122/999: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/1000:
bdate=datetime.datetime(2018,3,31)
sdate=[bdate+datetime.timedelta(days=i) for i in range(10)]
t=0
nc.SDATE,nc.STIME=dt2jul(sdate[t])
for h in range(25):
  nc['TFLAG'][h,:,0],nc['TFLAG'][h,:,1]=dt2jul(sdate[t]+datetime.timedelta(hours=h))
  for v in V[3]:
    nc[v][h,0,:,:]=nc[v][3,0,:,:]
122/1001: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
122/1002: var=np.zeros(shape=(len(V[3]),nt,nlay,nrow,ncol))
122/1003:
iv=0
for v in V[3]:
  var[iv,:,:,:,:]=nc[v][:,:,:,:]
  iv+=1
122/1004: nc.close()
122/1005: fname='area_CWBWRF_15k.20180331.nc_666X390'
122/1006: nc = netCDF4.Dataset(fname, 'r+')
122/1007:
iv=0
for v in V[3]:
  mn=np.min(nc[v][:,:,:,:],axis=(0,2,3))
  nc[v][:,:,:,:]=mn[None,:,None,None]  
  nc[v][:,:,:-1,:-1]=var[iv,:,:,:,:]
  iv+=1
122/1008: v
122/1009: nc[v][:,:,:-1,:-1].shape
122/1010: var[iv,:,:,:,:].shape
122/1011: nm.shape
122/1012:
iv=0
for v in V[3]:  
  nc[v][:,:,:,:]=0
  nc[v][:,:,:-1,:-1]=var[iv,:,:,:,:]
  iv+=1
122/1013: nc.close()
122/1014: pwd
122/1015: cd /home/cmaqruns/2018base/data/output_CCTM_v53_gcc_1803_run12
122/1016: ls
122/1017: fname='/nas1/cmaqruns/2018base/data/output_CCTM_v53_gcc_1804_run5/../output_CCTM_v53_gcc_1803_run12/CCTM_CGRID_v53_gcc_1803_run12_20180330_CWBWRF_15k_11.nc'
122/1018: nc = netCDF4.Dataset(fname, 'r+')
122/1019: nc.SDATE
122/1020: nc.SDATE=2018090
122/1021: v='TFLAG'
122/1022: nc[v][0,:,0]=2018090
122/1023: nc.close()
122/1024: nc.close()
122/1025: fname='/home/cmaqruns/2018base/data/emis/REAS/1804/area_CWBWRF_15k.20180331.nc'
122/1026: nc = netCDF4.Dataset(fname, 'r+')
122/1027: v='CO'
122/1028: np.min(nc[v][:])
122/1029: nc.close()
122/1030: cd /home/cmaqruns/2018base/data/emis/REAS/1804
122/1031: fname='/home/cmaqruns/2018base/data/emis/REAS/1804/area_CWBWRF_15k.20180331.nc'
122/1032: nc = netCDF4.Dataset(fname, 'r')
122/1033: v='TFLAG'
122/1034: np.min(nc[v][:])
122/1035: np.min(nc[v][:,:,0])
122/1036: np.max(nc[v][:,:,0])
122/1037: pwd
122/1038: cd /nas1/cmaqruns/2018base/data/ptse/REAS
122/1039: fname='teds11.1804.const.nc'
122/1040: nc = netCDF4.Dataset(fname, 'r')
122/1041: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/1042: V
122/1043: nc.GDNAM
122/1044: cp teds11.1804.const.nc CWBWRF_15k_11.1804.const.nc
122/1045: nc.GDNAM='{:16s}'.format('CWBWRF_15k')
122/1046: fname='CWBWRF_15k_11.1804.const.nc'
122/1047: nc = netCDF4.Dataset(fname, 'r+')
122/1048: nc.GDNAM='{:16s}'.format('CWBWRF_15k')
122/1049:
for i in 'XORIG YORIG P_GAM XCENT'.split():
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/1050: nc.NCOLS=665
122/1051: nc.NROWS=389
122/1052: V
122/1053: lat=nc['LATITUDE'][0,0,:,0]
122/1054: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
122/1055: nt,nlay,nrow,ncol
122/1056: nc.NROWS=23160
122/1057: nc.NCOLS=1
122/1058: lon=nc['LONGITUDE'][0,0,:,0]
122/1059:
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
x,y=pnyc(lonm,latm, inverse=False)
122/1060: x,y=pnyc(lon,lat, inverse=False)
122/1061:
for i in 'YCENT XCENT'.split():
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/1062: pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc1.P_ALP, lat_2=nc1.P_BET,lat_0=nc1.YCENT, lon_0=nc1.XCENT, x_0=0, y_0=0.0)
122/1063:
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
x,y=pnyc(lon,lat, inverse=False)
122/1064: V
122/1065: v='XLOCA'
122/1066: nc[v][0,0,:,0]=x
122/1067: v='YLOCA'
122/1068: nc[v][0,0,:,0]=y
122/1069: v='COL'
122/1070: IX=(x-nc.XORIG)/nc.XCELL
122/1071: np.min(nc[v][:])
122/1072: nc[v]
122/1073: min(IX)
122/1074: max(IX)
122/1075: max(lon)
122/1076: nc.XCELL
122/1077:
for i in 'XCELL YCELL'.split():
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/1078: nc.XCELL
122/1079: IX=(x-nc.XORIG)/nc.XCELL
122/1080: max(lon)
122/1081: max(IX)
122/1082: IX=(x-nc.XORIG)//nc.XCELL
122/1083: max(IX)
122/1084: v
122/1085: nc[v][0,0,:,0]=IX
122/1086: np.min(nc[v][:])
122/1087: np.max(nc[v][:])
122/1088: v='ROW'
122/1089: nc[v][0,0,:,0]=(y-nc.YORIG)//nc.YCELL
122/1090: V
122/1091: nc.close()
122/1092: pwd
122/1093: cd 1804
122/1094: ls
122/1095: fname='teds11.20180331.timvar.nc'
122/1096: nc = netCDF4.Dataset(fname, 'r')
122/1097: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/1098: nt,nlay,nrow,ncol
122/1099: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
122/1100: nt,nlay,nrow,ncol
122/1101: V[2]
122/1102: V[1]
122/1103: V[0]
122/1104: len(V[3])
122/1105: V[3]
122/1106: !psg CCTM|wc
122/1107: !top
122/1108: !finger
122/1109: !who
122/1110: cd /home/cmaqruns/2018base/data/bcon
122/1111: cd /nas1/cmaqruns/2018base/data/bcon/BC_from_EAC4
122/1112: fname='BCON_v53_1804_run6_regrid_20180404_CWBWRF_15k'
122/1113: nc = netCDF4.Dataset(fname, 'r')
122/1114: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/1115: nt,nlay,nb=(nc[V[2][-1]].shape[i] for i in range(3))
122/1116: var=np.zeros(shape=(len(V[2]),nt,nlay,nb))
122/1117:
iv=0
for v in V[2]:
  if v=='TFLAG':continue
  var[iv,:,:,:]=nc[v][:,:,:]
  iv+=1
122/1118: 1/0.008928
122/1119: 1/0.008928/60
122/1120: 6378137.000000/40457
122/1121: 40076/40457
122/1122: cd /home/cmaqruns/2018base/data/land
122/1123: fname='beld4.CWBWRF_15k.ncf'
122/1124: nc = netCDF4.Dataset(fname, 'r+')
122/1125: nc.GDNAM='{:16s}'.format('CWBWRF_15k')
122/1126: nc.UPNAM='{:16s}'.format('M3BELD')
122/1127: nc.close()
122/1128: cp beld4.CWBWRF_15k.ncf a
122/1129: fname='a'
122/1130: nc = netCDF4.Dataset(fname, 'r+')
122/1131: nc.GDNAM='{:16s}'.format('CWBWRF_15k')
122/1132: nc.UPNAM='{:16s}'.format('M3BELD')
122/1133: nc.close()
122/1134: mv a beld4.CWBWRF_15k.ncf
122/1135: cp beld4.CWBWRF_15k.ncf LU.CWBWRF_15k.ncf
122/1136: fname='/nas1/cmaqruns/2018base/data/bcon/BC_from_EAC4/BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k'
122/1137: nc0 = netCDF4.Dataset(fname, 'r')
122/1138: fname='BCON_v53_1804_run5_regrid_20180331_CWBWRF_15k'
122/1139: nc = netCDF4.Dataset(fname, 'r+')
122/1140: pwd
122/1141: cd ../bcon
122/1142: nc = netCDF4.Dataset(fname, 'r+')
122/1143:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
122/1144: nc.SDATE
122/1145: nc.UPNAM
122/1146: nc.GDNAM
122/1147: nc.close()
122/1148: nc.close()
122/1149:
s="MODIS_1         MODIS_2         MODIS_3         MODIS_4         MODIS_5         MODIS_6         MODIS_7         MODIS_8         M
ODIS_9         MODIS_10        MODIS_11        MODIS_12        MODIS_13        MODIS_14        MODIS_15        MODIS_16        MODIS_0         MODIS_Res1
 MODIS_Res2      MODIS_Res3      NLCD_11         NLCD_12         NLCD_21         NLCD_22         NLCD_23         NLCD_24         NLCD_31         NLCD_41
   NLCD_42         NLCD_43         NLCD_51         NLCD_52         NLCD_71         NLCD_72         NLCD_73         NLCD_74         NLCD_81         NLCD_82
     NLCD_90         NLCD_95         Hay             Hay_ir          Alfalfa         Alfalfa_ir      Other_Grass     Other_Grass_ir  Barley          Barley_ir
       BeansEdible     BeansEdible_ir  CornGrain       CornGrain_ir    CornSilage      CornSilage_ir   Cotton          Cotton_ir       Oats            Oats_ir
         Peanuts         Peanuts_ir      Potatoes        Potatoes_ir     Rice            Rice_ir         Rye             Rye_ir          SorghumGrain    Sorgh
umGrain_ir SorghumSilage   SorghumSilage_irSoybeans        Soybeans_ir     Wheat_Spring    Wheat_Spring_ir Wheat_Winter    Wheat_Winter_ir Other_Crop      Oth
er_Crop_ir   Canola          Canola_ir       Beans           Beans_ir        Acacia          Ailanthus       Alder           Apple           Ash             B
asswood        Beech           Birch           Bumelia_gum     Cajeput         Califor-laurel  Cascara-buckthorCastanea        Catalpa         Cedar_chamaecyp
 Cedar_thuja     Chestnut_buckeyeChinaberry      Cypress_cupress Cypress_taxodiumDogwood         Douglas_fir     East_hophornbeanElder           Elm
   Eucalyptus      Fir_balsam      Fir_CA_red      Fir_corkbark    Fir_fraser      Fir_grand       Fir_noble       Fir_Pacf_silver Fir_SantaLucia  Fir_Shasta_
red  Fir_spp         Fir_subalpine   Fir_white       Gleditsia_locustHackberry       Hawthorn        Hemlock         Hickory         Holly_American  Hornbeam
       Incense_cedar   Juniper         KY_coffeetree   Larch           Loblolly_bay    Madrone         Magnolia        Mahogany        Maple_bigleaf   Maple_b
igtooth  Maple_black     Maple_boxelder  Maple_FL        Maple_mtn       Maple_Norway    Maple_red       Maple_RkyMtn    Maple_silver    Maple_spp       Maple
_striped   Maple_sugar     Mesquite        Misc-hardwoods  Mixed_conifer_spMountain_ash    Mulberry        Nyssa           Oak_AZ_white    Oak_bear        Oak
_black       Oak_blackjack   Oak_blue        Oak_bluejack    Oak_bur         Oak_CA_black    Oak_CA_live     Oak_CA_white    Oak_canyon_live Oak_chestnut    O
ak_chinkapin   Oak_delta_post  Oak_Durand      Oak_Emery       Oak_Engelmann   Oak_evergreen_spOak_Gambel      Oak_interio_liveOak_laurel      Oak_live
 Oak_Mexicanblue Oak_Northrn_pin Oak_Northrn_red Oak_nuttall     Oak_OR_white    Oak_overcup     Oak_pin         Oak_post        Oak_scarlet     Oak_scrub
   Oak_shingle     Oak_Shumrd_red  Oak_silverleaf  Oak_Southrn_red Oak_spp         Oak_swamp_cnut  Oak_swamp_red   Oak_swamp_white Oak_turkey      Oak_water
     Oak_white       Oak_willow      Osage-orange    Paulownia       Pawpaw          Persimmon       Pine_Apache     Pine_Austrian   Pine_AZ         Pine_Bish
op     Pine_blackjack  Pine_brstlcone  Pine_chihuahua  Pine_Coulter    Pine_digger     Pine_Ewhite     Pine_foxtail    Pine_jack       Pine_Jeffrey    Pine_kn
obcone   Pine_limber     Pine_loblolly   Pine_lodgepole  Pine_longleaf   Pine_Monterey   Pine_pinyon     Pine_pinyon_brdrPine_pinyon_cmn Pine_pitch      Pine_
pond       Pine_ponderosa  Pine_red        Pine_sand       Pine_scotch     Pine_shortleaf  Pine_slash      Pine_spruce     Pine_sugar      Pine_Swwhite    Pin
e_tablemtn   Pine_VA         Pine_Washoe     Pine_whitebark  Pine_Wwhite     Pine_yellow     Populus         Prunus          Redbay          Robinia_locust  S
assafras       Sequoia         Serviceberry    Silverbell      Smoketree       Soapberry_westrnSourwood        Sparkleberry    Spruce_black    Spruce_blue
 Spruce_Brewer   Spruce_EnglemannSpruce_Norway   Spruce_red      Spruce_Sitka    Spruce_spp      Spruce_white    Sweetgum        Sycamore        Tallowtree-ch
insTamarix         Tanoak          Torreya         Tung-oil-tree   Unknown_tree    Walnut          Water-elm       Willow          Yellow_poplar   Yellowwood
     Yucca_Mojave    "
122/1150: !vi afdas
122/1151: !vi afdas
122/1152: pwd
122/1153: cd ../land
122/1154: lst
122/1155: ls
122/1156: fname='LU.CWBWRF_15k.ncf'
122/1157: nc = netCDF4.Dataset(fname, 'r+')
122/1158: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/1159: s=''
122/1160:
for v in V[3]:
    s+='{:16s}'.format(v)
122/1161: s
122/1162: nc.NVARS
122/1163: len(V[3])
122/1164: nc.close()
122/1165: !lst /nas1/cmaqruns/2018base/data/mcip/1804_run5/CWBWRF_15k/
122/1166: cd /home/cmaqruns/2018base/data/land/1804
122/1167: bdate
122/1168: for d in range(1,10)
122/1169: for d in range(1,10)
122/1170:
bdate=datetime.datetime(2018,3,31)
sdate=[bdate+datetime.timedelta(days=i) for i in range(10)]
ib=1
122/1171:
for t in range(ib,10):
    ymd=sdate[t].strftime('%Y%m%d')
    fname='2018_CWBWRF_15k_time'+ymd+'_bench.nc'
    nc = netCDF4.Dataset(fname,'r+')
    if t==ib:V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
    nc.SDATE,nc.STIME=dt2jul(sdate[t])
    h=0
    nc['TFLAG'][h,:,0],nc['TFLAG'][h,:,1]=dt2jul(sdate[t]+datetime.timedelta(hours=h))
    nc.close()
122/1172: !lst
122/1173: pwd
122/1174: cd ../
122/1175: lst
122/1176: ls
122/1177: fname='LU.CWBWRF_15k.ncf'
122/1178: nc = netCDF4.Dataset(fname, 'r')
122/1179: v='USGS_shrubland'
122/1180: nc[v]
122/1181: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/1182: V
122/1183: [v for v in V[3] if 'USGS' in v]
122/1184: fname
122/1185: ls
122/1186: fname='beld4.EAsia_81K.ncf'
122/1187: nc = netCDF4.Dataset(fname, 'r')
122/1188: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/1189: [v for v in V[3] if 'USGS' in v]
122/1190: [len(v) for v in V]
122/1191: fname='USGS_EAsia_81K.nc'
122/1192: nc = netCDF4.Dataset(fname, 'r')
122/1193: [v for v in V[3] if 'USGS' in v]
122/1194: [len(v) for v in V]
122/1195:
pi=3.14159265359
peri_x=40075.02
peri_y=40008
r_x=peri_x/2./pi
r_y=peri_y/2./pi
122/1196:
path=''
fname='NOx_allHeights_2015-01-01T00_2015-12-31T00.nc'
122/1197:
nc = netCDF4.Dataset(fname,'r')
NOx=np.array(nc.variables['NOx'][:,:,:])
lst={'lat':list(nc.variables['latitude'][:])}
lst.update({'lon':list(nc.variables['longitude'][:])})
lst.update({'time':[i for i in range(31)]})
122/1198: pwd
122/1199: cd /nas1/TEDS/en.ilmatieteenlaitos.fi/surveying-maritime-emissions
122/1200:
nc = netCDF4.Dataset(fname,'r')
NOx=np.array(nc.variables['NOx'][:,:,:])
lst={'lat':list(nc.variables['latitude'][:])}
lst.update({'lon':list(nc.variables['longitude'][:])})
lst.update({'time':[i for i in range(31)]})
122/1201: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
122/1202: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/1203: V
122/1204: nc['time'].shape
122/1205: nt=nc['time'].shape[0]
122/1206: nt
122/1207: lst.update({'time':[i for i in range(nt)]})
122/1208: lst['time'][:5]
122/1209:
df=read_csv('shp_sum.csv')
dfs=read_csv('shp_moz.csv')
dfs.iloc[0,0]='1,3,5-trimethylbenzene'
col=['spec','moz','mw']
dfs.columns=col
spec=list(dfs.spec);moz=list(dfs.moz);mw=list(dfs.mw)
spz={i:j for i,j in zip(spec,moz)}
mws={i:j for i,j in zip(spec,mw)}
facG=10**6 /1E3 # 10^3 for kg/day, 10^6 is for mozart2camx
unit_SHIP={i:facG*mw for i,mw in zip(spec,mw)}
with open('aero_fac.txt','r') as f:
  facP={i.split()[0]:float(i.split()[1])/1E3 for i in f}
SHIP_part=['BC','OC'] #,'PM2.5','PM10_']
MOZT_part=['CB1','OC1']#,'SA1','DUST1']
S2M={i:j for i,j in zip(SHIP_part,MOZT_part)}
for i in SHIP_part:
  unit_SHIP.update({i:facP[S2M[i]]})
HRS=24
122/1210: cp ~/mac/TEDS/REAS3.1/origins/aero_fac.txt .
122/1211: unit_SHIP
122/1212:
with open('aero_fac.txt','r') as f:
  facP={i.split()[0]:float(i.split()[1])/1E3 for i in f}
SHIP_part=['BC','OC'] #,'PM2.5','PM10_']
MOZT_part=['CB1','OC1']#,'SA1','DUST1']
S2M={i:j for i,j in zip(SHIP_part,MOZT_part)}
for i in SHIP_part:
  unit_SHIP.update({i:facP[S2M[i]]})
HRS=24
122/1213: V
122/1214: nc['latitude'].shape
122/1215: nrow=nc['latitude'].shape[0]
122/1216: nrow=nc['longitude'].shape[0]
122/1217: nrow=nc['latitude'].shape[0]
122/1218: ncol=nc['longitude'].shape[0]
122/1219: V[0]
122/1220: ncol,nrow,nt=(nc[V[0][i]].shape[0] for i in range(3))
122/1221: ncol,nrow,n
122/1222: ncol,nrow,nt
122/1223: lst['longitude'][:5]
122/1224: lst.keys()
122/1225: lst['lon'][:5]
122/1226: lst['lon'][-5:]
122/1227: dlon,dlat=360/ncol,180/nrow
122/1228: lat=nc['latitude'][:]
122/1229: rad=abs(lat/90.)*pi/2.
122/1230: rad[:5]
122/1231: lat=np.array(nc['latitude'][:])
122/1232: rad=abs(lat/90.)*pi/2.
122/1233: rad[:5]
122/1234: r=(r_x*np.cos(rad)+r_y*np.sin(pi/2.-rad))/2.
122/1235: dx=2.*pi*r * dlon/360.
122/1236: dx.shape
122/1237: dy=dlat/180.*(peri_x*np.cos(rad)**2+peri_y*np.sin(rad)**2)/2.
122/1238: dy.shape
122/1239: dx[:5]
122/1240: dx[700:705]
122/1241: dy[:5]
122/1242: dy[700:705]
122/1243: area=dx*dy
122/1244: area.shape
122/1245:
dx=list(dx)+[dx[-1]]
dx=np.array([(dx[i]+dx[i+1])/2. for i in range(nrow)])
122/1246: len(dx),nrow
122/1247: nrow=nc['latitude'].shape[0]
122/1248: nrow
122/1249:
dx=2.*pi*r * dlon/360.
dx=list(dx)+[dx[-1]]
dx=np.array([(dx[i]+dx[i+1])/2. for i in range(nrow)])
122/1250: dx.shape
122/1251: dx[:5]
122/1252: dx[700:705]
122/1253: area=dx*dy
122/1254: area3d=np.zeros(shape=NOx.shape)
122/1255: NOx.shape
122/1256: NOx[0,:5,:5]
122/1257: NOx[0,700:705,2000:2005]
122/1258: NOx[0,700:705,3000:3005]
122/1259: NOx[0,700:705,3004:3010]
122/1260: area[700:705]
122/1261: NOx=NOx/area[None,:,None]
122/1262: NOx[0,700:705,3004:3010]
122/1263: m=1
122/1264: lst.update({'time':[(datetime.datetime(2015,m,1)+datetime.timedelta(days=d)).toordinal() for d in range(nt)]})
122/1265: lst['time'][:5]
122/1266: nc['time'][:5]
122/1267: nc['time']
122/1268: moz
122/1269: len(moz),len(set(moz))
122/1270: df.head()
122/1271: m=4
122/1272: dfm=df.loc[(df.mon==m)].reset_index(drop=True)
122/1273: len(dfm)
122/1274: dfm
122/1275: spec
122/1276: 'shipD0_'+'{:2d}'.format(m)+'.nc'
122/1277: 'shipD0_'+'{:02d}'.format(m)+'.nc
122/1278: 'shipD0_'+'{:02d}'.format(m)+'.nc'
122/1279: y=2018
122/1280: bdate=datetime.datetime(y,m,1)+datetime.timedelta(days=-1)
122/1281: bdate
122/1282: from calendar import monthrange
122/1283: ntm=(monthrange(y,m)[1]+2)*24+1
122/1284: ntm
122/1285: sdate=[bdate+datetime.timedelta(days=t) for t in range(ntm)]
122/1286: sdate[-5:]
122/1287: sdate[:5]
122/1288: sdate=[bdate+datetime.timedelta(hours=t) for t in range(ntm)]
122/1289: sdate[:5]
122/1290: sdate[-5:]
122/1291: from dtconvertor import dt2jul, jul2dt
122/1292: yrmn
122/1293: yrmn='1804'
122/1294: fname='shipD0_'+yrmn+'.nc'
122/1295: os.system('cp templateD0.nc '+fname)
122/1296: nc = netCDF4.Dataset(fname,'r+')
122/1297:
for t in range(dtm):
  nc['TFLAG'][t,:,0]=dt2jul(sdate[t])[0]
  nc['TFLAG'][t,:,1]=dt2jul(sdate[t])[1]
122/1298:
for t in range(ntm):
  nc['TFLAG'][t,:,0]=dt2jul(sdate[t])[0]
  nc['TFLAG'][t,:,1]=dt2jul(sdate[t])[1]
122/1299: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/1300: V[3][:5]
122/1301: spec
122/1302: dfm.head()
122/1303: df.head()
122/1304: dfs.head()
122/1305: vi REAS2CMAQ.csv
122/1306: !vi REAS2CMAQ.csv
122/1307: reas=read_csv('REAS2CMAQ.csv')
122/1308: sp_re={i:j for i,j in zip(reas.REAS,reas.CMAQ)}
122/1309: sp_re
122/1310: sp_wt={i:j for i,j in zip(reas.REAS,reas.wt)}
122/1311: sp_mole={i:j for i,j in zip(reas.REAS,reas.mole)}
122/1312: sp_re.keys()
122/1313: reas.head()
122/1314: reas
122/1315:
spec=list(dfs.REAS);cmaq=list(dfs.CMAQ);mw=list(dfs.mw);mole=list(dfs.mw)
spz={i:j for i,j in zip(spec,cmaq)}
mws={i:j for i,j in zip(spec,mw)}
mol={i:j for i,j in zip(spec,mole)}
facG=1E3 # 10^3 for kg/day to g-hr
unit_SHIP={i:facG*mw for i,mw in zip(spec,mw)}
122/1316:
dfs=read_csv('REAS2CMAQ.csv')
dfs.iloc[0,0]='1,3,5-trimethylbenzene'

spec=list(dfs.REAS);cmaq=list(dfs.CMAQ);mw=list(dfs.mw);mole=list(dfs.mw)
spz={i:j for i,j in zip(spec,cmaq)}
mws={i:j for i,j in zip(spec,mw)}
mol={i:j for i,j in zip(spec,mole)}
facG=1E3 # 10^3 for kg/day to g-hr
unit_SHIP={i:facG*mw for i,mw in zip(spec,mw)}
122/1317: dfs.head()
122/1318:
spec=list(dfs.REAS);cmaq=list(dfs.CMAQ);mw=list(dfs.wt);mole=list(dfs.mole)
spz={i:j for i,j in zip(spec,cmaq)}
mws={i:j for i,j in zip(spec,mw)}
mol={i:j for i,j in zip(spec,mole)}
facG=1E3 # 10^3 for kg/day to g-hr
unit_SHIP={i:facG*mw for i,mw in zip(spec,mw)}
122/1319: unit_SHIP
122/1320: unit_SHIP={i:facG/mw for i,mw in zip(spec,mw)}
122/1321: unit_SHIP
122/1322: spz
122/1323: a=set(dfs.CMAQ)
122/1324: [i for i in a if list(dfs.CMAQ).count(i)>1]
122/1325: CBM=[i for i in set(cmaq) if cmaq.count(i)>1]
122/1326: CBM
122/1327: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/1328:
for v in V[3]:
  nc[v][:]=0
122/1329: j0=[datetime.strptime(dt,%j) for dt in sdate]
122/1330: j0=[datetime.strptime(dt,"%j") for dt in sdate]
122/1331: j0=[datetime.datetime.strptime(dt,"%j") for dt in sdate]
122/1332: j0=[dt.strptime("%j") for dt in sdate]
122/1333: j0=[datetime.strptime(dt,"%j") for dt in sdate]
122/1334: j0=[datetime.datetime.strptime(dt,"%j") for dt in sdate]
122/1335: j0=[datetime.datetime.strftime(dt,"%j") for dt in sdate]
122/1336: j0[:5]
122/1337: j0=[int(datetime.datetime.strftime(dt,"%j")) for dt in sdate]
122/1338: j0[:5]
122/1339: j0[-5:]
122/1340: js=np.array([datetime.strptime(dt,%j) for dt in sdate],dtype=int)
122/1341: js=np.array([datetime.strptime(dt,"%j") for dt in sdate],dtype=int)
122/1342: js=np.array([int(datetime.datetime.strftime(dt,"%j")) for dt in sdate],dtype=int)
122/1343: j=min(js)
122/1344: j
122/1345: j=max(js)
122/1346: j
122/1347: j=90
122/1348: idt=np.where(j)
122/1349: len(idt[0])
122/1350: idt
122/1351: idt=np.where(js==j)
122/1352: len(idt[0])
122/1353: j=122
122/1354: idt=np.where(js==j)
122/1355: len(idt[0])
122/1356: idt[0]
122/1357: ntm
122/1358: spec[:5]
122/1359: s='BC'
122/1360: unit_SHIP[s]
122/1361: nc.close()
122/1362: !lst
122/1363: rm shipD0_1804.nc
122/1364: fname='shipD0_'+yrmn+'.nc'
122/1365: os.system('cp templateD0.nc '+fname)
122/1366:
for t in range(ntm):
  nc['TFLAG'][t,:,0]=dt2jul(sdate[t])[0]
  nc['TFLAG'][t,:,1]=dt2jul(sdate[t])[1]
122/1367: nc = netCDF4.Dataset(fname,'r+')
122/1368:
for t in range(ntm):
  nc['TFLAG'][t,:,0]=dt2jul(sdate[t])[0]
  nc['TFLAG'][t,:,1]=dt2jul(sdate[t])[1]
122/1369:
for t in range(24):
  nc['TFLAG'][t,:,0]=dt2jul(sdate[t])[0]
  nc['TFLAG'][t,:,1]=dt2jul(sdate[t])[1]
122/1370: os.system('cp templateD0.nc '+fname)
122/1371: nc = netCDF4.Dataset(fname,'r+')
122/1372:
for t in range(24):
  nc['TFLAG'][t,:,0]=dt2jul(sdate[t])[0]
  nc['TFLAG'][t,:,1]=dt2jul(sdate[t])[1]
122/1373: s='CO'
122/1374:   rat=list(dfm.loc[dfm.spec==s,'sum_file'])[0]/vNOx
122/1375: vNOx=list(dfm.loc[dfm.spec=='NOx','sum_file'])[0]
122/1376:   rat=list(dfm.loc[dfm.spec==s,'sum_file'])[0]/vNOx
122/1377: rat
122/1378: j=90
122/1379:     idt=np.where(j)[0]
122/1380: idt=np.where(js==j)[0]
122/1381: idt
122/1382: NOx2d=NOx[js[idt[0]],:,:]
122/1383: unit_SHIP[s]
122/1384:
facG=1E3/24/3600. # 10^3 for kg/day to gmole/s
unit_SHIP={i:facG/mw for i,mw in zip(spec,mw)}
122/1385: unit_SHIP[s]
122/1386: spz[s] in CBM
122/1387: nc[spz[s]][idt[0][:],0,:,:]=NOx2d[None,:,:]*rat*unit_SHIP[s]
122/1388: nc[spz[s]][idt[0][:],0,:,:].shape
122/1389: spz[s]
122/1390: nc[spz[s]][idt[:],0,:,:]=NOx2d[None,:,:]*rat*unit_SHIP[s]
122/1391: nc[spz[s]][idt[:],0,:,:]=NOx2d[None,None,:,:]*rat*unit_SHIP[s]
122/1392: nc[spz[s]][idt[:],:,:,:]=NOx2d[None,None,:,:]*rat*unit_SHIP[s]
122/1393:
    arr=np.zeros(shape=nc[spz[s]][idt[:],:,:,:].shape)
    arr[:,:,:,:]=NOx2d[None,None,:,:]
122/1394:
fname=path+'NOx_allHeights_2015-01-01T00_2015-12-31T00.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
ncol,nrow,nt=(nc[V[0][i]].shape[0] for i in range(3))
NOx=np.array(nc.variables['NOx'][:,:,:])
122/1395: area.shape
122/1396: NOx=NOx/area[None,:,None]
122/1397: NOx.shape
122/1398: NOx2d=NOx[js[idt[0]],:,:]
122/1399: NOx2d.shape
122/1400: arr=np.zeros(shape=(len(idt),nrow,ncol))
122/1401: nc[spz[s]][idt[0]:idt[-1],0,:,:]+=arr[:,:,:]
122/1402: fname='shipD0_'+yrmn+'.nc'
122/1403: nc = netCDF4.Dataset(fname,'r+')
122/1404: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/1405: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
122/1406: nt,nlay,nrow,ncol
122/1407: NOx=NOx[min(js):max(js)+1,:,:]
122/1408: NOx.shape
122/1409: js=js-min(js)
122/1410: pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
122/1411: lonm, latm = np.meshgrid(lon, lat)
122/1412: x,y=pnyc(lonm,latm, inverse=False)
122/1413: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
122/1414: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
122/1415:
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
maxx,maxy=x1[-1,-1],y1[-1,-1]
minx,miny=x1[0,0],y1[0,0]
122/1416:
boo=(abs(x) <= (maxx - minx) /2+nc.XCELL*10) & (abs(y) <= (maxy - miny) /2+nc.YCELL*10)
idx = np.where(boo)
mp=len(idx[0])
xyc= [(x[idx[0][i],idx[1][i]],y[idx[0][i],idx[1][i]]) for i in range(mp)]
122/1417: mp
122/1418: len(NOx[0,:,:].flatten())
122/1419: ncol,nrow
122/1420: len(x)
122/1421: len(lonm)
122/1422: len(lon)
122/1423: len(lat)
122/1424: lonm, latm = np.meshgrid(lst['lon'],lst['lat'])
122/1425: len(latm)
122/1426: len(lonm)
122/1427: len(lst['lon'])
122/1428: len(lst['lat'])
122/1429: lonm, latm-0,0
122/1430: lonm, latm=0,0
122/1431: lonm, latm = np.meshgrid(lst['lon'],lst['lat'])
122/1432: len(lonm)
122/1433: lst['lon'][:5]
122/1434: lst['lon'][-5]
122/1435: lst['lat'][-5:]
122/1436: lst['lat'][:5]
122/1437: lon=lst['lon']
122/1438: lat=lst['lat']
122/1439: lonm, latm = np.meshgrid(lon, lat)
122/1440: lonm.shape
122/1441: x,y=pnyc(lonm,latm, inverse=False)
122/1442: x.shape
122/1443:
boo=(abs(x) <= (maxx - minx) /2+nc.XCELL*10) & (abs(y) <= (maxy - miny) /2+nc.YCELL*10)
idx = np.where(boo)
mp=len(idx[0])
xyc= [(x[idx[0][i],idx[1][i]],y[idx[0][i],idx[1][i]]) for i in range(mp)]
122/1444: mp
122/1445: s
122/1446: idt=np.where(js==j)[0]
122/1447: NOx2d=NOx[js[idt[0]],:,:]
122/1448: j
122/1449: js[:5]
122/1450: j=0
122/1451: idt=np.where(js==j)[0]
122/1452: NOx2d=NOx[js[idt[0]],:,:]
122/1453:
  var=np.zeros(shape=(nrow,ncol))
  c = NOx2d[idx[0][:],idx[1][:]]
122/1454: len(c)
122/1455: len(x1)
122/1456: from scipy.interpolate import griddata
122/1457:   var[:,:] = griddata(xyc, c[:], (x1, y1), method='linear')
122/1458:     arr=np.zeros(shape=(len(idt),nrow,ncol))
122/1459: arr.shape
122/1460:     arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
122/1461: nc[spz[s]][idt[0]:idt[-1],0,:,:] =arr[:,:,:]
122/1462: nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1463: nc.close()
122/1464: run ship2cmaq.py 1804
122/1465: run ship2cmaq.py 1804
122/1466: len(dx)
122/1467: nrow
122/1468: V[0]
122/1469: ncol
122/1470: run ship2cmaq.py 1804
122/1471: s
122/1472: mws.keys()
122/1473: mws[s]
122/1474: spz[s]
122/1475: spec.index(s)
122/1476: mws[s]
122/1477: dfm.loc[dfm.spec==s,'sum_file']
122/1478: dfm.head()
122/1479: dfm.spec=[i.lower() for i in dfm.spec]
122/1480: j
122/1481:
for j in range(min(js),max(js)+1):
  idt=np.where(js==j)[0]
  NOx2d=NOx[js[idt[0]],:,:]
  var=np.zeros(shape=(nrow,ncol))
  c = NOx2d[idx[0][:],idx[1][:]]
  var[:,:] = griddata(xyc, c[:], (x1, y1), method='linear')
  for s in spec:
    if spz[s]=='None': continue
    if mws[s]==0.:continue
    rat=list(dfm.loc[dfm.spec==s.lower(),'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1482: s
122/1483: dfm
122/1484:
for j in range(min(js),max(js)+1):
  idt=np.where(js==j)[0]
  NOx2d=NOx[js[idt[0]],:,:]
  var=np.zeros(shape=(nrow,ncol))
  c = NOx2d[idx[0][:],idx[1][:]]
  var[:,:] = griddata(xyc, c[:], (x1, y1), method='linear')
  for s in spec:
    if spz[s]=='None': continue
    if mws[s]==0.:continue
    ss=s.lower()
    if s=='butanes':ss='isobutane'
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1485:
    if ss=='butanes':ss='isobutane'
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1486:
  for s in spec:
    if spz[s]=='None': continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1487: s
122/1488: dfm
122/1489:
  for s in spec:
    if spz[s]=='None': continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    if ss=='ch4':ss='methan'
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1490: s
122/1491: ss
122/1492:
  for s in spec:
    if spz[s]=='None': continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    if ss=='ch4':ss='methane'
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1493: s
122/1494: spz[s]
122/1495:
  for s in spec:
    if spz[s]=='None': continue
    if spz[s] not in V[3]: continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    if ss=='ch4':ss='methane'
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1496: s
122/1497: ss
122/1498: dfm
122/1499:
  for s in spec:
    if spz[s]=='None': continue
    if spz[s] not in V[3]: continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    if ss=='ch4':ss='methane'
    if ss not in dfm.spec:
      print(ss)
      continue
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1500:
  for s in spec:
    if spz[s]=='None': continue
    if spz[s] not in V[3]: continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    if ss=='ch4':ss='methane'
    if ss not in list(dfm.spec):
      print(ss)
      continue
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1501: dfm
122/1502: j
122/1503:
  for s in spec:
    if spz[s]=='None': continue
    if spz[s] not in V[3]: continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    if ss=='ch4':ss='methane'
    if ss=='xylenes':ss='somers_of_xylene'
    if ss=='pentanes':ss='isomers_of_pentane'
    if ss=='other_aromatics':ss='c10_aromatics'
    if ss not in list(dfm.spec):
      print(ss)
      continue
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1504: dfm
122/1505: ss=somers_of_xylene
122/1506:
  for s in spec:
    if spz[s]=='None': continue
    if spz[s] not in V[3]: continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    if ss=='ch4':ss='methane'
    if ss=='xylenes':ss='somers_of_xylene'
    if ss=='pentanes':ss='isomers_of_pentane'
    if ss=='other_aromatics':ss='c10_aromatics'
    if ss not in list(dfm.spec):
      print(ss)
      continue
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1507: dfm
122/1508:
  for s in spec:
    if spz[s]=='None': continue
    if spz[s] not in V[3]: continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    if ss=='ch4':ss='methane'
    if ss=='xylenes':ss='isomers_of_xylene'
    if ss=='pentanes':ss='isomers_of_pentane'
    if ss=='other_aromatics':ss='c10_aromatics'
    if ss not in list(dfm.spec):ss='nmvoc'
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1509:
  for s in spec:
    if spz[s]=='None': continue
    if spz[s] not in V[3]: continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    if ss=='ch4':ss='methane'
    if ss=='xylenes':ss='isomers_of_xylene'
    if ss=='pentanes':ss='isomers_of_pentane'
    if ss=='other_aromatics':ss='c10_aromatics'
#    if ss not in list(dfm.spec):ss='nmvoc'
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1510:
  for s in spec:
    if spz[s]=='None': continue
    if spz[s] not in V[3]: continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    if ss=='ch4':ss='methane'
    if ss=='xylenes':ss='isomers_of_xylene'
    if ss=='pentanes':ss='isomers_of_pentane'
    if ss=='other_aromatics':ss='c10_aromatics'
    if ss not in list(dfm.spec):
      print(ss)
      continue
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1511: dfm
122/1512:
  for s in spec:
    if spz[s]=='None': continue
    if spz[s] not in V[3]: continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    if ss=='ch4':ss='methane'
    if ss=='xylenes':ss='isomers_of_xylene'
    if ss=='pentanes':ss='isomers_of_pentane'
    if ss=='other_aromatics':ss='c10_aromatics'
    if ss not in list(dfm.spec):
      if ss in ['nh3','pm10','pm2.5']:continue
      ss='nmvoc'
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1513:
for v in V[3]:
    nc[v][idt[0]:idt[-1]+1,0,:,:]=0
122/1514:
for j in range(min(js),max(js)+1):
  idt=np.where(js==j)[0]
  NOx2d=NOx[js[idt[0]],:,:]
  var=np.zeros(shape=(nrow,ncol))
  c = NOx2d[idx[0][:],idx[1][:]]
  var[:,:] = griddata(xyc, c[:], (x1, y1), method='linear')
  for s in spec:
    if spz[s]=='None': continue
    if spz[s] not in V[3]: continue
    if mws[s]==0.:continue
    ss=s.lower()
    if ss=='butanes':ss='isobutane'
    if ss=='ch4':ss='methane'
    if ss=='xylenes':ss='isomers_of_xylene'
    if ss=='pentanes':ss='isomers_of_pentane'
    if ss=='other_aromatics':ss='c10_aromatics'
    if ss not in list(dfm.spec):
      if ss in ['nh3','pm10','pm2.5']:continue
      ss='nmvoc'
    rat=list(dfm.loc[dfm.spec==ss,'sum_file'])[0]/vNOx
    if s=='NOx': rat=1.
    arr=np.zeros(shape=(len(idt),nrow,ncol))
    arr[:,:,:]=var[None,:,:]*rat*unit_SHIP[s]
    if spz[s] in CBM:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:]+=arr[:,:,:]
    else:
      nc[spz[s]][idt[0]:idt[-1]+1,0,:,:] =arr[:,:,:]
122/1515: nc.close()
122/1516: pwd
123/1: 665/389
123/2: 11390.11/3600
123/3: 11390.11/60
123/4: 11390.11/60/24
123/5: 107276.49/60
123/6: 107276.49/60/24
123/7: 107276.49/3600
123/8: cd /nas1/aermruns/aermap
123/9:
import rasterio
fname='taiwan2020.tif'
123/10:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
123/11: data.shape
123/12: img.bounds
123/13: img.lnglat()
123/14: img.xy(0,0)
123/15: xmin,nx,dx,ymin,ny,dy=246453. 20 500. 2747820. 20 500.
123/16: xmin,nx,dx,ymin,ny,dy=246453., 20, 500., 2747820., 20, 500.,
123/17: nx,ny=int(nx),int(ny) xmax=xmin+(nx-1)*dx ymax=ymin+(ny-1)*dy
123/18:
nx,ny=int(nx),int(ny)
xmax=xmin+(nx-1)*dx 
ymax=ymin+(ny-1)*dy
123/19: x0,y0=img.xy(0,0)
123/20: dxm,dym=20,20
123/21: mn=img.shape
123/22: x1d = np.array([x0+dxm*i for i in range(mn[2])])
123/23: import numpy as np
123/24: x1d = np.array([x0+dxm*i for i in range(mn[2])])
123/25: mn
123/26: img.shape
123/27: x1d = np.array([x0+dxm*i for i in range(mn[1])])
123/28: y1d = np.array([y0+dym*i for i in range(mn[0])])
123/29: y1d[:5]
123/30: y0
123/31: x9
123/32: x0
123/33: dxm,dym=20,-20
123/34: y1d = np.array([y0+dym*i for i in range(mn[0])])
123/35: y1d[:5]
123/36: y1dr=y1d
123/37: y1dr.sort()
123/38: I1=bisect.bisect_left(x1d,xmin)-2;I2=bisect.bisect_left(x1d,xmax)+2;J1=bisect.bisect_left(y1dr,ymin)-2;J2=bisect.bisect_left(y1dr,ymax)+2
123/39: import bisect
123/40: I1=bisect.bisect_left(x1d,xmin)-2;I2=bisect.bisect_left(x1d,xmax)+2;J1=bisect.bisect_left(y1dr,ymin)-2;J2=bisect.bisect_left(y1dr,ymax)+2
123/41: I1,I2,J1,J2
123/42: c=d[-J2:-J1,I1:I2].flatten()
123/43: c=img[-J2:-J1,I1:I2].flatten()
123/44: c=data[0,-J2:-J1,I1:I2].flatten()
123/45: x=np.array([x1d[I1:I2] for j in range(J2-J1)]).flatten()
123/46: y=np.array([[j for i in range(I2-I1)] for j in y1d[-J2:-J1]]).flatten()
123/47: x_mesh = np.linspace(xmin, xmax, nx)
123/48: y_mesh = np.linspace(ymin, ymax, ny)
123/49: x_g, y_g = np.meshgrid(x_mesh, y_mesh)
123/50: ll = np.array([[twd97.towgs84(i, j) for i, j in zip(x_g[k], y_g[k])] for k in range(ny)])
123/51: import twd97
123/52: ll = np.array([[twd97.towgs84(i, j) for i, j in zip(x_g[k], y_g[k])] for k in range(ny)])
123/53: lat, lon = (ll[:, :, i] for i in [0, 1])
123/54: points = np.array(zip(x, y)).astype(float)
123/55: points=[(i,j) for i,j in zip(x.flatten(),y.flatten())]
123/56: len(point),len(c)
123/57: len(points),len(c)
123/58: from scipy.interpolate import griddata
123/59: import matplotlib._cntr as cntr
123/60: import legacycontour._cntr as cntr
123/61: grid_z2 = griddata(points, c, (x_g, y_g), method='linear')
123/62: grid_z2 = np.clip(grid_z2,0.,np.max(grid_z2))
123/63: c = cntr.Cntr(lon, lat, grid_z2)
123/64:
N = 10
mxgrd=max([10.,np.max(grid_z2)])
levels = np.linspace(0, mxgrd, N)
col = '#00FF0A #3FFF0A #7FFF0A #BFFF0A #FFFF0A #FECC0A #FD990A #FC660A #FB330A #FA000A'.replace('#', '').split()
if len(col) != N: print 'color scale not right, please redo from http://www.zonums.com/online/color_ramp/'
aa = '28'  # ''28'~ 40%, '4d' about 75%
rr, gg, bb = ([i[j:j + 2] for i in col] for j in [0, 2, 4])
col = [aa + b + g + r for b, g, r in zip(bb, gg, rr)]
123/65:
N = 10
mxgrd=max([10.,np.max(grid_z2)])
levels = np.linspace(0, mxgrd, N)
col = '#00FF0A #3FFF0A #7FFF0A #BFFF0A #FFFF0A #FECC0A #FD990A #FC660A #FB330A #FA000A'.replace('#', '').split()
if len(col) != N: print ('color scale not right, please redo from http://www.zonums.com/online/color_ramp/')
aa = '28'  # ''28'~ 40%, '4d' about 75%
rr, gg, bb = ([i[j:j + 2] for i in col] for j in [0, 2, 4])
col = [aa + b + g + r for b, g, r in zip(bb, gg, rr)]
123/66:
M = int(np.log10(levels[1])) - 1
levels = [round(lev, -M) for lev in levels]

#the Cntr method is valid only in previous version of matplotlib
c = cntr.Cntr(lon, lat, grid_z2)
# the tolerance to determine points are connected to the boundaries
tol = 1E-3
col0 = '4d6ecdcf'
col_line0 = 'cc2d3939'
123/67:
head1 = '<?xml version="1.0" encoding="UTF-8"?><kml xmlns="http://earth.google.com/kml/2.2"><Document><name><![CDATA[' + last + ']]></name>'
st_head = ''
st_med = '</color><width>1</width></LineStyle><PolyStyle><color>'
st_tail = '</color></PolyStyle></Style>'
for i in xrange(N):
  st_head += '<Style id="level' + str(i) + '"><LineStyle><color>' + col[i] + st_med + col[i] + st_tail
head2 = '</styleUrl><Polygon><outerBoundaryIs><LinearRing><tessellate>1</tessellate><coordinates>'
tail2 = '</coordinates></LinearRing></outerBoundaryIs></Polygon></Placemark>'
line = [head1 + st_head]
123/68: last='Hsinzhu'
123/69:
head1 = '<?xml version="1.0" encoding="UTF-8"?><kml xmlns="http://earth.google.com/kml/2.2"><Document><name><![CDATA[' + last + ']]></name>'
st_head = ''
st_med = '</color><width>1</width></LineStyle><PolyStyle><color>'
st_tail = '</color></PolyStyle></Style>'
for i in xrange(N):
  st_head += '<Style id="level' + str(i) + '"><LineStyle><color>' + col[i] + st_med + col[i] + st_tail
head2 = '</styleUrl><Polygon><outerBoundaryIs><LinearRing><tessellate>1</tessellate><coordinates>'
tail2 = '</coordinates></LinearRing></outerBoundaryIs></Polygon></Placemark>'
line = [head1 + st_head]
123/70:
head1 = '<?xml version="1.0" encoding="UTF-8"?><kml xmlns="http://earth.google.com/kml/2.2"><Document><name><![CDATA[' + last + ']]></name>'
st_head = ''
st_med = '</color><width>1</width></LineStyle><PolyStyle><color>'
st_tail = '</color></PolyStyle></Style>'
for i in range(N):
  st_head += '<Style id="level' + str(i) + '"><LineStyle><color>' + col[i] + st_med + col[i] + st_tail
head2 = '</styleUrl><Polygon><outerBoundaryIs><LinearRing><tessellate>1</tessellate><coordinates>'
tail2 = '</coordinates></LinearRing></outerBoundaryIs></Polygon></Placemark>'
line = [head1 + st_head]
123/71:
e, w, s, n = np.max(lon), np.min(lon), np.min(lat), np.max(lat)
for level in levels[:]:
  nlist = c.trace(level, level, 0)
  segs = nlist[:len(nlist) // 2]
  i = levels.index(level)
  for seg in segs:
    line.append('<Placemark><name>level:' + str(level) + '</name><styleUrl>#level' + str(i) + head2)
    leng = -9999
    for j in xrange(len(seg[:, 0])):
      line.append(str(seg[j, 0]) + ',' + str(seg[j, 1]) + ',0 ')
      if j > 0:
        leng = max(leng, np.sqrt((seg[j, 0] - seg[j - 1, 0]) ** 2 + (seg[j, 1] - seg[j - 1, 1]) ** 2))
    leng0 = np.sqrt((seg[j, 0] - seg[0, 0]) ** 2 + (seg[j, 1] - seg[0, 1]) ** 2)
    ewsn = np.zeros(shape=(4, 2))
    j = -1
    # end points not closed, add coner point(s) to close the polygons.
    if leng0 > leng and leng0 / leng > 5:
      if abs(seg[j, 0] - e) < tol: ewsn[0, 1] = 1
      if abs(seg[0, 0] - e) < tol: ewsn[0, 0] = 1
      if abs(seg[j, 0] - w) < tol: ewsn[1, 1] = 1
      if abs(seg[0, 0] - w) < tol: ewsn[1, 0] = 1
      if abs(seg[j, 1] - s) < tol: ewsn[2, 1] = 1
      if abs(seg[0, 1] - s) < tol: ewsn[2, 0] = 1
      if abs(seg[j, 1] - n) < tol: ewsn[3, 1] = 1
      if abs(seg[0, 1] - n) < tol: ewsn[3, 0] = 1
      if sum(ewsn[1, :] + ewsn[2, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.min(lat)) + ',0 ')
      if sum(ewsn[1, :] + ewsn[3, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[3, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[2, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.min(lat)) + ',0 ')
    # TODO: when contour pass half of the domain,must add two edge points.
    line.append(tail2)
123/72:
line.append('</Document></kml>')
with open(fname + '.kml', 'w') as f:
  [f.write(i) for i in line]

xy = np.array([[(i, j) for i, j in zip(x_g[k], y_g[k])] for k in xrange(ny)])
with open(fname + '_re.dat','w') as f:
  f.write('RE ELEVUNIT METERS\n')
  for j in range(ny):
    for i in range(nx):
      f.write('RE DISCCART '+str(xy[j,i,0])+' '+str(xy[j,i,1])+' '+str(grid_z2[j,i])+'\n')

#terrain grid file
with open(fname + '_TG.txt','w') as f:
  f.write(str(nx)+' '+str(ny)+' '+str(xmin)+' '+str(xmax)+' '+str(ymin)+' '+str(ymax)+' '+str(dx)+' '+str(dy)+'\n')
  for j in range(ny):
    ele=[str(int(grid_z2[j,i])) for i in range(nx)]
    st=ele[0]
    for i in range(1,nx):
      st+=' '+ele[i]
    f.write(st+'\n')
123/73:
line.append('</Document></kml>')
with open(fname + '.kml', 'w') as f:
  [f.write(i) for i in line]

xy = np.array([[(i, j) for i, j in zip(x_g[k], y_g[k])] for k in range(ny)])
with open(fname + '_re.dat','w') as f:
  f.write('RE ELEVUNIT METERS\n')
  for j in range(ny):
    for i in range(nx):
      f.write('RE DISCCART '+str(xy[j,i,0])+' '+str(xy[j,i,1])+' '+str(grid_z2[j,i])+'\n')

#terrain grid file
with open(fname + '_TG.txt','w') as f:
  f.write(str(nx)+' '+str(ny)+' '+str(xmin)+' '+str(xmax)+' '+str(ymin)+' '+str(ymax)+' '+str(dx)+' '+str(dy)+'\n')
  for j in range(ny):
    ele=[str(int(grid_z2[j,i])) for i in range(nx)]
    st=ele[0]
    for i in range(1,nx):
      st+=' '+ele[i]
    f.write(st+'\n')
123/74: cc=np.where(np.isnan(grid_z2),0,grid_z2)
123/75: np.max(cc)
123/76: c=data[0,-J2:-J1,I1:I2].flatten()
123/77: max(c)
123/78: min(c)
123/79: data[0,:5,:5]
123/80: points[:5]
123/81: xg[0,:5]
123/82: x_g[0,:5]
123/83: y_g[0,:5]
123/84:
import rasterio
fname='taiwan2020.tif'
123/85: %history -f his.txt
123/86: !vi his.txt
123/87: (img.bounds.right-img.bounds.left)/img.width,(img.bounds.top-img.bounds.bottom)/img.height
123/88: !vi his.txt
123/89: x.shape
123/90: points[:5]
123/91: c=np.where(c<0,0,c)
123/92: grid_z2 = griddata(points, c, (x_g, y_g), method='linear')
123/93: grid_z2.shape
123/94: ny,nx
123/95: fname='Hsinzhu'
123/96:
N = 10
mxgrd=max([10.,np.max(grid_z2)])
levels = np.linspace(0, mxgrd, N)
col = '#00FF0A #3FFF0A #7FFF0A #BFFF0A #FFFF0A #FECC0A #FD990A #FC660A #FB330A #FA000A'.replace('#', '').split()
if len(col) != N: print 'color scale not right, please redo from http://www.zonums.com/online/color_ramp/'
aa = '28'  # ''28'~ 40%, '4d' about 75%
rr, gg, bb = ([i[j:j + 2] for i in col] for j in [0, 2, 4])
col = [aa + b + g + r for b, g, r in zip(bb, gg, rr)]

# round the values of levels to 1 significant number at least, -2 at least 2 digits
M = int(np.log10(levels[1])) - 1
levels = [round(lev, -M) for lev in levels]

#the Cntr method is valid only in previous version of matplotlib
c = cntr.Cntr(lon, lat, grid_z2)
# the tolerance to determine points are connected to the boundaries
tol = 1E-3
col0 = '4d6ecdcf'
col_line0 = 'cc2d3939'


#writing the KML, see the KML official website
head1 = '<?xml version="1.0" encoding="UTF-8"?><kml xmlns="http://earth.google.com/kml/2.2"><Document><name><![CDATA[' + last + ']]></name>'
st_head = ''
st_med = '</color><width>1</width></LineStyle><PolyStyle><color>'
st_tail = '</color></PolyStyle></Style>'
for i in range(N):
  st_head += '<Style id="level' + str(i) + '"><LineStyle><color>' + col[i] + st_med + col[i] + st_tail
head2 = '</styleUrl><Polygon><outerBoundaryIs><LinearRing><tessellate>1</tessellate><coordinates>'
tail2 = '</coordinates></LinearRing></outerBoundaryIs></Polygon></Placemark>'
line = [head1 + st_head]
# repeat for the level lines
e, w, s, n = np.max(lon), np.min(lon), np.min(lat), np.max(lat)
for level in levels[:]:
  nlist = c.trace(level, level, 0)
  segs = nlist[:len(nlist) // 2]
  i = levels.index(level)
  for seg in segs:
    line.append('<Placemark><name>level:' + str(level) + '</name><styleUrl>#level' + str(i) + head2)
    leng = -9999
    for j in range(len(seg[:, 0])):
      line.append(str(seg[j, 0]) + ',' + str(seg[j, 1]) + ',0 ')
      if j > 0:
        leng = max(leng, np.sqrt((seg[j, 0] - seg[j - 1, 0]) ** 2 + (seg[j, 1] - seg[j - 1, 1]) ** 2))
    leng0 = np.sqrt((seg[j, 0] - seg[0, 0]) ** 2 + (seg[j, 1] - seg[0, 1]) ** 2)
    ewsn = np.zeros(shape=(4, 2))
    j = -1
    # end points not closed, add coner point(s) to close the polygons.
    if leng0 > leng and leng0 / leng > 5:
      if abs(seg[j, 0] - e) < tol: ewsn[0, 1] = 1
      if abs(seg[0, 0] - e) < tol: ewsn[0, 0] = 1
      if abs(seg[j, 0] - w) < tol: ewsn[1, 1] = 1
      if abs(seg[0, 0] - w) < tol: ewsn[1, 0] = 1
      if abs(seg[j, 1] - s) < tol: ewsn[2, 1] = 1
      if abs(seg[0, 1] - s) < tol: ewsn[2, 0] = 1
      if abs(seg[j, 1] - n) < tol: ewsn[3, 1] = 1
      if abs(seg[0, 1] - n) < tol: ewsn[3, 0] = 1
      if sum(ewsn[1, :] + ewsn[2, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.min(lat)) + ',0 ')
      if sum(ewsn[1, :] + ewsn[3, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[3, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[2, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.min(lat)) + ',0 ')
    # TODO: when contour pass half of the domain,must add two edge points.
    line.append(tail2)
line.append('</Document></kml>')
with open(fname + '.kml', 'w') as f:
  [f.write(i) for i in line]
123/97:
N = 10
mxgrd=max([10.,np.max(grid_z2)])
levels = np.linspace(0, mxgrd, N)
col = '#00FF0A #3FFF0A #7FFF0A #BFFF0A #FFFF0A #FECC0A #FD990A #FC660A #FB330A #FA000A'.replace('#', '').split()
if len(col) != N: print ('color scale not right, please redo from http://www.zonums.com/online/color_ramp/')
aa = '28'  # ''28'~ 40%, '4d' about 75%
rr, gg, bb = ([i[j:j + 2] for i in col] for j in [0, 2, 4])
col = [aa + b + g + r for b, g, r in zip(bb, gg, rr)]

# round the values of levels to 1 significant number at least, -2 at least 2 digits
M = int(np.log10(levels[1])) - 1
levels = [round(lev, -M) for lev in levels]

#the Cntr method is valid only in previous version of matplotlib
c = cntr.Cntr(lon, lat, grid_z2)
# the tolerance to determine points are connected to the boundaries
tol = 1E-3
col0 = '4d6ecdcf'
col_line0 = 'cc2d3939'


#writing the KML, see the KML official website
head1 = '<?xml version="1.0" encoding="UTF-8"?><kml xmlns="http://earth.google.com/kml/2.2"><Document><name><![CDATA[' + last + ']]></name>'
st_head = ''
st_med = '</color><width>1</width></LineStyle><PolyStyle><color>'
st_tail = '</color></PolyStyle></Style>'
for i in range(N):
  st_head += '<Style id="level' + str(i) + '"><LineStyle><color>' + col[i] + st_med + col[i] + st_tail
head2 = '</styleUrl><Polygon><outerBoundaryIs><LinearRing><tessellate>1</tessellate><coordinates>'
tail2 = '</coordinates></LinearRing></outerBoundaryIs></Polygon></Placemark>'
line = [head1 + st_head]
# repeat for the level lines
e, w, s, n = np.max(lon), np.min(lon), np.min(lat), np.max(lat)
for level in levels[:]:
  nlist = c.trace(level, level, 0)
  segs = nlist[:len(nlist) // 2]
  i = levels.index(level)
  for seg in segs:
    line.append('<Placemark><name>level:' + str(level) + '</name><styleUrl>#level' + str(i) + head2)
    leng = -9999
    for j in range(len(seg[:, 0])):
      line.append(str(seg[j, 0]) + ',' + str(seg[j, 1]) + ',0 ')
      if j > 0:
        leng = max(leng, np.sqrt((seg[j, 0] - seg[j - 1, 0]) ** 2 + (seg[j, 1] - seg[j - 1, 1]) ** 2))
    leng0 = np.sqrt((seg[j, 0] - seg[0, 0]) ** 2 + (seg[j, 1] - seg[0, 1]) ** 2)
    ewsn = np.zeros(shape=(4, 2))
    j = -1
    # end points not closed, add coner point(s) to close the polygons.
    if leng0 > leng and leng0 / leng > 5:
      if abs(seg[j, 0] - e) < tol: ewsn[0, 1] = 1
      if abs(seg[0, 0] - e) < tol: ewsn[0, 0] = 1
      if abs(seg[j, 0] - w) < tol: ewsn[1, 1] = 1
      if abs(seg[0, 0] - w) < tol: ewsn[1, 0] = 1
      if abs(seg[j, 1] - s) < tol: ewsn[2, 1] = 1
      if abs(seg[0, 1] - s) < tol: ewsn[2, 0] = 1
      if abs(seg[j, 1] - n) < tol: ewsn[3, 1] = 1
      if abs(seg[0, 1] - n) < tol: ewsn[3, 0] = 1
      if sum(ewsn[1, :] + ewsn[2, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.min(lat)) + ',0 ')
      if sum(ewsn[1, :] + ewsn[3, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[3, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[2, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.min(lat)) + ',0 ')
    # TODO: when contour pass half of the domain,must add two edge points.
    line.append(tail2)
line.append('</Document></kml>')
with open(fname + '.kml', 'w') as f:
  [f.write(i) for i in line]
123/98: !lst
123/99: pwd
123/100: !more Hsinzhu.kml
123/101: np.max(grid_z2)
123/102: np.min(grid_z2)
123/103: !grep where his.txt
123/104: cc=np.where(np.isnan(grid_z2),0,grid_z2)
123/105: np.max(cc)
123/106: np.max(c)
123/107: c=data[0,-J2:-J1,I1:I2].flatten()
123/108: c=np.where(c<0,0,c)
123/109: np.max(c)
123/110: len(c)
123/111: len(points)
123/112: max(x)
123/113: min(x)
123/114: min(y)
123/115: max(y)
123/116: np.min(x_g),np.max(x_g)
123/117: np.min(y_g),np.max(y_g)
123/118: y0
123/119: dxm,dym
123/120: mn
123/121:
x1d = np.array([x0+dxm*i for i in range(mn[1])])
y1d = np.array([y0+dym*i for i in range(mn[0])])
123/122: y1dr=y1d;y1dr.sort()
123/123:
I1=bisect.bisect_left(x1d,xmin)-2;I2=bisect.bisect_left(x1d,xmax)+2
J1=bisect.bisect_left(y1dr,ymin)-2;J2=bisect.bisect_left(y1dr,ymax)+2
123/124: J1,J2
123/125: c=data[0,-J2:-J1,I1:I2].flatten()
123/126:
c=np.where(c<0,0,c)
x=np.array([x1d[I1:I2] for j in range(J2-J1)]).flatten()
y=np.array([[j for i in range(I2-I1)] for j in y1d[-J2:-J1]]).flatten()
123/127: np.min(y),np.max(y)
123/128: y1dr[J1:J1+5]
123/129: ymin
123/130: y1d[-J1-5:-J1]
123/131: y1d[:5]
123/132: y1dr[-5:]
123/133:
y1d = np.array([y0+dym*i for i in range(mn[0])])
y1dr=y1d[:];y1dr.sort()
123/134: y1d[:5]
123/135: y1dr[-5:]
123/136: y0
123/137: dym
123/138: mn[0]
123/139: y1d = np.array([y0+dym*i for i in range(mn[0])])
123/140: y1d[:5]
123/141: y1dr=y1d[:]
123/142: y1dr.sort()
123/143: y1d[:5]
123/144: y1dr=y1d.values()
123/145: y1dr=y1d.data[:]
123/146: y1d = np.array([y0+dym*i for i in range(mn[0])])
123/147: y1dr=y1d.data[:]
123/148: y1dr[:5]
123/149: y1dr[:5]()
123/150:
x1d = np.array([x0+dxm*i for i in range(mn[1])])
y1d = np.array([y0+dym*i for i in range(mn[0])])
y1d.sort()
123/151:
I1=bisect.bisect_left(x1d,xmin)-2;I2=bisect.bisect_left(x1d,xmax)+2
J1=bisect.bisect_left(y1d,ymin)-2;J2=bisect.bisect_left(y1d,ymax)+2
c=data[0,-J2:-J1,I1:I2].flatten()
c=np.where(c<0,0,c)
x=np.array([x1d[I1:I2] for j in range(J2-J1)]).flatten()
y=np.array([[j for i in range(I2-I1)] for j in y1d[-J2:-J1]]).flatten()
123/152: y[:5]
123/153: ymin
123/154: data=np.flip(img.read()[0,:,:],[0])
123/155: y=np.array([[j for i in range(I2-I1)] for j in y1d[J1:J2]]).flatten()
123/156: y[:5]
123/157: ymin
123/158: points=[(i,j) for i,j in zip(x,y)]
123/159: grid_z2 = griddata(points, c, (x_g, y_g), method='linear')
123/160:
N = 10
mxgrd=max([10.,np.max(grid_z2)])
levels = np.linspace(0, mxgrd, N)
col = '#00FF0A #3FFF0A #7FFF0A #BFFF0A #FFFF0A #FECC0A #FD990A #FC660A #FB330A #FA000A'.replace('#', '').split()
if len(col) != N: print ('color scale not right, please redo from http://www.zonums.com/online/color_ramp/')
aa = '28'  # ''28'~ 40%, '4d' about 75%
rr, gg, bb = ([i[j:j + 2] for i in col] for j in [0, 2, 4])
col = [aa + b + g + r for b, g, r in zip(bb, gg, rr)]

# round the values of levels to 1 significant number at least, -2 at least 2 digits
M = int(np.log10(levels[1])) - 1
levels = [round(lev, -M) for lev in levels]

#the Cntr method is valid only in previous version of matplotlib
c = cntr.Cntr(lon, lat, grid_z2)
# the tolerance to determine points are connected to the boundaries
tol = 1E-3
col0 = '4d6ecdcf'
col_line0 = 'cc2d3939'


#writing the KML, see the KML official website
head1 = '<?xml version="1.0" encoding="UTF-8"?><kml xmlns="http://earth.google.com/kml/2.2"><Document><name><![CDATA[' + last + ']]></name>'
st_head = ''
st_med = '</color><width>1</width></LineStyle><PolyStyle><color>'
st_tail = '</color></PolyStyle></Style>'
for i in range(N):
  st_head += '<Style id="level' + str(i) + '"><LineStyle><color>' + col[i] + st_med + col[i] + st_tail
head2 = '</styleUrl><Polygon><outerBoundaryIs><LinearRing><tessellate>1</tessellate><coordinates>'
tail2 = '</coordinates></LinearRing></outerBoundaryIs></Polygon></Placemark>'
line = [head1 + st_head]
# repeat for the level lines
e, w, s, n = np.max(lon), np.min(lon), np.min(lat), np.max(lat)
for level in levels[:]:
  nlist = c.trace(level, level, 0)
  segs = nlist[:len(nlist) // 2]
  i = levels.index(level)
  for seg in segs:
    line.append('<Placemark><name>level:' + str(level) + '</name><styleUrl>#level' + str(i) + head2)
    leng = -9999
    for j in range(len(seg[:, 0])):
      line.append(str(seg[j, 0]) + ',' + str(seg[j, 1]) + ',0 ')
      if j > 0:
        leng = max(leng, np.sqrt((seg[j, 0] - seg[j - 1, 0]) ** 2 + (seg[j, 1] - seg[j - 1, 1]) ** 2))
    leng0 = np.sqrt((seg[j, 0] - seg[0, 0]) ** 2 + (seg[j, 1] - seg[0, 1]) ** 2)
    ewsn = np.zeros(shape=(4, 2))
    j = -1
    # end points not closed, add coner point(s) to close the polygons.
    if leng0 > leng and leng0 / leng > 5:
      if abs(seg[j, 0] - e) < tol: ewsn[0, 1] = 1
      if abs(seg[0, 0] - e) < tol: ewsn[0, 0] = 1
      if abs(seg[j, 0] - w) < tol: ewsn[1, 1] = 1
      if abs(seg[0, 0] - w) < tol: ewsn[1, 0] = 1
      if abs(seg[j, 1] - s) < tol: ewsn[2, 1] = 1
      if abs(seg[0, 1] - s) < tol: ewsn[2, 0] = 1
      if abs(seg[j, 1] - n) < tol: ewsn[3, 1] = 1
      if abs(seg[0, 1] - n) < tol: ewsn[3, 0] = 1
      if sum(ewsn[1, :] + ewsn[2, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.min(lat)) + ',0 ')
      if sum(ewsn[1, :] + ewsn[3, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[3, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[2, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.min(lat)) + ',0 ')
    # TODO: when contour pass half of the domain,must add two edge points.
    line.append(tail2)
line.append('</Document></kml>')
with open(fname + '.kml', 'w') as f:
  [f.write(i) for i in line]
123/161:
c=data[0,J1:J2,I1:I2].flatten()
c=np.where(c<0,0,c)
123/162:
c=data[J1:J2,I1:I2].flatten()
c=np.where(c<0,0,c)
123/163: grid_z2 = griddata(points, c, (x_g, y_g), method='linear')
123/164:
N = 10
mxgrd=max([10.,np.max(grid_z2)])
levels = np.linspace(0, mxgrd, N)
col = '#00FF0A #3FFF0A #7FFF0A #BFFF0A #FFFF0A #FECC0A #FD990A #FC660A #FB330A #FA000A'.replace('#', '').split()
if len(col) != N: print ('color scale not right, please redo from http://www.zonums.com/online/color_ramp/')
aa = '28'  # ''28'~ 40%, '4d' about 75%
rr, gg, bb = ([i[j:j + 2] for i in col] for j in [0, 2, 4])
col = [aa + b + g + r for b, g, r in zip(bb, gg, rr)]

# round the values of levels to 1 significant number at least, -2 at least 2 digits
M = int(np.log10(levels[1])) - 1
levels = [round(lev, -M) for lev in levels]

#the Cntr method is valid only in previous version of matplotlib
c = cntr.Cntr(lon, lat, grid_z2)
# the tolerance to determine points are connected to the boundaries
tol = 1E-3
col0 = '4d6ecdcf'
col_line0 = 'cc2d3939'


#writing the KML, see the KML official website
head1 = '<?xml version="1.0" encoding="UTF-8"?><kml xmlns="http://earth.google.com/kml/2.2"><Document><name><![CDATA[' + last + ']]></name>'
st_head = ''
st_med = '</color><width>1</width></LineStyle><PolyStyle><color>'
st_tail = '</color></PolyStyle></Style>'
for i in range(N):
  st_head += '<Style id="level' + str(i) + '"><LineStyle><color>' + col[i] + st_med + col[i] + st_tail
head2 = '</styleUrl><Polygon><outerBoundaryIs><LinearRing><tessellate>1</tessellate><coordinates>'
tail2 = '</coordinates></LinearRing></outerBoundaryIs></Polygon></Placemark>'
line = [head1 + st_head]
# repeat for the level lines
e, w, s, n = np.max(lon), np.min(lon), np.min(lat), np.max(lat)
for level in levels[:]:
  nlist = c.trace(level, level, 0)
  segs = nlist[:len(nlist) // 2]
  i = levels.index(level)
  for seg in segs:
    line.append('<Placemark><name>level:' + str(level) + '</name><styleUrl>#level' + str(i) + head2)
    leng = -9999
    for j in range(len(seg[:, 0])):
      line.append(str(seg[j, 0]) + ',' + str(seg[j, 1]) + ',0 ')
      if j > 0:
        leng = max(leng, np.sqrt((seg[j, 0] - seg[j - 1, 0]) ** 2 + (seg[j, 1] - seg[j - 1, 1]) ** 2))
    leng0 = np.sqrt((seg[j, 0] - seg[0, 0]) ** 2 + (seg[j, 1] - seg[0, 1]) ** 2)
    ewsn = np.zeros(shape=(4, 2))
    j = -1
    # end points not closed, add coner point(s) to close the polygons.
    if leng0 > leng and leng0 / leng > 5:
      if abs(seg[j, 0] - e) < tol: ewsn[0, 1] = 1
      if abs(seg[0, 0] - e) < tol: ewsn[0, 0] = 1
      if abs(seg[j, 0] - w) < tol: ewsn[1, 1] = 1
      if abs(seg[0, 0] - w) < tol: ewsn[1, 0] = 1
      if abs(seg[j, 1] - s) < tol: ewsn[2, 1] = 1
      if abs(seg[0, 1] - s) < tol: ewsn[2, 0] = 1
      if abs(seg[j, 1] - n) < tol: ewsn[3, 1] = 1
      if abs(seg[0, 1] - n) < tol: ewsn[3, 0] = 1
      if sum(ewsn[1, :] + ewsn[2, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.min(lat)) + ',0 ')
      if sum(ewsn[1, :] + ewsn[3, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[3, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[2, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.min(lat)) + ',0 ')
    # TODO: when contour pass half of the domain,must add two edge points.
    line.append(tail2)
line.append('</Document></kml>')
with open(fname + '.kml', 'w') as f:
  [f.write(i) for i in line]
123/165: grid_z2.dtype
123/166: TIF
123/167: TIF,DEM,NUL=fname+'.tiff',last+'.dem',' >>'+dir+'geninp.out'
123/168: dir='.'
123/169: TIF,DEM,NUL=fname+'.tiff',last+'.dem',' >>'+dir+'geninp.out'
123/170: TIF
123/171: os.system('cp template.tiff '+TIF)
123/172: import os
123/173: os.system('cp template.tiff '+TIF)
123/174:
os.system('cp template.tiff '+TIF)
new_dataset = rasterio.open(TIF,'w',driver='GTiff',height=grid_z2.shape[0],width=grid_z2.shape[1],count=1,
dtype=grid_z2.dtype,crs='+proj=latlong',transform=transform,)
123/175: from rasterio.transform import Affine
123/176: nx
123/177: res = (x[-1] - x[0]) / nx
123/178: res
123/179: res = (x[-1] - x[0]) / (nx-1)
123/180: res
123/181: y[0]
123/182: x[0],xmin,x[-1],xmax
123/183: transform = Affine.translation(xmin - dx / 2, ymin - dy / 2) * Affine.scale(dx, dy)
123/184:
new_dataset = rasterio.open(TIF,'w',driver='GTiff',height=grid_z2.shape[0],width=grid_z2.shape[1],count=1,
dtype=grid_z2.dtype,crs='+proj=latlong',transform=transform,)
123/185: new_dataset.write(grid_z2, 1)
123/186: new_dataset.close()
123/187: !lst
123/188: !tiffinfo Hsinzhu.tiff
123/189: fname='Hsinzhu.tiff'
123/190:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
123/191: nx,ny
123/192: data
123/193: np.max(data)
123/194: !lsS
123/195: rm Hsinzhu.tif
123/196: grid_z2=data[:]
123/197:
N = 10
mxgrd=max([10.,np.max(grid_z2)])
levels = np.linspace(0, mxgrd, N)
col = '#00FF0A #3FFF0A #7FFF0A #BFFF0A #FFFF0A #FECC0A #FD990A #FC660A #FB330A #FA000A'.replace('#', '').split()
if len(col) != N: print ('color scale not right, please redo from http://www.zonums.com/online/color_ramp/')
aa = '28'  # ''28'~ 40%, '4d' about 75%
rr, gg, bb = ([i[j:j + 2] for i in col] for j in [0, 2, 4])
col = [aa + b + g + r for b, g, r in zip(bb, gg, rr)]

# round the values of levels to 1 significant number at least, -2 at least 2 digits
M = int(np.log10(levels[1])) - 1
levels = [round(lev, -M) for lev in levels]

#the Cntr method is valid only in previous version of matplotlib
c = cntr.Cntr(lon, lat, grid_z2)
# the tolerance to determine points are connected to the boundaries
tol = 1E-3
col0 = '4d6ecdcf'
col_line0 = 'cc2d3939'


#writing the KML, see the KML official website
head1 = '<?xml version="1.0" encoding="UTF-8"?><kml xmlns="http://earth.google.com/kml/2.2"><Document><name><![CDATA[' + last + ']]></name>'
st_head = ''
st_med = '</color><width>1</width></LineStyle><PolyStyle><color>'
st_tail = '</color></PolyStyle></Style>'
for i in range(N):
  st_head += '<Style id="level' + str(i) + '"><LineStyle><color>' + col[i] + st_med + col[i] + st_tail
head2 = '</styleUrl><Polygon><outerBoundaryIs><LinearRing><tessellate>1</tessellate><coordinates>'
tail2 = '</coordinates></LinearRing></outerBoundaryIs></Polygon></Placemark>'
line = [head1 + st_head]
# repeat for the level lines
e, w, s, n = np.max(lon), np.min(lon), np.min(lat), np.max(lat)
for level in levels[:]:
  nlist = c.trace(level, level, 0)
  segs = nlist[:len(nlist) // 2]
  i = levels.index(level)
  for seg in segs:
    line.append('<Placemark><name>level:' + str(level) + '</name><styleUrl>#level' + str(i) + head2)
    leng = -9999
    for j in range(len(seg[:, 0])):
      line.append(str(seg[j, 0]) + ',' + str(seg[j, 1]) + ',0 ')
      if j > 0:
        leng = max(leng, np.sqrt((seg[j, 0] - seg[j - 1, 0]) ** 2 + (seg[j, 1] - seg[j - 1, 1]) ** 2))
    leng0 = np.sqrt((seg[j, 0] - seg[0, 0]) ** 2 + (seg[j, 1] - seg[0, 1]) ** 2)
    ewsn = np.zeros(shape=(4, 2))
    j = -1
    # end points not closed, add coner point(s) to close the polygons.
    if leng0 > leng and leng0 / leng > 5:
      if abs(seg[j, 0] - e) < tol: ewsn[0, 1] = 1
      if abs(seg[0, 0] - e) < tol: ewsn[0, 0] = 1
      if abs(seg[j, 0] - w) < tol: ewsn[1, 1] = 1
      if abs(seg[0, 0] - w) < tol: ewsn[1, 0] = 1
      if abs(seg[j, 1] - s) < tol: ewsn[2, 1] = 1
      if abs(seg[0, 1] - s) < tol: ewsn[2, 0] = 1
      if abs(seg[j, 1] - n) < tol: ewsn[3, 1] = 1
      if abs(seg[0, 1] - n) < tol: ewsn[3, 0] = 1
      if sum(ewsn[1, :] + ewsn[2, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.min(lat)) + ',0 ')
      if sum(ewsn[1, :] + ewsn[3, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[3, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[2, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.min(lat)) + ',0 ')
    # TODO: when contour pass half of the domain,must add two edge points.
    line.append(tail2)
line.append('</Document></kml>')
with open(fname + '.kml', 'w') as f:
  [f.write(i) for i in line]
123/198: grid_z2=data[:,:]
123/199:
N = 10
mxgrd=max([10.,np.max(grid_z2)])
levels = np.linspace(0, mxgrd, N)
col = '#00FF0A #3FFF0A #7FFF0A #BFFF0A #FFFF0A #FECC0A #FD990A #FC660A #FB330A #FA000A'.replace('#', '').split()
if len(col) != N: print ('color scale not right, please redo from http://www.zonums.com/online/color_ramp/')
aa = '28'  # ''28'~ 40%, '4d' about 75%
rr, gg, bb = ([i[j:j + 2] for i in col] for j in [0, 2, 4])
col = [aa + b + g + r for b, g, r in zip(bb, gg, rr)]

# round the values of levels to 1 significant number at least, -2 at least 2 digits
M = int(np.log10(levels[1])) - 1
levels = [round(lev, -M) for lev in levels]

#the Cntr method is valid only in previous version of matplotlib
c = cntr.Cntr(lon, lat, grid_z2)
# the tolerance to determine points are connected to the boundaries
tol = 1E-3
col0 = '4d6ecdcf'
col_line0 = 'cc2d3939'


#writing the KML, see the KML official website
head1 = '<?xml version="1.0" encoding="UTF-8"?><kml xmlns="http://earth.google.com/kml/2.2"><Document><name><![CDATA[' + last + ']]></name>'
st_head = ''
st_med = '</color><width>1</width></LineStyle><PolyStyle><color>'
st_tail = '</color></PolyStyle></Style>'
for i in range(N):
  st_head += '<Style id="level' + str(i) + '"><LineStyle><color>' + col[i] + st_med + col[i] + st_tail
head2 = '</styleUrl><Polygon><outerBoundaryIs><LinearRing><tessellate>1</tessellate><coordinates>'
tail2 = '</coordinates></LinearRing></outerBoundaryIs></Polygon></Placemark>'
line = [head1 + st_head]
# repeat for the level lines
e, w, s, n = np.max(lon), np.min(lon), np.min(lat), np.max(lat)
for level in levels[:]:
  nlist = c.trace(level, level, 0)
  segs = nlist[:len(nlist) // 2]
  i = levels.index(level)
  for seg in segs:
    line.append('<Placemark><name>level:' + str(level) + '</name><styleUrl>#level' + str(i) + head2)
    leng = -9999
    for j in range(len(seg[:, 0])):
      line.append(str(seg[j, 0]) + ',' + str(seg[j, 1]) + ',0 ')
      if j > 0:
        leng = max(leng, np.sqrt((seg[j, 0] - seg[j - 1, 0]) ** 2 + (seg[j, 1] - seg[j - 1, 1]) ** 2))
    leng0 = np.sqrt((seg[j, 0] - seg[0, 0]) ** 2 + (seg[j, 1] - seg[0, 1]) ** 2)
    ewsn = np.zeros(shape=(4, 2))
    j = -1
    # end points not closed, add coner point(s) to close the polygons.
    if leng0 > leng and leng0 / leng > 5:
      if abs(seg[j, 0] - e) < tol: ewsn[0, 1] = 1
      if abs(seg[0, 0] - e) < tol: ewsn[0, 0] = 1
      if abs(seg[j, 0] - w) < tol: ewsn[1, 1] = 1
      if abs(seg[0, 0] - w) < tol: ewsn[1, 0] = 1
      if abs(seg[j, 1] - s) < tol: ewsn[2, 1] = 1
      if abs(seg[0, 1] - s) < tol: ewsn[2, 0] = 1
      if abs(seg[j, 1] - n) < tol: ewsn[3, 1] = 1
      if abs(seg[0, 1] - n) < tol: ewsn[3, 0] = 1
      if sum(ewsn[1, :] + ewsn[2, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.min(lat)) + ',0 ')
      if sum(ewsn[1, :] + ewsn[3, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[3, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[2, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.min(lat)) + ',0 ')
    # TODO: when contour pass half of the domain,must add two edge points.
    line.append(tail2)
line.append('</Document></kml>')
with open(fname + '.kml', 'w') as f:
  [f.write(i) for i in line]
123/200: grid_z2.shape
123/201: grid_z2=data[0,:,:]
123/202:
N = 10
mxgrd=max([10.,np.max(grid_z2)])
levels = np.linspace(0, mxgrd, N)
col = '#00FF0A #3FFF0A #7FFF0A #BFFF0A #FFFF0A #FECC0A #FD990A #FC660A #FB330A #FA000A'.replace('#', '').split()
if len(col) != N: print ('color scale not right, please redo from http://www.zonums.com/online/color_ramp/')
aa = '28'  # ''28'~ 40%, '4d' about 75%
rr, gg, bb = ([i[j:j + 2] for i in col] for j in [0, 2, 4])
col = [aa + b + g + r for b, g, r in zip(bb, gg, rr)]

# round the values of levels to 1 significant number at least, -2 at least 2 digits
M = int(np.log10(levels[1])) - 1
levels = [round(lev, -M) for lev in levels]

#the Cntr method is valid only in previous version of matplotlib
c = cntr.Cntr(lon, lat, grid_z2)
# the tolerance to determine points are connected to the boundaries
tol = 1E-3
col0 = '4d6ecdcf'
col_line0 = 'cc2d3939'


#writing the KML, see the KML official website
head1 = '<?xml version="1.0" encoding="UTF-8"?><kml xmlns="http://earth.google.com/kml/2.2"><Document><name><![CDATA[' + last + ']]></name>'
st_head = ''
st_med = '</color><width>1</width></LineStyle><PolyStyle><color>'
st_tail = '</color></PolyStyle></Style>'
for i in range(N):
  st_head += '<Style id="level' + str(i) + '"><LineStyle><color>' + col[i] + st_med + col[i] + st_tail
head2 = '</styleUrl><Polygon><outerBoundaryIs><LinearRing><tessellate>1</tessellate><coordinates>'
tail2 = '</coordinates></LinearRing></outerBoundaryIs></Polygon></Placemark>'
line = [head1 + st_head]
# repeat for the level lines
e, w, s, n = np.max(lon), np.min(lon), np.min(lat), np.max(lat)
for level in levels[:]:
  nlist = c.trace(level, level, 0)
  segs = nlist[:len(nlist) // 2]
  i = levels.index(level)
  for seg in segs:
    line.append('<Placemark><name>level:' + str(level) + '</name><styleUrl>#level' + str(i) + head2)
    leng = -9999
    for j in range(len(seg[:, 0])):
      line.append(str(seg[j, 0]) + ',' + str(seg[j, 1]) + ',0 ')
      if j > 0:
        leng = max(leng, np.sqrt((seg[j, 0] - seg[j - 1, 0]) ** 2 + (seg[j, 1] - seg[j - 1, 1]) ** 2))
    leng0 = np.sqrt((seg[j, 0] - seg[0, 0]) ** 2 + (seg[j, 1] - seg[0, 1]) ** 2)
    ewsn = np.zeros(shape=(4, 2))
    j = -1
    # end points not closed, add coner point(s) to close the polygons.
    if leng0 > leng and leng0 / leng > 5:
      if abs(seg[j, 0] - e) < tol: ewsn[0, 1] = 1
      if abs(seg[0, 0] - e) < tol: ewsn[0, 0] = 1
      if abs(seg[j, 0] - w) < tol: ewsn[1, 1] = 1
      if abs(seg[0, 0] - w) < tol: ewsn[1, 0] = 1
      if abs(seg[j, 1] - s) < tol: ewsn[2, 1] = 1
      if abs(seg[0, 1] - s) < tol: ewsn[2, 0] = 1
      if abs(seg[j, 1] - n) < tol: ewsn[3, 1] = 1
      if abs(seg[0, 1] - n) < tol: ewsn[3, 0] = 1
      if sum(ewsn[1, :] + ewsn[2, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.min(lat)) + ',0 ')
      if sum(ewsn[1, :] + ewsn[3, :]) == 2: line.append(str(np.min(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[3, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.max(lat)) + ',0 ')
      if sum(ewsn[0, :] + ewsn[2, :]) == 2: line.append(str(np.max(lon)) + ',' + str(np.min(lat)) + ',0 ')
    # TODO: when contour pass half of the domain,must add two edge points.
    line.append(tail2)
line.append('</Document></kml>')
with open(fname + '.kml', 'w') as f:
  [f.write(i) for i in line]
123/203: !lst
123/204: !gdal_translate
123/205:
llmin=(np.min(lon),np.min(lat)) #long/lati
llmax=(np.max(lon),np.max(lat))
llSE=str(llmax[0])+' '+str(llmin[1])
llNW=str(llmin[0])+' '+str(llmax[1])+' '+llSE
pth1='/opt/anaconda3/bin/'
pth2='/opt/anaconda3/envs/ncl_stable/bin/'
gd_data=';export PATH='+pth1+':'+pth2+':$PATH;GDAL_DATA=/opt/anaconda3/envs/py37/share/gdal '
os.system('echo "before gdal"'+NUL)
gd='gdal_translate'
cmd='cd '+dir+gd_data+gd+' -of USGSDEM -ot Float32 -projwin '+llNW+' '+TIF+' '+DEM+NUL
os.system('echo "'+cmd+'"'+NUL)
os.system(cmd)
123/206: cmd
123/207: img.shape
123/208: img.width
123/209: img.height
123/210: !lst
123/211: img.bounds
123/212: dir(img)
123/213: dir(img())
123/214:
new_dataset = rasterio.open(TIF,'w',driver='GTiff',height=grid_z2.shape[0],width=grid_z2.shape[1],count=1,
  dtype=grid_z2.dtype,crs='+proj=latlong',transform=transform,)
data=np.flip(grid_z2,[0])
new_dataset.write(data, 1)
new_dataset.close()
123/215:
llmin=(np.min(lon),np.min(lat)) #long/lati
llmax=(np.max(lon),np.max(lat))
llSE=str(llmax[0])+' '+str(llmin[1])
llNW=str(llmin[0])+' '+str(llmax[1])+' '+llSE
pth1='/opt/anaconda3/bin/'
pth2='/opt/anaconda3/envs/ncl_stable/bin/'
gd_data=';export PATH='+pth1+':'+pth2+':$PATH;GDAL_DATA=/opt/anaconda3/envs/py37/share/gdal '
os.system('echo "before gdal"'+NUL)
gd='gdal_translate'
cmd='cd '+dir+gd_data+gd+' -of USGSDEM -ot Float32 -projwin '+llNW+' '+TIF+' '+DEM+NUL
os.system('echo "'+cmd+'"'+NUL)
os.system(cmd)
123/216: cmd
123/217: fname='Hsinzhu.tiff'
123/218:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
123/219: img.bounds
123/220: img.transform
123/221: transform = Affine.translation(xmin - dx / 2, ymax + dy / 2) * Affine.scale(dx, -dy)
123/222:
new_dataset = rasterio.open(TIF,'w',driver='GTiff',height=grid_z2.shape[0],width=grid_z2.shape[1],count=1,
  dtype=grid_z2.dtype,crs='+proj=latlong',transform=transform,)
123/223:
data=np.flip(grid_z2,[0])
new_dataset.write(data, 1)
new_dataset.close()
123/224: os.system(cmd)
123/225: transform = Affine.translation(xmin - dx / 2, ymax + dy / 2) * Affine.scale(-dy, dx)
123/226:
new_dataset = rasterio.open(TIF,'w',driver='GTiff',height=grid_z2.shape[0],width=grid_z2.shape[1],count=1,
  dtype=grid_z2.dtype,crs='+proj=latlong',transform=transform,)
123/227:
data=np.flip(grid_z2,[0])
new_dataset.write(data, 1)
new_dataset.close()
123/228: os.system(cmd)
123/229: fname='Hsinzhu.tiff'
123/230:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
123/231: (img.bounds.right-img.bounds.left)/img.width,-(img.bounds.top-img.bounds.bottom)/img.height
123/232: transform = Affine.translation(xmin - dx / 2, ymax + dy / 2) * Affine.scale(dx, -dy)
123/233:
new_dataset = rasterio.open(TIF,'w',driver='GTiff',height=grid_z2.shape[0],width=grid_z2.shape[1],count=1,
  dtype=grid_z2.dtype,crs='+proj=latlong',transform=transform,)
123/234:
data=np.flip(grid_z2,[0])
new_dataset.write(data, 1)
new_dataset.close()
123/235:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
123/236: (img.bounds.right-img.bounds.left)/img.width,-(img.bounds.top-img.bounds.bottom)/img.height
123/237: img.bounds
123/238: os.system(cmd)
123/239: !lst
123/240: !tiffinfo Hsinzhu.tiff
123/241: img.lnglat()
123/242: dir(rasterio.open)
123/243: dir(rasterio.open())
123/244: rasterio.open
123/245: transform
123/246: img.lnglat()
123/247:
resx,resy=(np.max(lon)-np.min(lon))/(nx-1),(np.max(lat)-np.min(lat))/(ny-1),
transform = Affine.translation(np.min(lon) - resx / 2, np.min(lat) - resy / 2) * Affine.scale(resx, -resy)
new_dataset = rasterio.open(TIF,'w',driver='GTiff',height=grid_z2.shape[0],width=grid_z2.shape[1],count=1,
  dtype=grid_z2.dtype,crs='+proj=latlong',transform=transform,)
123/248:
data=np.flip(grid_z2,[0])
new_dataset.write(data, 1)
new_dataset.close()
123/249: os.system(cmd)
123/250: transform = Affine.translation(np.min(lon) - resx / 2, np.min(lat) - resy / 2) * Affine.scale(resx, resy)
123/251:
new_dataset = rasterio.open(TIF,'w',driver='GTiff',height=grid_z2.shape[0],width=grid_z2.shape[1],count=1,
  dtype=grid_z2.dtype,crs='+proj=latlong',transform=transform,)
123/252:
data=np.flip(grid_z2,[0])
new_dataset.write(data, 1)
new_dataset.close()
123/253: os.system(cmd)
123/254:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
123/255: img.lnglat()
123/256: img.xy(0,0)
123/257: lon[0,0]
123/258: lat[0,0]
123/259: img.bounds
123/260: transform = Affine.translation(np.min(lon) - resx / 2, np.max(lat) + resy / 2) * Affine.scale(resx, -resy)
123/261:
new_dataset = rasterio.open(TIF,'w',driver='GTiff',height=grid_z2.shape[0],width=grid_z2.shape[1],count=1,
  dtype=grid_z2.dtype,crs='+proj=latlong',transform=transform,)
123/262:
data=np.flip(grid_z2,[0])
new_dataset.write(data, 1)
new_dataset.close()
123/263: os.system(cmd)
123/264: !lst
123/265: more Hsinzhu.dem
123/266:
llmin=pnyc(xmin-dx*5-Xcent, ymin-dx*5-Ycent, inverse=True) #long/lati
llmax=pnyc(xmax+dx*5-Xcent, ymax+dy*5-Ycent, inverse=True)
uxmn1,uymn1=utm.from_latlon(llmin[1],llmin[0])[0:2]
uxmn2,uymx1=utm.from_latlon(llmax[1],llmin[0])[0:2]
uxmx1,uymn2=utm.from_latlon(llmin[1],llmax[0])[0:2]
uxmx2,uymx2=utm.from_latlon(llmax[1],llmax[0])[0:2]
co,an,z='   DOMAINXY  ','   ANCHORXY  ',' 51 '
UTMrange=co+str(int(max(uxmn1,uxmn2)))+' '+str(int(max(uymn1,uymn2)))+z+str(int(min(uxmx1,uxmx2)))+' '+str(int(min(uymx1,uymx2)))+z
xmid,ymid=(xmin+xmax)/2., (ymin+ymax)/2.
llanc=pnyc(xmid-Xcent, ymid-Ycent, inverse=True)
uxanc,uyanc=utm.from_latlon(llanc[1],llanc[0])[0:2]
UTMancha=an+str(int(xmid))+' '+str(int(ymid))+' '+str(int(uxanc))+' '+str(int(uyanc))+z+'0'
123/267:
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
123/268: from pyproj import Proj
123/269:
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
123/270:
Xcent=(xmin+xmax)/2
Ycent=(ymin+ymax)/2
Latitude_Pole, Longitude_Pole=twd97.towgs84(Xcent, Ycent)
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
123/271:
llmin=pnyc(xmin-dx*5-Xcent, ymin-dx*5-Ycent, inverse=True) #long/lati
llmax=pnyc(xmax+dx*5-Xcent, ymax+dy*5-Ycent, inverse=True)
uxmn1,uymn1=utm.from_latlon(llmin[1],llmin[0])[0:2]
uxmn2,uymx1=utm.from_latlon(llmax[1],llmin[0])[0:2]
uxmx1,uymn2=utm.from_latlon(llmin[1],llmax[0])[0:2]
uxmx2,uymx2=utm.from_latlon(llmax[1],llmax[0])[0:2]
co,an,z='   DOMAINXY  ','   ANCHORXY  ',' 51 '
UTMrange=co+str(int(max(uxmn1,uxmn2)))+' '+str(int(max(uymn1,uymn2)))+z+str(int(min(uxmx1,uxmx2)))+' '+str(int(min(uymx1,uymx2)))+z
xmid,ymid=(xmin+xmax)/2., (ymin+ymax)/2.
llanc=pnyc(xmid-Xcent, ymid-Ycent, inverse=True)
uxanc,uyanc=utm.from_latlon(llanc[1],llanc[0])[0:2]
UTMancha=an+str(int(xmid))+' '+str(int(ymid))+' '+str(int(uxanc))+' '+str(int(uyanc))+z+'0'
123/272: import twd97, utm
123/273: !pip install utm
123/274: import twd97, utm
123/275:
llmin=pnyc(xmin-dx*5-Xcent, ymin-dx*5-Ycent, inverse=True) #long/lati
llmax=pnyc(xmax+dx*5-Xcent, ymax+dy*5-Ycent, inverse=True)
uxmn1,uymn1=utm.from_latlon(llmin[1],llmin[0])[0:2]
uxmn2,uymx1=utm.from_latlon(llmax[1],llmin[0])[0:2]
uxmx1,uymn2=utm.from_latlon(llmin[1],llmax[0])[0:2]
uxmx2,uymx2=utm.from_latlon(llmax[1],llmax[0])[0:2]
co,an,z='   DOMAINXY  ','   ANCHORXY  ',' 51 '
UTMrange=co+str(int(max(uxmn1,uxmn2)))+' '+str(int(max(uymn1,uymn2)))+z+str(int(min(uxmx1,uxmx2)))+' '+str(int(min(uymx1,uymx2)))+z
xmid,ymid=(xmin+xmax)/2., (ymin+ymax)/2.
llanc=pnyc(xmid-Xcent, ymid-Ycent, inverse=True)
uxanc,uyanc=utm.from_latlon(llanc[1],llanc[0])[0:2]
UTMancha=an+str(int(xmid))+' '+str(int(ymid))+' '+str(int(uxanc))+' '+str(int(uyanc))+z+'0'
123/276:
text_file = open("aermap.inp", "r")
d=[line for line in text_file]
keywd=[i[3:11] for i in d]
ifile=keywd.index('DATAFILE')
idmxy=keywd.index('DOMAINXY')
ianxy=keywd.index('ANCHORXY')
ihead=keywd.index('ELEVUNIT')
iend=d.index('RE FINISHED\n')
123/277:
llmin=pnyc(xmin-dx*5-Xcent, ymin-dx*5-Ycent, inverse=True) #long/lati
llmax=pnyc(xmax+dx*5-Xcent, ymax+dy*5-Ycent, inverse=True)
uxmn1,uymn1=utm.from_latlon(llmin[1],llmin[0])[0:2]
uxmn2,uymx1=utm.from_latlon(llmax[1],llmin[0])[0:2]
uxmx1,uymn2=utm.from_latlon(llmin[1],llmax[0])[0:2]
uxmx2,uymx2=utm.from_latlon(llmax[1],llmax[0])[0:2]
co,an,z='   DOMAINXY  ','   ANCHORXY  ',' 51 '
UTMrange=co+str(int(max(uxmn1,uxmn2)))+' '+str(int(max(uymn1,uymn2)))+z+str(int(min(uxmx1,uxmx2)))+' '+str(int(min(uymx1,uymx2)))+z
xmid,ymid=(xmin+xmax)/2., (ymin+ymax)/2.
llanc=pnyc(xmid-Xcent, ymid-Ycent, inverse=True)
uxanc,uyanc=utm.from_latlon(llanc[1],llanc[0])[0:2]
UTMancha=an+str(int(xmid))+' '+str(int(ymid))+' '+str(int(uxanc))+' '+str(int(uyanc))+z+'0'
123/278:
text_file = open("aermap.inp", "r")
d=[line for line in text_file]
keywd=[i[3:11] for i in d]
ifile=keywd.index('DATAFILE')
idmxy=keywd.index('DOMAINXY')
ianxy=keywd.index('ANCHORXY')
ihead=keywd.index('ELEVUNIT')
iend=d.index('RE FINISHED\n')
123/279:
text_file = open("aermap.inp", "w")

x0,y0=xmin,ymin
sta='RE GRIDCART '+last+' STA\n'
args[0]=last
STR='RE GRIDCART {:s} XYINC {:s} {:s} {:s} {:s} {:s} {:s}\n'.format(*args)
s=[sta,STR,sta.replace('STA','END')]
for l in range(ihead+1):
#  if l == ifile:
#    text_file.write( "%s" % '   DATAFILE  '+DEM+'\n')
  if l == idmxy:
    text_file.write( "%s" % UTMrange+'\n')
  elif l == ianxy:
    text_file.write( "%s" % UTMancha+'\n')
  else:
    text_file.write( "%s" % d[l])
for l in range(len(s)):
    text_file.write( "%s" % s[l])
for l in range(iend,len(d)):
    text_file.write( "%s" % d[l])
text_file.close()
123/280: cmd
123/281: args='Hsinzhu,246453., 20, 500., 2747820., 20, 500.'.split(',')
123/282: STR='RE GRIDCART {:s} XYINC {:s} {:s} {:s} {:s} {:s} {:s}\n'.format(*args)
123/283: STR
123/284:
text_file = open("aermap.inp", "w")

x0,y0=xmin,ymin
sta='RE GRIDCART '+last+' STA\n'
args[0]=last
STR='RE GRIDCART {:s} XYINC {:s} {:s} {:s} {:s} {:s} {:s}\n'.format(*args)
s=[sta,STR,sta.replace('STA','END')]
for l in range(ihead+1):
#  if l == ifile:
#    text_file.write( "%s" % '   DATAFILE  '+DEM+'\n')
  if l == idmxy:
    text_file.write( "%s" % UTMrange+'\n')
  elif l == ianxy:
    text_file.write( "%s" % UTMancha+'\n')
  else:
    text_file.write( "%s" % d[l])
for l in range(len(s)):
    text_file.write( "%s" % s[l])
for l in range(iend,len(d)):
    text_file.write( "%s" % d[l])
text_file.close()
123/285: !lst
123/286: !mroe aermap.inp
123/287: more aermap.inp
123/288: xmin
123/289: fname
123/290: fname='Hsinzhu'
123/291: lon[0,0]
123/292: lat[0,0]
123/293: lat[-1,-1]
123/294:
x_mesh = np.linspace(xmin-dx, xmax+dx, nx+2)
y_mesh = np.linspace(ymin-dy, ymax+dy, ny+2)
# 2-d mesh coordinates, both in TWD97 and WGS84
x_g, y_g = np.meshgrid(x_mesh, y_mesh)
xgl=x_g-Xcent
ygl=y_g-Ycent
lon,lat=pnyc(xgl, ygl, inverse=True)
123/295:
I1=bisect.bisect_left(x1d,x_mesh[0])-2;I2=bisect.bisect_left(x1d,x_mesh[-1])+2
J1=bisect.bisect_left(y1d,y_mesh[0])-2;J2=bisect.bisect_left(y1d,y_mesh[-1])+2
123/296:
fname='taiwan2020.tif'
img = rasterio.open(fname)
data=np.flip(img.read()[0,:,:],[0])
123/297:
c=data[0,J1:J2,I1:I2].flatten()
c=np.where(c<0,0,c)
x=np.array([x1d[I1:I2] for j in range(J2-J1)]).flatten()
y=np.array([[j for i in range(I2-I1)] for j in y1d[J1:J2]]).flatten()
123/298:
c=data[J1:J2,I1:I2].flatten()
c=np.where(c<0,0,c)
x=np.array([x1d[I1:I2] for j in range(J2-J1)]).flatten()
y=np.array([[j for i in range(I2-I1)] for j in y1d[J1:J2]]).flatten()
123/299:
points=[(i,j) for i,j in zip(x,y)]
grid_z2 = griddata(points, c, (x_g, y_g), method='linear')
123/300: fname
123/301: fname='Hsinzhu'
123/302:
TIF,DEM,NUL=fname+'.tiff',last+'.dem',' >>'+dir+'geninp.out'
os.system('cp template.tiff '+TIF)
resx,resy=(np.max(lon)-np.min(lon))/(nx-1),(np.max(lat)-np.min(lat))/(ny-1),
transform = Affine.translation(np.min(lon) - resx / 2, np.max(lat) + resy / 2) * Affine.scale(resx, -resy)
new_dataset = rasterio.open(TIF,'w',driver='GTiff',height=grid_z2.shape[0],width=grid_z2.shape[1],count=1,
  dtype=grid_z2.dtype,crs='+proj=latlong',transform=transform,)
data=np.flip(grid_z2,[0])
new_dataset.write(data, 1)
new_dataset.close()
123/303: lst
123/304: !lst
123/305: os.system(cmd)
123/306: !lst
123/307:
llmin=pnyc(xmin-Xcent, ymin-Ycent, inverse=True) #long/lati
llmax=pnyc(xmax-Xcent, ymax-Ycent, inverse=True)
uxmn1,uymn1=utm.from_latlon(llmin[1],llmin[0])[0:2]
uxmn2,uymx1=utm.from_latlon(llmax[1],llmin[0])[0:2]
uxmx1,uymn2=utm.from_latlon(llmin[1],llmax[0])[0:2]
uxmx2,uymx2=utm.from_latlon(llmax[1],llmax[0])[0:2]
co,an,z='   DOMAINXY  ','   ANCHORXY  ',' 51 '
UTMrange=co+str(int(max(uxmn1,uxmn2)))+' '+str(int(max(uymn1,uymn2)))+z+str(int(min(uxmx1,uxmx2)))+' '+str(int(min(uymx1,uymx2)))+z
xmid,ymid=(xmin+xmax)/2., (ymin+ymax)/2.
llanc=pnyc(xmid-Xcent, ymid-Ycent, inverse=True)
uxanc,uyanc=utm.from_latlon(llanc[1],llanc[0])[0:2]
UTMancha=an+str(int(xmid))+' '+str(int(ymid))+' '+str(int(uxanc))+' '+str(int(uyanc))+z+'0'

#change the contain of aermap
text_file = open("aermap.inp", "r+")
d=[line for line in text_file]
keywd=[i[3:11] for i in d]
ifile=keywd.index('DATAFILE')
idmxy=keywd.index('DOMAINXY')
ianxy=keywd.index('ANCHORXY')
ihead=keywd.index('ELEVUNIT')
iend=d.index('RE FINISHED\n')

text_file = open("aermap.inp", "w")

x0,y0=xmin,ymin
sta='RE GRIDCART '+last+' STA\n'
args[0]=last
STR='RE GRIDCART {:s} XYINC {:s} {:s} {:s} {:s} {:s} {:s}\n'.format(*args)
s=[sta,STR,sta.replace('STA','END')]
for l in range(ihead+1):
#  if l == ifile:
#    text_file.write( "%s" % '   DATAFILE  '+DEM+'\n')
  if l == idmxy:
    text_file.write( "%s" % UTMrange+'\n')
  elif l == ianxy:
    text_file.write( "%s" % UTMancha+'\n')
  else:
    text_file.write( "%s" % d[l])
for l in range(len(s)):
    text_file.write( "%s" % s[l])
for l in range(iend,len(d)):
    text_file.write( "%s" % d[l])
text_file.close()
123/308: %history -f his.txt
123/309: !vi his.txt
123/310: STR
123/311: run gen_inp.py Hsinzhu XYINC 246453.  20  500.  2747820.  20  500.
123/312: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/313: !lst
123/314: !tail isc.out
123/315: !vi aermap.out
123/316: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/317: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/318: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/319: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/320: grid_z2.shape
123/321: lon.shape
123/322: lat.shape
123/323: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/324: xmid,Xcent
123/325: llanc
123/326: ymid-Ycent
123/327: Latitude_Pole, Longitude_Pole
123/328: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/329: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/330: cmd
123/331: fname='Hsinzhu.tiff'
123/332:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
123/333: img.bounds
123/334: llSE
123/335: img.bounds[:]
123/336: l,b,r,t=img.bounds[:]
123/337: l,b,r,t
123/338: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/339: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/340: cmd
123/341: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/342: fname='Hsinzhu.tiff'
123/343:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
123/344: nx,ny,nz
123/345: dxm,dym=(img.bounds.right-img.bounds.left)/img.width,-(img.bounds.top-img.bounds.bottom)/img.height
123/346: dxm,dym
123/347: l,b,r,t=img.bounds[:]
123/348: l,b,r,t
123/349: lon[0,0]
123/350: lon[-1,0]
123/351: resx,resy
123/352: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/353: I1,I2,J1,J2
123/354: fname
123/355: fname='Hsinzhu.tiff'
123/356:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
123/357: l,b,r,t=img.bounds[:]
123/358: l,b,r,t
123/359: pync(xmin-Xcent,ymin-Ycent,inverse=True)
123/360: pnyc(xmin-Xcent,ymin-Ycent,inverse=True)
123/361: pnyc(xmin-Xcent,ymax-Ycent,inverse=True)
123/362: uxmn1,uxmn2
123/363: uxanc
123/364: uxanc-uxmn1
123/365: uxanc-uxmn2
123/366: a=utm.from_latlon(lat,lon)
123/367: a.shape
123/368: a[0].shape
123/369: np.min(a[0]),np.max(a[0])
123/370: np.min(a[0][M:-M,M:-M]),np.max(a[0][M:-M,M:-M])
123/371: M
123/372: M=3
123/373: np.min(a[0][M:-M,M:-M]),np.max(a[0][M:-M,M:-M])
123/374: np.min(a[0]),np.max(a[0])
123/375: np.min(a[1]),np.max(a[1])
123/376: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/377: np.max(a[0][:,0]),np.min(a[0][:,-1])
123/378: np.max(a[1][0,:]),np.min(a[1][-1,0])
123/379: M
123/380: M=3
123/381: np.max(a[1][0,M:-M]),np.min(a[1][-1,M:-M])
123/382: np.max(a[0][M:-M,0]),np.min(a[0][M:-M,-1])
123/383: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/384: M
123/385: cmd
123/386: llNW
123/387: M=4
123/388: lon[-M,0]
123/389: lon[-1,0]
123/390: resx
123/391: l,b,r,t
123/392:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
123/393: fname='Hsinzhu.tiff'
123/394:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
123/395: l,b,r,t=img.bounds[:]
123/396: l,b,r,t
123/397: llNW
123/398: lon[-M,M]
123/399: M
123/400: lat[-M,M]
123/401: resx
123/402: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/403: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/404: fname='Hsinzhu.tiff'
123/405:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
123/406: data.shape
123/407: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/408: xmax-xmin
123/409: run gen_inp.py Hsinzhu 246453.  19  500.  2747820.  19  500.
123/410: dxm,dym
123/411:
xmax=xmin+(nx-1)*dx
ymax=ymin+(ny-1)*dy
123/412: x_mesh = np.linspace(xmin, xmax, nx)
123/413: x_mesh[:5]
123/414: x_mesh[-5:]
123/415: 255453.-246453.
123/416: dx,dy
123/417: len(x_mesh)
123/418: nx
123/419: nx=20
123/420: xmax=xmin+(nx-1)*dx
123/421: x_mesh = np.linspace(xmin, xmax, nx)
123/422: x_mesh[:5]
123/423: x_mesh[-5:]
123/424: M=3
123/425: x_mesh = np.linspace(xmin-dx*M, xmax+dx*M, nx+2*M)
123/426: x_mesh[:5]
123/427: x_mesh[M:M+5]
123/428: x_mesh[-M-5:-M]
123/429: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/430: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/431: a=utm.from_latlon(lat,lon)
123/432: np.max(a[0][M:-M,M]),np.min(a[0][M:-M,-M])
123/433:
xy=utm.from_latlon(lat,lon)
uxmn1,uxmx1=np.max(xy[0][M:-M,M]),np.min(xy[0][M:-M,-M])
123/434: M=5
123/435:
xy=utm.from_latlon(lat,lon)
uxmn1,uxmx1=np.max(xy[0][M:-M,M]),np.min(xy[0][M:-M,-M])
123/436: uxmn1,uxmx
123/437: uxmn1,uxmx1
123/438: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/439: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/440: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/441: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/442: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/443: run gen_inp.py Hsinzhu 246453.  20  500.  2747820.  20  500.
123/444: run gen_inp.py Hsinzhu 246453.  201  50.  2747820.  201  50.
123/445: run gen_inp.py Hsinzhu 246453.  101  100.  2747820.  101  100.
123/446: run gen_inp.py Hsinzhu 246453.  101  100.  2747820.  101  100.
123/447: run gen_inp.py Hsinzhu 246453.  101  100.  2747820.  101  100.
123/448: run gen_inp.py Hsinzhu 246453.  101  100.  2747820.  101  100.
123/449: run gen_inp.py Hsinzhu 246453.  201  50.  2747820.  201  50.
123/450: run gen_inp.py Hsinzhu 246453.  51  20.  2747820.  51  20.
123/451: uxmn1,uxmx1=np.min(xy[0][M:-M,M],dtype=int),np.max(xy[0][M:-M,-M])
123/452: M
123/453: xy[0].shape
123/454: xy=utm.from_latlon(lat,lon)
123/455: xy[0].shape
123/456: uxmn1,uxmx1=np.min(xy[0][M:-M,M],dtype=int),np.max(xy[0][M:-M,-M])
123/457: uxmn1,uxmx1=int(np.min(xy[0][M:-M,M])),int(np.max(xy[0][M:-M,-M]))
123/458: run gen_inp.py Hsinzhu 246453.  51  20.  2747820.  51  20.
123/459: os.environ("bs")
123/460: os.environ["bs"]
123/461: !echo $bs
123/462: os.environ["PATH"]
123/463: os.environ["HOSTNAME"]
123/464: img.lnglat()
123/465: pwd
123/466: run gen_inp.py Hsinzhu 246453.  51  20.  2747820.  51  20.
123/467: run gen_inp2.py Hsinzhu 246453.  51  20.  2747820.  51  20.
123/468: pwd
123/469: run gen_inp2.py Hsinzhu 246453.  51  20.  2747820.  51  20.
123/470: pwd
123/471: !lst
123/472: d
123/473: d=0
123/474: run gen_inp2.py Hsinzhu 246453.  51  20.  2747820.  51  20.
124/1: run gen_inp2.py Hsinzhu 246453.  51  20.  2747820.  51  20.
124/2: line
124/3: UTMancha
124/4: d[:5]
124/5: len(d)
124/6: pwd
124/7: more aermap.inp
124/8: run gen_inp2.py Hsinzhu 246453.  51  20.  2747820.  51  20.
124/9: fname
124/10: run gen_inp2.py Hsinzhu 246453.  51  20.  2747820.  51  20.
125/1: run gen_inp2.py Hsinzhu 246453.  51  20.  2747820.  51  20.
125/2: import rasterio
125/3: fname='Hsinzhu.tiff'
125/4:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
125/5: l,b,r,t=img.bounds[:]
125/6: l,b,r,t
125/7: img.lnglat()
125/8: (120.96370742445437+120.97620159183366)/2
125/9: (120.96370742445437+120.97620159183366)/2==img.lnglat()[0]
125/10: run tif2kml.py template.tiff
125/11: !lst
125/12: run tif2kml.py template.tiff
125/13: !vi tif2kml.py
125/14: run tif2kml.py template.tiff
125/15: !lst
125/16: !v
125/17: !vi tif2kml.py
125/18: more template.tiff.kml.kml
125/19: !lst
125/20: run tif2kml.py Hsinzhu.tiff
125/21: !lst
126/1: run tif2kml.py Hsinzhu.tiff
126/2: fname='Hsinzhu.tiff'
126/3:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
data=img.read()
126/4: img.proj
126/5: dir(img)
126/6: img.transform
126/7: dir(img)[:30]
126/8: dir(img)[30:60]
126/9: dir(img)[60:100]
126/10: (81+135)*2
126/11: pwd
126/12: cd /nas1/WRF4.0/WPS/
126/13: !lsS
126/14: fname='geo_em.d03_3Km.nc'
126/15: nc = netCDF4.Dataset(fname,'r')
126/16: import netCDF4
126/17: nc = netCDF4.Dataset(fname,'r')
126/18: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
126/19: V
126/20: [i for i in V[0] if 'LON' in i]
126/21: [i for i in V[1] if 'LON' in i]
126/22: [i for i in V[2] if 'LON' in i]
126/23: v='XLONG_M'
126/24: nc[v]
126/25: v='XLONG_U'
126/26: nc[v][0,0,0]
126/27: v='XLAT_V'
126/28: nc[v][0,0,0]
126/29: !lsS
126/30: pwd
126/31: cd ../WRF-chem
126/32: cd ..
126/33: !lsd
126/34: cd WRF_chem/WPS
126/35: !lsS
126/36: fname='geo_em.d01.nc_121.7359'
126/37: nc = netCDF4.Dataset(fname,'r')
126/38: !grep bisect ~/bin/*py
126/39: from bisect import bisect
126/40: breakpoints=nc[v][0,:,0]
126/41: score=13.17
126/42: score=13.1744
126/43: i = bisect(breakpoints, score)
126/44: breakpoints[i],breakpoints[i+1]
126/45: i
126/46: breakpoints[i-1],breakpoints[i]
126/47: score=104.56
126/48: v='XLONG_U'
126/49: breakpoints=nc[v][0,0,:]
126/50: i = bisect(breakpoints, score)
126/51: breakpoints[i-1],breakpoints[i]
126/52: i
126/53: !grep 44 /etc/hosts
126/54: v='XLAT_V'
126/55: breakpoints=nc[v][0,:,0]
126/56: fname
126/57: score=13.1744
126/58: i = bisect(breakpoints, score)
126/59: i
126/60: breakpoints[i-1],breakpoints[i]
126/61: fname='geo_em.d03_3Km.nc'
126/62: nc = netCDF4.Dataset(fname,'r')
126/63: pwd
126/64: cd ../../WPS
126/65: nc = netCDF4.Dataset(fname,'r')
126/66: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
126/67: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
126/68: nrow,nco
126/69: nrow,ncol
126/70: v='XLAT_V'
126/71: nc[v][0,0,ncol//2]
126/72: v='XLONG_U'
126/73: nc[v][0,nrow//2,0]
126/74: fname='geo_em.d01.nc_121.7359'
126/75: v
126/76: breakpoints=nc[v][0,nrow//2,:]
126/77: score=103.3
126/78: i = bisect(breakpoints, score)
126/79: breakpoints[i-1],breakpoints[i]
126/80: breakpoints[:5]
126/81: i
126/82: fname='geo_em.d01.nc_121.7359'
126/83: nc = netCDF4.Dataset(fname,'r')
126/84: pwd
126/85: ls *nc
126/86: fname='geo_em.d01.nc'
126/87: nc = netCDF4.Dataset(fname,'r')
126/88: dir(nc)
126/89: dir(nc)[:30]
126/90: nc.MOAD_CEN_LAT
126/91: nc.CEN_LON
126/92: v
126/93: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
126/94: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
126/95: breakpoints=nc[v][0,nrow//2,:]
126/96: i = bisect(breakpoints, score)
126/97: breakpoints[i-1],breakpoints[i]
126/98: i
126/99: v='XLAT_V'
126/100: score=14.15
126/101: breakpoints=nc[v][0,:,ncol//2]
126/102: i = bisect(breakpoints, score)
126/103: breakpoints[i-1],breakpoints[i]
126/104: i
126/105: %history -f his.txt
126/106: !vi his.txt
126/107: 1166/2*3/15
126/108: (671-1166/5)/2
126/109: 48*11/60
126/110: 0.061*0.1456
126/111: 0.00794/0.1456 *10
126/112: cd /nas1/WRF4.0/WRF_chem/201804_run56
126/113: fname='met_em.d01.2018-03-30_00:00:00.nc'
126/114: nc = netCDF4.Dataset(fname,'r')
126/115: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
126/116: V
126/117: v='
126/118: v='TT'
126/119: nc[v]
126/120: nc[0,0,:5,:5]
126/121: nc[v][0,0,:5,:5]
126/122: fname='metoa_em.d01.2018-03-30_00:00:00.nc'
126/123: nc = netCDF4.Dataset(fname,'r')
126/124: nc[v][0,0,:5,:5]
126/125: 4000/16
126/126: 671*15/81
126/127: 395*15/81
126/128: 1166*.8
126/129: 681*.8
126/130: (667*15-395*9)/2./15
126/131: (555*15-395*9)/2./18
126/132: (352*15-226*9)/2./18
126/133: (351*9-583*3)/2./9
126/134: (225*9-5355*3)/2./9
126/135: (225*9-355*3)/2./9
126/136: 1166*0.9
126/137: 681*0.9
126/138: (351*9-1049*3)/2./9
126/139: (351*9-931*3)/2./9
126/140: (225*9-531*3)/2./9
126/141: pwd
126/142: fname='wrfout_d01_2018-03-30_00:00:00'
126/143: nc = netCDF4.Dataset(fname,'r')
126/144: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
126/145: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
126/146: [i for i in V[3] if 'P' in i]
126/147: v='PB'
126/148: nc[v]
126/149: nc[v][:5,1,1,1]
126/150: v='P'
126/151: nc[v][:5,1,1,1]
126/152: nc[v][:5,:3,:3,:3]
126/153:
def isMonotonic(A):

    return (all(A[i] <= A[i + 1] for i in range(len(A) - 1)) or
            all(A[i] >= A[i + 1] for i in range(len(A) - 1)))
126/154: isMonotonic(nc[v][0,:,0,0])
126/155:
def isMonotonic(A):

    return (all(A[i] >= A[i + 1] for i in range(len(A) - 1)) or
            all(A[i] <= A[i + 1] for i in range(len(A) - 1)))
126/156: isMonotonic(nc[v][0,:,0,0])
126/157: isMonotonic([6, 5, 4, 4])
126/158: nc[v][0,:,0,0]
126/159: a=list(nc[v][0,:,0,0])
126/160: isMonotonic(a)
126/161:
def isMonotonic(A):

    return (all(A[i] <= A[i + 1] for i in range(len(A) - 1)) or
            all(A[i] >= A[i + 1] for i in range(len(A) - 1)))
126/162: isMonotonic([6, 5, 4, 4])
126/163: isMonotonic(a)
126/164: a
126/165: v='PB'
126/166: a=list(nc[v][0,:,0,0])
126/167: a
126/168: isMonotonic(a)
126/169:
for t in range(5):
    for j in range(3):
        for i in range(3):
            if isMonotonic(nc[v][t,:,j,i]):print(t,j,i)
126/170:
for t in range(5):
    for j in range(3):
        for i in range(3):
            if not isMonotonic(nc[v][t,:,j,i]):print(t,j,i)
126/171: t,j,i=(4, 0, 0,)
126/172: a=list(nc[v][t,:,j,i])
126/173: a
126/174:
for t in range(5):
    for j in range(3):
        for i in range(3):
            if not isMonotonic(nc[v][t,:,j,i]):print(t,j,i)
126/175: pres=nc['PB'][:]+nc['P'][:]
126/176:
for t in range(5):
    for j in range(3):
        for i in range(3):
            if not isMonotonic(pres[t,:,j,i]):print(t,j,i)
126/177: t,j,i=(4, 0, 0,)
126/178: a=list(pres[t,:,j,i])
126/179: a
126/180: t,j,i=(4, 0, 1,)
126/181: a=list(pres[t,:,j,i])
126/182: a
126/183: [i for i in V[3] if 'H' in i]
126/184: [i for i in V[2] if 'H' in i]
126/185: v='HGT'
126/186: np.where(nc[v][:]==0.9243630)
126/187: fname
126/188: nc[v].shape
126/189:
for t in range(5):
    for j in range(3):
        for i in range(3):
            if not isMonotonic(pres[t,:,j,i]):print(t,j,i)
126/190:
for t in range(24):
    for j in range(394):
        for i in range(670):
            if not isMonotonic(pres[t,:,j,i]):print(t,j,i)
126/191: ls name*
126/192: cd ../*N1
126/193: cd ../*N2
126/194: pwd
126/195: cd ..
126/196: lsd
126/197: !lsd
126/198: cd 201804_run56N2
126/199: ls name*
126/200: vi namelist.inputN1N2N3
126/201: !vi namelist.inputN1N2N3
126/202: nc.close()
126/203: nc = netCDF4.Dataset(fname,'r')
126/204:
for t in range(24):
    for j in range(394):
        for i in range(670):
            if not isMonotonic(pres[t,:,j,i]):print(t,j,i)
126/205: t,j,i=8 0 51
126/206: t,j,i=8, 0, 51
126/207: a=list(pres[t,:,j,i])
126/208: a
126/209:
for t in range(24):
    for j in range(394):
        for i in range(670):
            if not isMonotonic(pres[t,:,j,i]) and np.min(pres[t,:,j,i])>0:print(t,j,i)
126/210: np.zeros(shape=(nt,nrow,ncol))
126/211: a=np.zeros(shape=(nt,nrow,ncol))
126/212:
for t in range(24):
    for j in range(394):
        for i in range(670):
            if not isMonotonic(pres[t,:,j,i]) and np.min(pres[t,:,j,i])>0:a[t,j,i]=1
126/213: nc.close()
126/214: idx=np.where(a>0)
126/215: len(idx[0])
126/216: set(idx[0])
126/217: len(set(idx[1]))
126/218: len(set(idx[2]))
126/219: ls *nc
126/220: !ncdump -h uv10.nc|H
126/221: a.shape
126/222: !ncks -v Times,U10 met_em.d01.2018-04-09_12\:00\:00.nc a.nc
126/223: !ncks -v Times,U10 wrfout_d01_2018-03-30_00\:00\:00 a.nc
126/224: !ncks -O -d time,0 -v Times,U10 wrfout_d01_2018-03-30_00\:00\:00 a.nc
126/225: v
126/226: nc[v]
126/227: nc = netCDF4.Dataset(fname,'r')
126/228: nc[v]
126/229: !ncks -O -d Time,0 -v Times,U10 wrfout_d01_2018-03-30_00\:00\:00 a.nc
126/230: fname='a.nc'
126/231: nc = netCDF4.Dataset(fname,'r+')
126/232: nc.close()
126/233: !ncks -O -v Times,U10 wrfout_d01_2018-03-30_00\:00\:00 a.nc
126/234: nc = netCDF4.Dataset(fname,'r+')
126/235: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
126/236: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
126/237: nc['U10'][:,:,:]=a[:,:,:]
126/238: nc['U10'].shape
126/239: a.shape
126/240: pwd
126/241: cd ../*56
126/242: !ncks -O -v Times,U10 wrfout_d01_2018-03-30_00\:00\:00 a.nc
126/243: nc.close()
126/244: nc = netCDF4.Dataset(fname,'r+')
126/245: nc['U10'][:,:,:]=a[:,:,:]
126/246: nc['U10'].shape
126/247: a.shape
126/248: pwd
126/249: cd ../*56
126/250: cd /nas1/WRF4.0/WRF_chem/201804_run56
126/251: !ncks -O -v Times,U10 wrfout_d01_2018-03-30_00\:00\:00 a.nc
126/252: nc.close()
126/253: nc = netCDF4.Dataset(fname,'r+')
126/254: nc['U10'].shape
126/255: a.shape
126/256: nc['U10'][:,:,:]=a[:,:,:]
126/257: nc.close()
126/258: pwd
126/259: np.where(a[2,:,:]>0)
126/260: idx=np.where(a>0)
126/261: idx
126/262: fname='wrfout_d01_2018-03-31_00:00:00'
126/263: a=np.zeros(shape=(nt,nrow,ncol))
126/264: nc = netCDF4.Dataset(fname,'r')
126/265: pres=nc['PB'][:]+nc['P'][:]
126/266:
for t in range(24):
    for j in range(394):
        for i in range(670):
            if not isMonotonic(pres[t,:,j,i]) and np.min(pres[t,:,j,i])>0:a[t,j,i]=1
126/267: fname='wrfout_d01_2018-03-30_00:00:00'
126/268: nc.close()
126/269: nc = netCDF4.Dataset(fname,'r+')
126/270: nc['PB'][:,:,0,:]=nc['PB'][:,:,1,:]
126/271: nc['P'][:,:,0,:]=nc['P'][:,:,1,:]
126/272: nc['P'][0,:,:,:]=nc['P'][1,:,:,:]
126/273: nc['PB'][0,:,:,:]=nc['PB'][1,:,:,:]
126/274: nc.close()
126/275: nc = netCDF4.Dataset(fname,'r')
126/276: a=np.zeros(shape=(nt,nrow,ncol))
126/277: pres=nc['PB'][:]+nc['P'][:]
126/278:
for t in range(10):
    for j in range(394):
        for i in range(670):
            if not isMonotonic(pres[t,:,j,i]):a[t,j,i]=1
126/279: nc.close()
126/280: fname='a.nc'
126/281: nc = netCDF4.Dataset(fname,'r+')
126/282: nc['U10'][:,:,:]=a[:,:,:]
126/283: nc.close()
126/284: idx=np.where(a>0)
126/285: len(set(idx[2]))
126/286: pwd
126/287: fname
126/288: fname='wrfout_d01_2018-03-30_00:00:00'
126/289: nc = netCDF4.Dataset(fname,'r+')
126/290: nc['PB'][:,:,1,:]=nc['PB'][:,:,2,:]
126/291: nc['PB'][:,:,0,:]=nc['PB'][:,:,2,:]
126/292: nc['P'][:,:,1,:]=nc['P'][:,:,2,:]
126/293: nc['P'][:,:,0,:]=nc['P'][:,:,2,:]
126/294: nc.close()
126/295: (555*18-401*9)/2./18
126/296: 401*9
126/297: 1166*3
126/298: (555*18-411*9)/2./18
126/299: 681*3
126/300: (325*18-2516*9)/2./18
126/301: (325*18-251*9)/2./18
126/302: 251*9
126/303: (411*9-1166*3)/2/9
126/304: (251*9-681*3)/2/9
126/305: 48*12
126/306: 24*12
126/307: 11*24
126/308: pwd
126/309: cd /nas1/cmaqruns/2018base/data/mcip/1804_run5/CWBWRF_15k
126/310: cd /nas1/aermruns/terrByC_NO
126/311: df=read_csv('../teds_SRCPARAM/teds11/tedsSRCPARAM.csv')
126/312: from pandas import *
126/313: df=read_csv('../teds_SRCPARAM/teds11/tedsSRCPARAM.csv')
126/314: len(df)
126/315: df.head()
126/316: str='LOCATION G3300513P001 POINT 338980 2756833 0. SRCPARAM G3300513P001 100. 13.0  15.0  4'
126/317: df.loc[0,'STR']
126/318:
pv=read_csv('point_QCteds11.csv')
pv=pv.loc[pv.HY1>0].reset_index(drop=True)
pv.TEMP+=273
pv.PM25_EMI*=1000.*1000./3600/pv.HY1
pv.HEI=[round(i,1) for i in pv.HEI]
126/319: df1=pivot_table(pv,index='C_NO',values=['HEI'],aggfunc=np.max).reset_index()
126/320:
pv['C_NO']=[i[:8] for i in pv.CP_NO]
df1=pivot_table(pv,index='C_NO',values=['HEI'],aggfunc=np.max).reset_index()
126/321: df1.head()
126/322: maxHEI={i:j for i,j in zip(df1.C_NO,df1.HEI)}
126/323: pv2=pv.sort_values('C_NO','HEI')
126/324: pv2=pv.sort_values(on=['C_NO','HEI'])
126/325: pv2=pv.sort_values(index=['C_NO','HEI'])
126/326: pv2=pv.sort_values(by=['C_NO','HEI'])
126/327: pv2.head()
126/328: pv2=pv.sort_values(by=['C_NO','HEI'],ascending=False)
126/329: pv2.head()
126/330: df=DataFrame({})
126/331: pv2=pv.sort_values(by=['C_NO','HEI'],ascending=False).reset_index(drop=True)
126/332:
for c in set(pv2.C_NO):
    a=pv2.loc[pv2.C_NO==c].reset_index(drop=True)
    df=df.append(a,ignore_index=True)
126/333: len(df)
126/334: df.head()
126/335: df.tail()
126/336: pv2.head()
126/337: a=df.sort_values(by='HEI')
126/338: a.tail()
126/339: history
126/340: run wr_inptxt.py
126/341: pwd
126/342: (220-60)*0.75
126/343: 720*81
126/344: 13*1.852
126/345: 25*1.852
126/346: cd I:\nas1\TEDS\EDGARv5\TNR_Ship
126/347: cd /nas1/TEDS/EDGARv5/TNR_Ship
126/348: ls
126/349: fname='v50_SO2_2015_3_TNR_Ship.0.1x0.1.nc'
126/350: nc = netCDF4.Dataset(fname,'r')
126/351: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
126/352: V
126/353: v='emi_so2'
126/354: np.min(nc[v][:])
126/355: np.min(nc[v][:]>0)
126/356: idx=np.where(nc[v][:]>0)
126/357: a=nc[v][idx[0],idx[1]]
126/358: a=nc[v][idx[0][:],idx[1][:]]
126/359: len(idx[0])
126/360: nc[v].shape
126/361: 1800*3600
126/362: a=np.zeros(shape=680105)
126/363: a[:]=nc[v][idx[0][:],idx[1][:]]
126/364: arr=np.array(nc[v][:,:])
126/365: a[:]=arr[idx[0][:],idx[1][:]]
126/366: np.min(a)
126/367: np.max(a)
126/368: nc[v]
126/369: len(idx[0])
126/370: mx=np.max(a)
126/371:
for i in range(1,11):
    s=mx/10*i
    print(len(np.where(arr<s)))
126/372:
for i in range(1,11):
    s=mx/10*i
    print(len(np.where(arr<s)[0]))
126/373:
s=mx
for i in range(1,11):
    s/=10
    print(len(np.where(arr<s)[0]))
126/374:
s=mx
for i in range(1,21):
    s/=10
    print(len(np.where(arr<s)[0]))
126/375: s
126/376: np.min(a)
126/377:
s=mx
for i in range(1,21):
    s/=10
    print(len(np.where(a<s)[0]))
126/378: la=list(a)
126/379: la.sort()
126/380: la[int(len(a)/2)]
126/381: pwd
126/382: fname
126/383: run filt.py
126/384: run filt.py
126/385: run filt.py
126/386: run filt.py
126/387: fname='v50_SO2_2015_3_TNR_Ship.0.1x0.1N.nc'
126/388: nc = netCDF4.Dataset(fname,'r')
126/389: arr=np.array(nc[v][:,:])
126/390: idx=np.where(nc[v][:]>0)
126/391: len(idx[0])
126/392: run filt.py
126/393: (arr[2:5,2:5]==0.)any()
126/394: (arr[2:5,2:5]==0).any()
126/395: arr[2:5,2:5]
126/396: idx[0][:5]
126/397: idx[1][:5]
126/398: arr[512,3599]
126/399: arr[510:516,3597:3603]
126/400: (arr[510:516,3597:3603]<=0).any()
126/401: run filtS.py
126/402:  pwd
126/403: cd ../worldbank_density/
126/404: fname='shipdensity_global.tif'
126/405: import rasterio
126/406:
img = rasterio.open(fname)
nx,ny,nz=img.width,img.height,img.count
126/407: nx,ny,nz
126/408: dxm,dym=(img.bounds.right-img.bounds.left)/img.width,-(img.bounds.top-img.bounds.bottom)/img.height #間距
126/409: dxm,dym
126/410: x0,y0=img.xy(0,0)
126/411: l,b,r,t=img.bounds[:]
126/412: x0,y0
126/413: lonCent,latCent=img.lnglat()
126/414:
data=img.read() #shape=(1,ny,nx)，南北方向為北向南
data=np.flip(data[0,:,:],[0]) #轉向
126/415: np.sum(data)
126/416: data[:5,:5]
126/417: pwd
126/418: fname='templateD6.nc'
126/419: nc = netCDF4.Dataset(fname,'r')
126/420: pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
126/421:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
126/422:
x1,y1=np.meshgrid(x1d,y1d)
maxx,maxy=x1[-1,-1],y1[-1,-1]
minx,miny=x1[0,0],y1[0,0]
126/423: lon1,lat1=pnyc(x1,y1, inverse=True)
126/424: np.min(lon1),np.min(lat1)
126/425: np.max(lon1),np.max(lat1)
126/426: x0,y0,l,b,r,t
126/427: dxm,dym
126/428: lon=np.array([x0+dxm*i for i in range(nx)])
126/429: lat=np.array([b+dym*(i+0.5) for i in range(ny)])
126/430: ny
126/431: y0-ny*dym
126/432: dym
126/433: lat=np.array([b-dym*(i+0.5) for i in range(ny)])
126/434: y0+ny*dym
126/435: lat=np.array([y0+dym*(i) for i in range(ny)])
126/436: lat=np.flip(np.array([y0+dym*(i) for i in range(ny)]))
126/437: lat[:5]
126/438: lon[:5]
126/439: lat[-5:]
126/440: y0
126/441: i_x=np.where((lon>=100)&(lon<=134))[0]
126/442: i_x[:5]
126/443: i_x[-5:]
126/444: i_y=np.where((lat>=15)&(lat<=42))[0]
126/445: data1=data[i_x[0]:i_x[-1]+1,i_y[0]:i_y[-1]+1]
126/446: data1.shape
126/447: data.shape
126/448: data1=data[i_y[0]:i_y[-1]+1,i_x[0]:i_x[-1]+1]
126/449: data1.shape
126/450: !lst
126/451: fname='v50.nc'
126/452: nc = netCDF4.Dataset(fname,'r+')
126/453: data1.shape
126/454: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
126/455: V
126/456: v
126/457: nc[v].shape
126/458:
for j in range(1800,5400+1):
    nc[v][j,:]=0.
126/459: nc.close()
126/460: nc = netCDF4.Dataset(fname,'r+')
126/461:
for i in range(3600,6800+1):
    nc[v][i,:]=0.
126/462: nc.close()
126/463: nc = netCDF4.Dataset(fname,'r+')
126/464: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
126/465: nc[v].shape
126/466: nc.close()
126/467: nc = netCDF4.Dataset(fname,'r+')
126/468: nc[v].shape
126/469: nc[v][:,:]=data1[:,:]
126/470: nc.close()
126/471: np.sum(data1)
126/472: nc = netCDF4.Dataset(fname,'r+')
126/473: nc['lon']=lon[i_x[0]:i_x[-1]+1]
126/474: nc['lon'][:]=lon[i_x[0]:i_x[-1]+1]
126/475: nc['lat'][:]=lon[i_y[0]:i_y[-1]+1]
126/476: nc.close()
126/477: nc = netCDF4.Dataset(fname,'r+')
126/478: nc['lon'][:5]
126/479: nc['lon'][-5:]
126/480: dir(nc)
126/481: dir(nc)[:20]
126/482: dir(nc)[20:40]
126/483: nc['lat'][-5:]
126/484: nc['lat'][:]=lat[i_y[0]:i_y[-1]+1]
126/485: nc.close()
126/486: %history -f his.txt
126/487: !lst
126/488: run dens2nc.py
126/489: run dens2nc.py
126/490: run dens2nc.py
127/1:
import rasterio
import os
from pyproj import Proj



fname='templateD6.nc'
nc = netCDF4.Dataset(fname,'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
lon1,lat1=pnyc(x1,y1, inverse=True)
mnx,mny=np.min(lon1),np.min(lat1)
mxx,mxy=np.max(lon1),np.max(lat1)
nc.close()

kinds=['Glob','Comm','Fish','Leis','OGas']
fnames=['shipdensity_global.tif','ShipDensity_Commercial1.tif','ShipDensity_Fishing1.tif','ShipDensity_Leisure1.tif','ShipDensity_OilGas1.tif']
knames={k:n for k,n in zip(kinds,fnames)}
127/2:
import numpy as np
import netCDF4
127/3:
import rasterio
import os
from pyproj import Proj



fname='templateD6.nc'
nc = netCDF4.Dataset(fname,'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
lon1,lat1=pnyc(x1,y1, inverse=True)
mnx,mny=np.min(lon1),np.min(lat1)
mxx,mxy=np.max(lon1),np.max(lat1)
nc.close()

kinds=['Glob','Comm','Fish','Leis','OGas']
fnames=['shipdensity_global.tif','ShipDensity_Commercial1.tif','ShipDensity_Fishing1.tif','ShipDensity_Leisure1.tif','ShipDensity_OilGas1.tif']
knames={k:n for k,n in zip(kinds,fnames)}
127/4: pwd
127/5: cd /nas1/TEDS/EDGARv5/worldbank_density
127/6:
import rasterio
import os
from pyproj import Proj



fname='templateD6.nc'
nc = netCDF4.Dataset(fname,'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
lon1,lat1=pnyc(x1,y1, inverse=True)
mnx,mny=np.min(lon1),np.min(lat1)
mxx,mxy=np.max(lon1),np.max(lat1)
nc.close()

kinds=['Glob','Comm','Fish','Leis','OGas']
fnames=['shipdensity_global.tif','ShipDensity_Commercial1.tif','ShipDensity_Fishing1.tif','ShipDensity_Leisure1.tif','ShipDensity_OilGas1.tif']
knames={k:n for k,n in zip(kinds,fnames)}
127/7: k='Glob'
127/8:
  fname=knames[k]
  fnameO='Dens'+k+'D6.nc'
  img = rasterio.open(fname)
  data=img.read()
  data=np.flip(data[0,:,:],[0])
  nx,ny,nz=img.width,img.height,img.count
  dxm,dym=(img.bounds.right-img.bounds.left)/img.width,-(img.bounds.top-img.bounds.bottom)/img.height #間距
  x0,y0=img.xy(0,0)
  lon=np.array([x0+dxm*i for i in range(nx)])
  lat=np.flip(np.array([y0+dym*(i) for i in range(ny)]))
  ix=np.where((lon>=mnx)&(lon<=mxx))[0]
  iy=np.where((lat>=mny)&(lat<=mxy))[0]
  data1=data[iy[0]:iy[-1]+1,ix[0]:ix[-1]+1]
  os.system('cp densiD6.nc '+fnameO)
  nc = netCDF4.Dataset(fnameO,'r+')
127/9: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
127/10: V
127/11: v=V[1][0]
127/12: nc[v][:,:]=data1[:,:]
127/13: data1.shape
127/14: mnx,mny
127/15:
mnx,mny=min([100,np.min(lon1)]),min([15,np.min(lat1)])
mxx,mxy=max([134,np.max(lon1)]),max([42,np.max(lat1)])
127/16: mnx,mny
127/17: mxx,mxy
127/18:
  ix=np.where((lon>=mnx)&(lon<=mxx))[0]
  iy=np.where((lat>=mny)&(lat<=mxy))[0]
  data1=data[iy[0]:iy[-1]+1,ix[0]:ix[-1]+1]
127/19:     nc[v][:,:]=data1[:,:]
127/20: nc.close()
127/21:
for k in kinds[1:]:
  fname=knames[k]
  fnameO='Dens'+k+'D6.nc'
  img = rasterio.open(fname)
  data=img.read()
  data=np.flip(data[0,:,:],[0])
  nx,ny,nz=img.width,img.height,img.count
  dxm,dym=(img.bounds.right-img.bounds.left)/img.width,-(img.bounds.top-img.bounds.bottom)/img.height #間距
  x0,y0=img.xy(0,0)
  lon=np.array([x0+dxm*i for i in range(nx)])
  lat=np.flip(np.array([y0+dym*(i) for i in range(ny)]))
  ix=np.where((lon>=mnx)&(lon<=mxx))[0]
  iy=np.where((lat>=mny)&(lat<=mxy))[0]
  data1=data[iy[0]:iy[-1]+1,ix[0]:ix[-1]+1]
  os.system('cp densiD6.nc '+fnameO)
  nc = netCDF4.Dataset(fnameO,'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  v=V[1][0]
  try:
    nc[v][:,:]=data1[:,:]
    close(nc)
  except:
    print('fail filling '+k)
128/1:
import numpy as np
import netCDF4
import rasterio
import os
from pyproj import Proj



fname='templateD6.nc'
nc = netCDF4.Dataset(fname,'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
lon1,lat1=pnyc(x1,y1, inverse=True)
mnx,mny=min([100,np.min(lon1)]),min([15,np.min(lat1)])
mxx,mxy=max([134,np.max(lon1)]),max([42,np.max(lat1)])
nc.close()

kinds=['Glob','Comm','Fish','Leis','OGas']
fnames=['shipdensity_global.tif','ShipDensity_Commercial1.tif','ShipDensity_Fishing1.tif','ShipDensity_Leisure1.tif','ShipDensity_OilGas1.tif']
knames={k:n for k,n in zip(kinds,fnames)}
128/2: k='Comm'
128/3:
  fname=knames[k]
  fnameO='Dens'+k+'D6.nc'
  img = rasterio.open(fname)
  data=img.read()
  data=np.flip(data[0,:,:],[0])
  nx,ny,nz=img.width,img.height,img.count
  dxm,dym=(img.bounds.right-img.bounds.left)/img.width,-(img.bounds.top-img.bounds.bottom)/img.height #間距
  x0,y0=img.xy(0,0)
  lon=np.array([x0+dxm*i for i in range(nx)])
  lat=np.flip(np.array([y0+dym*(i) for i in range(ny)]))
  ix=np.where((lon>=mnx)&(lon<=mxx))[0]
  iy=np.where((lat>=mny)&(lat<=mxy))[0]
  data1=data[iy[0]:iy[-1]+1,ix[0]:ix[-1]+1]
  os.system('cp densiD6.nc '+fnameO)
  nc = netCDF4.Dataset(fnameO,'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  v=V[1][0]
128/4: data.shape
128/5: data1.shape
128/6: nc[v].shape
128/7:
  try:
    nc[v][:,:]=data1[:,:]
    close(nc)
  except:
    print('fail filling '+k)
128/8:   nc = netCDF4.Dataset(fnameO,'r+')
128/9:
  try:
    nc[v][:,:]=data1[:,:]
    nc.close()
  except:
    print('fail filling '+k)
128/10: fname='DensGlobD6.nc'
128/11: nc1 = netCDF4.Dataset(fname,'r')
128/12: fname='DensCommD6.nc'
128/13: nc2 = netCDF4.Dataset(fname,'r')
128/14: (nc1[v][:,:]==nc2[v][:,:]).all()
128/15: nx1,ny1=nc[v].shape
128/16: nx1,ny1
128/17: lat1,lon1=nc['lat'][:],nc['lon'][:]
128/18: lat1[:5]
128/19: pwd
128/20: ls
128/21: cd ../*Ship
128/22: ls
128/23: cd /nas1/TEDS/EDGARv5/TNR_Ship
128/24: ls
128/25:
spec='BC CO NH3 NMVOC NOx OC PM10 PM2.5 SO2'.split()
m='4'
fnames=['v50_'+s+'_2015_4_TNR_Ship.0.1x0.1.nc' for s in spec]
nc = netCDF4.Dataset(fnames[0],'r')
lat0,lon0=nc['lat'][:],nc['lon'][:]
nc.close()
128/26: lat0[:5]
128/27: I10=[(i-lon0[0])/0.1 for i in lon1]
128/28: I10[:5]
128/29: I10=[int((i-lon0[0])/0.1) for i in lon1]
128/30: I10[:5]
128/31: I10[-5:]
128/32: J10=[int((j-lat0[0])/0.1) for i in lat1]
128/33: J10=[int((j-lat0[0])/0.1) for j in lat1]
128/34: J10[:5]
128/35: J10[-5:]
128/36: JI=np.meshgrid(J10,I10)
128/37: JI[:5,:5]
128/38: JI.shape
128/39:
I10=np.array([int((i-lon0[0])/0.1) for i in lon1])
J10=np.array([int((j-lat0[0])/0.1) for j in lat1])
JI=np.meshgrid(J10,I10)
128/40: JI.shape
128/41: I10.shape
128/42: J10.shape
128/43:
I10=np.array([int((i-lon0[0])/0.1) for i in lon1])
J10=np.array([int((j-lat0[0])/0.1) for j in lat1])
JI=np.meshgrid(J10,I10,dtype=int)
128/44: len(JI)
128/45: JI
128/46:
I10=[int((i-lon0[0])/0.1) for i in lon1]
J10=[int((j-lat0[0])/0.1) for j in lat1]
JI=np.meshgrid(J10,I10)
128/47: len(JI)
128/48: nx1,ny1
128/49: JI=np.zeros(shape=(nx1,ny1),dtype=int)
128/50: JI=np.zeros(shape=(ny1,nx1),dtype=int)
128/51:
JI=np.zeros(shape=(2,ny1,nx1),dtype=int)
JI[0,:,:]=J10[:,None]
JI[1,:,:]=I10[None,:]
128/52:
I10=np.array([int((i-lon0[0])/0.1) for i in lon1])
J10=np.array([int((j-lat0[0])/0.1) for j in lat1])
128/53:
JI=np.zeros(shape=(2,ny1,nx1),dtype=int)
JI[0,:,:]=J10[:,None]
JI[1,:,:]=I10[None,:]
128/54: J10.shape
128/55: ny1
128/56:
ny1,nx1=nc[v].shape
JI=np.zeros(shape=(2,ny1,nx1),dtype=int)
JI[0,:,:]=J10[:,None]
JI[1,:,:]=I10[None,:]
128/57: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
128/58: V
128/59: v='emi_bc'
128/60:
ny1,nx1=nc[v].shape
JI=np.zeros(shape=(2,ny1,nx1),dtype=int)
JI[0,:,:]=J10[:,None]
JI[1,:,:]=I10[None,:]
128/61: fname
128/62: nc = netCDF4.Dataset(fname,'r')
128/63: pwd
128/64: fname='DensGlobD6.nc'
128/65: nc = netCDF4.Dataset(fname,'r')
128/66:
ny1,nx1=nc[v].shape
JI=np.zeros(shape=(2,ny1,nx1),dtype=int)
JI[0,:,:]=J10[:,None]
JI[1,:,:]=I10[None,:]
128/67: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
128/68: v=V[1][0]
128/69:
ny1,nx1=nc[v].shape
JI=np.zeros(shape=(2,ny1,nx1),dtype=int)
JI[0,:,:]=J10[:,None]
JI[1,:,:]=I10[None,:]
128/70:
ny0,nx0=J10[-1]-J10[0]+1,I10[-1]-I10[0]+1
D=np.zeros(shape=(ny0,nx0))
128/71: D.shape
128/72: lat1[-1]
128/73:
sI10=list(I10)
sI10.sort()
128/74: len(sI10)
128/75:
sI10=list(set(I10))
sI10.sort()
128/76: len(sI10)
128/77:
sI10,sJ10=list(set(I10)), list(set(J10))
sI10.sort(); sJ10.sort()
128/78: I10[18:25]
128/79: I10[:5]
128/80: I10[:20]
128/81: I10[28:35]
128/82: J10[:20]
128/83: J10[28:35]
128/84: list(I10[:25]).index(I10[0]+1)
128/85: list(I10[:25]).index(I10[0]+1)
128/86: list(J10[:25]).index(J10[0]+1)
128/87:
i0=list(I10[:25]).index(I10[0]+1)
j0=list(J10[:25]).index(J10[0]+1)
128/88: D=np.zeros(shape=(ny0,nx0))
128/89: fname
128/90:
for j in range(ny0):
  j1,j2=0,j0
  for i in range(nx0):
    i1,i2=0,i0
    D[j,i]=np.sum(nc[v][j1:j2,i1:i2])
    i2=i1
    i1+=20
  j2=j1
  j1+=20
128/91: D[:5,:5]
128/92: np.max(D)
128/93: j1,j2,i1,i2
128/94: i0,j0
128/95:
j1,j2=0,j0
for j in range(ny0):
  i1,i2=0,i0
  for i in range(nx0):
    D[j,i]=np.sum(nc[v][j1:j2,i1:i2])
    i2=i1
    i1+=20
  j2=j1
  j1+=20
128/96: np.max(D)
128/97: j1,j2,i1,i2
128/98: np.max(nc[v][:,:])
128/99: nx0,ny0
128/100:
j1,j2=0,j0
for j in range(ny0):
  i1,i2=0,i0
  for i in range(nx0):
    D[j,i]=np.sum(nc[v][j1:j2,i1:i2])
    i1=i2
    i2+=20
  j1=j2
  j2+=20
128/101: np.max(D)
128/102: SumDens=np.zeros(shape=(ny1,nx1))
128/103:
j1,j2=0,j0
for j in range(ny0):
  i1,i2=0,i0
  for i in range(nx0):
    SumDens[j1:j2,i1:i2]=np.sum(nc[v][j1:j2,i1:i2])
    i1=i2
    i2+=20
  j1=j2
  j2+=20
128/104: fnames
128/105:
nc = netCDF4.Dataset(fnames[0],'r')
lat0,lon0=nc['lat'][:],nc['lon'][:]
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[1][0]
EmsGlb=nc[v][:,:]
128/106: len(I10)
128/107:
fname='DensGlobD6.nc'
nc = netCDF4.Dataset(fname,'r')
lat1,lon1=nc['lat'][:],nc['lon'][:]
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[1][0]
ny1,nx1=nc[v].shape
dens=nc[v][:,:]
nc.close()
128/108:
fnameO=fname.replace('.nc',spec[0]+'.nc')
os.system('cp DensGlobD6.nc '+fnameO)
nc = netCDF4.Dataset(fnameO,'r+')
I10=np.array([int((i-lon0[0])/0.1) for i in lon1])
J10=np.array([int((j-lat0[0])/0.1) for j in lat1])
i0=list(I10[:25]).index(I10[0]+1)
j0=list(J10[:25]).index(J10[0]+1)
sI10,sJ10=list(set(I10));list(set(J10))
sI10.sort();sJ10.sort()
nx0,ny0=len(sI10),len(sJ10)
128/109:
sI10,sJ10=list(set(I10)),list(set(J10))
sI10.sort();sJ10.sort()
nx0,ny0=len(sI10),len(sJ10)
128/110:
j1,j2=0,j0
for j in range(ny0):
  i1,i2=0,i0
  for i in range(nx0):
    SumDens=np.sum(dens[j1:j2,i1:i2])
    if SumDens>0:
      rat=dens[j1:j2,i1:i2]/SumDens
      nc[v][j1:j2,i1:i2]=EmsGlb[sJ10[j],sJ10[j]]*rat
    else:
      nc[v][j1:j2,i1:i2]=0.
    i1=i2
    i2+=20
  j1=j2
  j2+=20
128/111: i,j
128/112: j1,j2,i1,i2
128/113: ny1,nx1
128/114: ny0,nx0
128/115: i0,j0
128/116:
j1,j2=0,j0
for j in range(ny0):
  i1,i2=0,i0
  for i in range(nx0):
    SumDens=np.sum(dens[j1:j2,i1:i2])
    if SumDens>0:
      rat=dens[j1:j2,i1:i2]/SumDens
      nc[v][j1:j2,i1:i2]=EmsGlb[sJ10[j],sJ10[j]]*rat
    else:
      nc[v][j1:j2,i1:i2]=0.
    i1=i2
    i2+=20
    i2=max(nx1,i2)
  j1=j2
  j2+=20
  j2=max(ny1,j2)
128/117: nc.close()
128/118: fnameO
128/119: nc = netCDF4.Dataset(fnameO,'r')
128/120: v
128/121: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
128/122: v=V[1][0]
128/123: v
128/124: np.max(nc[v][:,:])
128/125: SumDens
128/126: nc = netCDF4.Dataset(fnameO,'r+')
128/127:
j1,j2=0,j0
for j in range(ny0):
  i1,i2=0,i0
  for i in range(nx0):
    SumDens=np.sum(dens[j1:j2,i1:i2])
    if SumDens>0:
      rat=dens[j1:j2,i1:i2]/SumDens
      nc[v][j1:j2,i1:i2]=EmsGlb[sJ10[j],sI10[i]]*rat
    else:
      nc[v][j1:j2,i1:i2]=0.
    i1=i2
    i2+=20
    i2=max(nx1,i2)
  j1=j2
  j2+=20
  j2=max(ny1,j2)
128/128:  nc[v][:,:]=0.
128/129:
j1,j2=0,j0
for j in range(ny0):
  i1,i2=0,i0
  for i in range(nx0):
    SumDens=np.sum(dens[j1:j2,i1:i2])
    if SumDens>0:
      rat=dens[j1:j2,i1:i2]/SumDens
      nc[v][j1:j2,i1:i2]=EmsGlb[sJ10[j],sI10[i]]*rat
    else:
      nc[v][j1:j2,i1:i2]=0.
    i1=i2
    i2=min(nx1,i2+20)
  j1=j2
  j2=min(ny1,j2+20)
128/130: nc.close()
128/131: rat.shape
128/132: j1,j2,i1,i2
128/133: nc = netCDF4.Dataset(fnameO,'r+')
128/134: np.max(nc[v][:,:])
128/135: j,i
128/136: EmsGlb[sJ10[j],sI10[i]]
128/137: sJ10[j],sI10[i]
128/138: rat
128/139: SumDens
128/140:
fname='DensGlobD6.nc'
nc = netCDF4.Dataset(fname,'r')
128/141:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[1][0]
ny1,nx1=nc[v].shape
dens=np.array(nc[v][:,:])
128/142:
nc = netCDF4.Dataset(fnames[0],'r')
lat0,lon0=nc['lat'][:],nc['lon'][:]
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[1][0]
EmsGlb=nc[v][:,:]
128/143:
nc = netCDF4.Dataset(fnames[0],'r')
lat0,lon0=nc['lat'][:],nc['lon'][:]
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[1][0]
EmsGlb=np.array(nc[v][:,:])
128/144: nc = netCDF4.Dataset(fnameO,'r+')
128/145: v
128/146:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[1][0]
128/147: nc[v][:,:]=0
128/148:
j1,j2=0,j0
for j in range(ny0):
  i1,i2=0,i0
  for i in range(nx0):
    SumDens=np.sum(dens[j1:j2,i1:i2])
    if SumDens>0:
      rat=dens[j1:j2,i1:i2]/SumDens
      nc[v][j1:j2,i1:i2]=EmsGlb[sJ10[j],sI10[i]]*rat
    else:
      nc[v][j1:j2,i1:i2]=0.
    i1=i2
    i2=min(nx1,i2+20)
  j1=j2
  j2=min(ny1,j2+20)
nc.close()
128/149: nc = netCDF4.Dataset(fnameO,'r+')
128/150: lat1,lon1=nc['lat'][:],nc['lon'][:]
128/151: np.max(lat1)
128/152: np.max(lon1)
128/153: v
128/154: np.max(nc[v][:,:])
128/155: np.min(nc[v][:,:])
128/156: (nc[v][:,:]>=0).all()
128/157: nc.close()
128/158: !lst
128/159: rm DensGlobD6BC.nc
128/160: !lst
128/161: run TNR2WBDens.py
128/162: run TNR2WBDens.py
128/163:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[1][0]
128/164: v
128/165: run TNR2WBDens.py
128/166: run TNR2WBDens.py
128/167: !lst
128/168: fname='DensGlobD6BC.nc'
128/169: nc = netCDF4.Dataset(fnameO,'r+')
128/170: np.max(nc[v][:,:])
128/171: run TNR2WBDens.py
128/172: np.max(arr)
128/173: run TNR2WBDens.py
128/174: run TNR2WBDens.py
128/175: run TNR2WBDens.py
128/176: pwd
128/177:
spec='BC CO NH3 NMVOC NOx OC PM10 PM2.5 SO2'.split()
nspec=len(spec)
specn={spec[i]:i for i in range(nspec)}

ny,nx=5400,6800
var=np.zeros(shape=(9+1,ny,nx))
for s in spec:
  fname='DensGlobD6'+s+'.nc'
  nc = netCDF4.Dataset(fname,'r')
  v='emi_so2'
  var[specn[s],:,:]=nc[v][:,:]
var[-1,:,:]=var[specn['PM10'],:,:]-var[specn['PM2.5'],:,:]
var=np.where(var<0,0,var)
spec+=['PMC']
specn.update({'PMC':len(spec)-1})
128/178:
lonM=nc['lon'][:]
latM=nc['lat'][:]
lonm, latm = np.meshgrid(lonM, latM)
for ll in ['lon','lat']:
  exec(ll+'n={l:'+ll+'M.index(l) for l in '+ll+'M}')
128/179:
lonM=list(nc['lon'][:])
latM=list(nc['lat'][:])
lonm, latm = np.meshgrid(lonM, latM)
for ll in ['lon','lat']:
  exec(ll+'n={l:'+ll+'M.index(l) for l in '+ll+'M}')
128/180: lonM[:5]
128/181: lonn[100.01219]
128/182: list(lonn)[:5]
128/183: lonn[100.02219]
128/184:
for ll in ['lon','lat']:
  exec(ll+'n={'+ll+'M[l]:l for l in range(len('+ll+'M))}')
128/185: lonn[100.02219]
128/186: list(lonn.keys())[:5]
128/187: list(lonn.values())[:5]
128/188: a=100.02219
128/189: lonn[a]
128/190: a in lonn
128/191: a=lonM[5]
128/192: lonn[a]
128/193: a
128/194: a=100.02719
128/195: lonn[a]
128/196: DD='D6'
128/197:
tail=DD+'.nc'
fname='template'+tail
nc = netCDF4.Dataset(fname, 'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
X2d,Y2d=np.meshgrid(x1d,y1d)
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
lon, lat= pnyc(X1d,Y2d, inverse=True)
128/198: lon, lat= pnyc(X2d,Y2d, inverse=True)
128/199:     lon_ss, lat_ss= np.zeros(shape=(nrow+1,ncol+1), dtype=int)-1, np.zeros(shape=(nrow+1,ncol+1), dtype=int)-1
128/200: ll='lon'
128/201:
      llss =ll+'_ss'
      lls = np.zeros(shape=(nrow,ncol), dtype=int)-1
      exec('lls=np.searchsorted('+ll+'M,'+ll+')')
128/202:
      exec(llss+'[:nrow,:ncol]=lls[:,:]')
      for i in range(ncol):
        exec(llss+'[nrow,i]=lls[-1,i]*2-lls[-2,i]')
128/203:
      for j in range(nrow):
        exec(llss+'[j,ncol]=lls[j,-1]*2-lls[j,-2]')
      exec(llss+'[nrow,ncol]=lls[-1,-1]*2-lls[-2,-2]')
128/204: lon_ss.shape
128/205: lon_ss[:5,:5]
128/206: lon_ss[5,5]
128/207: lon[5,5]
128/208: lonM[1967:1970]
128/209: len(set(lon_ss))==len(lon_ss) and len(set(lat_ss))==len(lat_ss)
128/210: len(set(lon_ss))==len(lon_ss)
128/211: len(set(lon_ss[:]))==len(lon_ss[:])
128/212: len(set(lon_ss.flatten()))
128/213: len(lon_ss.flatten())
128/214:
EDGAR2EMIS={'BC':'PEC','OC':'POA','PM2.5':'FPRM','PMC':'CPRM'}
mw={i:1 for i in EDGAR2EMIS}
mw.update({'CO':28,'NH3':17,'NMVOC':58,'NOx':46,'SO2':64})
EDGAR2EMIS.update({'NOx':'NO2'})
EDGAR2EMIS.update({i:i for i in 'CO NH3 SO2'.split()})
VOCs=['ALD2','ALDX','BENZ','ETH','ETHA','ETHY','ETOH','FORM','HONO','IOLE','ISOP','KET','MEOH','OLE','PAR','PRPA','TERP','TOL','XYL']
nv=len(VOCs)
128/215: V[3]
128/216: run EDGAR2cmaqD6.py D6
128/217: zz=np.sum(var[ispec,lat_ss[j,i]:lat_ss[j+1,i+1],lon_ss[j,i]:lon_ss[j+1,i+1]],axis=(0,1))
128/218: run EDGAR2cmaqD6.py D6
128/219: run EDGAR2cmaqD6.py D6
128/220: run EDGAR2cmaqD6.py D6
128/221: mwv=mw[v]
128/222:
v='NMVOC'
ispec=specn[v]
mwv=mw[v]
128/223:
for j in range(nrow):
  for i in range(ncol):
    zz=np.mean(var[ispec,lat_ss[j,i]:lat_ss[j+1,i+1],lon_ss[j,i]:lon_ss[j+1,i+1]],axis=(0,1))
    for v in VOCs:
      iv=VOCs.index(v)
      nc[v][0,0,j,i]=zz*Vspl[iv,j,i]/mwv*1000.*nc.XCELL*nc.YCELL
128/224: nc.close()
128/225: !lst
128/226: pwd
128/227:
fnames=['v50_'+s+'_2015_4_TNR_Ship.0.1x0.1.nc' for s in spec]
nc = netCDF4.Dataset(fnames[0],'r')
lat0,lon0=nc['lat'][:],nc['lon'][:]
128/228: lat0[:5]
128/229: lon0[:5]
128/230: sI10[:5]
128/231: os.system('cp '+fnames[0]+' a')
128/232: fname='a'
128/233: nc = netCDF4.Dataset(fnameO,'r+')
128/234: nc = netCDF4.Dataset(fname,'r+')
128/235:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v=V[1][0]
128/236: v
128/237: arr=np.array(nc[v][:,:])
128/238: nx0,ny0=len(sI10),len(sJ10)
128/239: data=np.zeros(shape=(ny0,nx0))
128/240:
for j in range(ny0):
  for i in range(nx0):
    data[j,i]=arr[sJ10[j],sI10[i]]
128/241: nc[v][:ny0,:nx0]=data[:,:]
128/242: nc.close()
128/243: fname='EDGARD6.nc'
128/244: nc = netCDF4.Dataset(fname,'r+')
128/245: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
128/246:
for v in V[3]:
    nc[v][:]*=400
128/247: nc.close()
128/248: zz=np.mean(var[ispec,lat_ss[j,i]:lat_ss[j+1,i+1],lon_ss[j,i]:lon_ss[j+1,i+1]],axis=(0,1))
128/249: zz
128/250: lats=lat_ss[j+1,i+1].flatten()
128/251: lons=lon_ss[j+1,i+1].flatten()
128/252: z=var[ispec,lats[:],lons[:]]
128/253: len(z)
128/254: z.shape
128/255: lats
128/256: lons
128/257: z
128/258: var[ispec,lat_ss[j,i]:lat_ss[j+1,i+1],lon_ss[j,i]:lon_ss[j+1,i+1]].shape
128/259: lat_ss[j,i],lat_ss[j+1,i+1]
128/260: run EDGAR2cmaqD2.py D6
128/261: 4500/12
128/262: 2700/12
129/1:
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt

map = Basemap(llcrnrlon=-0.5,llcrnrlat=39.8,urcrnrlon=4.,urcrnrlat=43.,
             resolution='i', projection='tmerc', lat_0 = 39.5, lon_0 = 1)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.drawcoastlines()
129/2: !pip install basemap
129/3:
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt

map = Basemap(llcrnrlon=-0.5,llcrnrlat=39.8,urcrnrlon=4.,urcrnrlat=43.,
             resolution='i', projection='tmerc', lat_0 = 39.5, lon_0 = 1)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.drawcoastlines()
129/4: %matplotlib inline
129/5:
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt

map = Basemap(llcrnrlon=-0.5,llcrnrlat=39.8,urcrnrlon=4.,urcrnrlat=43.,
             resolution='i', projection='tmerc', lat_0 = 39.5, lon_0 = 1)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.drawcoastlines()
129/6:
import utm
import numpy as np
Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
Xcent, Ycent = twd97.fromwgs84(Latitude_Pole, Longitude_Pole)
x1d=[i*3000 for i in range(83)]
y1d=[i*3000 for i in range(137)]
x,y=np.meshgrid(x1d,y1d)
ll = np.array([twd97.towgs84(i*1000+Xcent,j*1000+Ycent) for i, j in zip(x.flatten(), y.flatten())])
lat, lon = (ll[:, i] for i in [0, 1])
129/7:
import utm,twd97
import numpy as np
Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
Xcent, Ycent = twd97.fromwgs84(Latitude_Pole, Longitude_Pole)
x1d=[i*3000 for i in range(83)]
y1d=[i*3000 for i in range(137)]
x,y=np.meshgrid(x1d,y1d)
ll = np.array([twd97.towgs84(i*1000+Xcent,j*1000+Ycent) for i, j in zip(x.flatten(), y.flatten())])
lat, lon = (ll[:, i] for i in [0, 1])
129/8:
import utm,twd97
import numpy as np
Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
Xcent, Ycent = twd97.fromwgs84(Latitude_Pole, Longitude_Pole)
x1d=[i*3 for i in range(83)]
y1d=[i*3 for i in range(137)]
x,y=np.meshgrid(x1d,y1d)
ll = np.array([twd97.towgs84(i*1000+Xcent,j*1000+Ycent) for i, j in zip(x.flatten(), y.flatten())])
lat, lon = (ll[:, i] for i in [0, 1])
129/9: lat[0],lon[0],lat[-1],lon[-1]
129/10:
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=on[-1],
             resolution='i', projection='tmerc', lat_0 = (lat[0]+lat[-1])/2, lon_0 = (lon[0]+lon[-1])/2)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.drawcoastlines()
129/11:
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=lon[-1],
             resolution='i', projection='tmerc', lat_0 = (lat[0]+lat[-1])/2, lon_0 = (lon[0]+lon[-1])/2)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.drawcoastlines()
129/12: (lat[0]+lat[-1])/2
129/13: (lat[-1]-lat[0])/137,(lon[-1]-lon[0])/83
129/14:
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=lon[-1],
             resolution='i', projection='tmerc', lat_0 = lat[0]+0.026726026768930884, lon_0 = lon[0]+0.029927146880506128)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.drawcoastlines()
129/15:
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=lat[-1],
             resolution='i', projection='tmerc', lat_0 = lat[0]+0.026726026768930884, lon_0 = lon[0]+0.029927146880506128)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.drawcoastlines()
129/16:
import utm,twd97
import numpy as np
Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
Xcent, Ycent = twd97.fromwgs84(Latitude_Pole, Longitude_Pole)
x1d=[i*3 for i in range(-82/2,84/2)]
y1d=[i*3 for i in range(-136/2,138/2)]
x,y=np.meshgrid(x1d,y1d)
ll = np.array([twd97.towgs84(i*1000+Xcent,j*1000+Ycent) for i, j in zip(x.flatten(), y.flatten())])
lat, lon = (ll[:, i] for i in [0, 1])
129/17:
import utm,twd97
import numpy as np
Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
Xcent, Ycent = twd97.fromwgs84(Latitude_Pole, Longitude_Pole)
x1d=[i*3 for i in range(-82//2,84//2)]
y1d=[i*3 for i in range(-136//2,138//2)]
x,y=np.meshgrid(x1d,y1d)
ll = np.array([twd97.towgs84(i*1000+Xcent,j*1000+Ycent) for i, j in zip(x.flatten(), y.flatten())])
lat, lon = (ll[:, i] for i in [0, 1])
129/18:
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=lat[-1],
             resolution='i', projection='tmerc', lat_0 = lat[0]+0.026726026768930884, lon_0 = lon[0]+0.029927146880506128)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.drawcoastlines()
129/19:
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=lat[-1],
             resolution='i', projection='tmerc', lat_0 = lat[0]+0.026726026768930884, lon_0 = lon[0]+0.029927146880506128)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.readshapefile('TWN_COUNTY.shp', 'TWN_COUNTY')
map.drawcoastlines()
129/20:
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=lat[-1],
             resolution='i', projection='tmerc', lat_0 = lat[0]+0.026726026768930884, lon_0 = lon[0]+0.029927146880506128)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.readshapefile('TWN_COUNTY', 'TWN_COUNTY')
map.drawcoastlines()
129/21:
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=lat[-1],
             resolution='i', projection='tmerc', lat_0 = lat[0]+0.026726026768930884, lon_0 = lon[0]+0.029927146880506128)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.readshapefile('TWN_COUNTY', 'TWN_COUNTY')
map.drawcoastlines()
129/22:
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=lat[-1],
             resolution='i', projection='tmerc', lat_0 = lat[0]+0.026726026768930884, lon_0 = lon[0]+0.029927146880506128)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.readshapefile('TWN_COUNTY', 'TWN_COUNTY',coding='big5')
map.drawcoastlines()
129/23:
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=lat[-1],
             resolution='i', projection='tmerc', lat_0 = lat[0]+0.026726026768930884, lon_0 = lon[0]+0.029927146880506128)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.readshapefile('TWN_COUNTY', 'TWN_COUNTY',encoding='big5')
map.drawcoastlines()
129/24:
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=lat[-1],
             resolution='i', projection='tmerc', lat_0 = lat[0]+0.026726026768930884, lon_0 = lon[0]+0.029927146880506128)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.readshapefile('TWN_COUNTY', 'TWN_COUNTY')
map.drawcoastlines()
129/25:
# coding='utf8'
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=lat[-1],
             resolution='i', projection='tmerc', lat_0 = lat[0]+0.026726026768930884, lon_0 = lon[0]+0.029927146880506128)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.readshapefile('TWN_COUNTY', 'TWN_COUNTY')
map.drawcoastlines()
129/26: !pip install pyshp
129/27:
# coding='utf8'
map = Basemap(llcrnrlon=lon[0],llcrnrlat=lat[0],urcrnrlon=lon[-1],urcrnrlat=lat[-1],
             resolution='i', projection='tmerc', lat_0 = lat[0]+0.026726026768930884, lon_0 = lon[0]+0.029927146880506128)

map.drawmapboundary(fill_color='aqua')
map.fillcontinents(color='#ddaa66',lake_color='aqua')
map.readshapefile('TWN_COUNTY', 'TWN_COUNTY', default_encoding='big5')
map.drawcoastlines()
129/28: 748/8
129/29: 748/9
129/30: 747/9
129/31:
import os
import sys
import datetime as dt
import numpy as np
import matplotlib.image as mpimg
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties
from mpl_toolkits.basemap import Basemap
import pytz
import gmplot
from dateutil.parser import parse
import warnings
import twd97
128/263: 160*.7
128/264: (220-57)
128/265: cd /nas1/Data/cwb/WRF_3Km/2022/20220328
128/266: !lst
128/267: fname='M-A0061-000.nc'
128/268: nc = netCDF4.Dataset(fname,'r')
128/269: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
128/270: V
128/271: a=list(dir(nc))
128/272: len(a)
128/273: a[:20]
128/274: a[20:40]
128/275: a[40:60]
128/276: *
128/277: 366*6
128/278: 16+31+5
128/279: 124+56+20
128/280: cd /nas2/cmaqruns/2019force/Perf_Tools/Air_Evaluate_tool/Taiwan_d4
128/281: df=read_csv('./Data/Obs/2019NMHC_PerHour.csv')
128/282: df.head()
128/283: for i in df.columns:print(i)
128/284: df=read_csv('./Data/Obs/2019NMHC_PerHour.csv',encoding='big5')
128/285: df=read_csv('./Data/Obs/2019NMHC_PerHour.csv',encoding='utf8')
128/286: for i in df.columns:print(i)
128/287: df=read_csv('./Data/Obs/2019NMHC_PerHour.csv',encoding='utf-8-sig', index_col=0)
128/288: for i in df.columns:print(i)
128/289: df.head()
128/290: stn='竹東'
128/291: df[stn]
128/292: df=read_csv('./Data/Obs/2019O3_PerHour.csv',encoding='utf-8-sig', index_col=0)
128/293: df[stn]
128/294: cd /nas2/cmaqruns/2019force/output/2019-01/grid03/mcip
128/295: !lst
128/296: ls -lrth|tail
128/297: pwd
128/298: ls
128/299: fname='zh.nc'
128/300: nc = netCDF4.Dataset(fname,'r')
128/301: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
128/302: V
128/303: v='ZH'
128/304: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
128/305: nc[v][0,:,0,ncol-1]
128/306: 411*12
128/307: 411-280
128/308: 131*12
128/309: cd /nas1/TEDS/teds11/biog
128/310: ls
128/311: lst
128/312: !lst
128/313: fname='biog201901K.ncf'
128/314: nc = netCDF4.Dataset(fname,'r+')
128/315: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
128/316:
for v in V[3]:
    nc[v]=0.
128/317:
for v in V[3]:
    nc[v][:]=0.
128/318: nc.close()
128/319: !lst
128/320: !lst
128/321: !lst
128/322: fname='biog201901K1.ncf'
128/323: nc = netCDF4.Dataset(fname,'r+')
128/324:
for v in V[3]:
    nc[v][:]=0.
128/325: nc.close()
128/326: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
128/327: nc = netCDF4.Dataset(fname,'r+')
128/328: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
128/329: x,y=np.zeros(shape=(nrow,ncol))
128/330: x,y=np.zeros(shape=(nrow,ncol)),np.zeros(shape=(nrow,ncol))
128/331: x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
128/332: y1d=[nc.YORIG+nc.YCELL*i for i in range(ncol)]
128/333: x1d[:5]
128/334: x1d[-5:]
128/335:
fname='fortBE.413_teds10.biog02.nc'
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
128/336: nt0,nlay0,nrow0,ncol0=nc0.variables[V0[3][0]].shape
128/337: from pyproj import Proj
128/338:
Latitude_Pole, Longitude_Pole = nc.YCENT, nc.XCENT
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
128/339: x,y=np.meshgrid(x1d,y1d)
128/340: lon, lat = pnyc(x, y, inverse=True)
128/341: Latitude_Pole, Longitude_Pole = nc0.YCENT, nc0.XCENT
128/342: y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
128/343: x,y=np.meshgrid(x1d,y1d)
128/344: lon, lat = pnyc(x, y, inverse=True)
128/345: lon.shape
128/346: lat.shape
128/347:
Latitude_Pole, Longitude_Pole = nc0.YCENT, nc0.XCENT
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
128/348: x,y=pnyc(lon,lat, inverse=False)
128/349: x[:5,:5]
128/350:
fname='fortBE.413_teds10.biog02.nc'
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nrow0,ncol0=nc0.variables[V0[3][0]].shape
x1d0=[nc0.XORIG+nc0.XCELL*i for i in range(ncol0)]
y1d0=[nc0.YORIG+nc0.YCELL*i for i in range(nrow0)]
128/351: from bisect import bisect
128/352: bisect.bisect_left(x1d,x1d0[0])
128/353: import bisect
128/354: bisect.bisect_left(x1d,x1d0[0])
128/355: x1d0[0]
128/356: x0,y0=np.meshgrid(x1d0,y1d0)
128/357:
Latitude_Pole, Longitude_Pole = nc0.YCENT, nc0.XCENT
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
128/358: x,y=pnyc(lon,lat, inverse=False)
128/359: bisect.bisect_left(x[0,:],x1d0[0])
128/360: x[0,10:20]
128/361: nc0.XORIG
128/362: bisect.bisect_left(x[0,:],x1d0[-1])
128/363: 92-16+1
128/364: nt0,nlay0,nrow0,ncol0
128/365: x.shape
128/366: x1d0[-1],x[0,-1]
128/367: bisect.bisect_left(y[:,16],y1d0[-1])
128/368: y.shape
128/369: bisect.bisect_left(y[:,16],y1d0[0])
128/370: bisect.bisect_left(y1d0,y[0,16])
128/371: bisect.bisect_left(y1d0,y[-1,16])
128/372: len(y1d0)
128/373: bisect.bisect_left(x1d0,x[0,:])
128/374: bisect.bisect_left(x1d0,x[0,-1])
128/375: bisect.bisect_left(x1d0,x[-1,-1])
128/376: i0,j0=np.zeros(shape=x.shape,dtype=int)-1,np.zeros(shape=x.shape,dtype=int)-1
128/377:
for j in range(131):
    for i in range(92)
128/378: bisect.bisect_left(x1d0,x[-1,0])
128/379: bisect.bisect_left(x[-1,:],x1d0[-1])
128/380: bisect.bisect_left(x[-1,:],x1d0[0])
128/381: from scipy.interpolate import griddata
128/382: bisect.bisect_left(x[0,:],x1d0[0])
128/383: max(bisect.bisect_left(x[0,:],x1d0[0]),bisect.bisect_left(x[-1,:],x1d0[0]))
128/384: i0=np.searchsorted(x0,x)
128/385:
maxx,maxy=x0[-1,-1],y0[-1,-1]
minx,miny=x0[0,0],y0[0,0]
128/386: boo=(maxx>=x>= minx) & (maxy>=y>=miny)
128/387: boo=(maxx>=x& x >= minx) & (maxy>=y& y>=miny)
128/388: boo=(maxx>=x) & ( x >= minx) & (maxy>=y) & (y>=miny)
128/389:
idx = np.where(boo)
mp=len(idx[0])
xyc= [(x[idx[0][i],idx[1][i]],y[idx[0][i],idx[1][i]]) for i in range(mp)]
128/390: mp
128/391: 92*131
128/392: from dtconvertor import dt2jul, jul2dt
128/393: sdatetime=[jul2dt(nc['TFLAG'][t,0,:]) for t in range(nt)]
128/394: sdatetime0=[jul2dt(nc0['TFLAG'][t,0,:]) for t in range(nt0)]
128/395: sdatetime[:5]
128/396: sdatetime0[:5]
128/397: sdatetime.index(sdatetime0[0])
128/398: sdatetime[-5:]
128/399: sdatetime0[-5:]
128/400: fname
128/401:
fname='fortBE.413_teds10.biog01.nc'
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay0,nrow0,ncol0=nc0.variables[V0[3][0]].shape
x1d0=[nc0.XORIG+nc0.XCELL*i for i in range(ncol0)]
y1d0=[nc0.YORIG+nc0.YCELL*i for i in range(nrow0)]
128/402: sdatetime0=[jul2dt(nc0['TFLAG'][t,0,:]) for t in range(nt0)]
128/403: sdatetime0[-5:]
128/404: sdatetime.index(sdatetime0[0])
128/405: sdatetime.index(sdatetime0[-1])
128/406: !grep cale ~/bin/*py
128/407: import calendar
128/408: m=1
128/409: mn=calendar(2019,m)[1]
128/410: mn=calendar.monthrange(2019,m)[1]
128/411: mn
128/412: mn=calendar.monthrange(sdatetime[0].year,sdatetime[0].month)[1]
128/413: mn
128/414: calendar.monthrange(sdatetime[0].year,sdatetime[0].month)[1]
128/415: sdatetime[0].year,sdatetime[0].month
128/416: itb=sdatetime.index(sdatetime0[0])
128/417: sdatetime[:itb]=[sdatetime[t]+timedelta(days=mn) for t in range(itb)]
128/418: sdatetime[:itb]=[sdatetime[t]+datetime.timedelta(days=mn) for t in range(itb)]
128/419: import datetime
128/420: sdatetime[:itb]=[sdatetime[t]+datetime.timedelta(days=mn) for t in range(itb)]
128/421: sdatetime[itb-5:itb+5]
128/422: sdatetime0.index(sdatetime[5])
128/423: sdatetime0.index(sdatetime[itb])
128/424: sdatetime0.index(sdatetime[itb-3])
128/425: set(sdatetime)-set(sdatetime0)
128/426: idxt=[sdatetime0.index(sdt) for sdt in sdatetime]
128/427: idxt[:5]
128/428: idxt[itb]
128/429: idxt[-5:]
128/430: len(idxt)
128/431: nt
128/432: xyc.shape
128/433: len(xyc)
128/434: mp
128/435: v=V[3][0]
128/436: v
128/437: v not in V0[3]
128/438: V0[3][0]
128/439: v=V0[3][0]
128/440: v in V[3][0]
128/441: set(V0[3])-set(V[3])
128/442: set(V0[3])& set(V[3])
128/443: set(V0[3])&& set(V[3])
128/444: v='ACET'
128/445: t=5
128/446: it=idxt[t]
128/447: c = np.array([nc0[v][it,0,idx[0][i], idx[1][i]] for i in range(mp)])
128/448: boo=(maxx>x) & ( x > minx) & (maxy>y) & (y>miny)
128/449:
idx = np.where(boo)
mp=len(idx[0])
xyc= [(x[idx[0][i],idx[1][i]],y[idx[0][i],idx[1][i]]) for i in range(mp)]
128/450: mp
128/451: len(idx)
128/452: max(idx[0])
128/453: max(idx[1])
128/454: boo=(x < maxx) & ( x > minx) & (y < maxy) & (y > miny)
128/455: idx = np.where(boo)
128/456: max(idx[1])
128/457:
maxx,maxy=x[-1,-1],y[-1,-1]
minx,miny=x[0,ib],y[0,ib]
128/458: ib=max(bisect.bisect_left(x[0,:],x1d0[0]),bisect.bisect_left(x[-1,:],x1d0[0]))
128/459:
maxx,maxy=x[-1,-1],y[-1,-1]
minx,miny=x[0,ib],y[0,ib]
128/460: boo=(x0 < maxx) & ( x0 > minx) & (y0 < maxy) & (y0 > miny)
128/461:
idx = np.where(boo)
mp=len(idx[0])
xyc= [(x0[idx[0][i],idx[1][i]],y0[idx[0][i],idx[1][i]]) for i in range(mp)]
128/462: c = np.array([nc0[v][it,0,idx[0][i], idx[1][i]] for i in range(mp)])
128/463: c.shape
128/464: zz=griddata(xyc, c[:], (x[:,ib:], y[:,ib:]), method='linear')
128/465: zz.shape
128/466: 72+ib
128/467: ib
128/468: 72+ib+7
128/469: ncol
128/470: 92-16
128/471: zz=np.where(np.isnan(zz),0,zz)
128/472: nc[v][t,:,ib:]=zz
128/473: ncol
128/474: nrow
128/475: nc[v].shape
128/476: zz.shape
128/477: nc[v][t,0,:,ib:]=zz
128/478: nc[v][t,0,:,ib:]=0
128/479: nc.close()
128/480: run biog201901K.py
128/481: import netCDF4
128/482: run biog201901K.py
128/483: run biog201901K.py
128/484: am
128/485: run biog201901K.py
128/486:
m=1
am='{:02d}'.format(m)
fname='biog2019'+am+'K1.ncf'
nc = netCDF4.Dataset(fname,'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay1,nrow1,ncol1=nc[V[3][0]].shape
sdatetime=[jul2dt(nc['TFLAG'][t,0,:]) for t in range(nt)]

Latitude_Pole, Longitude_Pole = nc.YCENT, nc.XCENT
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol1)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow1)]
x1,y1=np.meshgrid(x1d,y1d)
lon, lat = pnyc(x1, y1, inverse=True)

fname='fortBE.413_teds10.biog'+am+'.nc'
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay,nrow,ncol=nc0.variables[V0[3][0]].shape

sdatetime0=[jul2dt(nc0['TFLAG'][t,0,:]) for t in range(nt0)]
itb=sdatetime.index(sdatetime0[0])
128/487:
mn=calendar.monthrange(sdatetime[0].year,sdatetime[0].month)[1]
sdatetime[:itb]=[sdatetime[t]+datetime.timedelta(days=mn) for t in range(itb)]
idxt=[sdatetime0.index(sdt) for sdt in sdatetime]

x1d=[nc0.XORIG+nc0.XCELL*i for i in range(ncol)]
y1d=[nc0.YORIG+nc0.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(x1d,y1d)

#transposition to nc0 coordinates
Latitude_Pole, Longitude_Pole = nc0.YCENT, nc0.XCENT
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
x1,y1=pnyc(lon,lat, inverse=False)
ib=max(bisect.bisect_left(x1[0,:],x1d[0]),bisect.bisect_left(x1[-1,:],x1d[0]))

idx=np.zeros(shape=(nrow1,ncol1,2),dtype=np.int)
128/488:
for j in range(nrow1):
  min_j=min([bisect(list(y[:,ii]),y1[j,0]) for ii in range(ncol)])-1
  min_i=min([bisect(list(x[jj,:]),x1[j,0]) for jj in range(nrow)])-1
  max_j=max([bisect(list(y[:,ii]),y1[j,-1]) for ii in range(ncol)])+1
  max_i=max([bisect(list(x[jj,:]),x1[j,-1]) for jj in range(nrow)])+1
  for i in range(ib,ncol1):
    for jj in range(min_j,max_j):
      for ii in range(min_i,max_i):
        if x[jj,ii]<=x1[j,i]<=x[jj+1,ii+1] and y[jj,ii]<=y1[j,i]<=y[jj+1,ii+1]:(idx[j,i,0],idx[j,i,1])=(jj,ii)
128/489: from bisect import bisect,  bisect_left
128/490: ib=max(bisect_left(x1[0,:],x1d[0]),bisect_left(x1[-1,:],x1d[0]))
128/491: ib
128/492:
idx=np.zeros(shape=(nrow1,ncol1,2),dtype=np.int)
for j in range(nrow1):
  min_j=min([bisect(list(y[:,ii]),y1[j,0]) for ii in range(ncol)])-1
  min_i=min([bisect(list(x[jj,:]),x1[j,0]) for jj in range(nrow)])-1
  max_j=max([bisect(list(y[:,ii]),y1[j,-1]) for ii in range(ncol)])+1
  max_i=max([bisect(list(x[jj,:]),x1[j,-1]) for jj in range(nrow)])+1
  for i in range(ib,ncol1):
    for jj in range(min_j,max_j):
      for ii in range(min_i,max_i):
        if x[jj,ii]<=x1[j,i]<=x[jj+1,ii+1] and y[jj,ii]<=y1[j,i]<=y[jj+1,ii+1]:(idx[j,i,0],idx[j,i,1])=(jj,ii)
128/493:
idx0=np.where(idx==0)
for j,i in zip(idx0[0],idx0[1]):
  if i<ib:continue
  min_j=min([bisect(list(y[:,ii]),y1[j,i]) for ii in range(ncol)])-1
  min_i=min([bisect(list(x[jj,:]),x1[j,i]) for jj in range(nrow)])-1
  max_j=max([bisect(list(y[:,ii]),y1[j,i]) for ii in range(ncol)])+1
  max_i=max([bisect(list(x[jj,:]),x1[j,i]) for jj in range(nrow)])+1
  xr=x[min_j:max_j,min_i:max_i]
  yr=y[min_j:max_j,min_i:max_i]
  di=np.sqrt((xr-x1[j,i])**2+(yr-y1[j,i])**2)
  dis=di.flatten()
  dis.sort()
  ii,jj=[],[]
  for k in range(4):
    min_k=np.where(di==dis[k])
    jj.append(min_j+min_k[0])
    ii.append(min_i+min_k[1])
  idx[j,i,0],idx[j,i,1]=(min(jj),min(ii))
128/494:
wts=np.zeros(shape=(nrow1,ncol1,4),dtype=np.float64)
one=np.ones(shape=(nrow1,ncol1),dtype=np.int64)
128/495:
kk=0
for jj in [0,1]:
  for ii in [0,1]:
    xr,yr=x[idx[:,:,0]+one*jj,idx[:,:,1]+one*ii],y[idx[:,:,0]+one*jj,idx[:,:,1]+one*ii]
    wts[:,:,kk]=one/((xr-x1)**2+(yr-y1)**2)
    kk+=1
sum_wts=np.sum(wts,axis=2)
for kk in range(4):
   wts[:,:,kk]=wts[:,:,kk]/sum_wts[:,:]
fname = 'idxD4.bin'
with FortranFile(fname, 'w') as f:
  f.write_record(idx)
fname = 'wtsD4.bin'
with FortranFile(fname, 'w') as f:
  f.write_record(wts)
128/496: from scipy.io import FortranFile
128/497:
kk=0
for jj in [0,1]:
  for ii in [0,1]:
    xr,yr=x[idx[:,:,0]+one*jj,idx[:,:,1]+one*ii],y[idx[:,:,0]+one*jj,idx[:,:,1]+one*ii]
    wts[:,:,kk]=one/((xr-x1)**2+(yr-y1)**2)
    kk+=1
sum_wts=np.sum(wts,axis=2)
for kk in range(4):
   wts[:,:,kk]=wts[:,:,kk]/sum_wts[:,:]
fname = 'idxD4.bin'
with FortranFile(fname, 'w') as f:
  f.write_record(idx)
fname = 'wtsD4.bin'
with FortranFile(fname, 'w') as f:
  f.write_record(wts)
128/498: idx.shape
128/499:
for v in V[3]:
  if v not in V0[3]:continue
  for t in range(itb,nt):
    it=idxt[t]
    kk=0
    for jj in [0,1]:
      for ii in [0,1]:
        vr=nc0[v][it,0,idx[:,:,0]+one*jj,idx[:,:,1]+one*ii]
        nc[v][t,0,:,ib:]+=vr[:,ib:]*wts[:,ib:,kk]
        kk+=1
  for t in range(itb):
    it=idxt[t]
    nc[v][t,0,:,ib:]= nc[v][it,0,:,ib:]
  print(v)
128/500: one.shape
128/501:
for v in V[3]:
  if v not in V0[3]:continue
  for t in range(itb,nt):
    it=idxt[t]
    kk=0
    for jj in [0,1]:
      for ii in [0,1]:
        vr=nc0[v][it,0,(idx[:,:,0]+one*jj).flatten(),(idx[:,:,1]+one*ii).flatten()]
        nc[v][t,0,:,ib:]+=vr[:,ib:]*wts[:,ib:,kk]
        kk+=1
  for t in range(itb):
    it=idxt[t]
    nc[v][t,0,:,ib:]= nc[v][it,0,:,ib:]
  print(v)
130/1:
import numpy as np
from pyproj import Proj
from scipy.interpolate import griddata
from dtconvertor import dt2jul, jul2dt
import netCDF4
import calendar
import datetime
from bisect import bisect,  bisect_left
from scipy.io import FortranFile

m=1
am='{:02d}'.format(m)
fname='biog2019'+am+'K1.ncf'
nc = netCDF4.Dataset(fname,'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay1,nrow1,ncol1=nc[V[3][0]].shape
sdatetime=[jul2dt(nc['TFLAG'][t,0,:]) for t in range(nt)]

Latitude_Pole, Longitude_Pole = nc.YCENT, nc.XCENT
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol1)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow1)]
x1,y1=np.meshgrid(x1d,y1d)
lon, lat = pnyc(x1, y1, inverse=True)

fname='fortBE.413_teds10.biog'+am+'.nc'
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay,nrow,ncol=nc0.variables[V0[3][0]].shape

sdatetime0=[jul2dt(nc0['TFLAG'][t,0,:]) for t in range(nt0)]
itb=sdatetime.index(sdatetime0[0])
131/1:
import numpy as np
from pyproj import Proj
from scipy.interpolate import griddata
from dtconvertor import dt2jul, jul2dt
import netCDF4
import calendar
import datetime
from bisect import bisect,  bisect_left
from scipy.io import FortranFile

m=1
am='{:02d}'.format(m)
fname='biog2019'+am+'K1.ncf'
nc = netCDF4.Dataset(fname,'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay1,nrow1,ncol1=nc[V[3][0]].shape
sdatetime=[jul2dt(nc['TFLAG'][t,0,:]) for t in range(nt)]

Latitude_Pole, Longitude_Pole = nc.YCENT, nc.XCENT
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol1)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow1)]
x1,y1=np.meshgrid(x1d,y1d)
lon, lat = pnyc(x1, y1, inverse=True)

fname='fortBE.413_teds10.biog'+am+'.nc'
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay,nrow,ncol=nc0.variables[V0[3][0]].shape

sdatetime0=[jul2dt(nc0['TFLAG'][t,0,:]) for t in range(nt0)]
itb=sdatetime.index(sdatetime0[0])
131/2:
mn=calendar.monthrange(sdatetime[0].year,sdatetime[0].month)[1]
sdatetime[:itb]=[sdatetime[t]+datetime.timedelta(days=mn) for t in range(itb)]
idxt=[sdatetime0.index(sdt) for sdt in sdatetime]

x1d=[nc0.XORIG+nc0.XCELL*i for i in range(ncol)]
y1d=[nc0.YORIG+nc0.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(x1d,y1d)

#transposition to nc0 coordinates
Latitude_Pole, Longitude_Pole = nc0.YCENT, nc0.XCENT
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
x1,y1=pnyc(lon,lat, inverse=False)
ib=max(bisect_left(x1[0,:],x1d[0]),bisect_left(x1[-1,:],x1d[0]))

idx=np.zeros(shape=(nrow1,ncol1,2),dtype=np.int)
for j in range(nrow1):
  min_j=min([bisect(list(y[:,ii]),y1[j,0]) for ii in range(ncol)])-1
  min_i=min([bisect(list(x[jj,:]),x1[j,0]) for jj in range(nrow)])-1
  max_j=max([bisect(list(y[:,ii]),y1[j,-1]) for ii in range(ncol)])+1
  max_i=max([bisect(list(x[jj,:]),x1[j,-1]) for jj in range(nrow)])+1
  for i in range(ib,ncol1):
    for jj in range(min_j,max_j):
      for ii in range(min_i,max_i):
        if x[jj,ii]<=x1[j,i]<=x[jj+1,ii+1] and y[jj,ii]<=y1[j,i]<=y[jj+1,ii+1]:(idx[j,i,0],idx[j,i,1])=(jj,ii)
131/3:
idx0=np.where(idx==0)
for j,i in zip(idx0[0],idx0[1]):
  if i<ib:continue
  min_j=min([bisect(list(y[:,ii]),y1[j,i]) for ii in range(ncol)])-1
  min_i=min([bisect(list(x[jj,:]),x1[j,i]) for jj in range(nrow)])-1
  max_j=max([bisect(list(y[:,ii]),y1[j,i]) for ii in range(ncol)])+1
  max_i=max([bisect(list(x[jj,:]),x1[j,i]) for jj in range(nrow)])+1
  xr=x[min_j:max_j,min_i:max_i]
  yr=y[min_j:max_j,min_i:max_i]
  di=np.sqrt((xr-x1[j,i])**2+(yr-y1[j,i])**2)
  dis=di.flatten()
  dis.sort()
  ii,jj=[],[]
  for k in range(4):
    min_k=np.where(di==dis[k])
    jj.append(min_j+min_k[0])
    ii.append(min_i+min_k[1])
  idx[j,i,0],idx[j,i,1]=(min(jj),min(ii))
131/4: v='ACET'
131/5: t=ib
131/6: t=itb
131/7:
    it=idxt[t]
    kk=0
131/8: ii,jj=0,0
131/9: vr=nc0[v][it,0,(idx[:,:,0]+one*jj).flatten(),(idx[:,:,1]+one*ii).flatten()]
131/10:
wts=np.zeros(shape=(nrow1,ncol1,4),dtype=np.float64)
one=np.ones(shape=(nrow1,ncol1),dtype=np.int64)
kk=0
for jj in [0,1]:
  for ii in [0,1]:
    xr,yr=x[idx[:,:,0]+one*jj,idx[:,:,1]+one*ii],y[idx[:,:,0]+one*jj,idx[:,:,1]+one*ii]
    wts[:,:,kk]=one/((xr-x1)**2+(yr-y1)**2)
    kk+=1
sum_wts=np.sum(wts,axis=2)
131/11:
for kk in range(4):
   wts[:,:,kk]=wts[:,:,kk]/sum_wts[:,:]
131/12: vr=nc0[v][it,0,(idx[:,:,0]+one*jj).flatten(),(idx[:,:,1]+one*ii).flatten()]
132/1:
import numpy as np
from pyproj import Proj
from scipy.interpolate import griddata
from dtconvertor import dt2jul, jul2dt
import netCDF4
import calendar
import datetime
from bisect import bisect,  bisect_left
from scipy.io import FortranFile

m=1
am='{:02d}'.format(m)
fname='biog2019'+am+'K1.ncf'
nc = netCDF4.Dataset(fname,'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay1,nrow1,ncol1=nc[V[3][0]].shape
sdatetime=[jul2dt(nc['TFLAG'][t,0,:]) for t in range(nt)]

Latitude_Pole, Longitude_Pole = nc.YCENT, nc.XCENT
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol1)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow1)]
x1,y1=np.meshgrid(x1d,y1d)
lon, lat = pnyc(x1, y1, inverse=True)

fname='fortBE.413_teds10.biog'+am+'.nc'
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay,nrow,ncol=nc0.variables[V0[3][0]].shape

sdatetime0=[jul2dt(nc0['TFLAG'][t,0,:]) for t in range(nt0)]
itb=sdatetime.index(sdatetime0[0])
132/2:
mn=calendar.monthrange(sdatetime[0].year,sdatetime[0].month)[1]
sdatetime[:itb]=[sdatetime[t]+datetime.timedelta(days=mn) for t in range(itb)]
idxt=[sdatetime0.index(sdt) for sdt in sdatetime]

x1d=[nc0.XORIG+nc0.XCELL*i for i in range(ncol)]
y1d=[nc0.YORIG+nc0.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(x1d,y1d)

#transposition to nc0 coordinates
Latitude_Pole, Longitude_Pole = nc0.YCENT, nc0.XCENT
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
x1,y1=pnyc(lon,lat, inverse=False)
ib=max(bisect_left(x1[0,:],x1d[0]),bisect_left(x1[-1,:],x1d[0]))

idx=np.zeros(shape=(nrow1,ncol1,2),dtype=np.int)
for j in range(nrow1):
  min_j=min([bisect(list(y[:,ii]),y1[j,0]) for ii in range(ncol)])-1
  min_i=min([bisect(list(x[jj,:]),x1[j,0]) for jj in range(nrow)])-1
  max_j=max([bisect(list(y[:,ii]),y1[j,-1]) for ii in range(ncol)])+1
  max_i=max([bisect(list(x[jj,:]),x1[j,-1]) for jj in range(nrow)])+1
  for i in range(ib,ncol1):
    for jj in range(min_j,max_j):
      for ii in range(min_i,max_i):
        if x[jj,ii]<=x1[j,i]<=x[jj+1,ii+1] and y[jj,ii]<=y1[j,i]<=y[jj+1,ii+1]:(idx[j,i,0],idx[j,i,1])=(jj,ii)
132/3:
idx0=np.where(idx==0)
for j,i in zip(idx0[0],idx0[1]):
  if i<ib:continue
  min_j=min([bisect(list(y[:,ii]),y1[j,i]) for ii in range(ncol)])-1
  min_i=min([bisect(list(x[jj,:]),x1[j,i]) for jj in range(nrow)])-1
  max_j=max([bisect(list(y[:,ii]),y1[j,i]) for ii in range(ncol)])+1
  max_i=max([bisect(list(x[jj,:]),x1[j,i]) for jj in range(nrow)])+1
  xr=x[min_j:max_j,min_i:max_i]
  yr=y[min_j:max_j,min_i:max_i]
  di=np.sqrt((xr-x1[j,i])**2+(yr-y1[j,i])**2)
  dis=di.flatten()
  dis.sort()
  ii,jj=[],[]
  for k in range(4):
    min_k=np.where(di==dis[k])
    jj.append(min_j+min_k[0])
    ii.append(min_i+min_k[1])
  idx[j,i,0],idx[j,i,1]=(min(jj),min(ii))
132/4:
wts=np.zeros(shape=(nrow1,ncol1,4),dtype=np.float64)
one=np.ones(shape=(nrow1,ncol1),dtype=np.int64)
kk=0
for jj in [0,1]:
  for ii in [0,1]:
    xr,yr=x[idx[:,:,0]+one*jj,idx[:,:,1]+one*ii],y[idx[:,:,0]+one*jj,idx[:,:,1]+one*ii]
    wts[:,:,kk]=one/((xr-x1)**2+(yr-y1)**2)
    kk+=1
sum_wts=np.sum(wts,axis=2)
132/5:
for kk in range(4):
   wts[:,:,kk]=wts[:,:,kk]/sum_wts[:,:]
132/6: v='ACET'
132/7: ii,jj=0,0
132/8: t=itb
132/9: it=idxt[t]
132/10: vr=np.zeros(shape=ncol,nrow)
132/11: vr=np.zeros(shape=(ncol,nrow))
132/12: vr=np.zeros(shape=(nrow1,ncol1))
132/13: vr[:,:]=nc0[v][it,0,idx[:,:,0]+one*jj,idx[:,:,1]+one*ii]
132/14: j,i=(idx[:,:,0]+one*jj).flatten(),(idx[:,:,1]+one*ii).flatten()
132/15: a=set(zip(j,i))
132/16: len(a)
132/17: len(zip(j,i))
132/18: len(i)
132/19: a=list(a)
132/20: a[:5]
132/21: j=[ii[0] for ii in a]
132/22: i=[ii[1] for ii in a]
132/23: vr=nc0[v][it,0,j,i]
133/1:
import numpy as np
from pyproj import Proj
from scipy.interpolate import griddata
from dtconvertor import dt2jul, jul2dt
import netCDF4
import calendar
import datetime
from bisect import bisect,  bisect_left
from scipy.io import FortranFile

m=1
am='{:02d}'.format(m)
fname='biog2019'+am+'K1.ncf'
nc = netCDF4.Dataset(fname,'r+')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay1,nrow1,ncol1=nc[V[3][0]].shape
sdatetime=[jul2dt(nc['TFLAG'][t,0,:]) for t in range(nt)]

Latitude_Pole, Longitude_Pole = nc.YCENT, nc.XCENT
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol1)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow1)]
x1,y1=np.meshgrid(x1d,y1d)
lon, lat = pnyc(x1, y1, inverse=True)

fname='fortBE.413_teds10.biog'+am+'.nc'
nc0 = netCDF4.Dataset(fname,'r')
V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
nt0,nlay,nrow,ncol=nc0.variables[V0[3][0]].shape

sdatetime0=[jul2dt(nc0['TFLAG'][t,0,:]) for t in range(nt0)]
itb=sdatetime.index(sdatetime0[0])

mn=calendar.monthrange(sdatetime[0].year,sdatetime[0].month)[1]
sdatetime[:itb]=[sdatetime[t]+datetime.timedelta(days=mn) for t in range(itb)]
133/2:
idxt=[sdatetime0.index(sdt) for sdt in sdatetime]

x1d=[nc0.XORIG+nc0.XCELL*i for i in range(ncol)]
y1d=[nc0.YORIG+nc0.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(x1d,y1d)

#transposition to nc0 coordinates
Latitude_Pole, Longitude_Pole = nc0.YCENT, nc0.XCENT
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
x1,y1=pnyc(lon,lat, inverse=False)
ib=max(bisect_left(x1[0,:],x1d[0]),bisect_left(x1[-1,:],x1d[0]))
133/3: ib
133/4: jb0=min(bisect_left(y1d[:],y[0,ib]),bisect_left(y1d[0],y[0,-1]))
133/5: jb0=min(bisect_left(y1d[:],y[0,ib]),bisect_left(y1d[:],y[0,-1]))
133/6: jb0
133/7: y[0,ib],y[0,-1]
133/8: bisect_left(y1d[:],y[0,ib])
133/9: y1d[:5]
133/10: jb0=min(bisect_left(y1d[:],y1[0,ib]),bisect_left(y1d[:],y1[0,-1]))
133/11: jb0
133/12: ib
133/13: v='ACET'
133/14: t=itb
133/15: it=idxt[t]
133/16: nc[v][t,0,:,ib:]=nc0[v][it,0,jb0:jb0+nrow1,:ncol-(ncol1-ib)]
133/17: ncol-(ncol1-ib)
133/18: nc[v][t,0,:,ib:]=nc0[v][it,0,jb0:jb0+nrow1,:(ncol1-ib)]
133/19: run biog201901K.py
133/20: run biog201901K.py
133/21: cd /nas1/cmaqruns/2019base/data/ptse/twn_DelXindaCG_Add3G
133/22:
import PseudoNetCDF as pnc
import numpy as np
import sys,os, subprocess, netCDF4
from calendar import monthrange
import datetime
import twd97
from pyproj import Proj
from dtconvertor import dt2jul, jul2dt
133/23:
pncg=subprocess.check_output('which pncgen',shell=True).decode('utf8').strip('\n')
python=subprocess.check_output('which python',shell=True).decode('utf8').strip('\n')
v1_xinda=[168616.09,2527230.05,80.,-11.0,363.0,19.8*3600.,1]
Xcent,Ycent= 248417-333.33*5,   2613022-3000. #tuning the coordinates
v1_xinda[0:2]=[v1_xinda[0]-Xcent,v1_xinda[1]-Ycent]
133/24:
fname1='/nas1/camxruns/2013_6.40/ptse/fortBE.14.hsinda.1.h80.n5.09Mp'
pt=pnc.pncopen(fname1,format='point_source')
for j in range(1,4):
  exec('v'+str(j)+'=list(filter(lambda x:pt.variables[x].ndim=='+str(j)+', [i for i in pt.variables]))')
nhr,nvar,dt=pt.variables[v3[0]].shape
nt,nopts=pt.variables[v2[0]].shape
d={}
for v in 'XYHDTV':
  var=v+'STK'
  d.update({v:np.array(list(pt.variables[var][:]))})
d.update({'I':np.array([i for i in range(nopts)])})
idx=np.where(abs(d['X']-v1_xinda[0])<3000)
idy=np.where(abs(d['Y'][idx[0]]-v1_xinda[1])<3000)
idh=np.where(abs(d['H'][idx[0]][idy[0]]-80)<5)
I=d['I'][idx[0]][idy[0]][idh[0]]
parms={v:d[v][I] for v in 'XYHDTV'}
emiss={v:np.array(pt.variables[v][0,I]) for v in v2 if pt.variables[v][0,I]>0}
133/25: I
133/26: parms
133/27: emiss
133/28: 6059/3600
133/29: m=0
133/30:
  mo='{:02d}'.format(m+1)
  fname='fortBE.413_teds11.ptse'+mo+'.nc'
  print(fname)
  os.system('cp ../twn/'+fname+' '+fname)
  nc = netCDF4.Dataset(fname,'r+')
  V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
  v='CP_NO'
  npts,dum=nc.variables[v].shape
  dd =[ ''.join([str(i, encoding='utf-8') for i in list(nc.variables[v][c, :])]) for c in range(npts)]
  idx=np.array([dd.index(i) for i in dd if xingda in i])
133/31:
Latitude_Pole, Longitude_Pole = 23.61000, 120.9900
Xcent, Ycent = twd97.fromwgs84(Latitude_Pole, Longitude_Pole)
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=10, lat_2=40,
        lat_0=Latitude_Pole, lon_0=Longitude_Pole, x_0=0, y_0=0.0)
lonm,latm=(120.20560197059724, 22.84382911621353)
x,y=pnyc(lonm,latm, inverse=False)
parms.update({'X':x,'Y':y})
xingda='S3200688'
names={7:['xcoord','ycoord','stkheight','stkdiam','stktemp','stkspeed']}
PRM='XYHDTV'
n2v={names[7][i]:PRM[i] for i in range(6)}
spec_old='CCRS CPRM FCRS FPRM NO NO2 SO2'.split()
133/32: idx=np.array([dd.index(i) for i in dd if xingda in i])
133/33: idx
133/34: !vi mk_ptAdd3G.py
134/1: !vi mk_ptAdd3G.py
134/2: !vi mk_ptAdd3G.py
133/35: ncks=subprocess.check_output('which ncks',shell=True).decode('utf8').strip('\n')
133/36: fname
133/37: mo
133/38: fnameO='New3G.ptse'+mo+'.nc'
133/39: os.system(ncks+' -O -d COL,'+str(idx[0])+ ../twn/'+fname+' '+fnameO)
133/40: os.system(ncks+' -O -d COL,'+str(idx[0])+' ../twn/'+fname+' '+fnameO)
133/41: !lst
133/42: run mk_ptAdd3G.py
134/3: !lst
134/4: !lst
134/5: !vi mk_ptAdd3G.py
133/43: run mk_ptAdd3G.py
133/44: run mk_ptAdd3G.py
133/45: !lst
133/46:
mo='01'
fname='../twn/fortBE.413_teds11.ptse'+mo+'.nc'
nc = netCDF4.Dataset(fname,'r')
print(fname)
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v='CP_NO'
npts,dum=nc.variables[v].shape
dd =[ ''.join([str(i, encoding='utf-8') for i in list(nc.variables[v][c, :])]) for c in range(npts)]
idx=np.array([dd.index(i) for i in dd if xingda in i])
fnameO='New3G.ptse00.nc'
os.system(ncks+' -O -d COL,'+str(idx[0])+fname+' '+fnameO)
133/47: os.system(ncks+' -O -d COL,'+str(idx[0])+' '+fname+' '+fnameO)
133/48:
for m in range(12):
  mo='{:02d}'.format(m+1)
  os.system('cp New3G.ptse00.nc '+fnameO)
  nc = netCDF4.Dataset(fnameO,'r+')
  for v in spec_old:
    nc.variables[v][:,idx[:]]=0.
  for n in n2v:
    nc.variables[n][0]=parms[n2v[n]]
  for v in [i for i in emiss if i in spec_old]:
    nc.variables[v][:,0]=emiss[v][0]*3.
  nc.close()
133/49:
for m in range(12):
  mo='{:02d}'.format(m+1)
  fnameO='New3G.ptse'+mo+'.nc'
  os.system('cp New3G.ptse00.nc '+fnameO)
  nc = netCDF4.Dataset(fnameO,'r+')
  for v in spec_old:
    nc.variables[v][:,idx[:]]=0.
  for n in n2v:
    nc.variables[n][0]=parms[n2v[n]]
  for v in [i for i in emiss if i in spec_old]:
    nc.variables[v][:,0]=emiss[v][0]*3.
  nc.close()
133/50:
for nco in ['ncks','ncatted']:
  exec(nco+"=subprocess.check_output('which "+ncko+"' ,shell=True).decode('utf8').strip('\n')")
133/51:
for nco in ['ncks','ncatted']:
  exec(nco+"=subprocess.check_output('which "+nco+"' ,shell=True).decode('utf8').strip('\n')")
133/52:
for nco in ['ncks','ncatted']:
  exec(nco+"=subprocess.check_output('which "+nco+"' ,shell=True).decode('utf8').strip('"+"\n')")
133/53: ncatted=subprocess.check_output('which ncatted',shell=True).decode('utf8').strip('\n')
133/54:
mo='01'
fname='../twn/fortBE.413_teds11.ptse'+mo+'.nc'
nc = netCDF4.Dataset(fname,'r')
print(fname)
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
v='CP_NO'
npts,dum=nc.variables[v].shape
dd =[ ''.join([str(i, encoding='utf-8') for i in list(nc.variables[v][c, :])]) for c in range(npts)]
idx=np.array([dd.index(i) for i in dd if xingda in i])
fnameO='New3G.ptse00.nc'
os.system(ncks+' -O -d COL,'+str(idx[0])+' '+fname+' '+fnameO)
os.system(ncatted+' -a NCOLS,global,o,i,1 '+fnameO)
133/55:
for m in range(12):
  mo='{:02d}'.format(m+1)
  fnameO='New3G.ptse'+mo+'.nc'
  os.system('cp New3G.ptse00.nc '+fnameO)
  nc = netCDF4.Dataset(fnameO,'r+')
  for v in spec_old:
    nc.variables[v][:,idx[:]]=0.
  for n in n2v:
    nc.variables[n][0]=parms[n2v[n]]
  for v in [i for i in emiss if i in spec_old]:
    nc.variables[v][:,0]=emiss[v][0]*3.
  nc.close()
  os.system(python+' ../twn/pt_constLL.py '+fnameO)
  os.system(python+' ../twn/pt_timvarLL.py '+fnameO)
135/1: run mk_ptAdd3G.py
135/2: cd /nas1/cmaqruns/2019base/data/ptse/twn_DelXindaCG_Add3G
135/3: run mk_ptAdd3G.py
135/4: run mk_ptAdd3G.py
134/6: !vi pt_constLL.py
135/5: run pt_constLL.py New3G.ptse01.nc
135/6: x
135/7: len(x)
134/7: !vi pt_constLL.py
135/8: nopts
134/8: !vi mk_ptAdd3G.py
135/9: run mk_ptAdd3G.py
135/10: run mk_ptAdd3G.py
135/11: fname='New3G.1903.timvar.nc'
135/12: nc = netCDF4.Dataset(fname,'r')
135/13: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
135/14: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
135/15: nt,nlay,nrow,ncol
135/16: len(V[3])
135/17: v='NO'
135/18: nc[v][:]
135/19: V[3]
135/20: 1.26*31
135/21: v
135/22: v='NO2'
135/23: nc[v][:5,0,0,0]
135/24: 1.26*31*46
134/9: cd /nas2/cmaqruns/2019force
134/10: !vi run.cctm.03.csh
135/25: pwd
135/26: !lst
135/27: ls
134/11: !vi run.cctm.03.csh
134/12: vi cctm.source.v5.3.1.ae7
134/13: !vi cctm.source.v5.3.1.ae7
134/14: !vi run.cctm.03.csh
134/15: !sudo vi /etc/hosts
136/1: import netCDF4
136/2: !lst
136/3: fname='New3G.1901.const.nc'
136/4: nc = netCDF4.Dataset(fname,'r+')
136/5: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
136/6: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
136/7: V
136/8: nc.XORIG
136/9: fname='cmaq_cb06r3_ae7_aq.01-20181225.38.TW3-d4.BaseEms.ncf'
136/10: nc0 = netCDF4.Dataset(fname,'r')
136/11: atts=['NCOLS',  'NROWS', 'P_ALP', 'P_BET', 'P_GAM', 'XCELL', 'XCENT', 'XORIG', 'YCELL', 'YCENT', 'YORIG']
136/12:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
136/13: from pyproj import Proj
136/14: pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
136/15: nc['LATITUDE'][0,0,:,0]
136/16: lat=nc['LATITUDE'][0,0,:,0]
136/17: lon=nc['LONGITUDE'][0,0,:,0]
136/18: x0,y0=pnyc(lat,lon, inverse=False)
136/19: x0,y0
136/20: lon
136/21: lat
136/22: pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
136/23: nc.P_ALP,nc.P_BET,nc.YCENT,nc.XCENT
136/24: x0,y0=pnyc(lon,lat, inverse=False)
136/25: x0,y0
136/26: V
136/27: nc['COL'][0,0,0,0]=int((x0-nc.XORIG)/nc.XCELL)
136/28: nc['ROW'][0,0,0,0]=int((y0-nc.YORIG)/nc.YCELL)
136/29: int((x0-nc.XORIG)/nc.XCELL),int((y0-nc.YORIG)/nc.YCELL)
136/30: v='STKCNT'
136/31: nc[v][:]
136/32: nc['XLOCA'][0,0,0,0], nc['YLOCA'][0,0,0,0]=x0,y0
136/33: nc.XORIG,nc.YORIG
136/34: nc.close()
136/35: fname='New3G.1901.timvar.nc'
136/36: nc = netCDF4.Dataset(fname,'r+')
136/37:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
136/38: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
136/39: V
136/40: V[0]
136/41: V[1]
136/42: V[2]
136/43: V[3][:5]
136/44: V[3][-5:]
136/45: nc.close()
136/46: fname='New3G.1901.const.nc'
136/47: nc = netCDF4.Dataset(fname,'r')
136/48: nc['COL']
136/49: nc['COL'][0,0,:,0]=np.array((x0[:]-nc.XORIG)/nc.XCELL,dtype=int)
136/50: import numpy as np
136/51: nc['COL'][0,0,:,0]=np.array((x0[:]-nc.XORIG)/nc.XCELL,dtype=int)
136/52: nc = netCDF4.Dataset(fname,'r+')
136/53: nc['COL'][0,0,:,0]=np.array((x0[:]-nc.XORIG)/nc.XCELL,dtype=int)
136/54:  nc['COL']
136/55:  nc['COL'][:]
136/56: history -f his.txt
136/57: !vi his.txt
136/58: !lst
136/59: fname
136/60: nc.close()
136/61: nc = netCDF4.Dataset(fname,'r+')
136/62: nc['XLOCA'][:]
136/63: nc['XLOCA']
136/64: nc.NROWS
136/65: nc.NROWS=1
136/66: nc.NCOLS=1
136/67: nc.NLAYS
136/68: nc.close()
136/69: !lst
136/70: fname='New3G.1901.timvar.nc'
136/71: nc = netCDF4.Dataset(fname,'r+')
136/72: nc.NROWS=1
136/73: nc.NCOLS=1
136/74: nc.close()
136/75: pwd
136/76: nc = netCDF4.Dataset(fname,'r+')
136/77: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
136/78: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
136/79: nt,nlay,nrow,ncol
136/80: nc.SDATE
136/81: atts=['SDATE','STIME', 'P_ALP', 'P_BET', 'P_GAM', 'XCELL', 'XCENT', 'XORIG', 'YCELL', 'YCENT', 'YORIG']
136/82:
for i in atts:
  if i not in dir(nc0):continue
  exec('nc.'+i+'=nc0.'+i)
136/83: V0=[list(filter(lambda x:nc0[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
136/84: V0
136/85: tflag0=nc0['TFLAG'][:,0,:]
136/86: nt0,nlay,nrow,ncol=(nc0[V0[3][0]].shape[i] for i in range(4))
136/87: nt0
136/88:
for t in nt0:
  for dt in 2:
    nc['TFLAG'][t,:,dt]=tflag0[t,dt]
136/89:
for t in range(nt0):
  for dt in range(2):
    nc['TFLAG'][t,:,dt]=tflag0[t,dt]
136/90:
for v in V[3]:
  nc[v][:,0,0,0]=nc[v][0,0,0,0]
136/91: nc.close()
136/92: lst
136/93: !lst
136/94: !pr_tflag.py New3G.1901.timvar.nc
136/95: !pr_tflag.py New3G.1901.timvar.nc|M
136/96: nc.close()
136/97: history
136/98: tflag0.shape
136/99: fname='New3G.1901.timvar.nc'
136/100: nc = netCDF4.Dataset(fname,'r')
136/101: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
136/102: dd={}
136/103:
for v in V[3]:
    if nc[v][0,0,0,0]==0.continue
136/104:
for v in V[3]:
    if nc[v][0,0,0,0]==0.:continue
    dd.update({v:nc[v][0,0,0,0]})
136/105: dd
136/106:
for v in V[3]:
    if nc[v][0,0,0,0]==0.:continue
    dd.update({v:nc[v][0,0,0,0].values()})
136/107: nc[v][0,0,0,0].value()
136/108: nc[v][0,0,0,0].values()
136/109: dir(nc[v][0,0,0,0])
136/110: dir(nc[v][0,0,0,0])[:30]
136/111: dir(nc[v][0,0,0,0])[30:60]
136/112: dir(nc[v][0,0,0,0])[60:90]
136/113: dir(nc[v][0,0,0,0])[90:120]
136/114: dir(nc[v][0,0,0,0])[120:150]
136/115: nc[v][0,0,0,0].data
136/116: nc[v][0,0,0,0].data[0]
136/117: nc[v][0,0,0,0].data()
136/118:
for v in V[3]:
    if nc[v][0,0,0,0]==0.:continue
    dd.update({v:nc[v][0,0,0,0].flatten()[0]})
136/119: dd
136/120: fname='New3G.1901.const.nc'
136/121: nc = netCDF4.Dataset(fname,'r')
136/122: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
136/123:
for v in V[3]:
    if nc[v][0,0,0,0]==0.:continue
    dd.update({v:nc[v][0,0,0,0].flatten()[0]})
136/124: dd
136/125: dd['STKVE']=-dd['STKVE']
136/126: dd
136/127: import os,sys, json
136/128: fnameO='N3G.json'
136/129:
with open(fnameO,'w', newline='\n') as jsonfile:
    json.dump(dd, jsonfile)
136/130:
with open(fnameO,'w') as jsonfile:
    json.dump(dd, jsonfile)
136/131: dd
136/132:
with open(fnameO,'w',newline='') as jsonfile:
    json.dump(dd, jsonfile)
136/133: dd
136/134:
for i in dd:
    dd.update({i:str(dd[i])})
136/135:
with open(fnameO,'w',newline='\n') as jsonfile:
    json.dump(dd, jsonfile)
136/136: !lst
136/137: cat N3G.json
136/138: history
136/139: history -f his.txt
137/1: 1/0.6
137/2: cd /nas2/cmaqruns/2019force/output/2019-01
137/3: fname='N3G.nc'
137/4: nc = netCDF4.Dataset(fname,'r')
137/5: import netCDF4
137/6: nc = netCDF4.Dataset(fname,'r')
137/7: v='TFLAG'
137/8: nc[v][:5,0,:]
137/9: 24*31
137/10: cd /nas2/cmaqruns/2019N3G/output/2019-01/grid03/smoke
137/11: !ls
137/12: !lsS
137/13: fname='cmaq_cb06r3_ae7_aq.01-20181225.38.TW3-d4.BaseEms.ncf'
137/14: nc = netCDF4.Dataset(fname,'r')
137/15: v='SO2'
137/16:
for k in range(24):
    print(k,np.sum(nc[v][:,k,:,:]))
137/17: import numpy as np
137/18:
for k in range(24):
    print(k,np.sum(nc[v][:,k,:,:]))
137/19: v='NO2'
137/20:
for k in range(24):
    print(k,np.sum(nc[v][:,k,:,:]))
137/21: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
137/22:
k,s=9,0
for v in V[3]:
    s+=np.sum(nc[v][:,k,:,:])
137/23: s
137/24: fname='cmaq_cb06r3_ae7_aq.01-20181225.38.TW3-d4.BaseEms.ncf_0-8NoTZPP'
137/25: nc = netCDF4.Dataset(fname,'r+')
137/26: nc.close()
137/27: nc = netCDF4.Dataset(fname,'r+')
137/28: k=4
137/29:
for v in V[3]:
    nc[v][:,4,87,39]=0.
    nc[v][:,4,86,39]=0.
137/30: nc.close()
137/31: nc = netCDF4.Dataset(fname,'r+')
137/32: nc.NLAYS
137/33: nc.NLAYS=9
137/34: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
137/35: nt,nlay,nrow,ncol
137/36: nc.close()
138/1: !vi combine.sh
138/2: !vi combine.sh
138/3: !vi combine.sh
137/37: cd /nas1/cmaqruns/2019base/data/ptse/twn
137/38: ls
137/39: fname='teds11.1901.const.nc'
137/40: nc = netCDF4.Dataset(fname,'r+')
137/41: nc = netCDF4.Dataset(fname,'r')
137/42: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
137/43: V
137/44: fname='fortBE.413_teds11.ptse01.nc'
137/45: nc = netCDF4.Dataset(fname,'r')
137/46: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
137/47: V
137/48: v='CP_NO'
137/49: L='L0200473'
137/50: nc[v].shape
137/51: cp_no=[''.join([i.decode('utf-8') for i in nc[v][t,:]]) for t in range(22516)]
137/52: t
137/53: t=0
137/54: nc[v][t,:]
137/55: cp_no=[''.join([i for i in nc[v][t,:]]) for t in range(22516)]
137/56: cp_no=[''.join([i for i in nc[v][t,:8]]) for t in range(22516)]
137/57: nc[v].shape
137/58: cp_no=[''.join([str(i) for i in nc[v][t,:]]) for t in range(22516)]
137/59: LL=[i for i in cp_no if L in i]
137/60: LL
137/61: L
137/62: cp_no[:5]
137/63: cp_no=[''.join([str(i, encoding='utf-8') for i in nc[v][t,:]]) for t in range(22516)]
137/64: cp_no[:5]
137/65: LL=[i for i in cp_no if L in i]
137/66: LL
137/67: V
137/68: set(LL)
137/69: L=LL[0]
137/70: cp_no.index(L)
137/71: v='stkheight'
137/72: nc[v].shape
137/73: hei=nc[v][:]
137/74: from pandas import *
137/75: df=DataFrame({'cp':cp_no,'he':hei})
137/76: df.loc[df.cp==L]
137/77: df.loc[(df.cp==L)&(df.he=250)]
137/78: df.loc[(df.cp==L)&(df.he==250)]
137/79: tzpp=df.loc[(df.cp==L)&(df.he==250)]
137/80: len(tzpp)
137/81: ls
137/82: fname='teds11.1901.const.nc'
137/83: nc = netCDF4.Dataset(fname,'r')
137/84: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
137/85: V
137/86: nt,nlay,nrow,ncol=(nc.variables[V[3][0]].shape[i] for i in range(4))
137/87: nt,nlay,nrow,ncol
137/88: len(df)
137/89:
for i tzpp.index:
    print([nc[v][0,0,i,0] for v in V[3]])
137/90: tzpp.index
137/91:
for i list(tzpp.index):
    print([nc[v][0,0,i,0] for v in V[3]])
137/92:
for i in tzpp.index:
    print([nc[v][0,0,i,0] for v in V[3]])
137/93:
for i in tzpp.index:
    print([np.array(nc[v][0,0,i,0])[0] for v in V[3]])
137/94:
for i in tzpp.index:
    print([np.array(nc[v][0,0,i,0]) for v in V[3]])
137/95:
for i in tzpp.index:
    print([nc[v][0,0,i,0].data for v in V[3]])
137/96:
for i in tzpp.index:
    print([nc[v][0,0,i,0].data[0] for v in V[3]])
137/97:
for i in tzpp.index:
    print([nc[v][0,0,i,0].data() for v in V[3]])
137/98:
for i in tzpp.index:
    print([nc[v][0,0,i,0].value for v in V[3]])
137/99:
for i in tzpp.index:
    print([nc[v][0,0,i,0].values for v in V[3]])
137/100:
for i in tzpp.index:
    print([nc[v][0,0,i,0].data[0] for v in V[3]])
137/101:
for i in tzpp.index:
    print([nc[v][0,0,i,0].data for v in V[3]])
137/102: [v for v in V[3]]
137/103:
for i in tzpp.index:
    print([nc[v][0,0,i,0].data for v in V[3]])
137/104: history
137/105: history -f his.txt
137/106: !vi his.txt
137/107: ls
137/108: pwd
137/109: !vi his.txt
137/110: ls
137/111: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
137/112: V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc])) for j in [1,2,3,4]]
137/113: pwd
137/114: cd /nas2/cmaq2019/download/input/201901/grid03/smoke
137/115: !lst
137/116: run add_tzpp.py
137/117: run add_tzpp.py
137/118: run add_tzpp.py
137/119: ls
137/120: run add_tzpp.py
137/121: tzpp.index[0]
137/122: l_tzpp=len(tzpp)
137/123: var=np.zeros(shape=(nv,nt,nlay,l_tzpp,ncol))
137/124: v
137/125: iv=V[3].index(v)
137/126: var[iv,:,:,:,:]=nc[v][:,:,tzpp.index[0]:tzpp.index[-1],:]
137/127: nc[v][:,:,tzpp.index[0]:tzpp.index[-1],:].shape
137/128: l_tzpp
137/129: tzpp.index
137/130: var5=nc[v][:]
137/131: var[iv,:,:,:,:]=var5[:,:,tzpp.index,:]
137/132: run add_tzpp.py
137/133: nc[v][:ibeg,:,:,:].shape
137/134: !lst
137/135: rm teds11.1901.timvar.nc
137/136: rm teds11.1901.timvar.nc
137/137: run add_tzpp.py
137/138: run add_tzpp.py
137/139: ibeg,nt0
137/140: nt0-ibeg
137/141: nt
137/142: nc[v][ibeg:nt0,:,:,:].shape
137/143: var[iv,:(nt0-ibeg),:,:,:].shape
137/144: l_tzpp
137/145: !lst
137/146: !ncdump -h teds11.1901.timvar.nc|H
137/147: root
137/148: !ncdump -h /nas1/cmaqruns/2019base/data/ptse/twn/teds11.1901.timvar.nc|H
137/149: root
137/150: run add_tzpp.py
137/151: !lst
137/152: !ncdump -h teds11.1901.timvar.nc|M
137/153:
fname='teds11.1901.const.nc'
nc0 = netCDF4.Dataset(root+fname,'r')
137/154: V0=[list(filter(lambda x:nc0.variables[x].ndim==j, [i for i in nc0.variables])) for j in [1,2,3,4]]
137/155: V0
137/156:
os.system(ncks+' -O -d ROW,1,'+str(l_tzpp)+' '+root+fname+' '+fname)
nc0 = netCDF4.Dataset(root+fname,'r')
nc = netCDF4.Dataset(fname,'r+')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
nv=len(V[3])
var=np.zeros(shape=(nv,nt,nlay,l_tzpp,ncol))
for v in V[3]:
  var4=nc0[v][:]
  iv=V[3].index(v)
  var[iv,:,:,:,:]=var4[:,:,tzpp.index,:]
137/157: var.shape
137/158: nc0 = netCDF4.Dataset(root+fname,'r')
137/159: nc = netCDF4.Dataset(fname,'r+')
137/160:
for v in V[3]:
  nc[v][0,0,:,0]=nc0[v][0,0,tzpp.index,0]
137/161:
fname='teds11.1901.const.nc'
os.system(ncks+' -O -d ROW,1,'+str(l_tzpp)+' '+root+fname+' '+fname)
nc0 = netCDF4.Dataset(root+fname,'r')
nc = netCDF4.Dataset(fname,'r+')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
for v in V[3]:
  nc[v][0,0,:,0]=nc0[v][0,0,tzpp.index,0]
for i in atts:
  if i not in dir(nc00):continue
  exec('nc.'+i+'=nc00.'+i)
nc.NROWS=l_tzpp
137/162:
fname='cmaq_cb06r3_ae7_aq.01-20181225.38.TW3-d4.BaseEms.ncf_0-8'
nc00 = netCDF4.Dataset(fname,'r')
137/163: run add_tzpp.py
137/164: !LST
137/165: !lst
137/166: pwd
137/167: !vi add_tzpp.py
137/168: fname='teds11.1901.const.nc'
137/169: nc = netCDF4.Dataset(fname,'r')
137/170: v='XLOCA'
137/171: nc[v]
137/172: nc[v][:]
137/173: nc.NCOLS
137/174: nc.NLAYS
137/175: v='TFLAG'
137/176: nc[v][:]
137/177: nc[v].shape
137/178: nc.SDATE
137/179: nc = netCDF4.Dataset(fname,'r+')
137/180: nc[v][:,0,0]=nc.SDATE
137/181: nc.close()
137/182: nc = netCDF4.Dataset(fname,'r+')
137/183: nc.TSTEP
137/184: ls
137/185: fname='New3G.1901.const.nc'
137/186: nc.TSTEP
137/187: nc[v][:]
137/188: fname='teds11.1901.const.nc'
137/189: nc = netCDF4.Dataset(fname,'r')
137/190: nc[v][:]
137/191: nc.close()
137/192: !lst
137/193: rm teds11.1901.timvar.nc teds11.1901.const.nc
137/194: !vi add_tzpp.py
137/195: run add_tzpp.py
137/196: !ldy
137/197: !lst
137/198: !tail add_tzpp.py
137/199: !wc add_tzpp.py
137/200: pwd
137/201: %history -g -f his.txt
137/202: fname='cmaq_cb06r3_ae7_aq.01-20181225.38.TW3-d4.BaseEms.ncf_0-8'
137/203: nc = netCDF4.Dataset(fname,'r')
137/204: V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc])) for j in [1,2,3,4]]
137/205: V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
137/206:
for v in V[3]:
  s=np.sum(nc[v][ibeg:,4,86:88,39])
  if s==0:continue
  print(v,s)
137/207: tflag0[-1,:]
137/208:
vv,ss=[],[]
for v in V[3]:
  s=np.sum(nc[v][ibeg:,4,86:88,39])
  if s==0:continue
  print(v,s)
  vv.append(v)
  ss.append(s)
137/209: tflag0[ibeg,:]
137/210: fname='teds11.1901.timvar.nc'
137/211: nc = netCDF4.Dataset(fname,'r')
137/212:
vv2,ss2=[],[]
for v in V[3]:
  s=np.sum(nc[v][ibeg:,4,86:88,39])
  if s==0:continue
  print(v,s)
  vv2.append(v)
  ss2.append(s)
137/213: V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
137/214:
vv2,ss2=[],[]
for v in V[3]:
  s=np.sum(nc[v][ibeg:,4,86:88,39])
  if s==0:continue
  print(v,s)
  vv2.append(v)
  ss2.append(s)
137/215:
vv2,ss2=[],[]
for v in V[3]:
  s=np.sum(nc[v][ibeg:,:,:,:])
  if s==0:continue
  print(v,s)
  vv2.append(v)
  ss2.append(s)
137/216: pwd
137/217: !lst
137/218: fname
137/219: !psg run.cctm
137/220: fname=root+'teds11.1901.timvar.nc'
137/221: nc = netCDF4.Dataset(fname,'r')
137/222:
vv2,ss2=[],[]
for v in V[3]:
  var4=np.array(nc[v][:])
  s=np.sum(var4[:,:,tzpp.index,:])
  if s==0:continue
  print(v,s)
  vv2.append(v)
  ss2.append(s)
137/223: tzpp.index
137/224: nc['SO2'][0,0,tzpp.index,0]
137/225: s=0
137/226: nt
137/227: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
137/228: nt
137/229:
s=0
for t in range(nt):
  s+=np.sum(nc['SO2'][t,0,tzpp.index,0])
137/230: s
137/231: tzpp
137/232: fname=root+'teds11.1907.timvar.nc'
137/233: nc = netCDF4.Dataset(fname,'r')
137/234:
s=0
for t in range(nt):
  s+=np.sum(nc['SO2'][t,0,tzpp.index,0])
137/235: s
137/236: fname=root+'teds11.1902.timvar.nc'
137/237: nc = netCDF4.Dataset(fname,'r')
137/238:
s=0
for t in range(nt):
  s+=np.sum(nc['SO2'][t,0,tzpp.index,0])
137/239: s
137/240: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
137/241: nt,nlay,nrow,ncol
137/242: len(df)
137/243: cd /nas1/TEDS/teds11/ptse
137/244: ls -lrth *nc
137/245: fname='fortBE.413_teds10.ptsE01.nc'
137/246: nc = netCDF4.Dataset(fname,'r')
137/247: V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
137/248: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
137/249: V
137/250: nt,nrow=(nc[V[1][0]].shape[i] for i in range(4))
137/251: nt,nrow=(nc[V[1][0]].shape[i] for i in range(2))
137/252: nt,nrow
137/253: v='CP_NO'
137/254:
nopt,ii=nc[v].shape
L='L0200473'
cp_no=[''.join([str(i, encoding='utf-8') for i in nc[v][t,:]]) for t in range(nopt)]
LL=[i for i in cp_no if L in i]
L=LL[0]
137/255: L
137/256:
v='stkheight'
hei=nc[v][:]
df=DataFrame({'cp':cp_no,'he':hei})
tzpp=df.loc[(df.cp==L)&(df.he==250)]
137/257: l_tzpp=len(tzpp)
137/258: l_tzpp
137/259: df
137/260: tzpp
137/261:
s=0
for t in range(nt):
  s+=np.sum(nc['SO2'][t,tzpp.index])
137/262: s
137/263: nc['SO2']
137/264: root
137/265: pwd
137/266: V
137/267: v='CP_NO'
137/268: nc[v]
137/269: nc[v].type
137/270: nc[v].type()
137/271: dir(nc[v])
137/272: nc[v].dtype
137/273: nc[v].dtype=='S1'
137/274: pwd
137/275: cd /nas1/cmaqruns/2019base/data/ptse/twn
137/276: !lst
137/277: fname='fortBE.413_teds11.ptse01.nc'
137/278: nc = netCDF4.Dataset(fname,'r')
137/279:
s=0
for t in range(nt):
  s+=np.sum(nc['SO2'][t,tzpp.index])
137/280: s
137/281: cd /nas2/cmaq2019/download/input/201901/grid03/smoke
137/282: lst
137/283: !lst
137/284: rm teds11.1901.timvar.nc teds11.1901.const.nc
137/285: run add_tzpp.py
137/286: fname='teds11.1901.timvar.nc'
137/287: nc = netCDF4.Dataset(fname,'r')
137/288: V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
137/289: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
137/290:
vv2,ss2=[],[]
for v in V[3]:
  var4=np.array(nc[v][:])
  s=np.sum(var4[:,:,tzpp.index,:])
  if s==0:continue
  print(v,s)
  vv2.append(v)
  ss2.append(s)
137/291:
vv2,ss2=[],[]
for v in V[3]:
  s=np.sum(nc[v][ibeg:,:,:,:])
  if s==0:continue
  print(v,s)
  vv2.append(v)
  ss2.append(s)
137/292: fname=root+'teds11.1901.timvar.nc'
137/293: nc = netCDF4.Dataset(fname,'r')
137/294:
vv2,ss2=[],[]
for v in V[3]:
  var4=np.array(nc[v][:])
  s=np.sum(var4[:,:,tzpp.index,:])
  if s==0:continue
  print(v,s)
  vv2.append(v)
  ss2.append(s)
137/295: V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
137/296:
vv2,ss2=[],[]
for v in V[3]:
  var4=np.array(nc[v][:])
  s1=np.sum(var4[:,:,:,:],axis=(0,1,3))
  s=np.sum(s1[tzpp.index])
  if s==0:continue
  print(v,s)
  vv2.append(v)
  ss2.append(s)
137/297: cd /nas1/cmaqruns/2019base/data/ptse/twn
137/298: !lst
137/299: fname='teds11.1901.timvar.nc'
137/300: v='CO'
137/301: c='CO'
137/302: nc.variables[c].set_auto_mask(False)
137/303: nc.variables[c].set_always_mask(False)
137/304: nc.variables[c][:].mask
137/305: nc.variables[c][:]
137/306: arr=np.array(nc.variables[c][:])
137/307: nc.variables[c][:]=arr[:]
137/308: nc = netCDF4.Dataset(fname,'r+')
137/309: nc.variables[c][:]=arr[:]
137/310: nc.close()
137/311: nc = netCDF4.Dataset(fname,'r')
137/312: nc.variables[c][:].mask
137/313: pwd
137/314: !lst
137/315: fname=root+'teds11.1901.timvar.nc'
137/316: nc = netCDF4.Dataset(fname,'r')
137/317:
vv2,ss2=[],[]
for v in V[3]:
  var4=np.array(nc[v][:])
  s=np.sum(var4[:,:,tzpp.index,:])
  if s==0:continue
  print(v,s)
  vv2.append(v)
  ss2.append(s)
137/318: cd /nas2/cmaq2019/download/input/201901/grid03/smoke
137/319: run add_tzpp.py
137/320: !lst
137/321: fname='teds11.1901.timvar.nc'
137/322: nc = netCDF4.Dataset(fname,'r')
137/323:
vv,ss=[],[]
for v in V[3]:
  s=np.sum(nc[v][ibeg:,0,:,0])
  if s==0:continue
  print(v,s)
  vv.append(v)
  ss.append(s)
137/324: V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
137/325:
vv,ss=[],[]
for v in V[3]:
  s=np.sum(nc[v][ibeg:,0,:,0])
  if s==0:continue
  print(v,s)
  vv.append(v)
  ss.append(s)
137/326: vv
137/327: ss
137/328: fname='cmaq_cb06r3_ae7_aq.01-20181225.38.TW3-d4.BaseEms.ncf_0-8'
137/329: nc = netCDF4.Dataset(fname,'r')
137/330:
vv1,ss1=[],[]
for v in V[3]:
  s=np.sum(nc[v][ibeg:,4,86:88,39])
  if s==0:continue
  print(v,s)
  vv1.append(v)
  ss1.append(s)
137/331: V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
137/332:
vv1,ss1=[],[]
for v in V[3]:
  s=np.sum(nc[v][ibeg:,4,86:88,39])
  if s==0:continue
  print(v,s)
  vv1.append(v)
  ss1.append(s)
137/333: dd=DataFrame({'vv':vv1,'ss':ss1})
137/334: a=dd.loc[dd.vv.map(lambda x:x[0]=='P')]
137/335: a
137/336: sum(a.ss)
137/337: pwd
137/338: cd ../mcip/
137/339: ll
137/340: fname='METCRO3D_Taiwan.nc'
137/341: nc = netCDF4.Dataset(fname,'r')
137/342: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
137/343: V
137/344: v='ZH'
137/345: nc[v][0,:,0,0]
137/346: print ([i for i in nc[v][0,:,0,0]])
137/347: a=nc[v][0,:,0,0]
137/348:
for i in range(10):
    print(round(a[i+1]-a[i],3))
137/349:
import numpy as np 
from scipy.stats import t
137/350: x = np.random.normal(size=100)
137/351: x = np.random.normal(size=1000)
137/352:
m = x.mean() 
s = x.std() 
dof = len(x)-1 
confidence = 0.95
137/353: t_crit = np.abs(t.ppf((1-confidence)/2,dof))
137/354: (m-s*t_crit/np.sqrt(len(x)), m+s*t_crit/np.sqrt(len(x)))
137/355: values = [np.random.choice(x,size=len(x),replace=True).mean() for i in range(1000)]
137/356: np.percentile(values,[100*(1-confidence)/2,100*(1-(1-confidence)/2)])
137/357: 24*7
137/358: cd /nas2/cmaqruns/2019force/output/2019-01
137/359:
import numpy as np
import netCDF4
import os,sys,subprocess
import datetime
from dtconvertor import dt2jul, jul2dt

rw=['r','r+']
137/360: !ln -s /home/kuang/bin/dtconvertor.py .
137/361:
import numpy as np
import netCDF4
import os,sys,subprocess
import datetime
from dtconvertor import dt2jul, jul2dt

rw=['r','r+']
137/362: fnames=['TZPP3.nc','TZPP3.ncD']
137/363:
nc=netCDF4.Dataset(fname[0],rw[0])
v4=list(filter(lambda x:nc[x].ndim==4, [i for i in nc.variables]))
nt,nlay,nrow,ncol=(nc[v4[0]].shape[i] for i in range(4))
nv=len(v4)
#store the dates
tflag=nc['TFLAG'][:,0,0]
days=list(set(tflag))
days.sort()
ndays=len(days)
ns=str(ndays)
#store the variables
var=np.zeros(shape=(nv,nt,nrow,ncol))
v0=[]
for v in v4:
  iv=v4.index(v)
  var[iv,:,:,:]=nc[v][:,0,:,:]
  if np.sum(var[iv,:,:,:])==0:v0.append(v)
nc.close()
#take daily-mean for each day
var_d=np.zeros(shape=(nv,ndays,nrow,ncol))
for d in days:
  id=days.index(d)
  idx=np.where(tflag==d)
  var_d[:,id,:,:]=np.mean(var[:,idx[0],:,:],axis=1)
137/364: fname=['TZPP3.nc','TZPP3.ncD']
137/365:
nc=netCDF4.Dataset(fname[0],rw[0])
v4=list(filter(lambda x:nc[x].ndim==4, [i for i in nc.variables]))
nt,nlay,nrow,ncol=(nc[v4[0]].shape[i] for i in range(4))
nv=len(v4)
#store the dates
tflag=nc['TFLAG'][:,0,0]
days=list(set(tflag))
days.sort()
ndays=len(days)
ns=str(ndays)
#store the variables
var=np.zeros(shape=(nv,nt,nrow,ncol))
v0=[]
for v in v4:
  iv=v4.index(v)
  var[iv,:,:,:]=nc[v][:,0,:,:]
  if np.sum(var[iv,:,:,:])==0:v0.append(v)
nc.close()
#take daily-mean for each day
var_d=np.zeros(shape=(nv,ndays,nrow,ncol))
for d in days:
  id=days.index(d)
  idx=np.where(tflag==d)
  var_d[:,id,:,:]=np.mean(var[:,idx[0],:,:],axis=1)
137/366: TIMESTAMP=subprocess.check_output("ncdump -h "+fname[0]+"|head |grep UNLIMITED|awk '{print $1}'",shell=True).decode('utf8').strip('\n')
137/367: TIMESTAMP
137/368:
ncks=subprocess.check_output('which ncks',shell=True).decode('utf8').strip('\n')
res=os.system(ncks+' -O -d '+TIMESTAMP+',1,'+ns+' '+fname[0]+' '+fname[1])
137/369: !lst
137/370: nc=netCDF4.Dataset(fname[1],rw[1])
137/371:
nc.SDATE,nc.STIME=days[0],0
day2=[dt2jul(jul2dt([i,0])+datetime.timedelta(days=1))[0] for i in days]
nc.EDATE,nc.ETIME=day2[-1],0
nc.TSTEP=240000
137/372:
for t in range(ndays):
  nc.variables['TFLAG'][t,:,0]=[days[t] for i in range(nv)]
  nc.variables['TFLAG'][t,:,1]=[0 for i in range(nv)]
137/373:
for t in range(ndays):
  nc.variables['TFLAG'][t,:nv,0]=[days[t] for i in range(nv)]
  nc.variables['TFLAG'][t,:nv,1]=[0 for i in range(nv)]
137/374:
if 'ETFLAG' in nc.variables:
  for t in range(ndays):
    nc.variables['ETFLAG'][t,:nv,0]=[day2[t] for i in range(nv)]
    nc.variables['ETFLAG'][t,:nv,1]=[0 for i in range(nv)]
137/375:
for v in v4:
  if v in v0:continue
  iv=v4.index(v)
  nc.variables[v][:,0,:,:]=var_d[iv,:,:,:]
137/376: nc.close()
139/1:
import numpy as np
import netCDF4
import os,sys,subprocess
import datetime
from dtconvertor import dt2jul, jul2dt

rw=['r','r+']
139/2: fname='/nas2/cmaq2019/download/input/201901/grid03/mcip/METCRO2D_Taiwan.nc'
139/3: nc = netCDF4.Dataset(fname,'r')
139/4: fname='/nas2/cmaq2019/download/input/201901/grid03/mcip/METCRO3D_Taiwan.nc'
139/5: nc = netCDF4.Dataset(fname,'r')
139/6: t=0
139/7: v='ZH'
139/8: zh=np.mean(nc[v][t,:,:,:],axis=(1,2))
139/9: zh
139/10: v='DENS'
139/11: dens=np.mean(nc[v][t,:,:,:],axis=(1,2))
139/12: fname='/nas3/cmaqruns/2018base/data/mcip/1804_run5/CWBWRF_15k/METCRO2D_1804_run5.nc'
139/13: nc = netCDF4.Dataset(fname,'r')
139/14: dens0=np.mean(nc[v][t,:,:,:],axis=(1,2))
139/15: fname='/nas3/cmaqruns/2018base/data/mcip/1804_run5/CWBWRF_15k/METCRO3D_1804_run5.nc'
139/16: nc = netCDF4.Dataset(fname,'r')
139/17: dens0=np.mean(nc[v][t,:,:,:],axis=(1,2))
139/18: v='ZH'
139/19: zh0=np.mean(nc[v][t,:,:,:],axis=(1,2))
139/20: zh
139/21: zh0
139/22: dens
139/23: dens0
139/24: dens0[:10]
139/25: zh0[:10]
139/26: dens[:10]
139/27: zh[:10]
139/28: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/29: V
139/30: v='TA'
139/31: fname
139/32: ta0=np.mean(nc[v][t,:,:,:],axis=(1,2))
139/33: fname='/nas2/cmaq2019/download/input/201901/grid03/mcip/METCRO3D_Taiwan.nc'
139/34: nc = netCDF4.Dataset(fname,'r')
139/35: ta=np.mean(nc[v][t,:,:,:],axis=(1,2))
139/36: ta[:10],ta0[:10]
139/37: v='PRES'
139/38: pr=np.mean(nc[v][t,:,:,:],axis=(1,2))
139/39: fname='/nas3/cmaqruns/2018base/data/mcip/1804_run5/CWBWRF_15k/METCRO3D_1804_run5.nc'
139/40: nc = netCDF4.Dataset(fname,'r')
139/41: pr0=np.mean(nc[v][t,:,:,:],axis=(1,2))
139/42: pr[:10],pr0[:10]
139/43: V
139/44: v='DENSA_J'
139/45: nc[v]
139/46: fname
139/47: densj0=np.mean(nc[v][t,:,:,:],axis=(1,2))
139/48: fname='/nas2/cmaq2019/download/input/201901/grid03/mcip/METCRO3D_Taiwan.nc'
139/49: nc = netCDF4.Dataset(fname,'r')
139/50: densj=np.mean(nc[v][t,:,:,:],axis=(1,2))
139/51: densj0[:10],densj[:10]
139/52: dens0[:10],dens[:10]
139/53: V
139/54: v='ZH'
139/55: nc[v]
139/56: v='ZF'
139/57: nc[v]
139/58: zh0[:10],xh[:10]
139/59: zh0[:10],zh[:10]
139/60: fname='/nas2/cmaq2019/download/input/201901/grid03/mcip/METCRO2D_Taiwan.nc'
139/61: nc = netCDF4.Dataset(fname,'r')
139/62: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/63: V
139/64: v='TEMP2'
139/65: np.where(nc[v][:]==0)
139/66: fname='/nas2/cmaqruns/2019force/output/2019-01/out.conc.nc'
139/67: nc = netCDF4.Dataset(fname,'r')
139/68: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/69: V
139/70: v='K'
139/71: np.where(nc[v][:]==0)
139/72: v='K_prime'
139/73: np.where(nc[v][:]==0)
139/74: pwd
139/75: cd /nas2/cmaqruns/2019force/output/2019-01
139/76: !lst
139/77: fname='K24TZR.nc'
139/78: nc = netCDF4.Dataset(fname,'r')
139/79:
def rotate_about_a_point(target_point,center_point,angle_rs):
  cp=np.subtract(target_point,center_point)
  px=cp[0]*math.cos(math.radians(angle_rs))+cp[1]*-math.sin(math.radians(angle_rs))
  py=cp[0]*math.sin(math.radians(angle_rs))+cp[1]*math.cos(math.radians(angle_rs))
  return(np.add([px,py],center_point))
139/80: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/81: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
139/82: x_mesh=np.array([nc.XORIG+i*nc.XCELL for i in range(ncol)])
139/83: y_mesh=np.array([nc.YORIG+j*nc.YCELL for j in range(nrow)])
139/84: x, y = np.meshgrid(x_mesh, y_mesh)
139/85: P=[(i,j) for i,j in zip(x.flatten(),y.flatten())]
139/86: orig=(x[87,39],y[87,39])
139/87: import math
139/88: diri=90-math.atan2((y[101,49]-y[70,29]),(x[101,49]-x[70,29]))*180/math.pi
139/89: diri
139/90: angl=90-math.atan2((y[101,49]-y[70,29]),(x[101,49]-x[70,29]))*180/math.pi
139/91: Pn=[rotate_about_a_point(pnt,orig,angl) for pnt in P]
139/92: Pn[86*ncol+39]
139/93: orig
139/94: Pn[86*ncol+40]
139/95: pnt=orig
139/96: rotate_about_a_point(pnt,orig,angl)
139/97: Pn.index(png)
139/98: Pn.index(pnt)
139/99: np.where(Pn==pnt)
139/100: pnt in P
139/101: pnt in Pn
139/102: Pnt[:5]
139/103: Pn[:5]
139/104: P[:5]
139/105: Pn=[tuple(rotate_about_a_point(pnt,orig,angl)) for pnt in P]
139/106: pnt=orig
139/107: pnt in Pn
139/108: np.where(Pn==pnt)
139/109: np.where((Pn==pnt).all())
139/110: Pn.index(pnt)
139/111: 86*ncol+40
139/112: 87*ncol+40
139/113: 87*ncol+39
139/114: xn=[i[0] for i in Pn]
139/115: yn=[i[1] for i in Pn]
139/116: from scipy.interpolate import griddata
139/117: maxx,maxy=x[-1,-1],y[-1,-1]
139/118: minx,miny=x[0,0],y[0,0]
139/119: boo=(xn<=maxx+nc.XCELL*10) & (xn>=minx-nc.XCELL*10) & (yn<=maxy+nc.YCELL*10) & (yn>=miny-nc.YCELL*10)
139/120: idx = np.where(boo)
139/121: mp=len(idx[0])
139/122: mp
139/123: len(idx)
139/124: xyc=[(xn[idx[0][i]],yn[idx[0][i]]) for i in range(mp)]
139/125: var=nc['PM25_TOT'][0,0,:,:]
139/126: J=np.array([i//ncol for i in idx[0]])
139/127: I=np.array([i%ncol for i in idx[0]])
139/128: J[:5],I[:5]
139/129: c = np.array([var[J[i], I[i]] for i in range(mp)])
139/130: zz = griddata(xyc, c[:], (x, y), method='linear')
139/131: nc = netCDF4.Dataset(fname,'r+')
139/132: zz.shape
139/133: nc['PM25_TOT'][0,0,:,:]=zz[:,:]
139/134: nc.close()
139/135: fname='K24TZR.ncM'
139/136: nc = netCDF4.Dataset(fname,'r+')
139/137: var=nc['PM25_TOT'][0,0,:,:]
139/138: zz=np.zeros(shape=var.shape)
139/139: c = np.array([var[J[i], I[i]] for i in range(mp)])
139/140: zz[:,:] = griddata(xyc, c[:], (x, y), method='linear')
139/141: nc['PM25_TOT'][0,0,:,:]=zz[:,:]
139/142: nc.close()
139/143: idx=np.where(np.isnan(zz))
139/144: len(idx[0])
139/145: zz=np.where(np.isnan(zz),0,zz)
139/146: nc = netCDF4.Dataset(fname,'r+')
139/147: nc['PM25_TOT'][0,0,:,:]=zz[:,:]
139/148: nc.close()
139/149: history -f his.txt
139/150: 5726.74+636.30457
139/151: 272.7862+5965
139/152: cd /nas2/cmaqruns/2019force/output/2019-01/grid03/smoke
139/153: !lst
139/154: fname='teds11.1901.const.nc'
139/155: nc = netCDF4.Dataset(fname,'r')
139/156: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/157: V
139/158: v='XLOCA'
139/159: nc[v][:]
139/160: v='YLOCA'
139/161: nc[v][:]
139/162: V
139/163: v='ROW'
139/164: nc[v][:]
139/165: v='COL'
139/166: nc[v][:]
139/167: V
139/168: v='LATITUDE'
139/169: nc[v][:]
139/170: v='LONGITUDE'
139/171: nc[v][:]
139/172: V
139/173: !lst
139/174: fname='teds11.1901.timvar.nc'
139/175: nc = netCDF4.Dataset(fname,'r')
139/176: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/177: V
139/178: V[:30]
139/179: V[3][:30]
139/180: V[2]
139/181: V[1]
139/182: V[0]
139/183: V[3][30:60]
139/184: pwd
139/185: fname='cmaq_cb06r3_ae7_aq.01-20181225.38.TW3-d4.BaseEms.ncf_dTZ'
139/186: !lst
139/187: nc = netCDF4.Dataset(fname,'r+')
139/188: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/189:
for v in V[3]:
    nc[v][:,4,87,39]=0.
    nc[v][:,4,86,39]=0.
139/190: nc.close()
139/191: 382/954000
139/192: 382/954000 *10000
140/1: run iso.py
140/2:
from netCDF4 import Dataset
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.cm import get_cmap
import cartopy.crs as crs
from cartopy.feature import NaturalEarthFeature

from wrf import (getvar, interplevel, to_np, latlon_coords, get_cartopy,
                 cartopy_xlim, cartopy_ylim)

# Open the NetCDF file
ncfile = Dataset("wrfout_d04_2019-01-29-31")
iso=850 #isobaric value
n=5     #frequency of wind bars
# Extract the pressure, geopotential height, and wind variables
p = getvar(ncfile, "pressure")
z = getvar(ncfile, "z", units="dm")
ua = getvar(ncfile, "ua", units="kt")
va = getvar(ncfile, "va", units="kt")
wspd = getvar(ncfile, "wspd_wdir", units="kts")[0,:]

# Interpolate geopotential height, u, and v winds to 500 hPa
ht_iso = interplevel(z, p, iso)
u_iso = interplevel(ua, p, iso)
v_iso = interplevel(va, p, iso)
wspd_iso = interplevel(wspd, p, iso)
lats, lons = latlon_coords(ht_iso)

# Get the map projection information
cart_proj = get_cartopy(ht_iso)

# Create the figure
fig = plt.figure(figsize=(12,9))
ax = plt.axes(projection=cart_proj)

# Download and add the states and coastlines
states = NaturalEarthFeature(category="cultural", scale="10m",
                             facecolor="none",
                             name="admin_1_states_provinces")
ax.add_feature(states, linewidth=0.5, edgecolor="black")
ax.coastlines('50m', linewidth=0.8)

# Add the 500 hPa geopotential height contours
levels = np.arange(np.min(ht_iso), np.max(ht_iso), 6.)
140/3: levels
140/4: ht_iso[:5,:5]
140/5: run iso.py
140/6: run iso.py
140/7: p.shape
140/8: shp = getvar(ncfile, "pressure").shape
140/9: shp
140/10: p0=np.zeros(shape=7,shp)
140/11: p0=np.zeros(shape=zip(7,shp))
140/12: p0=np.zeros(shape=zip([7],shp))
140/13: p0=np.zeros(shape=[7]+shp)
140/14: p0=np.zeros(shape=[7]+list(shp))
140/15:
shp = [7]+list(getvar(ncfile, "pressure").shape)
p0=np.zeros(shape=shp)
z0=np.zeros(shape=shp)
tlst=[i for i in range(4,41,6)]
for it in tlst:
  i=tlst.index(it)
  p0[i,:,:,:] = getvar(ncfile, "pressure",timeidx=it)
  z0[i,:,:,:] = getvar(ncfile, "cape_3d", timeidx=it)[0,:,:,:]
ht_iso0 = interplevel(z0, p0, iso)
140/16: (np.min(ht_iso0), np.max(ht_iso0)*1.001, (np.max(ht_iso0)*1.001-np.min(ht_iso0))/10)
140/17:
ncfile = Dataset("wrfout_d03_2019-01-29-31")
n=2     #frequency of wind bars
t=int(sys.argv[1])
# Extract the pressure, geopotential height, and wind variables
shp = [7]+list(getvar(ncfile, "pressure").shape)
z0=np.zeros(shape=shp)
tlst=[i for i in range(4,41,6)]
for it in tlst:
  i=tlst.index(it)
  z0[i,:,:,:] = getvar(ncfile, "cape_3d", timeidx=it)[0,0,:,:]
ht_iso0 = z0
levels = np.arange(0, np.max(ht_iso0)*1.001, np.max(ht_iso0)*1.001/10)
140/18:
ncfile = Dataset("wrfout_d03_2019-01-29-31")
n=2     #frequency of wind bars
t=4
# Extract the pressure, geopotential height, and wind variables
shp = [7]+list(getvar(ncfile, "pressure").shape)
z0=np.zeros(shape=shp)
tlst=[i for i in range(4,41,6)]
for it in tlst:
  i=tlst.index(it)
  z0[i,:,:,:] = getvar(ncfile, "cape_3d", timeidx=it)[0,0,:,:]
ht_iso0 = z0
levels = np.arange(0, np.max(ht_iso0)*1.001, np.max(ht_iso0)*1.001/10)
140/19: p.max(ht_iso0)
140/20: np.max(ht_iso0)
140/21: run cape_isob.py 4 850
140/22: levels
140/23: (np.min(ht_iso0), np.max(ht_iso0)*1.001, (np.max(ht_iso0)*1.001-np.min(ht_iso0))/10)
140/24: ht_iso[:5,:5]
140/25:
shp2 = [7]+list(getvar(ncfile, "pressure").shape)[1,2]
p0=np.zeros(shape=shp)
z0=np.zeros(shape=shp2)
140/26:
shp2 = [7]+list(getvar(ncfile, "pressure").shape)[1:]
p0=np.zeros(shape=shp)
z0=np.zeros(shape=shp2)
140/27:
shp2 = [7]+list(getvar(ncfile, "pressure").shape)[1:]
p0=np.zeros(shape=shp)
z0=np.zeros(shape=shp2)
tlst=[i for i in range(4,41,6)]
for it in tlst:
  i=tlst.index(it)
  p0[i,:,:,:] = getvar(ncfile, "pressure",timeidx=it)
  z0[i,:,:] = getvar(ncfile, "cape_2d", timeidx=it)[0,:,:]
ht_iso0 = z0
140/28: ht_iso0[:5,:5]
140/29: levels = np.arange(np.min(ht_iso0), np.max(ht_iso0)*1.001, (np.max(ht_iso0)*1.001-np.min(ht_iso0))/10)
140/30: (np.min(ht_iso0), np.max(ht_iso0)*1.001, (np.max(ht_iso0)*1.001-np.min(ht_iso0))/10)
140/31:
shp2 = [7]+list(getvar(ncfile, "pressure").shape)[1:]
p0=np.zeros(shape=shp)
z0=np.zeros(shape=shp2)
tlst=[i for i in range(4,41,6)]
for it in tlst:
  i=tlst.index(it)
  p0[i,:,:,:] = getvar(ncfile, "pressure",timeidx=it)
  z0[i,:,:] = getvar(ncfile, "cape_2d", timeidx=it)[0,:,:]
ht_iso0 = z0
z0=np.where(np.isnan(z0),0,z0)
140/32: (np.min(ht_iso0), np.max(ht_iso0)*1.001, (np.max(ht_iso0)*1.001-np.min(ht_iso0))/10)
140/33:
shp2 = [7]+list(getvar(ncfile, "pressure").shape)[1:]
p0=np.zeros(shape=shp)
z0=np.zeros(shape=shp2)
tlst=[i for i in range(4,41,6)]
for it in tlst:
  i=tlst.index(it)
  p0[i,:,:,:] = getvar(ncfile, "pressure",timeidx=it)
  z0[i,:,:] = getvar(ncfile, "cape_2d", timeidx=it)[0,:,:]
z0=np.where(np.isnan(z0),0,z0)
ht_iso0 = z0
140/34: (np.min(ht_iso0), np.max(ht_iso0)*1.001, (np.max(ht_iso0)*1.001-np.min(ht_iso0))/10)
141/1: run CIN_isob.py 4 1000
141/2: ht_iso[:5,:5]
141/3: run CIN_isob.py 4 850
141/4: ht_iso[:5,:5]
141/5: ht_iso0[:5,:5]
141/6: z0.shape
141/7: z0[0,:,30,25]
141/8: z0[0,:,0,5]
141/9: z0[5,:,0,5]
141/10: run CIN_isob.py 4 1000
141/11: run CIN_isob.py 16 1000
141/12: run K_index.py 1000
141/13: round(np.max(ht_iso0)+1,0)
141/14: round(np.max(ht_iso0)+1,0.)
141/15: round(np.max(ht_iso0)+1.,0)
141/16: round(np.max(ht_iso0)+1.,1)
141/17: np.round(np.max(ht_iso0)+1.,1)
141/18: np.round(np.max(ht_iso0)+1.,0)
139/193: 744/24
139/194: 912/24
139/195: 24*7
139/196: 92.6/3.6
142/1: run ver_ZhonBuTh.py 22
142/2: lats[:5,:5]
142/3: lats[-5,-5]
142/4: lons[-5,-5]
142/5: lons[:5,:5]
142/6: lons[-5:,-5:]
142/7: run ver_ZhonBuTh.py 22
142/8: run ver_ZhonBuTh.py 22
142/9: ncfile
139/197: cd /nas2/cmaqruns/2019force/output/2019-01
139/198: fname='N3G.nc'
139/199: nc = netCDF4.Dataset(fname,'r')
139/200: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/201: V
139/202: rat=np.zeros(shape=nc['CO'].shape)
139/203: rat=nc['PM25_TOT'][:]/nc['PM10'][:]
139/204: rat.shape
139/205: pm25m=np.max(['PM25_TOT'][:],axis=0)
139/206: pm25m=np.max(['PM25_TOT'][:],axis=(0))
139/207: pm25m=np.max(['PM25_TOT'][:])
139/208: pm25m=np.max(nc['PM25_TOT'][:],axis=0)
139/209: pm10m=np.max(nc['PM10'][:],axis=0)
139/210: rat=pm25m/pm10m
139/211: ls *N3G*
139/212: fname='N3G.ncM'
139/213: nc = netCDF4.Dataset(fname,'r+')
139/214: v='CO'
139/215: nc[v][:]=rat[:]
139/216: nc.close()
139/217: pwd
139/218: fname='N3G.ncM'
139/219: nc = netCDF4.Dataset(fname,'r+')
139/220: v
139/221: nc[v][:]=nc['PM25_TOT'][:]/nc['PM10'][:]
139/222: nc.close()
139/223: idx=np.where(nc['PM10'][:]>0)
139/224: nc = netCDF4.Dataset(fname,'r+')
139/225: idx=np.where(nc['PM10'][:]>0)
139/226: idx.shape
139/227: len(idx)
139/228: len(idx[0])
139/229: rat=nc['PM25_TOT'][:]/nc['PM10'][:]
139/230: idx=np.where(nc['PM10'][:]<=0)
139/231: rat[idx]=0
139/232: v
139/233: nc[v][:]=rat[:]
139/234: nc.close()
139/235: fname
139/236: nc = netCDF4.Dataset(fname,'r+')
139/237: idx=np.where(nc['PM10'][:]<=nc['PM25_TOT'][:])
139/238: rat=nc['PM25_TOT'][:]/nc['PM10'][:]
139/239: rat[idx]=0
139/240: nc[v][:]=rat[:]
139/241: nc.close()
143/1:
from netCDF4 import Dataset
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.cm import get_cmap
import cartopy.crs as crs
from cartopy.feature import NaturalEarthFeature
import sys
from datetime import datetime, timedelta
from wrf import (getvar, interplevel, to_np, latlon_coords, get_cartopy,
                 cartopy_xlim, cartopy_ylim)
fname='../isobaric/wrfout_d01_2019-01-29-31'
ncfile = Dataset(fname)
p = getvar(ncfile, "pressure",timeidx=0)
143/2: dir(p)
143/3:
fname='../vert_conc/K24aTZ.nc'
ncfile=Dataset(fname)
c=getvar(ncfile,"SO2",timeidx=0)
143/4:
fname='../vert_conc/K24aTZ.nc'
ncfile=Dataset(fname)
c=ncfile["SO2"]
143/5: cart_proj = get_cartopy(c)
143/6: cartopy_xlim(c)
143/7: run m3nc2gif.py
143/8: run m3nc2gif.py
143/9: run m3nc2gif.py
143/10: run m3nc2gif.py
143/11: run m3nc2gif.py
143/12: cart_proj = get_cartopy(p)
143/13: cart_proj
143/14: dir(cart_proj)
143/15: dir(crs)
143/16: run m3nc2gif.py
143/17: run m3nc2gif.py
143/18: run m3nc2gif.py
143/19: run m3nc2gif.py
143/20: crs.LambertConformal
143/21: run m3nc2gif.py
143/22: run m3nc2gif.py
143/23: run m3nc2gif.py
143/24: run m3nc2gif.py
143/25: run m3nc2gif.py
143/26: run m3nc2gif.py
143/27: run m3nc2gif.py
143/28: run m3nc2gif.py
143/29: dir(p)
143/30: dir(p.coordinates)
143/31: p.coordinates
143/32: p.coordinates['XLONG']
143/33: p.coordinates.XLONG
143/34: p.coords
143/35: p.coords['XLONG']
143/36: p.coords['XLONG','XLAT']
143/37: p.coords[['XLONG','XLAT']]
143/38: run m3nc2gif.py
143/39: p.shape
143/40:
    major_Xtick = range(119.5, 130.5,0.5)
    minor_Xtick = range(118, 130.5,0.5)
143/41:
    major_Xtick = arange(119.5, 130.5,0.5)
    minor_Xtick = arange(118, 130.5,0.5)
143/42:
    major_Xtick = np.arange(119.5, 130.5,0.5)
    minor_Xtick = np.arange(118, 130.5,0.5)
143/43: major_Xtick
143/44: run m3nc2gif.py
143/45: run m3nc2gif.py
143/46: run m3nc2gif.py
143/47: run m3nc2gif.py
143/48: run m3nc2gif.py
143/49: run m3nc2gif.py
143/50: run m3nc2gif.py
143/51: run m3nc2gif.py
143/52: run m3nc2gif.py
143/53: dc
143/54: mxv,mnv
143/55: run m3nc2gif.py
143/56: run m3nc2gif.py
143/57: run m3nc2gif.py
143/58: run m3nc2gif.py
143/59: run m3nc2gif.py
143/60: run m3nc2gif.py
143/61: run m3nc2gif.py
143/62: run m3nc2gif.py
143/63: run m3nc2gif.py
143/64: run m3nc2gif.py
143/65: mxv,mnv
143/66: run m3nc2gif.py
143/67: mxv,mnv
143/68: run m3nc2gif.py
143/69: run m3nc2gif.py
143/70: run m3nc2gif.py
143/71: run m3nc2gif.py
143/72: run m3nc2gif.py
143/73: run m3nc2gif.py
143/74: run m3nc2gif.py
143/75: run m3nc2gif.py
143/76: run m3nc2gif.py
143/77: mxv,mnv
143/78: dc
143/79: level
143/80: mxv,mnv
143/81: run m3nc2gif.py
143/82: dc
143/83: mxv,mnv
143/84: run m3nc2gif.py
143/85: run m3nc2gif.py
143/86: dc
143/87: level
143/88: np.max(a)
143/89: run m3nc2gif.py
143/90: mxv,mnv
143/91: dc
143/92: np.max(a)
143/93: level
143/94: run m3nc2gif.py
143/95: run m3nc2gif.py
143/96: mxv,mnv
143/97: mxv,mnv,mxv/mnv
143/98: np.max(a)
143/99: int(5-np.log10(100))
143/100: int(4-np.log10(1000))
143/101: int(4-np.log10(10000))
143/102: int(4-np.log10(100000))
143/103: int(3-np.log10(100000))
143/104: int(3-np.log10(10000))
143/105: int(3-np.log10(1000))
143/106: int(3-np.log10(100))
143/107: int(3-np.log10(10))
143/108: int(3-np.log10(0.1)
143/109: int(3-np.log10(0.1))
143/110: run m3nc2gif.py
143/111: int(3-np.log10(0.1))
143/112: run m3nc2gif.py
143/113: int(3-np.log10(0.1))
143/114: run m3nc2gif.py
143/115: int(3-np.log10(0.1))
143/116: run m3nc2gif.py
143/117: run m3nc2gif.py
143/118: mxv,mnv,mxv/mnv
143/119: '%E'.format(.01)
143/120: ':%E'.format(.01)
143/121: '{:%E}'.format(.01)
143/122: ':{%E}'.format(.01)
143/123: '{%e}'.format(.01)
143/124: '{%e}'%(.01)
143/125: '%e'%(.01)
143/126: '%e'%(10.01)
143/127: run m3nc2gif.py
143/128: a='%e'.format(10.01)
143/129: a
143/130: a={:%e}.format(10.01)
143/131: a={:e}.format(10.01)
143/132: a='{:e}'.format(10.01)
143/133: a
143/134: run m3nc2gif.py
143/135: run m3nc2gif.py
143/136: run m3nc2gif.py
143/137: mxv,mnv,mxv/mnv
143/138: dc
143/139: run m3nc2gif.py
143/140: dc
143/141: mxv,mnv,mxv/mnv
143/142: run m3nc2gif.py
143/143: run m3nc2gif.py
143/144: run m3nc2gif.py
143/145: xtics
143/146: ytics
143/147: ytics
143/148: run m3nc2gif.py
143/149: ytics
143/150: run m3nc2gif.py
143/151: run m3nc2gif.py
143/152: run m3nc2gif.py
143/153: run m3nc2gif.py
143/154: run m3nc2gif.py
143/155: xlab
143/156: ylab
143/157: run m3nc2gif.py
143/158: run m3nc2gif.py
143/159: run m3nc2gif.py
143/160: run m3nc2gif.py
143/161: run m3nc2gif.py
143/162: cartopy_xlim(p)
143/163: run m3nc2gif.py
143/164: [np.min(X[0,:]),np.max(X[0,:])
143/165: [np.min(X[0,:]),np.max(X[0,:])]
143/166: nc.XORIG
143/167: run m3nc2gif.py
143/168: run m3nc2gif.py
143/169: run m3nc2gif.py
143/170: cartopy_ylim(p)
143/171: [np.min(y1d),np.max(y1d)]
143/172: nc.YORIG
143/173: run m3nc2gif.py
143/174: run m3nc2gif.py
143/175: cartopy_ylim(p)+np.array([+20000,-20000])
143/176: run m3nc2gif.py
143/177: run m3nc2gif.py
143/178: run m3nc2gif.py
143/179: run m3nc2gif.py
139/242: a=[12345.12345, 1234.1234,123.123,12.12,1.1]
139/243: n=[int(3-np.log10(mxv)) for mxv in a]
139/244: [round(i,j) for i,j in zip(n,a)]
139/245: n
139/246: [round(i,j) for i,j in zip(a,n)]
139/247: a=[12345.12345, 1234.1234,123.123,12.12,1.1,0.12,0.123]
139/248: n=[int(3-np.log10(mxv)) for mxv in a]
139/249: [round(i,j) for i,j in zip(a,n)]
139/250: n
139/251: mxv=0.1234
139/252: int(3-np.log10(mxv))
139/253: mxv=0.01234
139/254: int(3-np.log10(mxv))
139/255: round(mxv,4)
139/256: cd /nas1/CAM-chem/Annuals
139/257: pwd
139/258: from pandas import *
139/259: df=read_csv('y_s_vO300.csv')
139/260: df.head()
139/261: pv=pivot_table(df,index='s',values=['v'],aggfunc=np.mean).reset_index()
139/262: pv.head()
139/263: df.set_index('s').to_csv('y_s_vO3T.csv')
139/264: pv['xianshi']=[i//1000 for i in pv.s]
139/265: pv['xiangzhenqu']=[i%1000 for i in pv.s]
139/266: col=['xianshi', 'xiangzhenqu','v']
139/267: pv[col].set_index('xianshi').to_csv('y_s_vO3T.csv')
139/268: pv[col].set_index('xianshi').to_csv('y_s_vO3T.csv',header=None)
139/269: cd /nas2/cmaqruns/2019force/output/2019-01
139/270: fname='N3G.nc'
139/271: nc = netCDF4.Dataset(fname,'r+')
139/272: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/273: V[3]
139/274: cd /nas2/cmaqruns/2019force/output/2019-01
139/275: fname='20190103.nc'
139/276: nc = netCDF4.Dataset(fname,'r+')
139/277: fname='20190102.nc'
139/278: fname='20190103.nc'
139/279: nc1 = netCDF4.Dataset(fname,'r+')
139/280: fname='20190102.nc'
139/281: nc = netCDF4.Dataset(fname,'r')
139/282: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/283: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
139/284:
for v in V[3]:
    for t in range(nt):
        nc1[v][t,:,:,:]=nc[v][t,:,:,:]
139/285: v='TFLAG'
139/286:
for t in range(nt):
    nc1[v][t,:,0]=nc1[v][0,:,0]
    nc1[v][t,0,1]=nc[v][t,0,1]
139/287: nc.close()
139/288: nc1.close()
139/289: fname
139/290: fname='20190102.nc'
139/291: nc = netCDF4.Dataset(fname,'r')
139/292: fname='20190103.nc'
139/293: nc1 = netCDF4.Dataset(fname,'r+')
139/294: v
139/295:
for t in range(nt):
    nc1[v][t,:,0]=nc1[v][0,:,0]
    nc1[v][t,0,1]=nc[v][t,0,1]
139/296:
for v in V[3]:
    for t in range(nt):
        nc1[v][t,:,:,:]=nc[v][t,:,:,:]
139/297: nc1.close()
139/298: fname='BASE.nc'
139/299: nc = netCDF4.Dataset(fname,'r')
139/300: nt
139/301: fname='20190103.nc'
139/302: nc1 = netCDF4.Dataset(fname,'r+')
139/303:
for v in V[3]:
    for t in range(nt):
        nc1[v][t,:,:,:]=nc[v][t+48,:,:,:]
139/304: nc1.close()
139/305: cd /nas2/cmaqruns/2019force/output/2019-01/grid03/cctm.XindaN3G-BASE/daily
139/306: lst
139/307: !lst
139/308: fname='CCTM_ACONC_v532_intel_Taiwan_20190102.nc'
139/309: nc = netCDF4.Dataset(fname,'r')
139/310: fname='CCTM_ACONC_v532_intel_Taiwan_20190103.nc'
139/311: nc1 = netCDF4.Dataset(fname,'r+')
139/312: v='TFLAG'
139/313:
for t in range(nt):
    nc1[v][t,:,0]=nc1[v][0,:,0]
    nc1[v][t,0,1]=nc[v][t,0,1]
139/314:
for v in V[3]:
    for t in range(nt):
        nc1[v][t,:,:,:]=nc[v][t,:,:,:]
139/315: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/316:
for v in V[3]:
    for t in range(nt):
        nc1[v][t,:,:,:]=nc[v][t,:,:,:]
139/317: nc1.close()
139/318: cd /nas2/cmaqruns/2019force/output/2019-01/grid03/cctm.XindaN3G/daily/APDIAG_b_n3g
139/319: fname='CCTM_APMDIAG_v532_intel_Taiwan_20190103.nc'
139/320: nc1 = netCDF4.Dataset(fname,'r+')
139/321: fname='/nas2/cmaq2019/download/input/201901/grid03/cctm.BASE/daily/CCTM_APMDIAG_v532_intel_Taiwan_20190103.nc'
139/322: nc = netCDF4.Dataset(fname,'r')
139/323: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/324: v='TFLAG'
139/325:
for t in range(nt):
    nc1[v][t,:,0]=nc1[v][0,:,0]
    nc1[v][t,0,1]=nc[v][t,0,1]
139/326:
for v in V[3]:
    for t in range(nt):
        nc1[v][t,:,:,:]=nc[v][t,:,:,:]
139/327: nc1.close()
139/328: cd /nas2/cmaqruns/2019force/output/2019-01/daily
139/329: !lst
139/330: ls
139/331: fnames=['CCTM_CGRID_v532_intel_Taiwan_201901'+'{:02d}'.format(i)+'.nc' for i in range(1,32)]
139/332: fnames[:5]
139/333: fnames[-5:]
139/334: fname='CCTM_CGRID_v532_intel_Taiwan_20190104.nc'
139/335: nc = netCDF4.Dataset(fname,'r')
139/336: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/337:
import numpy as np 
from scipy.stats import t
139/338:
for fname in fnames:
    nc = netCDF4.Dataset(fname,'r+')
    for v in V[3]:
        var=np.where(nc[v][:]>0,nc[v][:],0.)
        nc[v][:]=var[:]
    nc.close()
    print(fname)
139/339: cp ../../../cctm.BASE/daily/CCTM_CGRID_v532_intel_Taiwan_20190103.nc .
139/340: pwd
139/341: cp ../../cctm.BASE/daily/CCTM_CGRID_v532_intel_Taiwan_20190103.nc .
139/342: ls ../../
139/343: ls -lh ../../cctm.BASE/daily/CCTM_CGRID_v532_intel_Taiwan_2019010*
139/344: ls -lh ../../cctm.BASE/daily/CCTM_ACONC_v532_intel_Taiwan_2019010*
139/345: cp ../../cctm.BASE/daily/CCTM_ACONC_v532_intel_Taiwan_20190103.nc .
139/346: fname='CCTM_ACONC_v532_intel_Taiwan_20190103.nc'
139/347: nc1 = netCDF4.Dataset(fname,'r+')
139/348:
for v in V[3]:
    nc1[v][:]=0
139/349: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/350:
for v in V[3]:
    nc1[v][:]=0
139/351: nc = netCDF4.Dataset(fname,'r')
139/352: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
139/353:
for v in V[3]:
    nc1[v][:]=0
139/354:
for fname in fnames:
    nc = netCDF4.Dataset(fname,'r+')
    for v in V[3]:
        var=np.where(nc[v][:]>0,nc[v][:],0.)
        nc[v][:]=var[:]
    nc.close()
    print(fname)
139/355: fnames[-5:]
139/356: fnames=['CCTM_ACON_v532_intel_Taiwan_201901'+'{:02d}'.format(i)+'.nc' for i in range(1,32)]
139/357:
for fname in fnames:
    nc = netCDF4.Dataset(fname,'r+')
    for v in V[3]:
        var=np.where(nc[v][:]>0,nc[v][:],0.)
        nc[v][:]=var[:]
    nc.close()
    print(fname)
139/358: fnames=['CCTM_ACONC_v532_intel_Taiwan_201901'+'{:02d}'.format(i)+'.nc' for i in range(1,32)]
139/359:
for fname in fnames:
    nc = netCDF4.Dataset(fname,'r+')
    for v in V[3]:
        var=np.where(nc[v][:]>0,nc[v][:],0.)
        nc[v][:]=var[:]
    nc.close()
    print(fname)
145/1: cd /nas2/cmaqruns/2019force/output/2019-01/grid03/cctm.XindaN3G-BASE/daily
145/2:
import numpy as np 
from scipy.stats import t
145/3: import netCDF4
145/4: fnames=['CCTM_ACONC_v532_intel_Taiwan_201901'+'{:02d}'.format(i)+'.nc' for i in range(1,32)]
145/5:
for fname in fnames:
    nc = netCDF4.Dataset(fname,'r+')
    for v in V[3]:
        var=np.where(nc[v][:]>0,nc[v][:],0.)
        nc[v][:]=var[:]
    nc.close()
    print(fname)
145/6: fname='CCTM_ACONC_v532_intel_Taiwan_20190103.nc'
145/7: nc = netCDF4.Dataset(fname,'r')
145/8: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
145/9:
for fname in fnames:
    nc = netCDF4.Dataset(fname,'r+')
    for v in V[3]:
        var=np.where(nc[v][:]>0,nc[v][:],0.)
        nc[v][:]=var[:]
    nc.close()
    print(fname)
145/10: pwd
145/11: pwd
145/12: cd /nas2/cmaqruns/2019force/output/2019-01
145/13: fname='20190102.nc'
145/14: nc = netCDF4.Dataset(fname,'r')
145/15: fname='20190103.nc'
145/16: nc1 = netCDF4.Dataset(fname,'r+')
145/17: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
145/18: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
145/19:
for v in V[3]:
    for t in range(nt):
        nc1[v][t,:,:,:]=nc[v][t,:,:,:]
145/20: nc1.close()
145/21: cd /nas2/cmaqruns/2019force/output/2019-01/daily
145/22: fname='CCTM_ACONC_v532_intel_Taiwan_20190102.nc'
145/23: nc = netCDF4.Dataset(fname,'r')
145/24: fname='CCTM_ACONC_v532_intel_Taiwan_20190103.nc'
145/25: nc1 = netCDF4.Dataset(fname,'r+')
145/26: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
145/27: nt
145/28:
for v in V[3]:
    for t in range(nt):
        nc1[v][t,:,:,:]=nc[v][t,:,:,:]
145/29: nc1.close()
145/30: pwd
145/31: cd ../../cctm.XindaN3G-BASE/daily
145/32: lst
145/33: fname='CCTM_ACONC_v532_intel_Taiwan_20190103.nc'
145/34: nc1 = netCDF4.Dataset(fname,'r+')
145/35: fname='CCTM_ACONC_v532_intel_Taiwan_20190102.nc'
145/36: nc = netCDF4.Dataset(fname,'r')
145/37:
for v in V[3]:
    for t in range(nt):
        nc1[v][t,:,:,:]=nc[v][t,:,:,:]
145/38: nc1.close()
146/1: run ~/bin/m3nc2gif.py 0801
147/1: run ~/bin/m3nc2gif.py 0801
147/2: contours == None
147/3: contours is None
147/4:
try: x
except: print('x is not defined')
else: print(x)
148/1: import netCDF4
148/2:
import numpy as np 
from scipy.stats import t
148/3: fname='2019T470.nc'
148/4: nc = netCDF4.Dataset(fname,'r')
148/5: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
148/6: V
148/7:
with open('2019allnc.txt','r') as f:
    fnames=[i.strip('\n') for i in f]
148/8:
var0=np.ones(shape=(365,4,1200,1200))*-1
i=0
for fname in fnames:
  nc = netCDF4.Dataset(fname,'r')
  nlay=nc[v].shape[0]
  var0[i,:nlay,:,:]=np.array(nc[v][:,:,:])
  i+=1
148/9: v='Optical_Depth_047'
148/10:
var0=np.ones(shape=(365,4,1200,1200))*-1
i=0
for fname in fnames:
  nc = netCDF4.Dataset(fname,'r')
  nlay=nc[v].shape[0]
  var0[i,:nlay,:,:]=np.array(nc[v][:,:,:])
  i+=1
148/11:
varn=np.where(var0>=0,var0, np.nan)
varm = np.ma.masked_array(varn, np.isnan(varn))
weights = np.ones(shape=365)
average = np.ma.average(varm, axis=0, weights=weights)
var=average.filled(np.nan)
148/12: nc.close()
148/13: fname='2019T470.nc'
148/14: nc = netCDF4.Dataset(fname,'r+')
148/15: pwd
148/16: nc = netCDF4.Dataset(fname,'r+')
148/17: nc = netCDF4.Dataset(fname,'r+')
148/18: v
148/19: nc[v][0,0,:,:]=np.mean(np.flip(var[:,:,:],axis=1),axis=0)
148/20: nc.close()
148/21: !lst
148/22: fname='MCD19A2.A2019365.h28v06.006.2020002231719.nc'
148/23: nc = netCDF4.Dataset(fname,'r')
148/24: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
148/25: V
148/26: v='grid1km_latitude'
148/27: nc[v][600,600]
148/28: v='grid1km_longitude'
148/29: nc[v][600,600]
148/30: lon=nc[v].flatten()
148/31: lon=nc[v][:].flatten()
148/32: v='grid1km_latitude'
148/33: lat=nc[v][:].flatten()
148/34: var.shape
148/35: a=np.mean(var,axis=0).flatten()
148/36: from scipy.interpolate import griddata
148/37: from pyproj import Proj
148/38: fnameO='2019T470.nc'
148/39: nc = netCDF4.Dataset(fname,'r+')
148/40: nc1 = netCDF4.Dataset(fnameO,'r+')
148/41: pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc1.P_ALP, lat_2=nc1.P_BET, lat_0=nc1.YCENT, lon_0=nc1.XCENT, x_0=0, y_0=0.0)
148/42: fname='2019T470.nc'
148/43: nc = netCDF4.Dataset(fname,'r+')
148/44:
nc = netCDF4.Dataset(fname,'r')
v4=list(filter(lambda x:nc.variables[x].ndim==4, [i for i in nc.variables]))
nt,nlay,nrow,ncol=(nc.variables[v4[0]].shape[i] for i in range(4))
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
lon0, lat0= pnyc(X,Y, inverse=True)
148/45: lon0.shape
148/46: len(lon0)
148/47: x1,y1=np.meshgrid(X, Y)
148/48: lon0, lat0= pnyc(x1,y1, inverse=True)
148/49: lon0.shape
148/50: nrow,ncol
148/51: nbnd1=nrow*ncol
148/52: x,y=np.meshgrid(X, Y)
148/53: x.shape()
148/54: x.shape
148/55: x1,y1=pnyc(lon,lat, inverse=False)
148/56: x1.shape
148/57: nbnd1
148/58:
n=[-1 for i in range(nbnd1)]
for i in range(nbnd1):
    dist=(x-x1[i])**2+(y-y1[i])**2      #nearest grib data for bcon
    idx=np.where(dist==np.min(dist))[0]
    if type(idx)==list and len(idx)>1: idx=idx[0]
    n[i]=idx
148/59: x=x.flatten()
148/60: y=y.flatten()
148/61:
n=[-1 for i in range(nbnd1)]
for i in range(nbnd1):
    dist=(x[i]-x1)**2+(y[i]-y1)**2      #nearest grib data for bcon
    idx=np.where(dist==np.min(dist))[0]
    if type(idx)==list and len(idx)>1: idx=idx[0]
    n[i]=idx
148/62: !lst
148/63: fname='MCD19A2.A2019365.h29v06.006.2020002231541.nc'
148/64: nc = netCDF4.Dataset(fname,'r')
148/65: v='grid1km_latitude'
148/66: lat=list(lat)+list(nc[v][:].flatten())
148/67: v='grid1km_longitude'
148/68: lon=list(lon)+list(nc[v][:].flatten())
148/69: v='Optical_Depth_047'
148/70: a.shape
148/71: a=list(a)+list(nc[v][0,0,:,:].flatten())
148/72: a=list(a)+list(nc[v][0,:,:].flatten())
148/73: x1,y1=pnyc(lon,lat, inverse=False)
148/74: type(x1)
148/75: x1=np.array(x1)
148/76: y1=np.array(y1)
148/77:
n=[-1 for i in range(nbnd1)]
for i in range(nbnd1):
    dist=(x[i]-x1)**2+(y[i]-y1)**2      #nearest grib data for bcon
    idx=np.where(dist==np.min(dist))[0]
    if type(idx)==list and len(idx)>1: idx=idx[0]
    n[i]=idx
148/78: i
148/79: np.min(x),np.max(x), np.min(y),np.max(y)
148/80: np.min(x1),np.max(x1), np.min(y1),np.max(y1)
148/81:
n=[-1 for i in range(nbnd1)]
vr=[-1 for i in range(nbnd1)]
for i in range(10):
    minx=x[i]-5000;maxx=x[i]+5000
    miny=y[i]-5000;maxy=y[i]+5000
    boo=(x1>=minx)&(y1>=miny)&(x1<=maxx)&(y1<=maxy)
    idx1=np.where(boo)
    if len(idx1[0])==0:continue
    x11=x1[idx1[0]]
    y11=y1[idx1[0]]
    a11=a[idx1[0]]
    dist=(x[i]-x11)**2+(y[i]-y11)**2      #nearest grib data for bcon
    idx=np.where(dist==np.min(dist))[0]
    if type(idx)==list and len(idx)>1: idx=idx[0]
    vr[i]=a11[idx]
148/82: vr[:10]
148/83: idx1
148/84: minx,miny,maxx,maxy
148/85:
n=[-1 for i in range(nbnd1)]
vr=[-1 for i in range(nbnd1)]
for i in range(nbnd1):
    minx=x[i]-5000;maxx=x[i]+5000
    miny=y[i]-5000;maxy=y[i]+5000
    boo=(x1>=minx)&(y1>=miny)&(x1<=maxx)&(y1<=maxy)
    idx1=np.where(boo)
    if len(idx1[0])==0:continue
    x11=x1[idx1[0]]
    y11=y1[idx1[0]]
    a11=a[idx1[0]]
    dist=(x[i]-x11)**2+(y[i]-y11)**2      #nearest grib data for bcon
    idx=np.where(dist==np.min(dist))[0]
    if type(idx)==list and len(idx)>1: idx=idx[0]
    vr[i]=a11[idx]
148/86:
n=[-1 for i in range(nbnd1)]
vr=[-1 for i in range(nbnd1)]
for i in range(nbnd1):
    minx=x[i]-5000;maxx=x[i]+5000
    miny=y[i]-5000;maxy=y[i]+5000
    boo=(x1>=minx)&(y1>=miny)&(x1<=maxx)&(y1<=maxy)
    idx1=np.where(boo)
    if len(idx1[0])==0:continue
    x11=x1[idx1[0]]
    y11=y1[idx1[0]]
    a11=a[idx1[0]]
    dist=(x[i]-x11)**2+(y[i]-y11)**2      #nearest grib data for bcon
    idx=np.where(dist==np.min(dist))[0]
    if type(idx)==list and len(idx)>1: idx=idx[0]
    vr[i]=a11[idx]
    n[i]=idx1[0][idx]
148/87: i
148/88:
n=[-1 for i in range(nbnd1)]
vr=[-1 for i in range(nbnd1)]
for i in range(nbnd1):
    minx=x[i]-5000;maxx=x[i]+5000
    miny=y[i]-5000;maxy=y[i]+5000
    boo=(x1>=minx)&(y1>=miny)&(x1<=maxx)&(y1<=maxy)
    idx1=np.where(boo)
    if len(idx1[0])==0:continue
    x11=x1[idx1[0]]
    y11=y1[idx1[0]]
    a11=a[idx1[0]]
    dist=(x[i]-x11)**2+(y[i]-y11)**2      #nearest grib data for bcon
    idx=np.where(dist==np.min(dist))[0]
    if type(idx)==list and len(idx)>1: idx=idx[0]
    vr[i]=a11[idx]
    n[i]=idx1[0][idx]
148/89: i
148/90: idx1
148/91: idx1[0]
148/92: type(a)
148/93: a=np.array(a)
148/94: len(a)
148/95:
n=[-1 for i in range(nbnd1)]
vr=[-1 for i in range(nbnd1)]
for i in range(nbnd1):
    minx=x[i]-5000;maxx=x[i]+5000
    miny=y[i]-5000;maxy=y[i]+5000
    boo=(x1>=minx)&(y1>=miny)&(x1<=maxx)&(y1<=maxy)
    idx1=np.where(boo)
    if len(idx1[0])==0:continue
    x11=x1[idx1[0]]
    y11=y1[idx1[0]]
    a11=a[idx1[0]]
    dist=(x[i]-x11)**2+(y[i]-y11)**2      #nearest grib data for bcon
    idx=np.where(dist==np.min(dist))[0]
    if type(idx)==list and len(idx)>1: idx=idx[0]
    vr[i]=a11[idx]
    n[i]=idx1[0][idx]
148/96: from scipy.io import FortranFile
148/97: len(n)
148/98: fnameO='N1200_1200.bin'
148/99:
nar=np.array(n,dtype=int)
with FortranFile(fnameO, 'w') as f:
    f.write_record(nar)
148/100: np.max(n)
148/101: ls *txt
148/102: mv 2019allnc.txt 2019allnc28.txt
148/103: !head 2019allnc28.txt
148/104: fnames[:5]
148/105:
var0=np.ones(shape=(365,4,1200,1200))*-1
i=0
for fname in fnames:
  nc = netCDF4.Dataset(fname,'r')
  nlay=nc[v].shape[0]
  var0[i,:nlay,:,:]=np.array(nc[v][:,:,:])
  i+=1
148/106:
varn=np.where(var0>=0,var0, np.nan)
varm = np.ma.masked_array(varn, np.isnan(varn))
weights = np.ones(shape=365)
average = np.ma.average(varm, axis=0, weights=weights)
var=average.filled(np.nan)
148/107: var28=list(np.sum(var,axis=0).flatten())
148/108:
with open('2019allnc29.txt','r') as f:
    fnames=[i.strip('\n') for i in f]
148/109: v
148/110:
var0=np.ones(shape=(365,4,1200,1200))*-1
i=0
for fname in fnames:
  nc = netCDF4.Dataset(fname,'r')
  nlay=nc[v].shape[0]
  var0[i,:nlay,:,:]=np.array(nc[v][:,:,:])
  i+=1
varn=np.where(var0>=0,var0, np.nan)
varm = np.ma.masked_array(varn, np.isnan(varn))
weights = np.ones(shape=365)
average = np.ma.average(varm, axis=0, weights=weights)
var=average.filled(np.nan)
var29=list(np.sum(var,axis=0).flatten())
var=np.array(var28+var29)
148/111: var10=var[n]
148/112: type(n)
148/113: n=np.array(n,dtype=int)
148/114: var10=var[n[:]]
148/115: nrow,ncol
148/116: var=var10.reshape(nrow,ncol)
148/117: fname='2019T470.nc'
148/118: nc = netCDF4.Dataset(fname,'r+')
148/119: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
148/120: V
148/121: v
148/122: var.shape
148/123: nc[v][0,0,:,:]=var[:,:]
148/124: nc.close()
148/125: cd 2016
148/126: cp ../2019T470.nc 2016T470.nc
148/127: %history -f his.txt
148/128: cp his.txt D3T.py
148/129: !vi D3T.py
148/130: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
148/131: !vi D3T.py
148/132: !vi D3T.py
148/133: ls 201*.nc
148/134: !vi D3T.py
148/135:
fnameO='/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/MCD19A2.006/N1200_1200.bin'
with FortranFile(fnameO, 'r') as f:
    nar2 = f.read_record(dtype=np.int)
148/136: len(nar)
148/137: np.max(nar-nar2)
148/138: np.max(nar2-nar)
148/139: !vi D3T.py
148/140: !vi D3T.py
148/141: type(var)
148/142: type(var10)
148/143: !vi D3T.py
148/144: nc = netCDF4.Dataset(fname,'r')
148/145: !vi D3T.py
148/146: run D3T.py
148/147: nlay
148/148: var0.shape
148/149: !vi D3T.py
148/150: run D3T.py
148/151: nc = netCDF4.Dataset(fname,'r+')
148/152: nc[v][0,0,:,:]=var[:,:]
148/153: nc.close()
148/154: cd 2016
148/155: pwd
148/156: ls *py
148/157: !vi genN_D4T.py
148/158: ls temp*
148/159: ls ../temp*
148/160: mv tempTW.nc ..
148/161: !vi genN_D4T.py
148/162: !vi genN_D4T.py
148/163: ls ../*bin
148/164: run genN_D4T.py
148/165: !vi genN_D4T.py
148/166: run genN_D4T.py
148/167: !lst
148/168: fnameO='/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/MCD19A2.006/N393_276.bin'
148/169: !vi D4T.py
148/170:
with FortranFile(fnameO, 'r') as f:
  n2 = f.read_record(dtype=np.int)
148/171: len(nar)
148/172: len(n)
148/173: min(nar-n)
148/174: max(abs(nar-n))
148/175: type(n)
148/176: !cat ../gg.cs
148/177: !vi D4T.py
148/178: run D4T.py
148/179:
yr='2016'

# open a blank template nc file for D3 domain
fname=yr+'T550.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v=V[3][0]
148/180: v
148/181:
fnameO='/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/MCD19A2.006/N393_276.bin'
with FortranFile(fnameO, 'r') as f:
  n = f.read_record(dtype=np.int)

# initialize the lists
nd365=365
if int(yr)%4==0:nd365=366
weights = np.ones(shape=nd365)
var2829=[]
148/182: len(n)
148/183: hh='28'
148/184:
  with open(yr+'allnc'+hh+'.txt','r') as f:
    fnames=[i.strip('\n') for i in f]
  var0=np.ones(shape=(nd365,nlay,1200,1200))*-1
  i=0
  for fname in fnames:
    nc = netCDF4.Dataset(fname,'r')
    nl=nc[v].shape[0]
    var0[i,:nl,:,:]=np.array(nc[v][:,:,:])
    i+=1
  varn=np.where(var0>=0,var0, np.nan)
  varm = np.ma.masked_array(varn, np.isnan(varn))
  average = np.ma.average(varm, axis=0, weights=weights)
  var=average.filled(np.nan)
  var2829+=list(np.sum(var,axis=0).flatten())
148/185: hh='29'
148/186:
  with open(yr+'allnc'+hh+'.txt','r') as f:
    fnames=[i.strip('\n') for i in f]
  var0=np.ones(shape=(nd365,nlay,1200,1200))*-1
  i=0
  for fname in fnames:
    nc = netCDF4.Dataset(fname,'r')
    nl=nc[v].shape[0]
    var0[i,:nl,:,:]=np.array(nc[v][:,:,:])
    i+=1
  varn=np.where(var0>=0,var0, np.nan)
  varm = np.ma.masked_array(varn, np.isnan(varn))
  average = np.ma.average(varm, axis=0, weights=weights)
  var=average.filled(np.nan)
  var2829+=list(np.sum(var,axis=0).flatten())
148/187:
var10=np.array(var2829)[n]
var=var10.reshape(nrow,ncol)
148/188:
fname=yr+'T550.nc'
nc = netCDF4.Dataset(fname,'r+')
nc[v][0,0,:,:]=var[:,:]
nc.close()
148/189: !lst
148/190:
fname=yr+'T550.nc'
nc = netCDF4.Dataset(fname,'r+')
nc[v][0,0,:,:]=var[:,:]
nc.close()
148/191: pwd
148/192: cd ../../LGHAP.AOD.D001/
148/193: fname='LGHAP.AOD.D001.A20001231.nc'
148/194: nc = netCDF4.Dataset(fname,'r')
148/195: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
148/196: V
148/197: lat=nc['lat'][:]
148/198: lon=nc['lon'][:]
148/199: lat[:5,0]
148/200: lat.shape
148/201: lat[:5]
148/202: lon[:5]
148/203: V
148/204: pwd
148/205: ls ../MCD19A2.006/temp*
148/206: var=nc['AOD'][:]
148/207: nc.close()
148/208: fname='../MCD19A2.006/tempTW.nc'
148/209: nc = netCDF4.Dataset(fname,'r')
148/210: pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
148/211: lon2d,lat2d=np.meshgrid(lon, lat)
148/212: x1,y1=pnyc(lon2d,lat2d, inverse=False)
148/213:
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten()
y=y.flatten()
148/214: type(x1)
148/215: x1.shape
148/216:
boo=(x1>=minx)&(y1>=miny)&(x1<=maxx)&(y1<=maxy)
idx0=np.where(boo)
148/217: x1=x1.flatten();y1=y1.flatten()
148/218:
boo=(x1>=minx)&(y1>=miny)&(x1<=maxx)&(y1<=maxy)
idx0=np.where(boo)
148/219: len(idx0[0])
148/220: x1,y1=pnyc(lon2d,lat2d, inverse=False)
148/221: idx0[0][:5]
148/222: x1=x1.flatten();y1=y1.flatten()
148/223: x1[idx0[0][:5]],y1[idx0[0][:5]]
148/224: minx=x[0]-5000;maxx=x[-1]+5000;miny=y[0]-5000;maxy=y[-1]+5000
148/225:
boo=(x1>=minx)&(y1>=miny)&(x1<=maxx)&(y1<=maxy)
idx0=np.where(boo)
148/226: len(idx0[0])
148/227:
n=[-1 for i in range(nbnd1)]
for i in range(nbnd1):
  minx=x[i]-5000;maxx=x[i]+5000
  miny=y[i]-5000;maxy=y[i]+5000
  boo=(x10>=minx)&(y10>=miny)&(x10<=maxx)&(y10<=maxy)
  idx1=np.where(boo)
  if len(idx1[0])==0:continue
  x11=x10[idx1[0]]
  y11=y10[idx1[0]]
  dist=(x[i]-x11)**2+(y[i]-y11)**2      #nearest grib data for bcon
  idx=np.where(dist==np.min(dist))[0][0]
  if type(idx)==list and len(idx)>1: idx=idx[0]
  n[i]=idx0[0][idx1[0][idx]]
148/228: nbnd1
148/229:
x10=x1[idx0[0]]
y10=y1[idx0[0]]
148/230:
n=[-1 for i in range(nbnd1)]
for i in range(nbnd1):
  minx=x[i]-5000;maxx=x[i]+5000
  miny=y[i]-5000;maxy=y[i]+5000
  boo=(x10>=minx)&(y10>=miny)&(x10<=maxx)&(y10<=maxy)
  idx1=np.where(boo)
  if len(idx1[0])==0:continue
  x11=x10[idx1[0]]
  y11=y10[idx1[0]]
  dist=(x[i]-x11)**2+(y[i]-y11)**2      #nearest grib data for bcon
  idx=np.where(dist==np.min(dist))[0][0]
  if type(idx)==list and len(idx)>1: idx=idx[0]
  n[i]=idx0[0][idx1[0][idx]]
148/231: ls *bin
148/232: pwd
148/233:
fnameO='N393_276.bin'
nar=np.array(n,dtype=int)
with FortranFile(fnameO, 'w') as f:
    f.write_record(nar)
148/234: !lst
148/235: ls temp*
148/236: cp ../MCD19A2.006/tempTW.nc .
148/237: nc.close()
148/238: fname='tempTW.nc'
148/239: nc = netCDF4.Dataset(fname,'r+')
148/240: var10=var[nar]
148/241: var.shape
148/242: var=var.flatten()
148/243: var10=var[nar]
148/244: nrow,ncol
148/245: var=var10.reshape(nrow,ncol)
148/246: v
148/247: nc[v][0,0,:,:]=var[:,:]
148/248: nc.close()
148/249: fname='LGHAP.AOD.D001.A20001231.nc'
148/250: nc = netCDF4.Dataset(fname,'r')
148/251: var=nc['AOD'][:]
148/252: var[:5,:5]
148/253: var=np.array(nc['AOD'][:])
148/254: var[:5,:5]
148/255: var=var.flatten()
148/256: var10=var[nar]
148/257: fname='tempTW.nc'
148/258: nc = netCDF4.Dataset(fname,'r+')
148/259: var=var10.reshape(nrow,ncol)
148/260: nc[v][0,0,:,:]=var[:,:]
148/261: nc.close()
148/262: fname='LGHAP.AOD.D001.A20001231.nc'
148/263: nc = netCDF4.Dataset(fname,'r')
148/264: var=np.array(nc['AOD'][:]).T
148/265: var.shape
148/266: var=var.flatten()
148/267: var10=var[nar]
148/268: var=var10.reshape(nrow,ncol)
148/269: fname='tempTW.nc'
148/270: nc = netCDF4.Dataset(fname,'r+')
148/271: nc[v][0,0,:,:]=var[:,:]
148/272: nc.close()
148/273: pwd
148/274: np.max(var)
148/275: np.count(65535.0)
148/276: list(var.flatten()).count(65535.0)
148/277: var.shape
148/278: 393*276
148/279: 74317/108468
148/280: varn=np.where(var!=65535.0,var, np.nan)
148/281: weights.shape
148/282: varm = np.ma.masked_array(varn, np.isnan(varn))
148/283: var=var.filled(np.nan)
148/284: var.shape
148/285: fname
148/286: nc = netCDF4.Dataset(fname,'r+')
148/287: nc[v][0,0,:,:]=var[:,:]
148/288: nc.close()
148/289: varm.shape
148/290: nc = netCDF4.Dataset(fname,'r+')
148/291: nc[v][0,0,:,:]=varm[:,:]
148/292: nc.close()
148/293: np.max(var)
148/294: varn=np.where(var!=65535.0,var, -1)
148/295: nc = netCDF4.Dataset(fname,'r+')
148/296: nc[v][0,0,:,:]=varn[:,:]
148/297: nc.close()
148/298: len(idx0[0])
148/299: fname='LGHAP.AOD.D001.A20001231.nc'
148/300: pwd
148/301: history -f his.txt
148/302: import datetime
148/303: bdate=datetime.datetime(2019,1,1)
148/304: dates=[datetime.datetime.strptime(bdate+datetime.timedelta(days=i),'%Y%m%d') for i in range(365)]
148/305: dates=[datetime.datetime.strftime(bdate+datetime.timedelta(days=i),'%Y%m%d') for i in range(365)]
148/306: dates[:5]
148/307: dates[-5:]
148/308: fnames=['LGHAP.PM25.D001.A'+i+'.nc' for i in dates]
148/309: fnames[30]
148/310: ls -lh LGHAP.PM25.D001.A20190131.nc
148/311: pwd
148/312: cd ../*PM25*
148/313: ls -lh LGHAP.PM25.D001.A20190131.nc
148/314: pwd
148/315: cd /nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/LGHAP.PM25.D001
148/316: ls -lh LGHAP.PM25.D001.A20190131.nc
148/317: fname='LGHAP.PM25.D001.A20190131.nc'
148/318: nc = netCDF4.Dataset(fname,'r')
148/319: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
148/320: V
148/321: var0=nc['PM25'][:]
148/322:
var0[:]=0
for fname in fnames:
    nc = netCDF4.Dataset(fname,'r')
    var0[:]+=nc['PM25'][:]/365
148/323: cp LGHAP.PM25.D001.A20190131.nc A2019T.nc
148/324: fname='A2019T.nc'
148/325: nc = netCDF4.Dataset(fname,'r+')
148/326: nc['PM25'][:]=var0[:]
148/327: nc.close()
148/328: var=var0.flatten()
148/329:
var10=var[nar]
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r+')
var=var10.reshape(nrow,ncol)
148/330: np.max(var)
148/331: np.min(var)
148/332: nc[v][0,0,:,:]=var[:,:]
148/333: nc.close()
148/334: pwd
148/335: var=var0.T.flatten()
148/336:
var10=var[nar]
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r+')
var=var10.reshape(nrow,ncol)
148/337: nc[v][0,0,:,:]=var[:,:]
148/338: nc.close()
148/339: np.max(var)
148/340: np.min(var)
148/341: varn=np.where(var!=np.nan,var, -1)
148/342: nc = netCDF4.Dataset(fname,'r+')
148/343: nc[v][0,0,:,:]=varn[:,:]
148/344: nc.close()
148/345: fname
148/346: nc = netCDF4.Dataset(fname,'r+')
148/347: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
148/348: V
148/349: V3[0]
148/350: V[3][0]
148/351: v=V[3][0]
148/352: nc[v].long_name = "PM25_TOT        "
148/353: nc[v].units = "ug m-3          "
148/354: nc[v].var_desc = "ATOTI[0]*PM25AT[3]+ATOTJ[0]*PM25AC[3]+ATOTK[0]*PM25CO[3]                        " ;
148/355: nc[v]
148/356: %history -f his.txt
148/357: ls -lh LGHAP.PM25.D001.A20190131.nc
148/358: !ncdump -h LGHAP.PM25.D001.A20190131.nc|head
148/359: fnameO='N393_276.bin'
148/360:
with FortranFile(fnameO, 'r') as f:
  n2 = f.read_record(dtype=np.int)
148/361: np.min(n2-nar)
148/362: !top
148/363: !topu
148/364: !psg python
148/365: fname='A2019T.nc'
148/366: nc = netCDF4.Dataset(fname,'r+')
148/367: var=nc['PM25_TOT'][:]
148/368: np.max(var)
148/369: list(var.flatten()).count(23920276.0)
148/370: var=nc['PM25_TOT'][0,0,:,:]
148/371: list(var.flatten()).count(23920276.0)
148/372: v
148/373: nc[v]
148/374: varn=np.where(var!=23920276.0,var, -1)
148/375: nc[v][0,0,:,:]=varn[:,:]
148/376: nc.close()
148/377: pwd
148/378: cd ../
148/379: fname='GRIDCRO2D.nc'
148/380: nc = netCDF4.Dataset(fname,'r')
148/381: fid=nc['MSFX2']
148/382: fid=nc['MSFX2'][:]
148/383: fid.shape
148/384: fid=nc['MSFX2'][0,0,:,:]
148/385: fid.shape
148/386: max(fid)
148/387: np.max(fid)
148/388:
import esda
import pandas as pd
import geopandas as gpd
from geopandas import GeoDataFrame
import libpysal as lps
import numpy as np
import matplotlib.pyplot as plt
from shapely.geometry import Point
148/389: %pip install esda
148/390:
import esda
import pandas as pd
import geopandas as gpd
from geopandas import GeoDataFrame
import libpysal as lps
import numpy as np
import matplotlib.pyplot as plt
from shapely.geometry import Point
148/391: %pip install geopandas
148/392:
import esda
import pandas as pd
import geopandas as gpd
from geopandas import GeoDataFrame
import libpysal as lps
import numpy as np
import matplotlib.pyplot as plt
from shapely.geometry import Point
148/393: import netCDF4
148/394:
root='l:/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/'
gdf = gpd.read_file(root+'twTown1982.geojson')
fname=root+'GRIDCRO2D.nc'
nc = netCDF4.Dataset(fname,'r+')
df=pd.DataFrame({'LAT':np.array(nc.variables['LAT'][0,0,:,:]).flatten(),'LON':np.array(nc.variables['LON'][0,0,:,:]).flatten()})
ngdf=len(gdf);ndf=len(df)
148/395:
root='/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/'
gdf = gpd.read_file(root+'twTown1982.geojson')
fname=root+'GRIDCRO2D.nc'
nc = netCDF4.Dataset(fname,'r+')
df=pd.DataFrame({'LAT':np.array(nc.variables['LAT'][0,0,:,:]).flatten(),'LON':np.array(nc.variables['LON'][0,0,:,:]).flatten()})
ngdf=len(gdf);ndf=len(df)
townid=[]
148/396: df.head()
148/397: gdf.head()
148/398: ls -lh *csv
148/399: !head gridLL.csv
148/400:
fnameO='/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/LGHAP.PM25.D001/N393_276.bin'
with FortranFile(fnameO, 'r') as f:
  nar = f.read_record(dtype=np.int)
148/401: yr='2005'
148/402: fnameO='A'+yr+'.nc'
148/403:
bdate=datetime.datetime(int(yr),1,1)
nd365=365
if int(yr)%4==0:nd365=366
dates=[datetime.datetime.strftime(bdate+datetime.timedelta(days=i),'%Y%m%d') for i in range(nd365)]
fnames=['LGHAP.PM25.D001.A'+i+'.nc' for i in dates]
nc = netCDF4.Dataset(fnameO,'r+')
v4=list(filter(lambda x:nc.variables[x].ndim==4, [i for i in nc.variables]))
nt,nlay,nrow,ncol=(nc.variables[v4[0]].shape[i] for i in range(4))
var=np.zeros(shape=(nd365,nrow,ncol))
148/404: pwd
148/405: cd LGHAP.PM25.D001
148/406:
bdate=datetime.datetime(int(yr),1,1)
nd365=365
if int(yr)%4==0:nd365=366
dates=[datetime.datetime.strftime(bdate+datetime.timedelta(days=i),'%Y%m%d') for i in range(nd365)]
fnames=['LGHAP.PM25.D001.A'+i+'.nc' for i in dates]
nc = netCDF4.Dataset(fnameO,'r+')
v4=list(filter(lambda x:nc.variables[x].ndim==4, [i for i in nc.variables]))
nt,nlay,nrow,ncol=(nc.variables[v4[0]].shape[i] for i in range(4))
var=np.zeros(shape=(nd365,nrow,ncol))
148/407: v4
148/408: fname=fnames[0]
148/409: t=0
148/410:
  nc0 = netCDF4.Dataset(fname,'r')
  var0=nc0['PM25'][:]
  var0=var0.T.flatten()
  var0=var0[nar]
  var[t,:,:]=var0.reshape(nrow,ncol)
  t+=1

var=np.where(var!=np.nan,var, -1)
148/411: mx=np.max(var)
148/412: mx
148/413: mx > 10000
148/414: if mx > 10000:var=np.where(var!=mx,var, -1)
148/415: nc.SDATE=int(yr)*1000+1
148/416: nc[v4[0]][:,0,:,:]=var[:,:,:]
148/417: fnameO
148/418: fname='tempTW.nc'
148/419: nc = netCDF4.Dataset(fname,'r+')
148/420: nc.VGLVLS
148/421: nc.VGLVLS=array([1.])
148/422: nc.VGLVLS=np.array([1.])
148/423: nc.close()
148/424: nc = netCDF4.Dataset(fname,'r+')
148/425: v
148/426: nc[v].units = "ug m-3          "
148/427: nc[v].long_name = "PM25_TOT        "
148/428: nc[v].scale_factor=1
148/429: nc.close()
148/430: nc = netCDF4.Dataset(fname,'r')
148/431: nc.NVARS
148/432: nc.close()
148/433: run rdPM25.py 2005
148/434: fnameO
148/435: nc = netCDF4.Dataset(fname,'r+')
148/436: nc = netCDF4.Dataset(fnameO,'r+')
148/437: v4
148/438:
nc = netCDF4.Dataset(fnameO,'r+')
nc.SDATE=int(yr)*1000+1
for t in range(nd365):
  nc['TFLAG'][t,:,0]=int(yr)*1000+1+t
nc['TFLAG'][:,:,1]=0
nc[v4[0]][:,0,:,:]=var[:,:,:]
nc.close()
148/439:
nc = netCDF4.Dataset(fnameO,'r+')
nc.SDATE=int(yr)*1000+1
for t in range(nd365):
  nc['TFLAG'][t,:,0]=int(yr)*1000+1+t
nc['TFLAG'][:,:,1]=0
nc[v4[0]][:,0,:,:]=var[:,:,:]
nc.close()
148/440:
nc = netCDF4.Dataset(fnameO,'r+')
nc.SDATE=int(yr)*1000+1
for t in range(nd365):
  nc['TFLAG'][t,:,0]=int(yr)*1000+1+t
nc['TFLAG'][:,:,1]=0
nc[v4[0]][:,0,:,:]=var[:,:,:]
nc.close()
148/441: os.system('cp tempTW.nc '+fnameO)
148/442:
nc = netCDF4.Dataset(fnameO,'r+')
nc.SDATE=int(yr)*1000+1
for t in range(nd365):
  nc['TFLAG'][t,:,0]=int(yr)*1000+1+t
nc['TFLAG'][:,:,1]=0
nc[v4[0]][:,0,:,:]=var[:,:,:]
nc.close()
148/443: !lst
148/444: rm A2005.nc
148/445: os.system('cp tempTW.nc '+fnameO)
148/446: !lst
148/447: A2005.nc
148/448: nd365
148/449:
nc = netCDF4.Dataset(fnameO,'r+')
nc.SDATE=int(yr)*1000+1
for t in range(nd365):
  nc['TFLAG'][t,:,0]=int(yr)*1000+1+t
nc['TFLAG'][:,:,1]=0
nc[v4[0]][:,0,:,:]=var[:,:,:]
nc.close()
148/450: gdf.head()
148/451: nc = netCDF4.Dataset(fnameO,'r')
148/452:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))

pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
nbnd1=len(x)
148/453: lon, lat = pnyc(x, y, inverse=True)
148/454: ndf=len(x)
148/455: ngdf
148/456:
townid=[]
for i in range(ndf):
  found=0
  for j in range(ngdf):
    if df.Point[i].within(gdf.geometry[j]):
      townid.append(gdf.TOWNSN[j])
      found=1
      break
  if found==0:townid.append(-1)
148/457: df=pd.DataFrame({'LAT':lat,'LON':lon})
148/458: ndf=len(df)
148/459:
townid=[]
for i in range(ndf):
  found=0
  for j in range(ngdf):
    if df.Point[i].within(gdf.geometry[j]):
      townid.append(gdf.TOWNSN[j])
      found=1
      break
  if found==0:townid.append(-1)
148/460: df['Point']=[Point(i,j) for i,j in zip(lon,lat)]
148/461:
townid=[]
for i in range(ndf):
  found=0
  for j in range(ngdf):
    if df.Point[i].within(gdf.geometry[j]):
      townid.append(gdf.TOWNSN[j])
      found=1
      break
  if found==0:townid.append(-1)
148/462: townid[:5]
148/463: root='/nas1/Data/GIS/TWN_town/'
148/464: gdf = gpd.read_file(root+'TOWN_MOI_1090727_geo.json')
148/465: ngdf=
148/466: ngdf=len(gdf);ndf=len(df)
148/467: ngdf
148/468: gdf.head()
148/469: len(set(gdf.TOWNCODE))
148/470:
townid=[]
for i in range(ndf):
  found=0
  for j in range(ngdf):
    if df.Point[i].within(gdf.geometry[j]):
      townid.append(gdf.TOWNCODE[j])
      found=1
      break
  if found==0:townid.append(-1)
148/471: df.head()
148/472: df['TOWNCODE']=townid
148/473: a=df.loc[df.TOWNCODE>0]
148/474: a.head()
148/475: a=df.loc[df.TOWNCODE.map(x: if type(x)==str)]
148/476: grep map ~/bin/*py|M
148/477: ~grep map ~/bin/*py|M
148/478: !grep map ~/bin/*py|M
148/479: a=df.loc[df.TOWNCODE.map(lambda x: if type(x)==str)]
148/480: a=df.loc[df.TOWNCODE.map(lambda x x: if type(x)==str)]
148/481: a=df.loc[df.TOWNCODE.map(lambda x : if type(x)==str)]
148/482: a=df.loc[df.TOWNCODE.map(lambda x :if type(x)==str)]
148/483: a=df.loc[df.TOWNCODE.map(lambda x :type(x)==str)]
148/484: a.head()
148/485: gdf.head()
148/486: df['COUNTYCODE']=[int(i)//1000 for i in df.TOWNCODE]
148/487: a=df.loc[df.TOWNCODE.map(lambda x :type(x)==str)]
148/488: a.head()
148/489: df.head()
148/490: tn={i:j fori,j in zip(gdf.TOWNCODE, gdf.TOWNNAME)}
148/491: tn={i:j for i,j in zip(gdf.TOWNCODE, gdf.TOWNNAME)}
148/492: cn={i:j for i,j in zip(gdf.COUNTYCODE, gdf.COUNTYNAME)}
148/493: cn.update('-1':'海')
148/494: cn.update({'-1':'海'})
148/495: tn.update({'-1':'海'})
148/496: df['COUNTYCODE']=[str(int(i)//1000) for i in df.TOWNCODE]
148/497: df['COUNTYNAME']=[cn[i] for i in df.COUNTYCODE]
148/498: gdf.loc[gdf.COUNTYCODE.map(lambda x:'9020' in x)]
148/499: df['TOWNCODE']=[str(i) for i in df.TOWNCODE]
148/500: df.loc[df.TOWNCODE=='-1']='00000000'
148/501: df['COUNTYCODE']=[i[:5] for i in df.TOWNCODE]
148/502: cn.update({'00000':'海'})
148/503: tn.update({'00000000':'海'})
148/504: df['COUNTYNAME']=[cn[i] for i in df.COUNTYCODE]
148/505: df['TOWNNAME']=[tn[i] for i in df.TOWNCODE]
148/506: df.head()
148/507: a=df.loc[df.TOWNCODE.map(lambda x :int(x)>0)]
148/508: a.head()
148/509: a.tail()
148/510: df.set_index('LAT').to_csv('gridLL.csv')
148/511: pwd
148/512: history -f his.txt
148/513: !vi mk_gridLL.py
148/514: run mk_gridLL.py
148/515: !v
148/516: !vi mk_gridLL.py
148/517: run mk_gridLL.py
148/518: import numpy as np
148/519: !vi mk_gridLL.py
148/520: run mk_gridLL.py
148/521: !vi mk_gridLL.py
148/522: run mk_gridLL.py
148/523: !vi mk_gridLL.py
148/524: gdf.head()
148/525: !vi mk_gridLL.py
148/526: !head mk_gridLL.csv
148/527: !head gridLL.csv
148/528: a.head()
148/529: run mk_gridLL.py
148/530: len('00000000')
148/531: len(tn)
148/532: '00000000' in tn
148/533: i
148/534: '00000000' in list(df.TOWNCODE)
148/535: df.head()
148/536: df[0]
148/537: df.loc[0]
148/538: df.TOWNCODE[0]
148/539: !vi mk_gridLL.py
148/540: df.loc[df.TOWNCODE=='00000000`']='00000000'
148/541: df['TOWNNAME']=[tn[i] for i in df.TOWNCODE]
148/542: df.set_index('LAT').to_csv('gridLL.csv2')
148/543: !diff gridLL.csv2 gridLL.csv
148/544: df.head()
148/545: X[:5]
148/546: Y[5:]
148/547: Y[-5:]
148/548: lat[:5]
148/549: lon[:5]
148/550: df.
148/551: df.head()
148/552: df.LAT.mean()
148/553: np.mean(list(df.LAT))
148/554: len(df)
148/555: a==pd.DataFrame({'LAT':lat,'LON':lon})
148/556: type(lat)
148/557: type(lon)
148/558: a=pd.DataFrame({'LAT':lat,'LON':lon})
148/559: a.head()
148/560: df['Point']=[Point(i,j) for i,j in zip(lon,lat)]
148/561: df['TOWNCODE']=townid
148/562:
tn={i:j for i,j in zip(gdf.TOWNCODE, gdf.TOWNNAME)}
cn={i:j for i,j in zip(gdf.COUNTYCODE, gdf.COUNTYNAME)}
df['COUNTYCODE']=[i[:5] for i in df.TOWNCODE]
cn.update({'00000':'海'})
tn.update({'00000000':'海'})
df['COUNTYNAME']=[cn[i] for i in df.COUNTYCODE]
df['TOWNNAME']=[tn[i] for i in df.TOWNCODE]
df.set_index('LAT').to_csv('gridLL.csv')
148/563: df.loc[df.TOWNCODE=='00000000`']='00000000'
148/564: df.head()
148/565: df=pd.DataFrame({'LAT':lat,'LON':lon})
148/566: df['Point']=[Point(i,j) for i,j in zip(lon,lat)]
148/567: df['TOWNCODE']=townid
148/568: df.loc[df.TOWNCODE=='00000000`','TOWNCODE']='00000000'
148/569: df.head()
148/570:
df['COUNTYCODE']=[i[:5] for i in df.TOWNCODE]
df['COUNTYNAME']=[cn[i] for i in df.COUNTYCODE]
df['TOWNNAME']=[tn[i] for i in df.TOWNCODE]
148/571: df.set_index('LAT').to_csv('gridLL.csv2')
148/572: !diff gridLL.csv2 gridLL.csv
148/573: !head gridLL.csv
148/574: run mk_gridLL.py
148/575: pwd
148/576: cd ..
148/577: !more n0r-t\ \(2\).cgi
148/578: pwd
148/579: !top
148/580: cd ~/bin
148/581: ls m3nc*
148/582: !diff m3nc2gif.py m3nc2gifP.py
148/583: sdate
148/584: bdate
148/585: sdate=bdate.strftime("%Y-%m-%d_%H:00Z")
148/586: sdate
148/587: y,m,d=[i+'/' for i in sdate.split('_')[0].split('-')]
148/588: ymd
148/589: y,m,d
148/590: os.system('mkdir -p pngs'+y+m+d)
148/591: lsd pngs
148/592: ls pngs
148/593: !lsd
148/594: rm -fr pngs2005/
148/595: v
148/596:
    tmp='_'+'{:03d}'.format(t)
    png=v+tmp+'.png'
    png2.replace(tmp,ymdh)+'.png'
148/597:
    tmp='_'+'{:03d}'.format(t)
    png=v+tmp+'.png'
    png2=png.replace(tmp,ymdh)+'.png'
148/598:     ymdh=sdate.split(':')[0]
148/599:
    tmp='_'+'{:03d}'.format(t)
    png=v+tmp+'.png'
    png2=png.replace(tmp,ymdh)+'.png'
148/600: png2
148/601:
    tmp='{:03d}'.format(t)
    png=v+'_'+tmp+'.png'
    png2=png.replace(tmp,ymdh)+'.png'
148/602: png2
148/603:
    y,m,d=[i+'/' for i in sdate.split('_')[0].split('-')]
    dir_ymd='pngs/'+y+m+d+'/'
148/604: dir_ymd
148/605:
    dir_ymd='pngs/'+y+m+d
    os.system('mkdir -p '+dir_ymd)
    os.system('cp '+png+' '+dir_ymd+png2)
148/606: tmp
148/607: !grep TZPP his.txt
148/608: cd /nas2/cmaqruns/2019TZPP/output/Annual/aTZPP
148/609: ls *nc
148/610: cd ..
148/611: ls
148/612: ls */*.nc*
148/613: cd /nas2/cmaqruns/2019TZPP/output/Annual/aTZPP
148/614: fname='A2019.ncT'
148/615: nc = netCDF4.Dataset(fname,'r')
148/616: pwd
148/617: cd LGHAP.PM25.D001
148/618: nc = netCDF4.Dataset(fname,'r')
148/619: v
148/620: var=nc['PM25_TOT'][0,0,:,:]
148/621: df.head()
148/622: len(df)
148/623: len(var.flatten())
148/624: ls his*
148/625: pwd
148/626: df['var']=var.flatten()
148/627: df_tm=pivot_table(df,index='TOWNCODE',values='var',aggfunc=np.mean).reset_index()
148/628: from pandas import *
148/629: df_tm=pivot_table(df,index='TOWNCODE',values='var',aggfunc=np.mean).reset_index()
148/630: df_tm.head()
148/631: df0=df.loc[df.var>0].reset_indexc(drop=True)
148/632: df0=df.loc[df['var']>0].reset_indexc(drop=True)
148/633: df0=df.loc[df['var']>0].reset_index(drop=True)
148/634: df_tm=pivot_table(df0,index='TOWNCODE',values='var',aggfunc=np.mean).reset_index()
148/635: df_tm.head()
148/636:
df_tm['COUNTYCODE']=[i[:5] for i in df_tm.TOWNCODE]
df_tm['COUNTYNAME']=[cn[i] for i in df_tm.COUNTYCODE]
df_tm['TOWNNAME']=[tn[i] for i in df_tm.TOWNCODE]
148/637: df_tm.head()
148/638: len(df_tm)
148/639: df_tm.set_index('TOWNCODE').to_csv(fname+'.csv')
148/640: df_tm.sort_values(on='var')
148/641: df_tm.sort_values('var')
148/642: df_tm.sort_values('var',ascendant=False)
148/643: a=df_tm.sort_values('var',ascending=False)
148/644: a.head()
148/645: history -f his2.txt
148/646: !vi his2.txt
148/647: a.set_index('TOWNCODE').to_csv(fname+'.csv')
148/648: df.head()
148/649: df2=read_csv('gridLL.csv')
148/650: df.h
148/651: df2.head()
148/652: df2.tail()
148/653: '{5d}'.format(0)
148/654: '{%5d}'.format(0)
148/655: '{:08d}'.format(0)
148/656: mp.max(df2.TOWNCODE)
148/657: np.max(df2.TOWNCODE)
148/658: pwd
148/659: cd ..
148/660: df=read_csv('TEDS2019.ncT.csv')
148/661: df.columns
148/662: df.columns=['TOWNCODE', 'TEDS', 'COUNTYCODE', 'COUNTYNAME', 'TOWNNAME']
148/663: df_tm
148/664: df_tm.column
148/665: df_tm.columns
148/666: df_tm.columns=['TOWNCODE', 'LGHAP', 'COUNTYCODE', 'COUNTYNAME', 'TOWNNAME']
148/667: a=merge(df_tm,df)
148/668: df.head()
148/669: df_tm=df_tm.sort_values('TOWNCODE')
148/670: df=df.sort_values('TOWNCODE')
148/671: len(df)
148/672: len(df_tm)
148/673: a=join(df,df_tm)
148/674: a=joint(df,df_tm)
148/675: dfT=merge(df,df_tm,on=['TOWNCODE'],how='inner')
148/676: df.head()
148/677:
df.TOWNCODE=['{:08d}'.format(i) for i in df.TOWNCODE]
df.COUNTYCODE=['{:05d}'.format(i) for i in df.COUNTYCODE]
148/678: a=joint(df,df_tm)
148/679: a=join(df,df_tm)
148/680: dfT=merge(df,df_tm,on=['TOWNCODE'],how='inner')
148/681: dfT.head()
148/682: a=[i==j for i,j in zip(list(dfT.TOWNNAME_x),list(TOWNNAME_y))]
148/683: a=[i==j for i,j in zip(list(dfT.TOWNNAME_x),list(dfT.TOWNNAME_y))]
148/684: set(a)
148/685: dfT=dfT.sort_values('LGHAP',asending=False)
148/686: dfT=dfT.sort_values('LGHAP',ascending=False)
148/687: dfT.head()
148/688: col='COUNTYNAME TOWNCODE LGHAP'
148/689: df_tm[col].set_index('TOWNCODE').to_csv(fname+'3.csv',header=None)
148/690: col='COUNTYNAME TOWNCODE LGHAP'.split()
148/691: df_tm[col].set_index('TOWNCODE').to_csv(fname+'3.csv',header=None)
148/692: pwd
148/693: df_tm.head()
148/694: df_tm[col].loc[1:].set_index('TOWNCODE').to_csv(fname+'3.csv',header=None)
148/695: col='COUNTYCODE TOWNCODE LGHAP'.split()
148/696: df_tm[col].loc[1:].set_index('TOWNCODE').to_csv(fname+'3.csv',header=None)
148/697: df_tm['tc2']=[i[-3:] for i in df_tm.TOWNCODE]
148/698: col='COUNTYCODE tc2 LGHAP'.split()
148/699: df_tm.head()
148/700: df_tm[col].loc[1:].set_index('COUNTYCODE').to_csv(fname+'3.csv',header=None)
148/701: pwd
148/702: (0.7-0.62)/0.62
148/703: (6.58-5.3)/5.3
148/704: pwd
149/1: from pandas import *
149/2: fname='gridLL.csv'
149/3: df=read_csv(fname)
149/4: df.head()
149/5: a=df.loc[df.COUNTYCODE==64000]
149/6: a.head()
149/7: len(set(a.TOWNCODE))
149/8: a.loc[a.TOWNCODE==64000380]
149/9: fname='teds2019pm25.nc.csv'
149/10: df=read_csv(fname)
149/11: df.head()
149/12: min(df.var)
149/13: np.min(df.var)
149/14: import numpy as np
149/15: np.min(df.var)
149/16: np.min(list(df.var))
149/17: min(list(df.var))
149/18: np.min(df['var'])
149/19: fname='/nas2/cmaqruns/2022fcst/grid03/mcip/METCRO3D.nc'
149/20: nc = netCDF4.Dataset(fname,'r')
149/21: import netCDF4
149/22: nc = netCDF4.Dataset(fname,'r')
149/23: v='ZF'
149/24: nc[v][0,:,0,0]
149/25: v
149/26: nc[v][0,:,-1,-1]
149/27: fname='/nas2/cmaqruns/2022fcst/grid03/mcip/METCRO2D.nc'
149/28: nc = netCDF4.Dataset(fname,'r')
149/29: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
149/30: V
149/31: fname='/nas2/cmaqruns/2022fcst/grid03/mcip/METCRO3D.nc'
149/32: nc = netCDF4.Dataset(fname,'r')
149/33: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
149/34: V
149/35:
import xarray as xr
import rioxarray as rio
149/36: !pip install rioxarray
149/37:
import xarray as xr
import rioxarray as rio
149/38: import rioxarray as rio
150/1:
import xarray as xr
import rioxarray as rio
151/1:
import xarray as xr
import rioxarray as rio
151/2: !pip3 install rioxarray
151/3:
import xarray as xr
import rioxarray as rio
151/4: fname='/nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/PM25.nc'
151/5: ncfile = xr.open_dataset(fname)
151/6: pr = ncfile['PM25_TOT']
151/7: pr.coord['LAT']
151/8: pr.coords['LAT']
151/9: ncfile
151/10: fname='/nas2/cmaqruns/2022fcst/grid03/mcip/GRIDCRO2D.nc'
151/11: nc = netCDF4.Dataset(fname,'r')
151/12: import netCDF4
151/13: nc = netCDF4.Dataset(fname,'r')
151/14: lat=nc['LAT'][0,0,:,:]
151/15: lon=nc['LON'][0,0,:,:]
151/16: ncfile
151/17: pr
151/18: dir(pr)
151/19: dir(pr.coords)
151/20: pr.coords['lon'].shape
151/21: dir(pr.coords)
151/22: pr.coords.update({'lon':lon,'lat':lat})
151/23: pr['PM25'].shape
151/24: pr['PM25_TOT'].shape
151/25: pr.shape
151/26: dir(pr.coords)
151/27: dir(pr.coords.dims)
151/28: pr.coords.dims.index
151/29: pr.coords.dims
151/30: lon2=np.zeros(shape=pr.shape)
151/31: import numpy as np
151/32: lon2=np.zeros(shape=pr.shape)
151/33: lat2=np.zeros(shape=pr.shape)
151/34: lon2=lon[None,None,:,:]
151/35: lat2=lat[None,None,:,:]
151/36: pr.coords.update({'lon':lon2,'lat':lat2})
151/37: pr.coords.update({'lon':lon.flatten(),'lat':lat.flatten()})
151/38: fname='/nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/PM25.nc'
151/39: nc = netCDF4.Dataset(fname,'r')
151/40: pm25=np.array(nc['PM25_TOT'][:])
151/41: da = xr.DataArray(pm25,coords=dict(lon=(["ROW","COL"],lon),lat=(["ROW","COL"],lat)))
151/42: da = xr.DataArray(pm25,coords=dict(lon=(["dim_2","dim_3"],lon),lat=(["dim_2","dim_3"],lat)))
151/43: da.coords['lon'][:5,:5]
151/44: da.coords['lon'][-5:,-5:]
151/45: da.coords['lat'][-5:,-5:]
151/46: pr=da.rio.set_spatial_dims('lon', 'lat')
151/47: da=xr.Dataset(data_vars=dict(pm=(["t","z","y","x"],pm25)),coords=dict(lon=(["y","x"],lon),lat=(["y","x"],lat)))
151/48: da.coords['lat'][-5:,-5:]
151/49: pr=pr.rio.set_spatial_dims('lon', 'lat')
151/50: pr=pr.rio.set_spatial_dims("y", "x")
151/51: pr=da.rio.set_spatial_dims("y", "x")
151/52: pr=da.rio.set_spatial_dims('lon', 'lat')
151/53: pr=da.rio.set_spatial_dims("y", "x")
151/54: pr.rio.crs
151/55: pr.rio.set_crs("epsg:4326")
151/56: fname='/nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/PM25.gtiff'
151/57: pr.rio.to_raster(fname)
151/58: pm25=np.array(nc['PM25_TOT'][0,0,:,:])
151/59: da=xr.Dataset(data_vars=dict(pm=(["t","z","y","x"],pm25)),coords=dict(lon=(["y","x"],lon),lat=(["y","x"],lat)))
151/60: da=xr.Dataset(data_vars=dict(pm=(["y","x"],pm25)),coords=dict(lon=(["y","x"],lon),lat=(["y","x"],lat)))
151/61: pr=da.rio.set_spatial_dims("y", "x")
151/62: pr.rio.to_raster(fname)
151/63: da=xr.Dataset(data_vars=dict(pm=(["x","y"],pm25.T)),coords=dict(lon=(["x","y"],lon.T),lat=(["x","y"],lat.T)))
151/64: pr=da.rio.set_spatial_dims("x", "y")
151/65: pr.rio.set_crs("epsg:4326")
151/66: fname
151/67: !lst /nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/
151/68: pr.rio.to_raster(fname)
151/69: da=xr.Dataset(data_vars=dict(pm=(["x","y"],pm25.T)),coords=dict(x=(["x","y"],lon.T),y=(["x","y"],lat.T)))
151/70: da=xr.Dataset(data=pm25.T,dims=["x","y"],coords=dict(x=(["x","y"],lon.T),y=(["x","y"],lat.T)))
151/71: da=xr.DataArray(data=pm25.T,dims=["x","y"],coords=dict(x=(["x","y"],lon.T),y=(["x","y"],lat.T)))
151/72: da=xr.Dataset(data_vars=dict(pm=(["x","y"],pm25.T)),coords=dict(lon=(["x","y"],lon.T),lat=(["x","y"],lat.T)))
151/73: pr=da.rio.set_spatial_dims("lon", "lat")
151/74: pr=da.rio.set_spatial_dims("x", "y")
151/75: pr.rio.set_crs("epsg:4326")
151/76: pr.rio.to_raster(fname)
151/77:
da0 = xr.DataArray(
      data = pm25.T,
      dims = ["x","y"],
      coords = dict(
            x = (["x","y"], lon.T),
            y = (["x","y"], lat.T)
      )
)
151/78: lon.shape
151/79: pr.x
151/80: pr.lat
153/1: import numpy as np
153/2: import netCDF4
153/3:
import xarray as xr
import rioxarray as rio
153/4: fname='/nas2/cmaqruns/2022fcst/grid03/mcip/GRIDCRO2D.nc'
153/5: nc = netCDF4.Dataset(fname,'r')
153/6: lat=nc['LAT'][0,0,:,:]
153/7: lon=nc['LON'][0,0,:,:]
153/8:
fname='/nas2/cmaqruns/2022fcst/grid03/mcip/GRIDCRO2D.nc'
nc = netCDF4.Dataset(fname,'r')
Y=nc['LAT'][0,0,:,:];X=nc['LON'][0,0,:,:]
mnX,mnY,mxX,mxY=(min(X),min(Y),max(X),max(Y))
153/9:
Y=list(nc['LAT'][0,0,:,:].flatten());X=list(nc['LON'][0,0,:,:].flatten())
mnX,mnY,mxX,mxY=(min(X),min(Y),max(X),max(Y))
153/10: fname='/nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/PM25.nc'
153/11:
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
var=nc[V[3][0]][0,0,:,:]
dx=(mxX-mnX)/ncol;dx=(mxY-mnY)/nrol
x1_1d=[mnX+dx*i for i in ncol]
y1_1d=[mnY+dy*i for i in nrol]
x1,y1=np.meshgrid(x1_1d, y1_1d)
x1=x1.flatten();y1=y1.flatten()
153/12:
y1_1d=[mnY+dy*i for i in nrow]
x1,y1=np.meshgrid(x1_1d, y1_1d)
x1=x1.flatten();y1=y1.flatten()
153/13:
x1_1d=[mnX+dx*i for i in range(ncol)]
y1_1d=[mnY+dy*i for i in range(nrow)]
x1,y1=np.meshgrid(x1_1d, y1_1d)
x1=x1.flatten();y1=y1.flatten()
153/14:
dx=(mxX-mnX)/ncol;dy=(mxY-mnY)/nrol
x1_1d=[mnX+dx*i for i in range(ncol)]
y1_1d=[mnY+dy*i for i in range(nrow)]
x1,y1=np.meshgrid(x1_1d, y1_1d)
x1=x1.flatten();y1=y1.flatten()
153/15:
dx=(mxX-mnX)/ncol;dy=(mxY-mnY)/nrow
x1_1d=[mnX+dx*i for i in range(ncol)]
y1_1d=[mnY+dy*i for i in range(nrow)]
x1,y1=np.meshgrid(x1_1d, y1_1d)
x1=x1.flatten();y1=y1.flatten()
153/16: xyc=[(i,j) for i,j in zip(x1,y1)]
153/17: xyc=[(i,j) for i,j in zip(X,Y)]
153/18: from scipy.interpolate import griddata
153/19: var1=griddata(xyc, var[:], (x1, y1), method='linear')
153/20: var=nc[V[3][0]][0,0,:,:].flatten()
153/21: var1=griddata(xyc, var[:], (x1, y1), method='linear')
153/22: var[:10]
153/23: var1[:10]
153/24: xyc[:10]
153/25: xyc0=[(i,j) for i,j in zip(X,Y)]
153/26: xyc0[:10]
153/27: xyc1=[(i,j) for i,j in zip(x1,y1)]
153/28: xyc1[:10]
153/29: fname='/nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/PM25.nc_eqLL'
153/30: nc = netCDF4.Dataset(fname,'r')
153/31: pm25=np.array(nc['PM25_TOT'][0,0,:,:])
153/32: np.mean(pm25)
153/33: np.mean(nc['PM25_TOT'][0,0,:,:])
153/34: pm25=np.where(pm25==np.nan,0,pm25)
153/35: np.mean(pm25)
153/36: pwd
153/37: cd /nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily
153/38: run /home/kuang/bin/nc2gtiff.py PM25.nc
153/39: pm25=np.array(nc['PM25_TOT'][0,0,:,:])
153/40: fname
153/41: fname='/nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/PM25.nc_eqLL'
153/42: nc = netCDF4.Dataset(fname,'r')
153/43: pm25=np.array(nc['PM25_TOT'][0,0,:,:])
153/44: pm25=np.where(np.isnan(pm25),0,pm25)
153/45: np.mean(pm25)
153/46: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
153/47:
with open('eqLL.txt','r') as f:
    xy=[i.strip('\n') for i in f]
153/48: da=xr.Dataset(data_vars=dict(pm=(["x","y"],pm25.T)),coords=dict(x=(["x","y"],xy[:ncol]),lat=(["y"],xy[ncol:])))
153/49: pm25.shape
153/50: da=xr.Dataset(data_vars=dict(pm=(["x","y"],pm25.T)),coords=dict(x=(["x","y"],xy[:ncol]),y=(["y"],xy[ncol:])))
153/51: da=xr.Dataset(data_vars=dict(pm=(["x","y"],pm25.T)),coords=dict(x=(["x"],xy[:ncol]),y=(["y"],xy[ncol:])))
153/52: pr=da.rio.set_spatial_dims("x", "y")
153/53: pr.rio.set_crs("epsg:4326")
153/54: fname='/nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/PM25.gtiff'
153/55: pr.rio.to_raster(fname)
153/56: da=xr.Dataset(data_vars=dict(pm=(["lon","lat"],pm25.T)),coords=dict(lon=(["lon"],xy[:ncol]),lat=(["lat"],xy[ncol:])))
153/57: pr=da.rio.set_spatial_dims("lon", "lat")
153/58: pr.rio.set_crs("epsg:4326")
153/59: pr.rio.to_raster(fname)
153/60: pr.coords['lon']
153/61: pr.coords['lon'].shape
153/62: ncol
153/63: nrow
153/64: da=xr.Dataset(data_vars=dict(pm=(["lat","lon"],pm25)),coords=dict(lon=(["lon"],xy[:ncol]),lat=(["lat"],xy[ncol:])))
153/65: pr=da.rio.set_spatial_dims("lon", "lat")
153/66: pr.rio.set_crs("epsg:4326")
153/67: pr.rio.to_raster(fname)
153/68: pr.rio.to_raster(fname,driver="COG")
153/69: pwd
153/70: mnX,mnY,mxX,mxY
153/71: !df -h&
153/72:
try: pync
except: print('pync is not defined')
154/1:
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
from matplotlib.cm import get_cmap
from matplotlib.colors import from_levels_and_colors
from cartopy import crs
from cartopy.feature import NaturalEarthFeature, COLORS
from netCDF4 import Dataset
from wrf import (getvar, to_np, get_cartopy, latlon_coords, vertcross,
                 cartopy_xlim, cartopy_ylim, interpline, CoordPair)
import sys, os
from datetime import datetime, timedelta
from pyproj import Proj
from bisect import bisect
import subprocess
154/2:
def get_lev(N):
  if mxv/mnv>15:
    dc=(np.log10(mxv)-np.log10(mnv))/15
    level=[round(10**(dc*i+np.log10(mnv)),N) for i in range(15)]
    nm=colors.LogNorm(vmin=level[3], vmax=level[-3])
  else:
    i=int('{:e}'.format(mxv)[0])
    if i==7:i=6
    dc=i*10**int(np.log10(mxv))/nlev[i]
    level=[round(dc*i,N) for i in range(nlev[i])]
    nm=colors.Normalize(vmin=mnv, vmax=mxv)
  return level,nm

CVT='/usr/bin/convert'
#2種▒~Z▒~@▒▒~V~S▒~Z~T▒~U▒
nlev={i:10 for i in [1,2,4,7,8]}
nlev.update({i:15 for i in [3,6,9]})
154/3: !lst
154/4: fname='PM10.nc'
154/5:
nc=Dataset(fname)
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
yj=nc['TFLAG'][0,0,0];t0=int(nc['TFLAG'][0,0,1]/10000)
bdate=datetime(yj//1000,1,1,t0)+timedelta(days=int(yj%1000-1))
try:
  yj=nc.CDATE;ih=nc.CTIME//10000;im=nc.CTIME%10000//100 #current in UTC
except:
  yj=nc.SDATE;ih=nc.STIME//10000;im=nc.STIME%10000//100 #current in UTC
cdate=datetime(yj//1000,1,1,t0,im)+timedelta(days=int(yj%1000-1))
cdate=(cdate+timedelta(hours=8)).strftime("%Y-%m-%d_%H:%M")

x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
154/6: v='PM10'
154/7: t=0
154/8: var=nc[v][t,0,:,:]
154/9: fnam='GRIDCRO2D.nc'
154/10: nc = netCDF4.Dataset(fname,'r')
154/11: nc=Dataset(fname,'r')
154/12: LON=nc['LON'][0,0,:,:]
154/13: LON=nc.variables['LON'][0,0,:,:]
154/14: fname='GRIDCRO2D.nc'
154/15: nc=Dataset(fname,'r')
154/16: LON=nc.variables['LON'][0,0,:,:]
154/17: LON[:5,:5]
154/18: lon[:5,:5]
154/19: lons[:5,:5]
154/20: LON[-5:,-5:]
154/21: lons[-5:,-5:]
154/22:
ncfile = Dataset('wrfout_d04')
p = getvar(ncfile, "pressure",timeidx=0)
cart_proj = get_cartopy(p)
154/23: dir(cart_proj)
154/24: cart_proj.boundary
154/25: cart_proj.boundary()
154/26: cart_proj.bounds
154/27: cart_proj.project_geometry
154/28: cart_proj.coordinate_system
154/29: cart_proj.coordinate_system()
154/30:
ncfile = Dataset(fname)
p = getvar(ncfile, "LON",timeidx=0)
cart_proj = get_cartopy(p)
154/31: ncfile = Dataset(fname)
154/32: cart_proj = get_cartopy(ncfile)
154/33: cart_proj = get_cartopy(wrfin=ncfile)
154/34:
wrfin = Dataset('wrfout_d04')
p = getvar(wrfin, "pressure",timeidx=0)
154/35: p.shape
154/36: p.coords
154/37: latm,lonm=getvar(wrfin,'lat'),getvar(wrfin,'lon')
154/38: latm.shape
154/39: from scipy.interpolate import griddata
154/40: var.shape
154/41: latm,lonm=getvar(wrfin,'lat').flatten(),getvar(wrfin,'lon').flatten()
154/42: latm,lonm=np.array(getvar(wrfin,'lat')).flatten(),np.array(getvar(wrfin,'lon')).flatten()
154/43:
nc=Dataset('GRIDCRO2D.nc')
lons,lats=(nc['LON'][0,0,:,:].flatten(),nc['LAT'][0,0,:,:].flatten())
154/44: xyc=[(i,j) for i,j in zip(lons,lats)]
154/45: y1,x1=np.array(getvar(wrfin,'lat')).flatten(),np.array(getvar(wrfin,'lon')).flatten()
154/46: var.shape
154/47: var=var.flatten()
154/48: var1=griddata(xyc, var[:], (x1, y1), method='linear')
154/49: p.shape
154/50: var1=var1.reshape(shape=p.shape)
154/51: var1=var1.reshape(p.shape)
154/52: cartopy_xlim(p)
155/1: run csv2dbf.py Header.csv tiffs_old.csv
155/2: line
155/3: body = csv.DictReader(open(body_file, 'rb'), delimiter=',', quotechar='"')
155/4: body_file='tiffs_old.csv'
155/5: body = csv.DictReader(open(body_file, 'rb'), delimiter=',', quotechar='"')
155/6: body
155/7:
    for line in body:
        rec = db.newRecord()
        #note: you need to reset the dict everytime
        header = csv.DictReader(open(header_file, 'rb'), delimiter=',')    
        for item in header:
            #print datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
            if (item['TYPE']=='D'):
                rec[item['FIELDNAME']] = line[item['FIELDNAME']]
            elif (item['TYPE']=='N'):
                rec[item['FIELDNAME']] = int(line[item['FIELDNAME']])
            else:    
                rec[item['FIELDNAME']] = line[item['FIELDNAME']]
        rec.store()
155/8: output_file='tiffs_old.dbf'
155/9:     db = dbf.Dbf(output_file, new=True)
155/10:
    for line in body:
        rec = db.newRecord()
        #note: you need to reset the dict everytime
        header = csv.DictReader(open(header_file, 'rb'), delimiter=',')    
        for item in header:
            #print datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
            if (item['TYPE']=='D'):
                rec[item['FIELDNAME']] = line[item['FIELDNAME']]
            elif (item['TYPE']=='N'):
                rec[item['FIELDNAME']] = int(line[item['FIELDNAME']])
            else:    
                rec[item['FIELDNAME']] = line[item['FIELDNAME']]
        rec.store()
155/11: header_file='Header.csv'
155/12:
    for line in body:
        rec = db.newRecord()
        #note: you need to reset the dict everytime
        header = csv.DictReader(open(header_file, 'rb'), delimiter=',')    
        for item in header:
            #print datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
            if (item['TYPE']=='D'):
                rec[item['FIELDNAME']] = line[item['FIELDNAME']]
            elif (item['TYPE']=='N'):
                rec[item['FIELDNAME']] = int(line[item['FIELDNAME']])
            else:    
                rec[item['FIELDNAME']] = line[item['FIELDNAME']]
        rec.store()
155/13: print db
155/14: !lst
155/15: db.close()
155/16: !lst
155/17: rec
155/18: cat tiffs_old.csv
155/19: !vi tif*csv
155/20: run csv2dbf.py Header.csv tiffs_old.csv
155/21: !vi tif*csv
155/22: run csv2dbf.py Header.csv tiffs_old.csv
155/23: !lst
155/24: !vi tif*csv
155/25: !vi H*csv
155/26: run csv2dbf.py Header.csv tiffs_old.csv
155/27: pwd
155/28: ls
155/29: db
155/30: :q
156/1: import nltk
157/1: import netCDF4
157/2: fname='BCON_yesterday_SECN_9k'
157/3: nc = netCDF4.Dataset(fname,'r')
157/4: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
157/5: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
157/6: nt,nlay,nrow=(nc[V[2][0]].shape[i] for i in range(3))
157/7: nt
157/8: t=24
157/9: v='NO2'
157/10: nc[v][t,0,0]
157/11: nc[v][t,0,:]
157/12: import numpy as np
157/13: np.sum(nc[v][t,:,:],axis=1)
157/14: t=23
157/15: np.sum(nc[v][t,:,:],axis=1)
157/16: t=24
157/17: t=22
157/18: np.sum(nc[v][t,:,:],axis=1)
157/19: v='TFLAG'
157/20: nc[v][:,0,0]
157/21: nc[v][:,0,1]
157/22: nc.NVARS
157/23: set(nc[v][:,:,0])
157/24: set(np.array(nc[v][:,:,0]).flatten())
157/25: set(np.array(nc[v][23,:,1]).flatten())
157/26: set(np.array(nc[v][24,:,1]).flatten())
157/27: set(np.array(nc[v][22,:,1]).flatten())
157/28: t=21
157/29: np.sum(nc[v][t,:,:],axis=1)
157/30: v='NO2'
157/31: np.sum(nc[v][t,:,:],axis=1)
158/1: import netCDF4
158/2: fname='temp.nc'
158/3: nc = netCDF4.Dataset(fname,'r+')
158/4: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
158/5: nt,nlay,nrow=(nc[V[2][0]].shape[i] for i in range(3))
158/6: nc.NROWS=431
158/7: for i in range(nc.NROWS)
158/8:
for i in range(nc.NROWS):
    nc[V[3][0]][:,:,i,:]=0.
158/9: !ncdump -h temp.nc|M
158/10:
for i in range(nc.NROWS):
    nc[V[3][0]][0,0,i,0]=0.
158/11: nrows
158/12: nrow
158/13: nt,nlay,nrow
159/1: import netCDF4
159/2: fname='temp.nc'
159/3: nc = netCDF4.Dataset(fname,'r+')
159/4: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
159/5: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
159/6: nt,nlay,nrow,ncol
159/7: nc.NROWS=431
159/8:
for i in range(nc.NROWS):
    nc[V[3][0]][i,:,:,:]=0.
159/9: nc.close()
159/10: :wq
160/1: fname='temp.nc'
160/2: import netCDF4
160/3: nc = netCDF4.Dataset(fname,'r+')
160/4: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
160/5: nc.NCOLS
160/6: nc.NCOLS=539
160/7:
for i in range(nc.NCOLS):
    nc[V[3][0]][i,:,:,:]=0.
160/8: nc.close()
161/1: import netCDF4
161/2: fname='temp.nc'
161/3: nc = netCDF4.Dataset(fname,'r+')
161/4: nc.NROWS=431
161/5:
for i in range(nc.NCOLS):
    nc[V[3][0]][i,:,:,:]=0.
161/6: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
161/7:
for i in range(nc.NROWS):
    nc[V[3][0]][i,:,:,:]=0.
161/8: nc.close()
162/1: import netCDF4
162/2: fname='temp.nc'
162/3: nc = netCDF4.Dataset(fname,'r+')
162/4: nc.NCOLS=539
162/5: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
162/6: nc.NLAYS
162/7:
for i in range(nc.NCOLS):
    nc[V[3][0]][i,:,:,:]=0.
162/8: nc.close()
163/1: import netCDF4
163/2: fname='../Annual/TEDS/20190101.nc'
163/3: nc = netCDF4.Dataset(fname,'r')
163/4:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
#new coordinates
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
ncXCELL=nc.XCELL
ncYCELL=nc.YCELL
163/5: from pyproj import Proj
163/6:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
#new coordinates
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
ncXCELL=nc.XCELL
ncYCELL=nc.YCELL
163/7: import numpy as np
163/8:
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
#new coordinates
x1d=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
ncXCELL=nc.XCELL
ncYCELL=nc.YCELL
163/9: from pandas import *
163/10: pwd
163/11: fname
163/12: !lst
163/13: fname='COMBINE_20190101.csv'
163/14: df=read_csv(fname,header=None)
163/15: df.head()
163/16: x.shape
163/17: x1.shape
163/18: xg=x1.flatten()[int(131*92/2)]
163/19: yg=y1.flatten()[int(131*92/2)]
163/20: xg,yg
163/21: xg=x1[int(131/2),int(92/2)]
163/22: yg=y1[int(131/2),int(92/2)]
163/23: xg,yg
163/24: ncol.nrow
163/25: ncol,nrow
163/26: nc.YORIG
163/27: nc.YORIG+nrow*nc.YCELL
163/28: df.head()
163/29: col='siteid,column,row,longitude,latitude,date,Time,NO2,O3,PM10,PM25_TOT,SO2,VOC'.split(',')
163/30: df.columns=col
163/31: df.head()
163/32: lon={i:j for i,j in zip(df.longitude,df.siteid)}
163/33: lat={i:j for i,j in zip(df.latitude,df.siteid)}
163/34: lon[1],lat[1]
163/35: lon["1"],lat["1"]
163/36: lon={i:j for j,i in zip(df.longitude,df.siteid)}
163/37: lat={j:i for i,j in zip(df.latitude,df.siteid)}
163/38: lon["1"],lat["1"]
163/39: lon[1],lat[1]
163/40: df.head()
163/41: siteid=set(df.siteid)
163/42: len(siteid)
163/43: len(lon)
163/44: siteid=list(set(df.siteid))
163/45: lon1d=[lon[i] for i in siteid]
163/46: lat1d=[lat[i] for i in siteid]
163/47: xs,ys=pnyc(lon1d, lat1d, inverse=False)
163/48: xs[:5],ys[:5]
163/49: type(xs)
163/50: lon1d=np.array([lon[i] for i in siteid])
163/51: lat1d=np.array([lat[i] for i in siteid])
163/52: xs,ys=pnyc(lon1d, lat1d, inverse=False)
163/53: type(xs)
163/54:
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
x1,y1=np.meshgrid(x1d,y1d)
163/55: xg=x1.flatten()[int(131*92/2)]
163/56: yg=y1[int(131/2),int(92/2)]
163/57: xg,yg
163/58: wg=1/((xs-xg)**2+(ys-yg)**2)
163/59: wg[:5]
163/60: s=sum(wg)
163/61: s
163/62: wg=wg/s
163/63: wg[:5]
163/64: sum(wg)
163/65: SO2g=nc['SO2'][0,0,int(131/2),int(92/2)]
163/66: SO2g
163/67: SO2g=nc['SO2'][0,0,int(131/2),int(92/2)].value
163/68: SO2g=nc['SO2'][0,0,int(131/2),int(92/2)].data
163/69: SO2
163/70: SO2g
163/71: SO2g=nc['SO2'][0,0,int(131/2),int(92/2)].data[0]
163/72: SO2g=nc['SO2'][0,0,int(131/2),int(92/2)].data.value
163/73: SO2g=nc['SO2'][0,0,int(131/2),int(92/2)].data.values
163/74: SO2g=nc['SO2'][0,0,int(131/2),int(92/2)].data.value()
163/75: SO2g=nc['SO2'][0,0,int(131/2),int(92/2)].data
163/76: SO2g-1
163/77: SO2g
163/78: SO2g=nc['SO2'][:,0,int(131/2),int(92/2)].data
163/79: SO2g
163/80: df.head()
163/81: SO2s=df.SO2.reshape(24,78)
163/82: SO2s=np.array(df.SO2).reshape(24,78)
163/83: Y=SO2s*wg[None,:]
163/84: Y=SO2s[:,:]*wg[None,:]
163/85: type(wg)
163/86: SO2s.shape
163/87: wg.shape
163/88: Y=np.zeros(shape=SO2s.shape)
163/89: Y[:,:]=SO2s[:,:]*wg[None,:]
163/90: Y[:,:]=SO2s[:,:]*1
163/91: SO2s[:5,:5]
163/92: SO2s=np.array(np.array(df.SO2).reshape(24,78),dtype=float)
163/93: SO2s=np.array(df.SO2,dtype=float).reshape(24,78)
163/94: SO2s=np.array(list(df.SO2),dtype=float).reshape(24,78)
163/95: SO2s=np.array([float(i) for i in df.SO2],dtype=float).reshape(24,78)
163/96: df.SO2
163/97: df.SO2=[float(i) for i in df.SO2]
163/98: a=df.sort_values('SO2')
163/99: a.head
163/100: df=df.loc['O3'!='m'].reset_index(drop=True)
163/101: df=df.loc[df.O3!='m'].reset_index(drop=True)
163/102: len(df)
163/103: siteid=list(set(df.siteid))
163/104: len(siteid)
163/105: lon1d=np.array([lon[i] for i in siteid])
163/106: lat1d=np.array([lat[i] for i in siteid])
163/107: xs,ys=pnyc(lon1d, lat1d, inverse=False)
163/108: wg=1/((xs-xg)**2+(ys-yg)**2)
163/109: s=sum(wg)
163/110: wg=wg/s
163/111: SO2s=np.array([float(i) for i in df.SO2],dtype=float).reshape(24,76)
163/112: Y=SO2s[:,:]*wg[None,:]
163/113: X[:5]
163/114: SO2g
163/115: from sklearn.model_selection import train_test_split
163/116: SO2s.shape
163/117: cols=['SO2_'+i for i in siteid]
163/118: cols=['SO2_'+str(i) for i in siteid]
163/119: D={cols[i]:SO2s[:,i] for i in range(76)}
163/120: dfs=DataFrmae(D)
163/121: dfs=DataFrame(D)
163/122: dfs.head()
163/123: X_train, X_test, y_train, y_test = train_test_split(dfs,SO2g, train_size=0.7, test_size=0.3, random_state=100)
163/124: X_train.head()
163/125:
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
163/126:
import warnings
warnings.filterwarnings('ignore')
163/127:
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
163/128:
from sklearn.feature_selection import RFE
rfe = RFE(logreg, 78)             
rfe = rfe.fit(X_train, y_train)
163/129: from sklearn.feature_selection import RFE
163/130: rfe = RFE(logreg, 78)
163/131: rfe = rfe.fit(X_train, y_train)
163/132: y_train
163/133: X_train
163/134: rfe = RFE(logreg, 15)
163/135: rfe = rfe.fit(X_train, y_train)
163/136: dir(RFE)
163/137: from datetime import datetime, timedelta
163/138: bdate=datetime(2019,1,1)
163/139: fname
163/140: history
163/141: pwd
163/142: history -f a.txt
163/143: !vi a.txt
163/144: nd=30
163/145: dates=[bdate+timedelta(days=i) for i in range(nd)]
163/146: dates
163/147: !grep str2 ~/bin/*py|H
163/148: !grep str ~/bin/*py|H
163/149: fnames=['../Annual/TEDS/'+dates[].strftime('%m%d%H')+'.nc' for i in range(nd)]
163/150: fnames=['../Annual/TEDS/'+dates[i].strftime('%m%d%H')+'.nc' for i in range(nd)]
163/151: fnames[:5]
163/152: fname
163/153: !vi a.txt
163/154: run reg.py
163/155: run reg.py
163/156: run reg.py
163/157: SO2g[:5]
163/158: SO2g[-5:]
163/159: SO2s[:5,:5]
163/160: SO2s[-5:,:5]
163/161: SO2s[-5:,-5:]
163/162: !vi a.txt
163/163:
cols=['SO2_'+str(i) for i in siteid]
D={cols[i]:SO2s[:,i] for i in range(76)}
dfs=DataFrame(D)
X_train, X_test, y_train, y_test = train_test_split(dfs,SO2g, train_size=0.7, test_size=0.3, random_state=100)
163/164: !vi a.txt
163/165:
logreg = LogisticRegression()
rfe = RFE(logreg, 15)
rfe = rfe.fit(X_train, y_train)
163/166: X_train.shape
163/167: y_tran.shape
163/168: y_train.shape
163/169:
logreg = LogisticRegression()
rfe = RFE(logreg, 76)
rfe = rfe.fit(X_train, y_train)
163/170:
from sklearn import preprocessing
from sklearn import utils
163/171: lab_enc = preprocessing.LabelEncoder()
163/172: encoded = lab_enc.fit_transform(y_train)
163/173: print(utils.multiclass.type_of_target(y_train))
163/174:
X_train_sm = sm.add_constant(X_train)
logm2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()
163/175: import statsmodels.api as sm
163/176:
X_train_sm = sm.add_constant(X_train)
logm2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()
163/177: X_test_sm = sm.add_constant(X_test)
163/178: X_test_sm.head()
163/179: y_test_pred = res.predict(sm.add_constant(X_test))
163/180: y_test_pred[:10]
163/181: y_train[:10]
163/182:
lab = preprocessing.LabelEncoder()
y_transformed = lab.fit_transform(y_train)
163/183: y_transformed[:5]
163/184: y_train[:5]
163/185:
classifier = LogisticRegression()
classifier.fit(X_train, y_transformed)
163/186: rfe = rfe.fit(X_train, y_transform)
163/187: rfe = rfe.fit(X_train, y_transformed)
163/188: list(zip(X_train.columns, rfe.support_, rfe.ranking_))
163/189: rfe = RFE(logreg, 15)
163/190: rfe = rfe.fit(X_train, y_transformed)
163/191: list(zip(X_train.columns, rfe.support_, rfe.ranking_))
163/192: col = X_train.columns[rfe.support_]
163/193: len(col)
163/194: X_train = X_train[col]
163/195:
X_train_sm = sm.add_constant(X_train)
logm2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()
163/196: from statsmodels.stats.outliers_influence import variance_inflation_factor
163/197:
vif = DataFrame()
vif['Features'] = X_train.columns
vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif
163/198: X_train_sm.head()
163/199: X_train.head()
163/200: y_train_pred = res.predict(sm.add_constant(X_train))
163/201: y_train_pred[:10]
163/202: y_train[:10]
163/203: y_train_pred = y_train_pred.values.reshape(-1)
163/204: y_train_pred_final = DataFrame({'SO2g':y_train.values, 'SO2g_prob':y_train_pred})
163/205: y_train_pred_final = DataFrame({'SO2g':y_train, 'SO2g_prob':y_train_pred})
163/206: y_train_pred_final.head()
163/207: y_train_pred_final['Predicted'] = y_train_pred_final.SO2g_prob.map(lambda x: 1 if x > 0.5 else 0)
163/208: y_train_pred_final.head()
163/209: y_train_pred_final.tail()
163/210:
from pysal.model import spreg
from pysal.lib import weights
from scipy import stats
import numpy as np
import pandas as pd
import geopandas as gpd
import seaborn as sns
import osmnx as ox
163/211: !pip install pysal
164/1:
from pysal.model import spreg
from pysal.lib import weights
from scipy import stats
import numpy as np
import pandas as pd
import geopandas as gpd
import seaborn as sns
import osmnx as ox
sns.set(style="whitegrid")
164/2: !pip install osmnx
164/3:
from pysal.model import spreg
from pysal.lib import weights
from scipy import stats
import numpy as np
import pandas as pd
import geopandas as gpd
import seaborn as sns
import osmnx as ox
sns.set(style="whitegrid")
164/4: import osmnx as ox
164/5: !conda install osmnx
164/6: %conda install osmnx
165/1:
from pysal.model import spreg
from pysal.lib import weights
from scipy import stats
import numpy as np
import pandas as pd
import geopandas as gpd
import seaborn as sns
import osmnx as ox
sns.set(style="whitegrid")
165/2: !pip install mgwr
165/3: !pip install pysal
165/4: !pip install numpy=1.22
165/5: !pip install numpy==1.22
165/6: %pip install pyproj==3.3.0
165/7: %pip install osmnx==1.2.0
165/8: import osmnx as ox
165/9:
from pysal.model import spreg
from pysal.lib import weights
from scipy import stats
import numpy as np
import pandas as pd
import geopandas as gpd
import seaborn as sns
import osmnx as ox
sns.set(style="whitegrid")
165/10: from pysal.model import spreg
165/11: %pip install pysal
166/1: from pysal.model import spreg
166/2:
from pysal.model import spreg
from pysal.lib import weights
from scipy import stats
import numpy as np
import pandas as pd
import geopandas as gpd
import seaborn as sns
import osmnx as ox
sns.set(style="whitegrid")
166/3: run reg.py
166/4: X_train.head()
166/5: y_train[:10]
166/6: w_pool = weights.KNN.from_dataframe(X_train, k=20)
166/7: X_train.head()
167/1:
import numpy as np
import geopandas as gpd
import contextily as ctx
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
167/2:
import numpy as np
import geopandas as gpd
import contextily as ctx
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
167/3:
import numpy as np
import geopandas as gpd
import contextily as ctx
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
167/4:
gdf = gpd.read_file("/home/kuang/bin/TWN_COUNTY.shp")
gdf.head()
167/5:
boundary = gpd.read_file(shp_fname)
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color=”gray”)
gdf.plot(ax=ax, markersize=3.5, color=”black”)
ax.axis(“off”)
plt.axis(‘equal’)
plt.show()
167/6:
boundary = gpd.read_file(shp_fname)
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
gdf.plot(ax=ax, markersize=3.5, color="black")
ax.axis("off")
plt.axis("equal")
plt.show()
167/7:
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
gdf.head()
167/8:
boundary = gpd.read_file(shp_fname)
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
gdf.plot(ax=ax, markersize=3.5, color="black")
ax.axis("off")
plt.axis("equal")
plt.show()
167/9:
boundary = gpd.read_file(shp_fname)
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
gdf.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/10:
boundary = gpd.read_file(shp_fname)
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
gdf.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/11:
boundary = boundary.to_crs(epsg=3826)
gdf_proj = gdf.to_crs(boundary.crs)
boundary_shape = cascaded_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/12:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = gdf.to_crs(boundary.crs)
boundary_shape = cascaded_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/13:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = gdf.to_crs(boundary.crs)
boundary_shape = unaray_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/14:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = gdf.to_crs(boundary.crs)
boundary_shape = cascaded_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/15: set([i for i in gdf.geometry])
167/16: set([type(i) for i in gdf.geometry])
167/17:
#boundary = boundary.to_crs(epsg=4326)
gdf_proj = gdf.to_crs(boundary.crs)
boundary_shape = cascaded_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/18: boundary.crs
167/19:
boundary = gpd.read_file(shp_fname)
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
gdf.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/20: boundary.crs
167/21:
#boundary = boundary.to_crs(epsg=4326)
gdf_proj = gdf.to_crs(boundary.crs)
boundary_shape = cascaded_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/22:
#boundary = boundary.to_crs(epsg=4326)
gdf_proj = gdf.to_crs(boundary.crs)
boundary_shape = cascaded_union(gdf.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/23: Point(118.26338652735485 24.425307404678911)
167/24: Point(118.26338652735485, 24.425307404678911)
167/25:
from shapely import Point
a=Point(118.26338652735485, 24.425307404678911)
print(a)
167/26: [i for i in gdf.geometry if a in i]
167/27:
i=gdf.geometry[0]
a in i
167/28:
i=gdf.geometry[0]
a in i[:]
167/29:
i=gdf.geometry[0]
a.within(i[:])
167/30:
i=gdf.geometry[0]
a.within(i)
167/31: [i for i in gdf.geometry if a.within(i)]
167/32: a
167/33:
import numpy as np
import geopandas as gpd
import contextily as ctx
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
167/34:
#boundary = boundary.to_crs(epsg=4326)
gdf_proj = gdf.to_crs(boundary.crs)
boundary_shape = unary_union(gdf.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/35:
#boundary = boundary.to_crs(epsg=4326)
gdf_proj = gdf.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/36: [i for i in boundary.geometry if a.within(i)]
167/37: area=[i.area for i in boundary.geometry]
167/38: area[:5]
167/39: area.index(max(area))
167/40: len(area)
167/41: gdf.loc[243]
167/42:
boundary = gpd.read_file(gdf)
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
gdf.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/43:
boundary = gpd.read_file(shp_fname)
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
gdf.loc[gdf.COUNTYNAME=="雲林縣"].plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/44:
CNTYNAM=set(gdf.COUNTYNAME)
print(CNTYNAM)
167/45:
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
print(CNTYNAM)
167/46:
df0=gpd.DataFrame({})
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a]
    i=area.index(max(area))
    df0.df0.append(a.loc[i],ignore_index=True)
167/47:
df0=gpd.GeoDataFrame({})
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a]
    i=area.index(max(area))
    df0.df0.append(a.loc[i],ignore_index=True)
167/48: a
167/49:
df0=gpd.GeoDataFrame({})
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    df0.df0.append(a.loc[imax],ignore_index=True)
167/50:
df0=gpd.GeoDataFrame({})
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    df0=df0.append(a.loc[imax],ignore_index=True)
167/51:
df0=gpd.GeoDataFrame({})
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    df0=gpd.concat([df0,a.loc[imax]])
167/52:
df0=gpd.GeoDataFrame({})
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    df0=pd.concat([df0,a.loc[imax]])
167/53:
import numpy as np
import geopandas as gpd
import pandas as pd
import contextily as ctx
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
167/54:
df0=gpd.GeoDataFrame({})
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    df0=pd.concat([df0,a.loc[imax]])
167/55: df0
167/56: df0.head()
167/57:
df0=pd.DataFrame({})
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    df0=df0.append(a.loc[imax],ignore_index=True)
167/58:
df0=pd.DataFrame()
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    df0=df0.append(a.loc[imax],ignore_index=True)
167/59: df0
167/60: type(df0)
167/61: a
167/62: b=df0.append(a.loc[imax],ignore_index=True)
167/63: dir(df0)
167/64:
df0=pd.DataFrame({})
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    df0=pd.concat([df0,a.loc[imax]], ignore_index=True)
167/65: df0.head()
167/66: a.head()
167/67:
df0=pd.DataFrame({})
ifirst=1
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=df0,a.loc[imax]
        ifirst=0
    else:
         df0=pd.concat([df0,a.loc[imax]], ignore_index=True)
167/68: a.head()
167/69: df0.head()
167/70:
ifirst=1
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=a.loc[imax]
        ifirst=0
    else:
         df0=gpd.concat([df0,a.loc[imax]], ignore_index=True)
167/71:
ifirst=1
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=a.loc[imax]
        ifirst=0
    else:
         df0=gpd.GeoDataFrame(pd.concat([df0,a.loc[imax]], ignore_index=True))
167/72: df0.head()
167/73: a
167/74: df0.head()
167/75:
ifirst=1
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=a.loc[imax]
        ifirst=0
    else:
        n=len(df0)
        df0.loc[n]=a.loc[imax]
167/76: df0.head()
167/77: a.loc[imax]
167/78: a.loc[imax][:]
167/79:
ifirst=1
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=a.loc[imax]
        ifirst=0
    else:
        n=len(df0)
        df0.loc[n][:]=a.loc[imax]
167/80:
ifirst=1
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=a.loc[imax]
        ifirst=0
    else:
        n=len(df0)
        for v in df0.columns:
            df0.loc[n][v]=a.loc[imax][v]
167/81:
ifirst=1
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=a.loc[imax]
        ifirst=0
    else:
        n=len(df0)
        for v in df0.columns[:]:
            df0.loc[n][v]=a.loc[imax][v]
167/82: type(df0)
167/83:
ifirst=1
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=a.loc[imax].reset_index(drop=True)
        ifirst=0
    else:
        n=len(df0)
        for v in df0.columns[:]:
            df0.loc[n][v]=a.loc[imax][v]
167/84: type(df0)
167/85:
ifirst=1
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=gpd.GeoDataFrame(a.loc[imax].reset_index(drop=True))
        ifirst=0
    else:
        n=len(df0)
        for v in df0.columns[:]:
            df0.loc[n][v]=a.loc[imax][v]
167/86: type(df0)
167/87:
ifirst=1
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=gpd.GeoDataFrame(a.loc[imax].reset_index(drop=True))
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,pd.GeoDataFrame(a.loc[imax].reset_index(drop=True))]))
167/88:
ifirst=1
for c in CNTYNAM:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=gpd.GeoDataFrame(a.loc[imax].reset_index(drop=True))
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,gpd.GeoDataFrame(a.loc[imax].reset_index(drop=True))]))
167/89: df0.head()
167/90: gpd.GeoDataFrame(a.loc[imax].reset_index(drop=True))
167/91:
ifirst=1
for c in list(CNTYNAM)[:1]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=gpd.GeoDataFrame(a.loc[imax].reset_index(drop=True))
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,gpd.GeoDataFrame(a.loc[imax].reset_index(drop=True))]))
167/92: df0
167/93:
ifirst=1
for c in list(CNTYNAM)[:1]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=a[imax].reset_index(drop=True))
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,gpd.GeoDataFrame(a.loc[imax].reset_index(drop=True))]))
167/94:
ifirst=1
for c in list(CNTYNAM)[:1]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if ifirst==1:
        df0=a[imax].reset_index(drop=True)
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,gpd.GeoDataFrame(a.loc[imax].reset_index(drop=True))]))
167/95: imax
167/96:
ifirst=1
for c in list(CNTYNAM)[:1]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==0:
        b=a
    else:
        b=a[imax].reset_index(drop=True)
    if ifirst==1:       
        df0=b
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b]))
167/97: imax
167/98:
ifirst=1
for c in list(CNTYNAM)[:1]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==0:
        b=a
    else:
        b=a[imax].reset_index(drop=True)
    if ifirst==1:       
        df0=b
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b]))
167/99: a
167/100: a[0]
167/101:
ifirst=1
for c in list(CNTYNAM)[:1]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==0:
        b=a
    else:
        b=a.loc[imax].reset_index(drop=True)
    if ifirst==1:       
        df0=b
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b]))
167/102: df0
167/103: a
167/104: b
167/105: len(a)==0
167/106: a
167/107:
ifirst=1
for c in list(CNTYNAM)[:1]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a
    else:
        b=a.loc[imax].reset_index(drop=True)
    if ifirst==1:       
        df0=b
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b]))
167/108: b
167/109: df0
167/110:
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a
    else:
        b=a.loc[imax].reset_index(drop=True)
    if ifirst==1:       
        df0=b
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b]))
167/111: df0.head()
167/112: b
167/113: a
167/114: df0
167/115:
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a
    else:
        b=a.loc[imax].reset_index(drop=True)
    if ifirst==1:       
        df0=b
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
167/116:
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a
    else:
        b=a.loc[imax].reset_index(drop=True)
    if ifirst==1:       
        df0=b
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True)
        df0=df0.to_crs(epsg=4326))
167/117:
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a
    else:
        b=a.loc[imax].reset_index(drop=True)
    if ifirst==1:       
        df0=b
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
        df0=df0.to_crs(epsg=4326))
167/118:
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a
    else:
        b=a.loc[imax].reset_index(drop=True)
    if ifirst==1:       
        df0=b
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
        df0=df0.to_crs(epsg=4326)
167/119: df0
167/120: b
167/121:
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a
    else:
        b=a.loc[imax].reset_index(drop=True)
    if ifirst==1:       
        df0=b
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True)).to_crs(epsg=4326)
167/122:
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:       
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
167/123:
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[imax,:]reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:       
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
167/124:
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[imax,:].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:       
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
167/125:
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:       
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
167/126: df0
167/127: boundary_shape = unary_union(df0.geometry)
167/128:
#boundary = boundary.to_crs(epsg=4326)
gdf_proj = gdf.to_crs(boundary.crs)
boundary_shape = unary_union(df0.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/129:
boundary = gpd.read_file(shp_fname)
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
gdf.loc[gdf.COUNTYNAME=="雲林縣"].plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/130:
#boundary = boundary.to_crs(epsg=4326)
gdf_proj = gdf.to_crs(boundary.crs)
boundary_shape = unary_union(df0.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/131: df0.to_file('mainisland.shp')
167/132:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df0.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/133:
#boundary = boundary.to_crs(epsg=4326)
gdf_proj = gdf.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/134:
#boundary = boundary.to_crs(epsg=4326)
gdf_proj = df0.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/135: boundary.crs
167/136:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df0.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/137: gdf_proj.head()
167/138: gdf_proj.crs
167/139:
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stn.head()
167/140:
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]
print(stnpnt[:5])
167/141: df0.head()
167/142:
for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]}).to_crs(epsg=4326)
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
167/143:
for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
167/144: df0.tail()
167/145: df0.to_file('mainisland.shp')
167/146: df0.to_file('mainisland.shp',mode='o')
167/147: df0.to_file('mainisland.shp',mode='w')
167/148:
df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
167/149: df0.loc[21:25]
167/150:
df1=df0.loc[:22]
df1.to_file('mainisland.shp',mode='w')
167/151: df1.tail()
167/152:
df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
167/153:
df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
167/154:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/155:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/156:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/157: type(boundary_shape)
167/158:
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if ! p.within(boundary_shape):
        df2=df2.drop(i)
167/159:
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape):
        df2=df2.drop(i)
167/160: p
167/161: i
167/162: df2.loc[i,'geometry']
167/163: df2.head()
167/164:
df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape):
        df2=df2.drop(i)
167/165:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/166:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/167: voronoi_regions_from_coords(coords, boundary_shape)
167/168:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, pts = voronoi_regions_from_coords(coords, boundary_shape)
167/169:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
poly_shapes, poly_to_pt_assignments = voronoi_regions_from_coords(coords, boundary_shape)
167/170: poly_to_pt_assignments
167/171: poly_shapes
167/172: df2.head()
167/173: df2.tail()
167/174: poly_to_pt_assignments
167/175:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, poly_shapes,  poly_to_pt_assignments)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/176:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, poly_shapes,  df2.geometry, poly_to_pt_assignments)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/177: df2.geometry[:5]
167/178: np.array(df2.geometry)[:5]
167/179: np.array(df2.geometry.coord)[:5]
167/180: np.array(df2.geometry.coords)[:5]
167/181: [i.coords for i in np.array(df2.geometry)[:5]]
167/182: [i.coords() for i in np.array(df2.geometry)[:5]]
167/183: dir(Point)
167/184: stnpnt[0].x
167/185: [(i.x,i.y) for i in np.array(df2.geometry)[:5]]
167/186: pnts=np.array([(i.x,i.y) for i in np.array(df2.geometry)])
167/187:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, poly_shapes, pnts, poly_to_pt_assignments)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/188: poly_shapes
167/189: poly_shapes.values
167/190: poly_shapes.values()
167/191: list(poly_shapes.values())
167/192:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, list(poly_shapes.values()), pts, poly_to_pt_assignments)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/193:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
167/194:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/195: boundary_shape
167/196: type(boundary_shape)
167/197: print(coords)
167/198: type(coords)
167/199: type(region_polys)
167/200: region_polys
167/201: region_pts
167/202: boundary_shape
167/203: print(boundary_shape)
167/204:
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if p.within(boundary_shape):continue
    print (i)
167/205: i,p
167/206:
for i in df2.index:
    p=df2.loc[i,'geometry']
    if p.within(boundary_shape):continue
    print (i)
167/207:
for i in df2.index:
    p=df2.loc[i,'geometry']
    if p.within(boundary_shape):continue
    print (i)
df2=df2.reset_index(drop=True)
167/208:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/209:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/210:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
167/211:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/212: boundary_shape.shape
167/213: len(boundary_shape)
167/214: dir(boundary_shape)
167/215:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary.geometry, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/216:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/217:
a=[]
ps=[p for p in boundary_shape]
for i in range(len(ps)-1):
    a.append((ps[i]+ps[i+1])/2.)
167/218:
a=[]
ps=[p for p in boundary_shape.coords]
for i in range(len(ps)-1):
    a.append((ps[i]+ps[i+1])/2.)
167/219: boundary_shape.coords
167/220: boundary_shape.coords()
167/221: boundary_shape
167/222: type(boundary_shape)
167/223: type(boundary_shape.length)
167/224: boundary_shape.length
167/225: print(boundary_shape.coords)
167/226: print(boundary_shape)
167/227: print(np.array(boundary_shape))
167/228: print(np.array(boundary_shape.coords))
167/229: boundary_shape.interiors
167/230: print(boundary_shape.interiors)
167/231: [i for i in boundary_shape.interiors]
167/232: [i for i in boundary_shape.exteriors]
167/233: dir(boundary_shape)
167/234: [i for i in boundary_shape.exterior]
167/235: [i for i in boundary_shape.exterior()]
167/236: [i for i in boundary_shape.xy]
167/237: [(i,j) for i,j in boundary_shape.xy]
167/238: print(boundary_shape.xy)
167/239: print(boundary_shape.xy())
167/240: print(boundary_shape.type)
167/241: print(boundary_shape.boundary)
167/242: print(boundary_shape.boundary.length)
167/243: print(np.array(boundary_shape.boundary))
167/244: a=[i for i in boundary_shape.boundary]
167/245: a=[i for i in boundary_shape.boundary.xy]
167/246: len(a)
167/247: a
167/248: a=[(i.x,i.y) for i in boundary_shape.boundary.xy]
167/249: a=[(i[0],i[1]) for i in boundary_shape.boundary.xy]
167/250: len(a)
167/251: a
167/252: a=[i for i in boundary_shape.boundary.xy]
167/253: b=[(i,j) for i,j in zip(a[0],a[1])]
167/254: len(b)
167/255: b[:5]
167/256:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/257:
for i in df2.index:
    p=df2.loc[i,'geometry']
    print(boundary_shape.exterior.distance(p))
167/258:
df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape) or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)    
print(len(df2))
167/259:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/260:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
167/261:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/262:
import numpy as np
import geopandas as gpd
import pandas as pd
import contextily as ctx
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
167/263:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/264:
import numpy as np
import geopandas as gpd
import pandas as pd
import contextily as ctx
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
167/265:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/266:
import numpy as np
import geopandas as gpd
import pandas as pd
import contextily as ctx
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
167/267:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/268:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, None, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/269:
import numpy as np
import geopandas as gpd
import pandas as pd
import contextily as ctx
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
167/270:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/271:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, [boundary_shape], region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/272:
import numpy as np
import geopandas as gpd
import pandas as pd
import contextily as ctx
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from shapely import Point, Polygon
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
167/273: Polygon[b]==boundary_shape
167/274: Polygon(b)=boundary_shape
167/275: Polygon(b)==boundary_shape
167/276:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, b, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/277:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, Polygon(b), region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/278:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, [Polygon(b)], region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/279: Polygon(b).to_crs(epsg=4326)==boundary_shape
167/280: Polygon(b)==boundary_shape
167/281: boundary_shape.crs
167/282: boundary_shape.project
167/283: Polygon(b).project==boundary_shape.project
167/284: Polygon(b).project=boundary_shape.project
167/285: Polygon(b).project
167/286: boundary_shape.project
167/287:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, Polygon(b), region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/288: world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
167/289: [i for i in world.name if 'taiwan' in a.lower()]
167/290: [i for i in world.name if 'taiwan' in i.lower()]
167/291:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
area.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/292:
area = world[world.name == 'Taiwan']
area = area.to_crs(epsg=3395)    # convert to World Mercator CRS
area_shape = area.iloc[0].geometry
167/293:
area = world[world.name == 'Taiwan']
area = area.to_crs(epsg=4326)    # convert to World Mercator CRS
area_shape = area.iloc[0].geometry
167/294:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
area.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/295: world = gpd.read_file(gpd.datasets.get_path('naturalearth_highres'))
167/296: type(area_shape)
167/297: world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
167/298: type(boundary_shape)
167/299:
area = world[world.name == 'Taiwan']
area = area.to_crs(epsg=4326)    # convert to World Mercator CRS
area_shape = area.iloc[0].geometry 
region_polys, region_pts = voronoi_regions_from_coords(coords, area_shape)
import matplotlib.pyplot as plt
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area

fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, area_shape, region_polys, coords, region_pts)
plt.show()
167/300:
df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape) or boundary_shape.exterior.distance(p) < 0.02:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)    
print(len(df2))
167/301:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
boundary_shape.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/302:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df0.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/303:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
167/304:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
167/305:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/306:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, [boundary_shape], region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
167/307:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, [boundary_shape])
167/308:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
168/1:
"""
Example script that scatters random points across a country and generates the Voronoi regions for them. Both the regions
and their points will be plotted using the `plotting` sub-module of `geovoronoi`. This example will show the effect
of calculating the Voronoi regions separately per country sub-geometry (e.g. separately for islands) vs. calculating
the Voronoi regions for the whole country shape together.

Author: Markus Konrad <markus.konrad@wzb.eu>
January 2021
"""


import logging
from pprint import pprint

import matplotlib.pyplot as plt
import numpy as np
import geopandas as gpd

from geovoronoi import coords_to_points, voronoi_regions_from_coords
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area

logging.basicConfig(level=logging.INFO)
geovoronoi_log = logging.getLogger('geovoronoi')
geovoronoi_log.setLevel(logging.INFO)
geovoronoi_log.propagate = True

#%%

N_POINTS = 105
COUNTRY = 'Italy'

np.random.seed(123)

print('loading country `%s` from naturalearth_lowres' % COUNTRY)
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
area = world[world.name == COUNTRY]
assert len(area) == 1

print('CRS:', area.crs)   # gives epsg:4326 -> WGS 84

area = area.to_crs(epsg=3395)    # convert to World Mercator CRS
area_shape = area.iloc[0].geometry   # get the Polygon

# generate some random points within the bounds
minx, miny, maxx, maxy = area_shape.bounds

randx = np.random.uniform(minx, maxx, N_POINTS)
randy = np.random.uniform(miny, maxy, N_POINTS)
coords = np.vstack((randx, randy)).T

# use only the points inside the geographic area
pts = [p for p in coords_to_points(coords) if p.within(area_shape)]  # converts to shapely Point
del coords   # not used any more

print('will use %d of %d randomly generated points that are inside geographic area' % (len(pts), N_POINTS))

#%%

#
# Calculate the Voronoi regions, cut them with the geographic area shape and assign the points to them.
# Note how in Sardinia there's only one point which is not assigned to any Voronoi region.
# Since by default all sub-geometries (i.e. islands or other isolated features) in the geographic shape
# are treated separately (set `per_geom=False` to change this) and you need more than one point to generate
# a Voronoi region, this point is left unassigned. By setting `return_unassigned_points=True`, we can get a
# set of unassigned point indices:
#

region_polys, region_pts, unassigned_pts = voronoi_regions_from_coords(pts, area_shape,
                                                                       return_unassigned_points=True,
                                                                       per_geom=True)  # this is the default

print('Voronoi region to point assignments:')
pprint(region_pts)

print('Unassigned points:')
for i_pt in unassigned_pts:
    print('#%d: %.2f, %.2f' % (i_pt, pts[i_pt].x, pts[i_pt].y))

#%% plotting

fig, ax = subplot_for_map()

plot_voronoi_polys_with_points_in_area(ax, area_shape, region_polys, pts, region_pts,
                                       point_labels=list(map(str, range(len(pts)))))

ax.set_title('%d random points and their Voronoi regions in %s' % (len(pts), COUNTRY))

plt.tight_layout()
plt.savefig('random_points_across_italy.png')
plt.show()


#%%

# Now we change `per_geom` to False. Since all geometries of the country are treated as one during Voronoi region
# generation, also the single point on Sardinia gets a Voronoi region. Note however, that these regions now can
# span over all geometries, e.g. regions from an island can span over to the mainland or another island and vice versa.
# You can see this effect for point #39 as its region spans from Sicilia to the "tiptoe" of Italy's mainland.

region_polys2, region_pts2 = voronoi_regions_from_coords(pts, area_shape, per_geom=False)

print('Voronoi region to point assignments:')
pprint(region_pts)


#%% plotting

fig, ax = subplot_for_map()

plot_voronoi_polys_with_points_in_area(ax, area_shape, region_polys2, pts, region_pts2,
                                       point_labels=list(map(str, range(len(pts)))))

ax.set_title('%d random points and their Voronoi regions in %s (with per_geom=False)' % (len(pts), COUNTRY))

plt.tight_layout()
plt.savefig('random_points_across_italy_per_geom_false.png')
plt.show()
168/2: %pip install descartes
169/1:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from shapely.geometry import Point
import geopandas as gpd
import folium

from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords

url = 'https://en.wikipedia.org/wiki/List_of_Toronto_subway_stations'
existing = pd.read_html(url)[1]

response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
table = soup.find_all('table')[1]

links = []
for tr in table.findAll("tr"):
  trs = tr.findAll("th", scope='row')
  for each in trs:
    try:
      link = each.find('a')['href']
      links.append(link)
    except:
      pass

existing['Link'] = links

base_url = 'https://en.wikipedia.org'

def get_coords(link):
  r = requests.get(base_url + link)
  soup = BeautifulSoup(r.text, 'html.parser')
  coordstring = soup.find(class_="geo").text.strip()
  tuplestring = tuple(map(float, coordstring.split(';')))
  return (tuplestring[1], tuplestring[0])

existing['coords'] = existing['Link'].apply(get_coords)
169/2: %pip install folium
169/3:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from shapely.geometry import Point
import geopandas as gpd
import folium

from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords

url = 'https://en.wikipedia.org/wiki/List_of_Toronto_subway_stations'
existing = pd.read_html(url)[1]

response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
table = soup.find_all('table')[1]

links = []
for tr in table.findAll("tr"):
  trs = tr.findAll("th", scope='row')
  for each in trs:
    try:
      link = each.find('a')['href']
      links.append(link)
    except:
      pass

existing['Link'] = links

base_url = 'https://en.wikipedia.org'

def get_coords(link):
  r = requests.get(base_url + link)
  soup = BeautifulSoup(r.text, 'html.parser')
  coordstring = soup.find(class_="geo").text.strip()
  tuplestring = tuple(map(float, coordstring.split(';')))
  return (tuplestring[1], tuplestring[0])

existing['coords'] = existing['Link'].apply(get_coords)
169/4: %pip install lxml
169/5:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from shapely.geometry import Point
import geopandas as gpd
import folium

from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords

url = 'https://en.wikipedia.org/wiki/List_of_Toronto_subway_stations'
existing = pd.read_html(url)[1]

response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
table = soup.find_all('table')[1]

links = []
for tr in table.findAll("tr"):
  trs = tr.findAll("th", scope='row')
  for each in trs:
    try:
      link = each.find('a')['href']
      links.append(link)
    except:
      pass

existing['Link'] = links

base_url = 'https://en.wikipedia.org'

def get_coords(link):
  r = requests.get(base_url + link)
  soup = BeautifulSoup(r.text, 'html.parser')
  coordstring = soup.find(class_="geo").text.strip()
  tuplestring = tuple(map(float, coordstring.split(';')))
  return (tuplestring[1], tuplestring[0])

existing['coords'] = existing['Link'].apply(get_coords)
169/6:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from shapely.geometry import Point
import geopandas as gpd
import folium

from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords

url = 'https://en.wikipedia.org/wiki/List_of_Toronto_subway_stations'
existing = pd.read_html(url)[1]

response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
table = soup.find_all('table')[1]

links = []
for tr in table.findAll("tr"):
  trs = tr.findAll("th", scope='row')
  for each in trs:
    try:
      link = each.find('a')['href']
      links.append(link)
    except:
      pass

existing['Link'] = links

base_url = 'https://en.wikipedia.org'

def get_coords(link):
  r = requests.get(base_url + link)
  soup = BeautifulSoup(r.text, 'html.parser')
  coordstring = soup.find(class_="geo").text.strip()
  tuplestring = tuple(map(float, coordstring.split(';')))
  return (tuplestring[1], tuplestring[0])

existing['coords'] = existing['Link'].apply(get_coords)
169/7:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from shapely.geometry import Point
import geopandas as gpd
import folium

from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords

url = 'https://en.wikipedia.org/wiki/List_of_Toronto_subway_stations'
existing = pd.read_html(url)[1]

response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
table = soup.find_all('table')[1]

links = []
for tr in table.findAll("tr"):
  trs = tr.findAll("th", scope='row')
  for each in trs:
    try:
      link = each.find('a')['href']
      links.append(link)
    except:
      pass

existing['Link'] = links

base_url = 'https://en.wikipedia.org'

def get_coords(link):
  r = requests.get(base_url + link)
  soup = BeautifulSoup(r.text, 'html.parser')
  coordstring = soup.find(class_="geo").text.strip()
  tuplestring = tuple(map(float, coordstring.split(';')))
  return (tuplestring[1], tuplestring[0])

existing['coords'] = existing['Link'].apply(get_coords)
171/1:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from shapely.geometry import Point
import geopandas as gpd
import folium

from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords

url = 'https://en.wikipedia.org/wiki/List_of_Toronto_subway_stations'
existing = pd.read_html(url)[1]

response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
table = soup.find_all('table')[1]

links = []
for tr in table.findAll("tr"):
  trs = tr.findAll("th", scope='row')
  for each in trs:
    try:
      link = each.find('a')['href']
      links.append(link)
    except:
      pass

existing['Link'] = links

base_url = 'https://en.wikipedia.org'

def get_coords(link):
  r = requests.get(base_url + link)
  soup = BeautifulSoup(r.text, 'html.parser')
  coordstring = soup.find(class_="geo").text.strip()
  tuplestring = tuple(map(float, coordstring.split(';')))
  return (tuplestring[1], tuplestring[0])

existing['coords'] = existing['Link'].apply(get_coords)
171/2: existing.head()
171/3:
#Getting & Assigning Colours:
COLurl = 'https://en.wikipedia.org/wiki/Template:TTC_color'
 colorHash = pd.read_html(COLurl)[1]
 websafeCOLS = {colorHash['Color.2'][i]:
 colorHash['Web-safeHexadecimal.1'][i] for i in colorHash}
 colorHash = colorHash[1:6]
 colorHash = colorHash['Color.2'].to_dict()

#Scraper for line number:
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
table = soup.find_all('table')[1]

lines = []
for tr in table.findAll("tr"):
  trs = tr.findAll("td")
 for each in trs:
     try:
        link = each.find('a')['href']
        if link.split('/wiki/')[1][0:4] == 'Line':
            lines.append(link)
     except:
         pass
         
existing['LineLink'] = lines[1:]

#Next we can extract the line number from the URL link:

def extractLineNO(LineLink):
    return int(LineLink.split('/wiki/Line_')[1][0])
 
existing['LineNO'] = existing['LineLink'].apply(extractLineNO)
171/5:
#Getting & Assigning Colours:
COLurl = 'https://en.wikipedia.org/wiki/Template:TTC_color'
 colorHash = pd.read_html(COLurl)[1]
 websafeCOLS = {colorHash['Color.2'][i]:
 colorHash['Web-safeHexadecimal.1'][i] for i in colorHash}
 colorHash = colorHash[1:6]
 colorHash = colorHash['Color.2'].to_dict()

#Scraper for line number:
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
table = soup.find_all('table')[1]

lines = []
for tr in table.findAll("tr"):
    trs = tr.findAll("td")
    for each in trs:
        try:
            link = each.find('a')['href']
            if link.split('/wiki/')[1][0:4] == 'Line':
                lines.append(link)
        except:
            pass
         
existing['LineLink'] = lines[1:]

#Next we can extract the line number from the URL link:

def extractLineNO(LineLink):
    return int(LineLink.split('/wiki/Line_')[1][0])
 
existing['LineNO'] = existing['LineLink'].apply(extractLineNO)
171/6:
#Getting & Assigning Colours:
COLurl = 'https://en.wikipedia.org/wiki/Template:TTC_color'
colorHash = pd.read_html(COLurl)[1]
websafeCOLS = {colorHash['Color.2'][i]:
colorHash['Web-safeHexadecimal.1'][i] for i in colorHash}
colorHash = colorHash[1:6]
colorHash = colorHash['Color.2'].to_dict()

#Scraper for line number:
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
table = soup.find_all('table')[1]

lines = []
for tr in table.findAll("tr"):
    trs = tr.findAll("td")
    for each in trs:
        try:
            link = each.find('a')['href']
            if link.split('/wiki/')[1][0:4] == 'Line':
                lines.append(link)
        except:
            pass
         
existing['LineLink'] = lines[1:]

#Next we can extract the line number from the URL link:

def extractLineNO(LineLink):
    return int(LineLink.split('/wiki/Line_')[1][0])
 
existing['LineNO'] = existing['LineLink'].apply(extractLineNO)
171/7:
#Scraper for line number:
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
table = soup.find_all('table')[1]

lines = []
for tr in table.findAll("tr"):
    trs = tr.findAll("td")
    for each in trs:
        try:
            link = each.find('a')['href']
            if link.split('/wiki/')[1][0:4] == 'Line':
                lines.append(link)
        except:
            pass
         
existing['LineLink'] = lines[1:]

#Next we can extract the line number from the URL link:

def extractLineNO(LineLink):
    return int(LineLink.split('/wiki/Line_')[1][0])
 
existing['LineNO'] = existing['LineLink'].apply(extractLineNO)
174/1:
import numpy as np
import geopandas as gpd
import pandas as pd
import contextily as ctx
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from shapely import Point, Polygon
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
174/2: %pip install contextily
174/3:
import numpy as np
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from shapely import Point, Polygon
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
174/4:
import numpy as np
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point, Polygon
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
174/5:
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
gdf.head()
174/6:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
174/7:
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
print(CNTYNAM)
174/8:
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:       
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:        
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
174/9:
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]
print(stnpnt[:5])
174/10:
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]
print(stnpnt[:5])
174/11:
df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape) or boundary_shape.exterior.distance(p) < 0.02:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)    
print(len(df2))
174/12:
df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
174/13:
df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape) or boundary_shape.exterior.distance(p) < 0.02:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)    
print(len(df2))
174/14:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
174/15:
df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape) or boundary_shape.exterior.distance(p) < 0.02:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)    
print(len(df2))
174/16:
for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
174/17:
df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
174/18: len(df1),len(df2)
174/19:
df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape) or boundary_shape.exterior.distance(p) < 0.02:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)    
print(len(df2))
174/20:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
174/21:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
174/22:
fig, ax = subplot_for_map()
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Schools in Uppsala')
plt.tight_layout()
plt.show()
174/23:
df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
174/24:
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
174/25:
boundary = boundary.to_crs(epsg=4326)
gdf_proj = df2.to_crs(boundary.crs)
boundary_shape = unary_union(boundary.geometry)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
175/1: run voronoi.py
175/2: run voronoi.py
175/3: run voronoi.py
175/4: run voronoi.py
175/5:
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
175/6: region_pts
175/7: coords
175/8: region_polys
175/9: len(df2)
175/10: df2.head()
175/11: gdf.head()
175/12: df2.head()
175/13:
dfv=df2[:]
region_pts
175/14:
dfv=df2[:]
for i in region_pts:
    dfv.geometry[region_pts[i][0]]=region_polys[i]
175/15:
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
dfv.loc[dfv.COUNTYNAME=='Keelung'].plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
175/16:
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
dfv.loc[dfv.COUNTYNAME=='Tucheng'].plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
175/17:
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
dfv.loc[dfv.COUNTYNAME=='Hualian'].plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
175/18: df2
175/19:
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
dfv.loc[dfv.COUNTYNAME=='Puli'].plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
175/20:
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
dfv.loc[dfv.COUNTYNAME=='Alishan'].plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
175/21:
dfv=df2[:]
for i in region_pts:
    dfv.geometry[region_pts[i][0]]=region_polys[i]
dfv = dfv.to_crs(epsg=4326)
dfv.to_file('vor_stn.shp',mode='w')
175/22: dfv
175/23: ll=pd.read_csv('/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/LGHAP.PM25.D001/gridLL.csv')
175/24:
ll=pd.read_csv('/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/LGHAP.PM25.D001/gridLL.csv')
ll.head()
175/25:
ll['AQID']=0
for i in range(len(ll)):
    p=ll.Point[i]
    if not p.within(boundary_shape):continue
    j=0
    for b in dfv.geometry:
        if p.within(b):
            ll.AQID=dfv.COUNTYSN[j]
            break
        j+=1
175/26: p
175/27: [float(i) for i in p.replace('(','').strip(')').split()[1:]]
175/28: Point([float(i) for i in p.replace('(','').strip(')').split()[1:]])
175/29:
ll['AQID']=0
for i in range(len(ll)):
    p=ll.Point[i]
    p=Point([float(i) for i in p.replace('(','').strip(')').split()[1:]])
    if not p.within(boundary_shape):continue
    j=0
    for b in dfv.geometry:
        if p.within(b):
            ll.AQID=dfv.COUNTYSN[j]
            break
        j+=1
175/30:
a=ll.loc[ll.AQID!=0]
len(a)
176/1: import netCDF4
175/31:
a=ll.loc[ll.AQID!=0]
a.head()
175/32:
import netCDF4
nc = netCDF4.Dataset(fname,'r+')
nc['PM25_TOT'][:]=np.array(ll.AQID)
175/33: %pip install netCDF4
175/34: %pip install netCDF4
175/35: %pip install hdf5
175/36: %pip install h5py
175/37: %pip install netcdf4
175/38: !export HDF5_DIR=/opt/anaconda3/envs/py36/include;pip install netcdf4
175/39: ll.set_index('LAT').to_csv('gridLLvor.csv')
176/2: cd /nas2/cmaqruns/2022fcst/fusion/Voronoi
176/3: fname='tempTW.nc'
176/4: from pandas import *
176/5: nc = netCDF4.Dataset(fname,'r+')
176/6: df=read_csv('gridLLvor.csv')
176/7: df.head()
176/8: len(df)
176/9: set(df.AQID)
175/40: p
175/41: b
175/42: p.within(b)
175/43: j
175/44: dfv.COUNTYSN[j]
175/45: dfv.COUNTYSN[3]
175/46:
ll['AQID']=0
for i in range(len(ll)):
    p=ll.Point[i]
    p=Point([float(i) for i in p.replace('(','').strip(')').split()[1:]])
    if not p.within(boundary_shape):continue
    j=0
    for b in dfv.geometry:
        if p.within(b):
            ll.AQID[i]=dfv.COUNTYSN[j]
            break
        j+=1
175/47:
a=ll.loc[ll.AQID!=0]
a.head()
176/10: nc.close()
175/48:
import netCDF4
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r+')
nc['PM25_TOT'][:]=np.array(ll.AQID)
175/49:
import netCDF4
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r+')
nc['PM25_TOT'][0,0,:,:]=np.array(ll.AQID).reshape(393,276)
175/50: nc.close()
175/51: ll.set_index('LAT').to_csv('gridLLvor.csv')
177/1:
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
177/2: %pip install tensorflow
177/3:
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
177/4:
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
177/5:
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])
177/6:
predictions = model(x_train[:1]).numpy()
predictions
177/7: x_train[:1]
177/8: tf.nn.softmax(predictions).numpy()
177/9: loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
177/10: loss_fn(y_train[:1], predictions).numpy()
177/11:
model.compile(optimizer='adam',
              loss=loss_fn,
              metrics=['accuracy'])
177/12: model.fit(x_train, y_train, epochs=5)
177/13: model.evaluate(x_test,  y_test, verbose=2)
177/14:
probability_model = tf.keras.Sequential([
  model,
  tf.keras.layers.Softmax()
])
probability_model(x_test[:5])
177/15: x_test[:5]
177/16: x_test[:5].shape
177/17: probability_model(x_test[:5]).shape
177/18:
from __future__ import print_function
import gzip
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import numpy as np
import os
import shutil
import struct
import sys
177/19:
try: 
    from urllib.request import urlretrieve 
except ImportError: 
    from urllib import urlretrieve
%matplotlib inline
177/20:
def loadData(src, cimg):
    print ('Downloading ' + src)
    gzfname, h = urlretrieve(src, './delete.me')
    print ('Done.')
    
    try:
        with gzip.open(gzfname) as gz:
            n = struct.unpack('I', gz.read(4))
            
            # 判斷是否為 MNIST 資料集
            if n[0] != 0x3080000:
                raise Exception('Invalid file: unexpected magic number.')
                
            # 計算資料筆數
            n = struct.unpack('>I', gz.read(4))[0]
            
            if n != cimg:
                raise Exception('Invalid file: expected {0} entries.'.format(cimg))
                
            crow = struct.unpack('>I', gz.read(4))[0]
            ccol = struct.unpack('>I', gz.read(4))[0]
            
            if crow != 28 or ccol != 28:
                raise Exception('Invalid file: expected 28 rows/cols per image.')
                
            # 讀取資料
            res = np.fromstring(gz.read(cimg * crow * ccol), dtype = np.uint8)
    finally:
        os.remove(gzfname)
    return res.reshape((cimg, crow * ccol))
def loadLabels(src, cimg):
    print ('Downloading ' + src)
    gzfname, h = urlretrieve(src, './delete.me')
    print ('Done.')
    try:
        with gzip.open(gzfname) as gz:        
            n = struct.unpack('I', gz.read(4))
            
            # 判斷是否為 MNIST 資料集
            if n[0] != 0x1080000:
                raise Exception('Invalid file: unexpected magic number.')
                
            # 計算資料筆數
            n = struct.unpack('>I', gz.read(4))
            
            if n[0] != cimg:
                raise Exception('Invalid file: expected {0} rows.'.format(cimg))
                
            # 讀取資料
            res = np.fromstring(gz.read(cimg), dtype = np.uint8)
    finally:
        os.remove(gzfname)
    return res.reshape((cimg, 1))
def try_download(dataSrc, labelsSrc, cimg):

    data = loadData(dataSrc, cimg)
    labels = loadLabels(labelsSrc, cimg)
    
    return np.hstack((data, labels))
177/21:
# 訓練資料集：圖像及標籤資料
url_train_image = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'
url_train_labels = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'
num_train_samples = 60000

print("Downloading train data")
train = try_download(url_train_image, url_train_labels, num_train_samples)

# 測試資料集：圖像及標籤資料
url_test_image = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'
url_test_labels = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'
num_test_samples = 10000

print("Downloading test data")
test = try_download(url_test_image, url_test_labels, num_test_samples)
177/22:
# 從訓練資料集中，亂數挑選一個圖像資料，繪製圖像及印出對應的標籤資料
sample_number = 5001

plt.imshow(train[sample_number,:-1].reshape(28,28), cmap="gray_r")
plt.axis('off')

print("Image Label: ", train[sample_number,-1])
177/23:
def loadData(src, cimg):
    print ('Downloading ' + src)
    gzfname, h = urlretrieve(src, './delete.me')
    print ('Done.')
    
    try:
        with gzip.open(gzfname) as gz:
            n = struct.unpack('I', gz.read(4))
            
            # 判斷是否為 MNIST 資料集
            if n[0] != 0x3080000:
                raise Exception('Invalid file: unexpected magic number.')
                
            # 計算資料筆數
            n = struct.unpack('>I', gz.read(4))[0]
            
            if n != cimg:
                raise Exception('Invalid file: expected {0} entries.'.format(cimg))
                
            crow = struct.unpack('>I', gz.read(4))[0]
            ccol = struct.unpack('>I', gz.read(4))[0]
            
            if crow != 28 or ccol != 28:
                raise Exception('Invalid file: expected 28 rows/cols per image.')
                
            # 讀取資料
            res = np.fromstring(gz.read(cimg * crow * ccol), dtype = np.uint8)
    finally:
        os.remove(gzfname)
    return res.reshape((cimg, crow * ccol))
def loadLabels(src, cimg):
    print ('Downloading ' + src)
    gzfname, h = urlretrieve(src, './delete.me')
    print ('Done.')
    try:
        with gzip.open(gzfname) as gz:        
            n = struct.unpack('I', gz.read(4))
            
            # 判斷是否為 MNIST 資料集
            if n[0] != 0x1080000:
                raise Exception('Invalid file: unexpected magic number.')
                
            # 計算資料筆數
            n = struct.unpack('>I', gz.read(4))
            
            if n[0] != cimg:
                raise Exception('Invalid file: expected {0} rows.'.format(cimg))
                
            # 讀取資料
            res = np.fromstring(gz.read(cimg), dtype = np.uint8)
    finally:
        os.remove(gzfname)
    return res.reshape((cimg, 1))
def try_download(dataSrc, labelsSrc, cimg):

    data = loadData(dataSrc, cimg)
    labels = loadLabels(labelsSrc, cimg)
    
    return np.hstack((data, labels))
def savetxt(filename, ndarray):
    dir = os.path.dirname(filename)

    if not os.path.exists(dir):
        os.makedirs(dir)

    if not os.path.isfile(filename):
        print("Saving", filename )
        
        with open(filename, 'w') as f:
        
            labels = list(map(' '.join, np.eye(10, dtype=np.uint).astype(str)))
            
            for row in ndarray:
                row_str = row.astype(str)
                label_str = labels[row[-1]]
                feature_str = ' '.join(row_str[:-1])
                f.write('|labels {} |features {}\n'.format(label_str, feature_str))
    else:
        print("File already exists", filename)
177/24:
data_dir = os.path.join("..", "Examples", "Image", "DataSets", "MNIST")
if not os.path.exists(data_dir):
    data_dir = os.path.join("data", "MNIST")

print ('Writing train text file...')
savetxt(os.path.join(data_dir, "Train-28x28_cntk_text.txt"), train)

print ('Writing test text file...')
savetxt(os.path.join(data_dir, "Test-28x28_cntk_text.txt"), test)

print('Done')
177/25: ls
177/26:
from __future__ import print_function
import gzip
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import numpy as np
import os
import shutil
import struct
import sys
# 引用相關組件
# 相容性需求，若使用舊版pyton時，可使用新版python函式

import cntk as C
import cntk.tests.test_utils

# 測試並設定使用 CPU 或 GPU 作為目前測試環境
cntk.tests.test_utils.set_device_from_pytest_env() 
# 重新設定 CNTK 的亂數種子
C.cntk_py.set_fixed_random_seed(1)

# 設定繪圖組件繪圖在當前界面 - Jupyter Notebook
%matplotlib inline
177/27: %pip intall cntk
177/28: %pip install cntk
177/29: %pip install --upgrade --no-deps cntk
178/1:
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
178/2:
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
178/3:
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])
178/4:
predictions = model(x_train[:1]).numpy()
predictions
178/5: tf.nn.softmax(predictions).numpy()
178/6: loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
178/7: loss_fn(y_train[:1], predictions).numpy()
178/8:
model.compile(optimizer='adam',
              loss=loss_fn,
              metrics=['accuracy'])
178/9: model.fit(x_train, y_train, epochs=5)
178/10: model.evaluate(x_test,  y_test, verbose=2)
178/11:
probability_model = tf.keras.Sequential([
  model,
  tf.keras.layers.Softmax()
])
probability_model(x_test[:5])
178/12:
from __future__ import print_function
import gzip
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import numpy as np
import os
import shutil
import struct
import sys
# 引用相關組件
# 相容性需求，若使用舊版pyton時，可使用新版python函式

import cntk as C
import cntk.tests.test_utils

# 測試並設定使用 CPU 或 GPU 作為目前測試環境
cntk.tests.test_utils.set_device_from_pytest_env() 
# 重新設定 CNTK 的亂數種子
C.cntk_py.set_fixed_random_seed(1)

# 設定繪圖組件繪圖在當前界面 - Jupyter Notebook
%matplotlib inline
178/13: %pip install --user --upgrade --no-deps cntk
178/14:
from __future__ import print_function
import gzip
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import numpy as np
import os
import shutil
import struct
import sys
# 引用相關組件
# 相容性需求，若使用舊版pyton時，可使用新版python函式

import cntk as C
import cntk.tests.test_utils

# 測試並設定使用 CPU 或 GPU 作為目前測試環境
cntk.tests.test_utils.set_device_from_pytest_env() 
# 重新設定 CNTK 的亂數種子
C.cntk_py.set_fixed_random_seed(1)

# 設定繪圖組件繪圖在當前界面 - Jupyter Notebook
%matplotlib inline
179/1:
from __future__ import print_function
import gzip
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import numpy as np
import os
import shutil
import struct
import sys
# 引用相關組件
# 相容性需求，若使用舊版pyton時，可使用新版python函式

import cntk as C
import cntk.tests.test_utils

# 測試並設定使用 CPU 或 GPU 作為目前測試環境
cntk.tests.test_utils.set_device_from_pytest_env() 
# 重新設定 CNTK 的亂數種子
C.cntk_py.set_fixed_random_seed(1)

# 設定繪圖組件繪圖在當前界面 - Jupyter Notebook
%matplotlib inline
181/1:
# TensorFlow and tf.keras
import tensorflow as tf

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)
181/2:
fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()
181/3:
train_images = train_images / 255.0
test_images = test_images / 255.0
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=10)
181/4:
test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

print('\nTest accuracy:', test_acc)
181/5:
probability_model = tf.keras.Sequential([model, 
                                         tf.keras.layers.Softmax()])
predictions = probability_model.predict(test_images)
181/6: predictions[0]
181/7: np.argmax(predictions[0])
181/8: test_labels[0]
181/9:
def plot_image(i, predictions_array, true_label, img):
  true_label, img = true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img, cmap=plt.cm.binary)

  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'

  plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

def plot_value_array(i, predictions_array, true_label):
  true_label = true_label[i]
  plt.grid(False)
  plt.xticks(range(10))
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777")
  plt.ylim([0, 1])
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')
181/10:
i = 0
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions[i],  test_labels)
plt.show()
181/11:
i = 12
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions[i],  test_labels)
plt.show()
181/12:
# Grab an image from the test dataset.
img = test_images[1]
# Add the image to a batch where it's the only member.
img = (np.expand_dims(img,0))
predictions_single = probability_model.predict(img)

print(predictions_single)
181/13:
# Grab an image from the test dataset.
img = test_images[1]
# Add the image to a batch where it's the only member.
img = (np.expand_dims(img,0))
predictions_single = probability_model.predict(img)

print(predictions_single)
plot_value_array(1, predictions_single[0], test_labels)
_ = plt.xticks(range(10), class_names, rotation=45)
plt.show()
181/14:
# Grab an image from the test dataset.
img = test_images[1]
# Add the image to a batch where it's the only member.
img = (np.expand_dims(img,0))
predictions_single = probability_model.predict(img)

print(predictions_single)
plot_value_array(1, predictions_single[0], test_labels)
_ = plt.xticks(range(10), class_names, rotation=45)
plt.show()
print(np.argmax(predictions_single[0]))
182/1:
#https://www.tensorflow.org/tutorials/keras/text_classification
import matplotlib.pyplot as plt
import os
import re
import shutil
import string
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import losses
print(tf.__version__)
182/2:
url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"

dataset = tf.keras.utils.get_file("aclImdb_v1", url,
                                    untar=True, cache_dir='.',
                                    cache_subdir='')

dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')
182/3:
train_dir = os.path.join(dataset_dir, 'train')
sample_file = os.path.join(train_dir, 'pos/1181_9.txt')
with open(sample_file) as f:
  print(f.read())
182/4:
remove_dir = os.path.join(train_dir, 'unsup')
shutil.rmtree(remove_dir)
batch_size = 32
seed = 42

raw_train_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)
for text_batch, label_batch in raw_train_ds.take(1):
  for i in range(3):
    print("Review", text_batch.numpy()[i])
    print("Label", label_batch.numpy()[i])
182/5:
raw_val_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='validation', 
    seed=seed)
raw_test_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/test', 
    batch_size=batch_size)

#Prepare the dataset for training
def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
  return tf.strings.regex_replace(stripped_html,
                                  '[%s]' % re.escape(string.punctuation),
                                  '')
max_features = 10000
sequence_length = 250

vectorize_layer = layers.TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)
# Make a text-only dataset (without labels), then call adapt
train_text = raw_train_ds.map(lambda x, y: x)
vectorize_layer.adapt(train_text)
def vectorize_text(text, label):
  text = tf.expand_dims(text, -1)
  return vectorize_layer(text), label
# retrieve a batch (of 32 reviews and labels) from the dataset
text_batch, label_batch = next(iter(raw_train_ds))
first_review, first_label = text_batch[0], label_batch[0]
train_ds = raw_train_ds.map(vectorize_text)
val_ds = raw_val_ds.map(vectorize_text)
test_ds = raw_test_ds.map(vectorize_text)
#Configure the dataset for performance
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)
182/6:
#Create the model
embedding_dim = 16

model = tf.keras.Sequential([
  layers.Embedding(max_features + 1, embedding_dim),
  layers.Dropout(0.2),
  layers.GlobalAveragePooling1D(),
  layers.Dropout(0.2),
  layers.Dense(1)])

model.summary()
182/7:
#Loss function and optimizer 
model.compile(loss=losses.BinaryCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))
#Train the model
epochs = 10
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs)
loss, accuracy = model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)
acc = history_dict['binary_accuracy']
val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
182/8:
#Loss function and optimizer 
model.compile(loss=losses.BinaryCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))
#Train the model
epochs = 10
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs)
loss, accuracy = model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)
history_dict = history.history
acc = history_dict['binary_accuracy']
val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
182/9:
#Create the model
embedding_dim = 16

model = tf.keras.Sequential([
  layers.Embedding(max_features + 1, embedding_dim),
  layers.Dropout(0.2),
  layers.GlobalAveragePooling1D(),
  layers.Dropout(0.2),
  layers.Dense(1)])

model.summary()
182/10:
#Loss function and optimizer 
model.compile(loss=losses.BinaryCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))
#Train the model
epochs = 10
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs)
loss, accuracy = model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)
history_dict = history.history
acc = history_dict['binary_accuracy']
val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
182/11:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

plt.show()
182/12: print("1287 ---> ",vectorize_layer.get_vocabulary()[1287])
182/13: print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))
182/14:
export_model = tf.keras.Sequential([
  vectorize_layer,
  model,
  layers.Activation('sigmoid')
])

export_model.compile(
    loss=losses.BinaryCrossentropy(from_logits=False), optimizer="adam", metrics=['accuracy']
)

# Test it with `raw_test_ds`, which yields raw strings
loss, accuracy = export_model.evaluate(raw_test_ds)
print(accuracy)
183/1:
#https://www.tensorflow.org/tutorials/keras/text_classification
import matplotlib.pyplot as plt
import os
import re
import shutil
import string
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import losses
print(tf.__version__)
183/2:
url = "https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz"

dataset = tf.keras.utils.get_file("stack_overflow", url,
                                    untar=True, cache_dir='.',
                                    cache_subdir='')

dataset_dir = os.path.join(os.path.dirname(dataset), 'stack_overflow')
183/3:
train_dir = os.path.join(dataset_dir, 'train')
sample_file = os.path.join(train_dir, 'python/1937.txt')
with open(sample_file) as f:
  print(f.read())
183/4:
train_dir = os.path.join(dataset_dir, 'train')
sample_file = os.path.join(train_dir, 'python/1937.txt')
with open(sample_file) as f:
  print(f.read())
183/5:
batch_size = 32
seed = 42

raw_train_ds = tf.keras.utils.text_dataset_from_directory(
    'stack_overflow/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)
for text_batch, label_batch in raw_train_ds.take(1):
  for i in range(3):
    print("Review", text_batch.numpy()[i])
    print("Label", label_batch.numpy()[i])
183/6:
raw_val_ds = tf.keras.utils.text_dataset_from_directory(
    'stack_overflow/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='validation', 
    seed=seed)
raw_test_ds = tf.keras.utils.text_dataset_from_directory(
    'stack_overflow/test', 
    batch_size=batch_size)

#Prepare the dataset for training
def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
  return tf.strings.regex_replace(stripped_html,
                                  '[%s]' % re.escape(string.punctuation),
                                  '')
max_features = 10000
sequence_length = 250

vectorize_layer = layers.TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)
# Make a text-only dataset (without labels), then call adapt
train_text = raw_train_ds.map(lambda x, y: x)
vectorize_layer.adapt(train_text)
def vectorize_text(text, label):
  text = tf.expand_dims(text, -1)
  return vectorize_layer(text), label
# retrieve a batch (of 32 reviews and labels) from the dataset
text_batch, label_batch = next(iter(raw_train_ds))
first_review, first_label = text_batch[0], label_batch[0]
train_ds = raw_train_ds.map(vectorize_text)
val_ds = raw_val_ds.map(vectorize_text)
test_ds = raw_test_ds.map(vectorize_text)
#Configure the dataset for performance
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)
183/7:
#Create the model
embedding_dim = 16

model = tf.keras.Sequential([
  layers.Embedding(max_features + 1, embedding_dim),
  layers.Dropout(0.2),
  layers.GlobalAveragePooling1D(),
  layers.Dropout(0.2),
  layers.Dense(1)])

model.summary()
183/8:
#Create the model
embedding_dim = 16

model = tf.keras.Sequential([
  layers.Embedding(max_features + 1, embedding_dim),
  layers.Dropout(0.2),
  layers.GlobalAveragePooling1D(),
  layers.Dropout(0.2),
  layers.Dense(4)])

model.summary()
183/9:
#Loss function and optimizer 
model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=['accuracy'])
#Train the model
epochs = 10
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs)
loss, accuracy = model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)
history_dict = history.history
acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
183/10:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

plt.show()
183/11: print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))
183/12:
#Export the model
export_model = tf.keras.Sequential([
  vectorize_layer,
  model,
  layers.Activation('sigmoid')
])

export_model.compile(
    loss=losses.BinaryCrossentropy(from_logits=False), optimizer="adam", metrics=['accuracy']
)

# Test it with `raw_test_ds`, which yields raw strings
loss, accuracy = export_model.evaluate(raw_test_ds)
print(accuracy)
183/13:
#Export the model
export_model = tf.keras.Sequential([
  vectorize_layer,
  model,
  layers.Activation('sigmoid')
])

export_model.compile(
    loss=losses.Crossentropy(from_logits=False), optimizer="adam", metrics=['accuracy']
)

# Test it with `raw_test_ds`, which yields raw strings
loss, accuracy = export_model.evaluate(raw_test_ds)
print(accuracy)
183/14:
#Export the model
export_model = tf.keras.Sequential([
  vectorize_layer,
  model,
  layers.Activation('sigmoid')
])

export_model.compile(
    loss=losses.SparseCategoricalCrossentropy(from_logits=False), optimizer="adam", metrics=['accuracy']
)

# Test it with `raw_test_ds`, which yields raw strings
loss, accuracy = export_model.evaluate(raw_test_ds)
print(accuracy)
183/15:
loss, accuracy = model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)
183/16: first_review
183/17:
examples = [first_review,text_batch[10],text_batch[110]]
export_model.predict(examples)
183/18: raw_train_ds.take(2)
183/19:
examples=[]
for i in range(1,5):
  for text_batch, label_batch in raw_train_ds.take(i):
    examples.append(text_batch)
export_model.predict(examples)
183/20:
examples=[]
for i in range(1,2):
  for text_batch, label_batch in raw_train_ds.take(i):
    examples.append(text_batch)
export_model.predict(examples)
184/1: %pip install tensorflow-hub tensorflow-datasets
184/2: %pip install absl-py==0.12
184/3: %pip install absl-py==0.12
184/4: %pip install tensorflow-hub tensorflow-datasets
184/5:
mport os
import numpy as np

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds

print("Version: ", tf.__version__)
print("Eager mode: ", tf.executing_eagerly())
print("Hub version: ", hub.__version__)
print("GPU is", "available" if tf.config.list_physical_devices("GPU") else "NOT AVAILABLE")
184/6:
import os
import numpy as np

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds

print("Version: ", tf.__version__)
print("Eager mode: ", tf.executing_eagerly())
print("Hub version: ", hub.__version__)
print("GPU is", "available" if tf.config.list_physical_devices("GPU") else "NOT AVAILABLE")
184/7:
train_data, validation_data, test_data = tfds.load(
    name="imdb_reviews", 
    split=('train[:60%]', 'train[60%:]', 'test'),
    as_supervised=True)
184/8:
embedding = "https://tfhub.dev/google/nnlm-en-dim50/2"
hub_layer = hub.KerasLayer(embedding, input_shape=[], 
                           dtype=tf.string, trainable=True)
hub_layer(train_examples_batch[:3])
184/9:
#Build the model
embedding = "https://tfhub.dev/google/nnlm-en-dim50/2"
hub_layer = hub.KerasLayer(embedding, input_shape=[], 
                           dtype=tf.string, trainable=True)
train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))
hub_layer(train_examples_batch[:3])
184/10:
model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(1))

model.summary()
184/11:
model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
results = model.evaluate(test_data.batch(512), verbose=2)

for name, value in zip(model.metrics_names, results):
  print("%s: %.3f" % (name, value))
184/12:
model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
results = model.evaluate(test_data.batch(512), verbose=2)

for name, value in zip(model.metrics_names, results):
  print("%s: %.3f" % (name, value))
184/13:
model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model.fit(train_data.shuffle(10000).batch(512),
                    epochs=10,
                    validation_data=validation_data.batch(512),
                    verbose=1)
results = model.evaluate(test_data.batch(512), verbose=2)

for name, value in zip(model.metrics_names, results):
  print("%s: %.3f" % (name, value))
184/14:
#Build the model
def model(embedding):
    hub_layer = hub.KerasLayer(embedding, input_shape=[], 
                           dtype=tf.string, trainable=True)
    train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))
    model = tf.keras.Sequential()
    model.add(hub_layer)
    model.add(tf.keras.layers.Dense(16, activation='relu'))
    model.add(tf.keras.layers.Dense(1))
    model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
    history = model.fit(train_data.shuffle(10000).batch(512),
                    epochs=10,
                    validation_data=validation_data.batch(512),
                    verbose=1)
    results = model.evaluate(test_data.batch(512), verbose=2)

    for name, value in zip(model.metrics_names, results):
      print("%s: %.3f" % (name, value))
    return

urls=["https://tfhub.dev/google/nnlm-en-dim50/2",
      "https://tfhub.dev/google/nnlm-en-dim128/2",
      "https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2",
      "https://tfhub.dev/google/universal-sentence-encoder/4"]
for u in urls:
    model(u)
184/15:
#Build the model
def model(embedding):
    hub_layer = hub.KerasLayer(embedding, input_shape=[], 
                           dtype=tf.string, trainable=True)
    train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))
    model = tf.keras.Sequential()
    model.add(hub_layer)
    model.add(tf.keras.layers.Dense(16, activation='relu'))
    model.add(tf.keras.layers.Dense(1))
    model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
    history = model.fit(train_data.shuffle(10000).batch(512),
                    epochs=10,
                    validation_data=validation_data.batch(512),
                    verbose=1)
    results = model.evaluate(test_data.batch(512), verbose=2)

    for name, value in zip(model.metrics_names, results):
      print("%s: %.3f" % (name, value))
    return

urls=["https://tfhub.dev/google/nnlm-en-dim50/2",
      "https://tfhub.dev/google/nnlm-en-dim128/2",
      "https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2",
      "https://tfhub.dev/google/universal-sentence-encoder/4"]
nvs=[]
for u in urls:
    nvs.append(model(u))
print(nvs)
185/1: %pip install -q seaborn
185/2: %pip install --user -q seaborn
185/3:
#https://www.tensorflow.org/tutorials/keras/regression
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

# Make NumPy printouts easier to read.
np.set_printoptions(precision=3, suppress=True)
185/4:
#https://www.tensorflow.org/tutorials/keras/regression
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

# Make NumPy printouts easier to read.
np.set_printoptions(precision=3, suppress=True)
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

print(tf.__version__)
185/5:
url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'
column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',
                'Acceleration', 'Model Year', 'Origin']

raw_dataset = pd.read_csv(url, names=column_names,
                          na_values='?', comment='\t',
                          sep=' ', skipinitialspace=True)
dataset = raw_dataset.copy()
dataset.tail()
185/6:
dataset = dataset.dropna()
dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})
dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')
185/7: dataset.tail()
185/8:
train_dataset = dataset.sample(frac=0.8, random_state=0)
test_dataset = dataset.drop(train_dataset.index)
sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde')
185/9:
train_features = train_dataset.copy()
test_features = test_dataset.copy()

train_labels = train_features.pop('MPG')
test_labels = test_features.pop('MPG')
normalizer = tf.keras.layers.Normalization(axis=-1)
normalizer.adapt(np.array(train_features))
185/10:
print(normalizer.mean.numpy())
first = np.array(train_features[:1])

with np.printoptions(precision=2, suppress=True):
  print('First example:', first)
  print()
  print('Normalized:', normalizer(first).numpy())
185/11:
#Linear regression
horsepower = np.array(train_features['Horsepower'])

horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)
horsepower_normalizer.adapt(horsepower)
horsepower_model = tf.keras.Sequential([
    horsepower_normalizer,
    layers.Dense(units=1)
])
horsepower_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')
%%time
history = horsepower_model.fit(
    train_features['Horsepower'],
    train_labels,
    epochs=100,
    # Suppress logging.
    verbose=0,
    # Calculate validation results on 20% of the training data.
    validation_split = 0.2)
hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()
def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
  plt.ylim([0, 10])
  plt.xlabel('Epoch')
  plt.ylabel('Error [MPG]')
  plt.legend()
  plt.grid(True)
plot_loss(history)
185/12:
#Linear regression
horsepower = np.array(train_features['Horsepower'])

horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)
horsepower_normalizer.adapt(horsepower)
horsepower_model = tf.keras.Sequential([
    horsepower_normalizer,
    layers.Dense(units=1)
])
horsepower_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')
%time
history = horsepower_model.fit(
    train_features['Horsepower'],
    train_labels,
    epochs=100,
    # Suppress logging.
    verbose=0,
    # Calculate validation results on 20% of the training data.
    validation_split = 0.2)
hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()
def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
  plt.ylim([0, 10])
  plt.xlabel('Epoch')
  plt.ylabel('Error [MPG]')
  plt.legend()
  plt.grid(True)
plot_loss(history)
185/13:
#Collect the results on the test set for later:
test_results = {}

test_results['horsepower_model'] = horsepower_model.evaluate(
    test_features['Horsepower'],
    test_labels, verbose=0)
x = tf.linspace(0.0, 250, 251)
y = horsepower_model.predict(x)
def plot_horsepower(x, y):
  plt.scatter(train_features['Horsepower'], train_labels, label='Data')
  plt.plot(x, y, color='k', label='Predictions')
  plt.xlabel('Horsepower')
  plt.ylabel('MPG')
  plt.legend()
plot_horsepower(x, y)
185/14:
#Linear regression with multiple inputs
linear_model = tf.keras.Sequential([
    normalizer,
    layers.Dense(units=1)
])
linear_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')
%time
history = linear_model.fit(
    train_features,
    train_labels,
    epochs=100,
    # Suppress logging.
    verbose=0,
    # Calculate validation results on 20% of the training data.
    validation_split = 0.2)
plot_loss(history)
test_results['linear_model'] = linear_model.evaluate(
    test_features, test_labels, verbose=0)
185/15:
#Regression with a deep neural network (DNN)
def build_and_compile_model(norm):
  model = keras.Sequential([
      norm,
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(1)
  ])

  model.compile(loss='mean_absolute_error',
                optimizer=tf.keras.optimizers.Adam(0.001))
  return model
dnn_horsepower_model = build_and_compile_model(horsepower_normalizer)
%time
history = dnn_horsepower_model.fit(
    train_features['Horsepower'],
    train_labels,
    validation_split=0.2,
    verbose=0, epochs=100)
plot_loss(history)
x = tf.linspace(0.0, 250, 251)
y = dnn_horsepower_model.predict(x)
plot_horsepower(x, y)
test_results['dnn_horsepower_model'] = dnn_horsepower_model.evaluate(
    test_features['Horsepower'], test_labels,
    verbose=0)
185/16: plot_horsepower(x, y)
185/17:
#Regression with a deep neural network (DNN)
def build_and_compile_model(norm):
  model = keras.Sequential([
      norm,
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(1)
  ])

  model.compile(loss='mean_absolute_error',
                optimizer=tf.keras.optimizers.Adam(0.001))
  return model
dnn_horsepower_model = build_and_compile_model(horsepower_normalizer)
%time
history = dnn_horsepower_model.fit(
    train_features['Horsepower'],
    train_labels,
    validation_split=0.2,
    verbose=0, epochs=100)
plot_loss(history)
185/18:
x = tf.linspace(0.0, 250, 251)
y = dnn_horsepower_model.predict(x)
plot_horsepower(x, y)
test_results['dnn_horsepower_model'] = dnn_horsepower_model.evaluate(
    test_features['Horsepower'], test_labels,
    verbose=0)
plot_horsepower(x, y)
185/19:
#Regression using a DNN and multiple inputs
dnn_model = build_and_compile_model(normalizer)
%time
history = dnn_model.fit(
    train_features,
    train_labels,
    validation_split=0.2,
    verbose=0, epochs=100)
plot_loss(history)
185/20:
#Regression using a DNN and multiple inputs
dnn_model = build_and_compile_model(normalizer)
%time
history = dnn_model.fit(
    train_features,
    train_labels,
    validation_split=0.2,
    verbose=0, epochs=100)
plot_loss(history)
test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)
185/21:
#Performance
pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T
185/22:
#Make predictions
test_predictions = dnn_model.predict(test_features).flatten()

a = plt.axes(aspect='equal')
plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values [MPG]')
plt.ylabel('Predictions [MPG]')
lims = [0, 50]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)
185/23:
error = test_predictions - test_labels
plt.hist(error, bins=25)
plt.xlabel('Prediction Error [MPG]')
_ = plt.ylabel('Count')
185/24:
dnn_model.save('dnn_model')
reloaded = tf.keras.models.load_model('dnn_model')

test_results['reloaded'] = reloaded.evaluate(
    test_features, test_labels, verbose=0)
pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T
186/1:
import sys
import os
import numpy as np
import time
import torch
import torch.nn as nn
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from geotorchai.models.raster import DeepSatV2
from geotorchai.datasets.raster import EuroSAT
import matplotlib.pyplot as plt
186/2: %pip install torchvision
186/3:
import sys
import os
import numpy as np
import time
import torch
import torch.nn as nn
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from geotorchai.models.raster import DeepSatV2
from geotorchai.datasets.raster import EuroSAT
import matplotlib.pyplot as plt
186/4:
## Define parameters
epoch_nums = 100
learning_rate = 0.0002
batch_size = 16
validation_split = 0.2
shuffle_dataset = True
random_seed = int(time.time())
params = {'batch_size': batch_size, 'shuffle': False}

## make sure that PATH_TO_DATASET exists in the running directory
PATH_TO_DATASET = "data/eurosat"
MODEL_SAVE_PATH = "model-deepsatv2"
os.makedirs(MODEL_SAVE_PATH, exist_ok=True)
186/5:
## load data and calculate mean and std to perform normalization transform
## Set download=True if dataset is not available in the given path
fullData = EuroSAT(root = PATH_TO_DATASET, download=False)

full_loader = DataLoader(fullData, batch_size= batch_size)
channels_sum, channels_squared_sum, num_batches = 0, 0, 0
for i, sample in enumerate(full_loader):
    data_temp, _ = sample
    channels_sum += torch.mean(data_temp, dim=[0, 2, 3])
    channels_squared_sum += torch.mean(data_temp**2, dim=[0, 2, 3])
    num_batches += 1

mean = channels_sum / num_batches
std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5
186/6:
## load data and calculate mean and std to perform normalization transform
## Set download=True if dataset is not available in the given path
fullData = EuroSAT(root = PATH_TO_DATASET, download=True)

full_loader = DataLoader(fullData, batch_size= batch_size)
channels_sum, channels_squared_sum, num_batches = 0, 0, 0
for i, sample in enumerate(full_loader):
    data_temp, _ = sample
    channels_sum += torch.mean(data_temp, dim=[0, 2, 3])
    channels_squared_sum += torch.mean(data_temp**2, dim=[0, 2, 3])
    num_batches += 1

mean = channels_sum / num_batches
std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5
186/7:
## load data and calculate mean and std to perform normalization transform
## Set download=True if dataset is not available in the given path
fullData = EuroSAT(root = PATH_TO_DATASET, download=False)

full_loader = DataLoader(fullData, batch_size= batch_size)
channels_sum, channels_squared_sum, num_batches = 0, 0, 0
for i, sample in enumerate(full_loader):
    data_temp, _ = sample
    channels_sum += torch.mean(data_temp, dim=[0, 2, 3])
    channels_squared_sum += torch.mean(data_temp**2, dim=[0, 2, 3])
    num_batches += 1

mean = channels_sum / num_batches
std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5
186/8:
## load data and calculate mean and std to perform normalization transform
## Set download=True if dataset is not available in the given path
fullData = EuroSAT(root = PATH_TO_DATASET, download=False)

full_loader = DataLoader(fullData, batch_size= batch_size)
channels_sum, channels_squared_sum, num_batches = 0, 0, 0
for i, sample in enumerate(full_loader):
    data_temp, _ = sample
    channels_sum += torch.mean(data_temp, dim=[0, 2, 3])
    channels_squared_sum += torch.mean(data_temp**2, dim=[0, 2, 3])
    num_batches += 1

mean = channels_sum / num_batches
std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5
186/9:
## load data and calculate mean and std to perform normalization transform
## Set download=True if dataset is not available in the given path
fullData = EuroSAT(root = PATH_TO_DATASET, download=False)

full_loader = DataLoader(fullData, batch_size= batch_size)
channels_sum, channels_squared_sum, num_batches = 0, 0, 0
for i, sample in enumerate(full_loader):
    data_temp, _ = sample
    channels_sum += torch.mean(data_temp, dim=[0, 2, 3])
    channels_squared_sum += torch.mean(data_temp**2, dim=[0, 2, 3])
    num_batches += 1

mean = channels_sum / num_batches
std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5
186/10:
## load data and calculate mean and std to perform normalization transform
## Set download=True if dataset is not available in the given path
fullData = EuroSAT(root = PATH_TO_DATASET, download=False)

full_loader = DataLoader(fullData, batch_size= batch_size)
channels_sum, channels_squared_sum, num_batches = 0, 0, 0
for i, sample in enumerate(full_loader):
    data_temp, _ = sample
    channels_sum += torch.mean(data_temp, dim=[0, 2, 3])
    channels_squared_sum += torch.mean(data_temp**2, dim=[0, 2, 3])
    num_batches += 1

mean = channels_sum / num_batches
std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5
186/11: full_loader
186/12:
for i, sample in enumerate(full_loader):
    print(i,sample)
186/13: dir(full_loader)
186/14: fullData = EuroSAT(root = PATH_TO_DATASET, download=True)
186/15: fullData = EuroSAT(root = PATH_TO_DATASET, download=False)
186/16:
## load data and calculate mean and std to perform normalization transform
## Set download=True if dataset is not available in the given path
fullData = EuroSAT(root = PATH_TO_DATASET, download=False)

full_loader = DataLoader(fullData, batch_size= batch_size)
channels_sum, channels_squared_sum, num_batches = 0, 0, 0
for i, sample in enumerate(full_loader):
    data_temp, _ = sample
    channels_sum += torch.mean(data_temp, dim=[0, 2, 3])
    channels_squared_sum += torch.mean(data_temp**2, dim=[0, 2, 3])
    num_batches += 1

mean = channels_sum / num_batches
std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5
186/17: num_batches
186/18: sample
186/19:
## load data and calculate mean and std to perform normalization transform
## Set download=True if dataset is not available in the given path
fullData = EuroSAT(root = PATH_TO_DATASET, download=False)

full_loader = DataLoader(fullData, batch_size= batch_size)
channels_sum, channels_squared_sum, num_batches = 0, 0, 0
for i, sample in enumerate(full_loader):
    data_temp, _ = sample
    channels_sum += torch.mean(data_temp, dim=[0, 2, 3])
    channels_squared_sum += torch.mean(data_temp**2, dim=[0, 2, 3])
    num_batches += 1

mean = channels_sum / num_batches
std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5
186/20: num_batches
186/21:
## Define the transform operation
sat_transform = transforms.Normalize(mean, std)
## Load data with desired transformation and additional handcrafted features enabled
fullData = EuroSAT(root = PATH_TO_DATASET, include_additional_features = True, transform = sat_transform)
## find all class names and corresponding labels
print(fullData.get_class_labels())
186/22:
## Display various bands from an input image
input_data, label, features = fullData[1]
f, ((ax1, ax2, ax3, ax4, ax5, ax6, ax7), (ax8, ax9, ax10, ax11, ax12, ax13, ax14)) = plt.subplots(2, 7, figsize=(15,5))
ax14.axis('off')
        
ax1.set_title('Band-1')
ax1.imshow(input_data[0])

ax2.set_title('Band-2')
ax2.imshow(input_data[1])

ax3.set_title('Band-3')
ax3.imshow(input_data[2])

ax4.set_title('Band-4')
ax4.imshow(input_data[3])

ax5.set_title('Band-5')
ax5.imshow(input_data[4])

ax6.set_title('Band-6')
ax6.imshow(input_data[5])

ax7.set_title('Band-7')
ax7.imshow(input_data[6])

ax8.set_title('Band-8')
ax8.imshow(input_data[7])

ax9.set_title('Band-9')
ax9.imshow(input_data[8])

ax10.set_title('Band-10')
ax10.imshow(input_data[9])

ax11.set_title('Band-11')
ax11.imshow(input_data[10])

ax12.set_title('Band-12')
ax12.imshow(input_data[11])

ax13.set_title('Band-13')
ax13.imshow(input_data[12])
186/23:
## Display various bands from an input image
input_data, label, features = fullData[2]
f, ((ax1, ax2, ax3, ax4, ax5, ax6, ax7), (ax8, ax9, ax10, ax11, ax12, ax13, ax14)) = plt.subplots(2, 7, figsize=(15,5))
ax14.axis('off')
        
ax1.set_title('Band-1')
ax1.imshow(input_data[0])

ax2.set_title('Band-2')
ax2.imshow(input_data[1])

ax3.set_title('Band-3')
ax3.imshow(input_data[2])

ax4.set_title('Band-4')
ax4.imshow(input_data[3])

ax5.set_title('Band-5')
ax5.imshow(input_data[4])

ax6.set_title('Band-6')
ax6.imshow(input_data[5])

ax7.set_title('Band-7')
ax7.imshow(input_data[6])

ax8.set_title('Band-8')
ax8.imshow(input_data[7])

ax9.set_title('Band-9')
ax9.imshow(input_data[8])

ax10.set_title('Band-10')
ax10.imshow(input_data[9])

ax11.set_title('Band-11')
ax11.imshow(input_data[10])

ax12.set_title('Band-12')
ax12.imshow(input_data[11])

ax13.set_title('Band-13')
ax13.imshow(input_data[12])
186/24:
## Display various bands from an input image
input_data, label, features = fullData[3]
f, ((ax1, ax2, ax3, ax4, ax5, ax6, ax7), (ax8, ax9, ax10, ax11, ax12, ax13, ax14)) = plt.subplots(2, 7, figsize=(15,5))
ax14.axis('off')
        
ax1.set_title('Band-1')
ax1.imshow(input_data[0])

ax2.set_title('Band-2')
ax2.imshow(input_data[1])

ax3.set_title('Band-3')
ax3.imshow(input_data[2])

ax4.set_title('Band-4')
ax4.imshow(input_data[3])

ax5.set_title('Band-5')
ax5.imshow(input_data[4])

ax6.set_title('Band-6')
ax6.imshow(input_data[5])

ax7.set_title('Band-7')
ax7.imshow(input_data[6])

ax8.set_title('Band-8')
ax8.imshow(input_data[7])

ax9.set_title('Band-9')
ax9.imshow(input_data[8])

ax10.set_title('Band-10')
ax10.imshow(input_data[9])

ax11.set_title('Band-11')
ax11.imshow(input_data[10])

ax12.set_title('Band-12')
ax12.imshow(input_data[11])

ax13.set_title('Band-13')
ax13.imshow(input_data[12])
186/25:
## Display various bands from an input image
for i in range(10):
    input_data, label, features = fullData[i]
    f, ((ax1, ax2, ax3, ax4, ax5, ax6, ax7), (ax8, ax9, ax10, ax11, ax12, ax13, ax14)) = plt.subplots(2, 7, figsize=(15,5))
    ax14.axis('off')
        
    ax1.set_title('Band-1')
    ax1.imshow(input_data[0])

    ax2.set_title('Band-2')
    ax2.imshow(input_data[1])

    ax3.set_title('Band-3')
    ax3.imshow(input_data[2])

    ax4.set_title('Band-4')
    ax4.imshow(input_data[3])

    ax5.set_title('Band-5')
    ax5.imshow(input_data[4])

    ax6.set_title('Band-6')
    ax6.imshow(input_data[5])

    ax7.set_title('Band-7')
    ax7.imshow(input_data[6])

    ax8.set_title('Band-8')
    ax8.imshow(input_data[7])

    ax9.set_title('Band-9')
    ax9.imshow(input_data[8])

    ax10.set_title('Band-10')
    ax10.imshow(input_data[9])

    ax11.set_title('Band-11')
    ax11.imshow(input_data[10])

    ax12.set_title('Band-12')
    ax12.imshow(input_data[11])

    ax13.set_title('Band-13')
    ax13.imshow(input_data[12])
186/26:
## Initialize training and validation indices to split the dataset
dataset_size = len(fullData)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
if shuffle_dataset:
    np.random.seed(random_seed)
    np.random.shuffle(indices)
train_indices, val_indices = indices[split:], indices[:split]
## Define training and validation data sampler
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)

## Define training and validation data loader
train_loader = DataLoader(fullData, **params, sampler=train_sampler)
val_loader = DataLoader(fullData, **params, sampler=valid_sampler)

#Initialize Model and Hyperparameters
## set device to CPU or GPU
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")
## Define Model
model = DeepSatV2(13, 64, 64, 10, len(fullData.ADDITIONAL_FEATURES))
## Define hyper-parameters
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
model.to(device)
loss_fn.to(device)
186/27:
#Method for Returning Validation Accuracy
## Before starting training, define a method to calculate validation accuracy
def get_validation_accuracy(model, data_loader, device):
    model.eval()
    total_sample = 0
    correct = 0
    for i, sample in enumerate(data_loader):
        inputs, labels, features = sample
        inputs = inputs.to(device)
        features = features.type(torch.FloatTensor).to(device)
        labels = labels.to(device)

        outputs = model(inputs, features)
        total_sample += len(labels)

        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
    
    accuracy = 100 * correct / total_sample
    return accuracy
186/28:
#Train and Evaluate Model
## Perform training and validation
max_val_accuracy = None
for e in range(epoch_nums):
    for i, sample in enumerate(train_loader):
        inputs, labels, features = sample
        inputs = inputs.to(device)
        features = features.type(torch.FloatTensor).to(device)
        labels = labels.to(device)

        # Forward pass
        outputs = model(inputs, features)
        loss = loss_fn(outputs, labels)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print('Epoch [{}/{}], Training Loss: {:.4f}'.format(e + 1, epoch_nums, loss.item()))

    ## Perform model validation after finishing each epoch training
    val_accuracy = get_validation_accuracy(model, val_loader, device)
    print("Validation Accuracy: ", val_accuracy, "%")

    if max_val_accuracy == None or val_accuracy > max_val_accuracy:
        max_val_accuracy = val_accuracy
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print('Best model saved!')
186/29: max_val_accuracy == None or val_accuracy > max_val_accuracy
186/30: max_val_accuracy
186/31: val_accuracy, max_val_accuracy
186/32: torch.save(model.state_dict(), MODEL_SAVE_PATH)
186/33: torch.save(model.state_dict(), MODEL_SAVE_PATH+'file1')
186/34:
#Train and Evaluate Model
## Perform training and validation
max_val_accuracy = None
for e in range(epoch_nums):
    for i, sample in enumerate(train_loader):
        inputs, labels, features = sample
        inputs = inputs.to(device)
        features = features.type(torch.FloatTensor).to(device)
        labels = labels.to(device)

        # Forward pass
        outputs = model(inputs, features)
        loss = loss_fn(outputs, labels)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print('Epoch [{}/{}], Training Loss: {:.4f}'.format(e + 1, epoch_nums, loss.item()))

    ## Perform model validation after finishing each epoch training
    val_accuracy = get_validation_accuracy(model, val_loader, device)
    print("Validation Accuracy: ", val_accuracy, "%")

    if max_val_accuracy == None or val_accuracy > max_val_accuracy:
        max_val_accuracy = val_accuracy
        torch.save(model.state_dict(), MODEL_SAVE_PATH+file+str(e+1))
        print('Best model saved!')
186/35:
#Train and Evaluate Model
## Perform training and validation
max_val_accuracy = None
for e in range(epoch_nums):
    for i, sample in enumerate(train_loader):
        inputs, labels, features = sample
        inputs = inputs.to(device)
        features = features.type(torch.FloatTensor).to(device)
        labels = labels.to(device)

        # Forward pass
        outputs = model(inputs, features)
        loss = loss_fn(outputs, labels)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print('Epoch [{}/{}], Training Loss: {:.4f}'.format(e + 1, epoch_nums, loss.item()))

    ## Perform model validation after finishing each epoch training
    val_accuracy = get_validation_accuracy(model, val_loader, device)
    print("Validation Accuracy: ", val_accuracy, "%")

    if max_val_accuracy == None or val_accuracy > max_val_accuracy:
        max_val_accuracy = val_accuracy
        torch.save(model.state_dict(), MODEL_SAVE_PATH+'/epoch'+str(e+1))
        print('Best model saved!')
187/1:
from __future__ import division
from __future__ import print_function

import os
import time
import tensorflow as tf
import numpy as np

from graphsage.models import SampleAndAggregate, SAGEInfo, Node2VecModel
from graphsage.minibatch import EdgeMinibatchIterator
from graphsage.neigh_samplers import UniformNeighborSampler
from graphsage.utils import load_data
188/1:
from __future__ import division
from __future__ import print_function

import os
import time
import tensorflow as tf
import numpy as np

from graphsage.models import SampleAndAggregate, SAGEInfo, Node2VecModel
from graphsage.minibatch import EdgeMinibatchIterator
from graphsage.neigh_samplers import UniformNeighborSampler
from graphsage.utils import load_data
188/2:
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"

# Set random seed
seed = 123
np.random.seed(seed)
tf.set_random_seed(seed)

# Settings
flags = tf.app.flags
FLAGS = flags.FLAGS
188/3:
tf.app.flags.DEFINE_boolean('log_device_placement', False,
                            """Whether to log device placement.""")
#core params..
flags.DEFINE_string('model', 'graphsage', 'model names. See README for possible values.')
flags.DEFINE_float('learning_rate', 0.00001, 'initial learning rate.')
flags.DEFINE_string("model_size", "small", "Can be big or small; model specific def'ns")
flags.DEFINE_string('train_prefix', '', 'name of the object file that stores the training data. must be specified.')

# left to default values in main experiments
flags.DEFINE_integer('epochs', 1, 'number of epochs to train.')
flags.DEFINE_float('dropout', 0.0, 'dropout rate (1 - keep probability).')
flags.DEFINE_float('weight_decay', 0.0, 'weight for l2 loss on embedding matrix.')
flags.DEFINE_integer('max_degree', 100, 'maximum node degree.')
flags.DEFINE_integer('samples_1', 25, 'number of samples in layer 1')
flags.DEFINE_integer('samples_2', 10, 'number of users samples in layer 2')
flags.DEFINE_integer('dim_1', 128, 'Size of output dim (final is 2x this, if using concat)')
flags.DEFINE_integer('dim_2', 128, 'Size of output dim (final is 2x this, if using concat)')
flags.DEFINE_boolean('random_context', True, 'Whether to use random context or direct edges')
flags.DEFINE_integer('neg_sample_size', 20, 'number of negative samples')
flags.DEFINE_integer('batch_size', 512, 'minibatch size.')
flags.DEFINE_integer('n2v_test_epochs', 1, 'Number of new SGD epochs for n2v.')
flags.DEFINE_integer('identity_dim', 0, 'Set to positive value to use identity embedding features of that dimension. Default 0.')

#logging, saving, validation settings etc.
flags.DEFINE_boolean('save_embeddings', True, 'whether to save embeddings for all nodes after training')
flags.DEFINE_string('base_log_dir', '.', 'base directory for logging and saving embeddings')
flags.DEFINE_integer('validate_iter', 5000, "how often to run a validation minibatch.")
flags.DEFINE_integer('validate_batch_size', 256, "how many nodes per validation sample.")
flags.DEFINE_integer('gpu', 1, "which gpu to use.")
flags.DEFINE_integer('print_every', 50, "How often to print training info.")
flags.DEFINE_integer('max_total_steps', 10**10, "Maximum total number of iterations")

os.environ["CUDA_VISIBLE_DEVICES"]=str(FLAGS.gpu)

GPU_MEM_FRACTION = 0.8
188/4:
def log_dir():
    log_dir = FLAGS.base_log_dir + "/unsup-" + FLAGS.train_prefix.split("/")[-2]
    log_dir += "/{model:s}_{model_size:s}_{lr:0.6f}/".format(
            model=FLAGS.model,
            model_size=FLAGS.model_size,
            lr=FLAGS.learning_rate)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    return log_dir
188/5:
def evaluate(sess, model, minibatch_iter, size=None):
    t_test = time.time()
    feed_dict_val = minibatch_iter.val_feed_dict(size)
    outs_val = sess.run([model.loss, model.ranks, model.mrr],
                        feed_dict=feed_dict_val)
    return outs_val[0], outs_val[1], outs_val[2], (time.time() - t_test)
188/6:
def incremental_evaluate(sess, model, minibatch_iter, size):
    t_test = time.time()
    finished = False
    val_losses = []
    val_mrrs = []
    iter_num = 0
    while not finished:
        feed_dict_val, finished, _ = minibatch_iter.incremental_val_feed_dict(size, iter_num)
        iter_num += 1
        outs_val = sess.run([model.loss, model.ranks, model.mrr],
                            feed_dict=feed_dict_val)
        val_losses.append(outs_val[0])
        val_mrrs.append(outs_val[2])
    return np.mean(val_losses), np.mean(val_mrrs), (time.time() - t_test)
188/7:
def save_val_embeddings(sess, model, minibatch_iter, size, out_dir, mod=""):
    val_embeddings = []
    finished = False
    seen = set([])
    nodes = []
    iter_num = 0
    name = "val"
    while not finished:
        feed_dict_val, finished, edges = minibatch_iter.incremental_embed_feed_dict(size, iter_num)
        iter_num += 1
        outs_val = sess.run([model.loss, model.mrr, model.outputs1],
                            feed_dict=feed_dict_val)
        #ONLY SAVE FOR embeds1 because of planetoid
        for i, edge in enumerate(edges):
            if not edge[0] in seen:
                val_embeddings.append(outs_val[-1][i,:])
                nodes.append(edge[0])
                seen.add(edge[0])
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    val_embeddings = np.vstack(val_embeddings)
    np.save(out_dir + name + mod + ".npy",  val_embeddings)
    with open(out_dir + name + mod + ".txt", "w") as fp:
        fp.write("\n".join(map(str,nodes)))
188/8:
def construct_placeholders():
    # Define placeholders
    placeholders = {
        'batch1' : tf.placeholder(tf.int32, shape=(None), name='batch1'),
        'batch2' : tf.placeholder(tf.int32, shape=(None), name='batch2'),
        # negative samples for all nodes in the batch
        'neg_samples': tf.placeholder(tf.int32, shape=(None,),
            name='neg_sample_size'),
        'dropout': tf.placeholder_with_default(0., shape=(), name='dropout'),
        'batch_size' : tf.placeholder(tf.int32, name='batch_size'),
    }
    return placeholders
188/9:
def train(train_data, test_data=None):
    G = train_data[0]
    features = train_data[1]
    id_map = train_data[2]

    if not features is None:
        # pad with dummy zero vector
        features = np.vstack([features, np.zeros((features.shape[1],))])

    context_pairs = train_data[3] if FLAGS.random_context else None
    placeholders = construct_placeholders()
    minibatch = EdgeMinibatchIterator(G,
            id_map,
            placeholders, batch_size=FLAGS.batch_size,
            max_degree=FLAGS.max_degree,
            num_neg_samples=FLAGS.neg_sample_size,
            context_pairs = context_pairs)
    adj_info_ph = tf.placeholder(tf.int32, shape=minibatch.adj.shape)
    adj_info = tf.Variable(adj_info_ph, trainable=False, name="adj_info")

    if FLAGS.model == 'graphsage_mean':
        # Create model
        sampler = UniformNeighborSampler(adj_info)
        layer_infos = [SAGEInfo("node", sampler, FLAGS.samples_1, FLAGS.dim_1),
                            SAGEInfo("node", sampler, FLAGS.samples_2, FLAGS.dim_2)]

        model = SampleAndAggregate(placeholders,
                                     features,
                                     adj_info,
                                     minibatch.deg,
                                     layer_infos=layer_infos,
                                     model_size=FLAGS.model_size,
                                     identity_dim = FLAGS.identity_dim,
                                     logging=True)
    elif FLAGS.model == 'gcn':
        # Create model
        sampler = UniformNeighborSampler(adj_info)
        layer_infos = [SAGEInfo("node", sampler, FLAGS.samples_1, 2*FLAGS.dim_1),
                            SAGEInfo("node", sampler, FLAGS.samples_2, 2*FLAGS.dim_2)]

        model = SampleAndAggregate(placeholders,
                                     features,
                                     adj_info,
                                     minibatch.deg,
                                     layer_infos=layer_infos,
                                     aggregator_type="gcn",
                                     model_size=FLAGS.model_size,
                                     identity_dim = FLAGS.identity_dim,
                                     concat=False,
                                     logging=True)

    elif FLAGS.model == 'graphsage_seq':
        sampler = UniformNeighborSampler(adj_info)
        layer_infos = [SAGEInfo("node", sampler, FLAGS.samples_1, FLAGS.dim_1),
                            SAGEInfo("node", sampler, FLAGS.samples_2, FLAGS.dim_2)]

        model = SampleAndAggregate(placeholders,
                                     features,
                                     adj_info,
                                     minibatch.deg,
                                     layer_infos=layer_infos,
                                     identity_dim = FLAGS.identity_dim,
                                     aggregator_type="seq",
                                     model_size=FLAGS.model_size,
                                     logging=True)

    elif FLAGS.model == 'graphsage_maxpool':
        sampler = UniformNeighborSampler(adj_info)
        layer_infos = [SAGEInfo("node", sampler, FLAGS.samples_1, FLAGS.dim_1),
                            SAGEInfo("node", sampler, FLAGS.samples_2, FLAGS.dim_2)]

        model = SampleAndAggregate(placeholders,
                                    features,
                                    adj_info,
                                    minibatch.deg,
                                     layer_infos=layer_infos,
                                     aggregator_type="maxpool",
                                     model_size=FLAGS.model_size,
                                     identity_dim = FLAGS.identity_dim,
                                     logging=True)
    elif FLAGS.model == 'graphsage_meanpool':
        sampler = UniformNeighborSampler(adj_info)
        layer_infos = [SAGEInfo("node", sampler, FLAGS.samples_1, FLAGS.dim_1),
                            SAGEInfo("node", sampler, FLAGS.samples_2, FLAGS.dim_2)]

        model = SampleAndAggregate(placeholders,
                                    features,
                                    adj_info,
                                    minibatch.deg,
                                     layer_infos=layer_infos,
                                     aggregator_type="meanpool",
                                     model_size=FLAGS.model_size,
                                     identity_dim = FLAGS.identity_dim,
                                     logging=True)

    elif FLAGS.model == 'n2v':
        model = Node2VecModel(placeholders, features.shape[0],
                                       minibatch.deg,
                                       #2x because graphsage uses concat
                                       nodevec_dim=2*FLAGS.dim_1,
                                       lr=FLAGS.learning_rate)
    else:
        raise Exception('Error: model name unrecognized.')

    config = tf.ConfigProto(log_device_placement=FLAGS.log_device_placement)
    config.gpu_options.allow_growth = True
    #config.gpu_options.per_process_gpu_memory_fraction = GPU_MEM_FRACTION
    config.allow_soft_placement = True

    # Initialize session
    sess = tf.Session(config=config)
    merged = tf.summary.merge_all()
    summary_writer = tf.summary.FileWriter(log_dir(), sess.graph)

    # Init variables
    sess.run(tf.global_variables_initializer(), feed_dict={adj_info_ph: minibatch.adj})

    # Train model

    train_shadow_mrr = None
    shadow_mrr = None

    total_steps = 0
    avg_time = 0.0
    epoch_val_costs = []

    train_adj_info = tf.assign(adj_info, minibatch.adj)
    val_adj_info = tf.assign(adj_info, minibatch.test_adj)
    for epoch in range(FLAGS.epochs):
        minibatch.shuffle()

        iter = 0
        print('Epoch: %04d' % (epoch + 1))
        epoch_val_costs.append(0)
        while not minibatch.end():
            # Construct feed dictionary
            feed_dict = minibatch.next_minibatch_feed_dict()
            feed_dict.update({placeholders['dropout']: FLAGS.dropout})

            t = time.time()
            # Training step
            outs = sess.run([merged, model.opt_op, model.loss, model.ranks, model.aff_all,
                    model.mrr, model.outputs1], feed_dict=feed_dict)
            train_cost = outs[2]
            train_mrr = outs[5]
            if train_shadow_mrr is None:
                train_shadow_mrr = train_mrr#
            else:
                train_shadow_mrr -= (1-0.99) * (train_shadow_mrr - train_mrr)

            if iter % FLAGS.validate_iter == 0:
                # Validation
                sess.run(val_adj_info.op)
                val_cost, ranks, val_mrr, duration  = evaluate(sess, model, minibatch, size=FLAGS.validate_batch_size)
                sess.run(train_adj_info.op)
                epoch_val_costs[-1] += val_cost
            if shadow_mrr is None:
                shadow_mrr = val_mrr
            else:
                shadow_mrr -= (1-0.99) * (shadow_mrr - val_mrr)

            if total_steps % FLAGS.print_every == 0:
                summary_writer.add_summary(outs[0], total_steps)

            # Print results
            avg_time = (avg_time * total_steps + time.time() - t) / (total_steps + 1)

            if total_steps % FLAGS.print_every == 0:
                print("Iter:", '%04d' % iter,
                      "train_loss=", "{:.5f}".format(train_cost),
                      "train_mrr=", "{:.5f}".format(train_mrr),
                      "train_mrr_ema=", "{:.5f}".format(train_shadow_mrr), # exponential moving average
                      "val_loss=", "{:.5f}".format(val_cost),
                      "val_mrr=", "{:.5f}".format(val_mrr),
                      "val_mrr_ema=", "{:.5f}".format(shadow_mrr), # exponential moving average
                      "time=", "{:.5f}".format(avg_time))

            iter += 1
            total_steps += 1

            if total_steps > FLAGS.max_total_steps:
                break

        if total_steps > FLAGS.max_total_steps:
                break

    print("Optimization Finished!")
    if FLAGS.save_embeddings:
        sess.run(val_adj_info.op)

        save_val_embeddings(sess, model, minibatch, FLAGS.validate_batch_size, log_dir())

        if FLAGS.model == "n2v":
            # stopping the gradient for the already trained nodes
            train_ids = tf.constant([[id_map[n]] for n in G.nodes_iter() if not G.node[n]['val'] and not G.node[n]['test']],
                    dtype=tf.int32)
            test_ids = tf.constant([[id_map[n]] for n in G.nodes_iter() if G.node[n]['val'] or G.node[n]['test']],
                    dtype=tf.int32)
            update_nodes = tf.nn.embedding_lookup(model.context_embeds, tf.squeeze(test_ids))
            no_update_nodes = tf.nn.embedding_lookup(model.context_embeds,tf.squeeze(train_ids))
            update_nodes = tf.scatter_nd(test_ids, update_nodes, tf.shape(model.context_embeds))
            no_update_nodes = tf.stop_gradient(tf.scatter_nd(train_ids, no_update_nodes, tf.shape(model.context_embeds)))
            model.context_embeds = update_nodes + no_update_nodes
            sess.run(model.context_embeds)

            # run random walks
            from graphsage.utils import run_random_walks
            nodes = [n for n in G.nodes_iter() if G.node[n]["val"] or G.node[n]["test"]]
            start_time = time.time()
            pairs = run_random_walks(G, nodes, num_walks=50)
            walk_time = time.time() - start_time

            test_minibatch = EdgeMinibatchIterator(G,
                id_map,
                placeholders, batch_size=FLAGS.batch_size,
                max_degree=FLAGS.max_degree,
                num_neg_samples=FLAGS.neg_sample_size,
                context_pairs = pairs,
                n2v_retrain=True,
                fixed_n2v=True)

            start_time = time.time()
            print("Doing test training for n2v.")
            test_steps = 0
            for epoch in range(FLAGS.n2v_test_epochs):
                test_minibatch.shuffle()
                while not test_minibatch.end():
                    feed_dict = test_minibatch.next_minibatch_feed_dict()
                    feed_dict.update({placeholders['dropout']: FLAGS.dropout})
                    outs = sess.run([model.opt_op, model.loss, model.ranks, model.aff_all,
                        model.mrr, model.outputs1], feed_dict=feed_dict)
                    if test_steps % FLAGS.print_every == 0:
                        print("Iter:", '%04d' % test_steps,
                              "train_loss=", "{:.5f}".format(outs[1]),
                              "train_mrr=", "{:.5f}".format(outs[-2]))
                    test_steps += 1
            train_time = time.time() - start_time
            save_val_embeddings(sess, model, minibatch, FLAGS.validate_batch_size, log_dir(), mod="-test")
            print("Total time: ", train_time+walk_time)
            print("Walk time: ", walk_time)
            print("Train time: ", train_time)
188/10:
def main(argv=None):
    print("Loading training data..")
    train_data = load_data(FLAGS.train_prefix, load_walks=True)
    print("Done loading training data..")
    train(train_data)
188/11: FLAGS.train_prefix
188/12: FLAGS.train_prefix='./example_data/ppi'
188/13:
if __name__ == '__main__':
    tf.app.run()
188/14: FLAGS.model
188/15: FLAGS.model='graphsage_mean'
188/16:
if __name__ == '__main__':
    tf.app.run()
188/17: FLAGS.train_prefix
188/18: FLAGS.dropout
188/19: FLAGS.batch_size
188/20: FLAGS.max_degree
188/21: FLAGS.neg_sample_size
188/22: context_pairs
188/23: minibatch.adj.shape
188/24: run -m graphsage.unsupervised_train --train_prefix ./example_data/ppi --model graphsage_mean --max_total_steps 1000 --validate_iter 10
188/25: !python -m graphsage.unsupervised_train --train_prefix ./example_data/ppi --model graphsage_mean --max_total_steps 1000 --validate_iter 10
188/26: FLAGS.max_degree
188/27: !topu
188/28: prefix
188/29: dataset_dir
188/30: FLAGS.train_prefix
188/31: prefix=FLAGS.train_prefix
188/32: G_data = json.load(open(prefix + "-G.json"))
188/33: import json
188/34: G_data = json.load(open(prefix + "-G.json"))
188/35: dir(G_data)
188/36: G_data.keys
188/37: G_data.keys()
188/38: len(G_data['graph'])
188/39: G_data['graph']
188/40: [len(G_data[i]) for i in G_data.keys()]
188/41: [len(G_data[i]) for i in G_data.iterkeys]
188/42: i='nodes'
188/43: len(G_data[i])
188/44: i='links'
188/45: len(G_data[i])
188/46: i='multigraph'
188/47: len(G_data[i])
188/48: G_data[i]
188/49: i='directed'
188/50: G_data[i]
188/51: i='nodes'
188/52: type(G_data[i])
188/53: G_data[i][:5]
188/54: G_data[i][4]
188/55: type(G_data[i][4])
188/56: G_data[i][4].keys()
188/57: G_data[i][4][j]
188/58: j='test'
188/59: G_data[i][4][j]
188/60: j='label'
188/61: G_data[i][4][j]
188/62: len(G_data[i][4][j])
188/63: set(G_data[i][4][j])
188/64: G_data[i][4][j]
188/65: G_data[i][4].keys()
188/66: j='id'
188/67: G_data[i][4][j]
188/68: set(G_data[i][:][j])
188/69: G_data[i][5][j]
188/70: j='val'
188/71: G_data[i][5][j]
188/72: set([k for k in G_data[i][k][j]])
188/73: set([G_data[i][k][j] for k in range(1000) ])
188/74: len(G_data[i])
188/75: set([G_data[i][k][j] for k in range(14755) ])
188/76: len([G_data[i][k][j] for k in range(14755) if G_data[i][k][j]==True ])
188/77: v=[G_data[i][k][j] for k in range(14755) if G_data[i][k][j]==True ]
188/78: v[:5]
188/79: G_data[i][4].keys()
188/80: j='label'
188/81: v=[G_data[i][k][j] for k in range(14755) if G_data[i][k][j]==True ]
188/82: v[:5]
188/83: v=[G_data[i][k][j] for k in range(14755) if G_data[i][k]['val']==True ]
188/84: v[:5]
188/85: set(v[:5])
188/86: set(v[5])
188/87: i
188/88: k
188/89: j
188/90: G_data[i][4].keys()
188/91: j='feature'
188/92: G_data[i][5][j]
188/93: len(G_data[i][5][j])
188/94: len(G_data[i][50][j])
188/95: k
188/96: set([len(G_data[i][k][j]) for k in range(14754)])
189/1:
mport matplotlib.pyplot as plt
import networkx as nx

G = nx.grid_2d_graph(5, 5)  # 5x5 grid

# print the adjacency list
for line in nx.generate_adjlist(G):
    print line
189/2:
import matplotlib.pyplot as plt
import networkx as nx

G = nx.grid_2d_graph(5, 5)  # 5x5 grid

# print the adjacency list
for line in nx.generate_adjlist(G):
    print line
189/3: %pip install --user matplotlib
189/4: !pip install --user matplotlib
189/5:
import matplotlib.pyplot as plt
import networkx as nx

G = nx.grid_2d_graph(5, 5)  # 5x5 grid

# print the adjacency list
for line in nx.generate_adjlist(G):
    print line
189/6:
nx.write_edgelist(G, path="grid.edgelist", delimiter=":")
# read edgelist from grid.edgelist
H = nx.read_edgelist(path="grid.edgelist", delimiter=":")

pos = nx.spring_layout(H, seed=200)
nx.draw(H, pos)
plt.show()
189/7:
nx.write_edgelist(G, path="grid.edgelist", delimiter=":")
# read edgelist from grid.edgelist
H = nx.read_edgelist(path="grid.edgelist", delimiter=":")

pos = nx.spring_layout(H)
nx.draw(H, pos)
plt.show()
189/8:
G = nx.grid_2d_graph(4, 4)
nx.write_edgelist(G, path="grid.edgelist", delimiter=":")
# read edgelist from grid.edgelist
H = nx.read_edgelist(path="grid.edgelist", delimiter=":")

pos = nx.spring_layout(H)
nx.draw(H, pos)
plt.show()
189/9:
G = nx.grid_2d_graph(14, 14)
nx.write_edgelist(G, path="grid.edgelist", delimiter=":")
# read edgelist from grid.edgelist
H = nx.read_edgelist(path="grid.edgelist", delimiter=":")

pos = nx.spring_layout(H)
nx.draw(H, pos)
plt.show()
189/10:
G = nx.grid_2d_graph(144, 144)
nx.write_edgelist(G, path="grid.edgelist", delimiter=":")
# read edgelist from grid.edgelist
H = nx.read_edgelist(path="grid.edgelist", delimiter=":")

pos = nx.spring_layout(H)
nx.draw(H, pos)
plt.show()
189/11:
G = nx.grid_2d_graph(14, 14)
nx.write_edgelist(G, path="grid.edgelist", delimiter=":")
# read edgelist from grid.edgelist
H = nx.read_edgelist(path="grid.edgelist", delimiter=":")

pos = nx.spring_layout(H)
nx.draw(H, pos)
plt.show()
189/12:
G = nx.grid_2d_graph(14, 14)
nx.write_edgelist(G, path="grid.edgelist", delimiter=":")
# read edgelist from grid.edgelist
H = nx.read_edgelist(path="grid.edgelist", delimiter=":")

pos = nx.spring_layout(H)
nx.draw(H, pos)
plt.show()
189/13:
G = nx.grid_2d_graph(14, 14)
nx.write_edgelist(G, path="grid.edgelist", delimiter=":")
# read edgelist from grid.edgelist
H = nx.read_edgelist(path="grid.edgelist", delimiter=":")

pos = nx.spring_layout(H)
nx.draw(H, pos)
plt.show()
189/14:
G = nx.grid_2d_graph(14, 14)
nx.write_edgelist(G, path="grid.edgelist", delimiter=":")
# read edgelist from grid.edgelist
H = nx.read_edgelist(path="grid.edgelist", delimiter=":")

pos = nx.spring_layout(H)
nx.draw(H, pos)
plt.show()
189/15:
G = nx.grid_2d_graph(14, 14)
nx.write_edgelist(G, path="grid.edgelist", delimiter=":")
# read edgelist from grid.edgelist
H = nx.read_edgelist(path="grid.edgelist", delimiter=":")

pos = nx.spring_layout(H)
nx.draw(H, pos)
plt.show()
189/16:
G = nx.grid_2d_graph(14, 14)
nx.write_edgelist(G, path="grid.edgelist", delimiter=":")
# read edgelist from grid.edgelist
H = nx.read_edgelist(path="grid.edgelist", delimiter=":")

pos = nx.spring_layout(H)
nx.draw(H, pos)
plt.show()
190/1:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas
190/2:
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
190/3:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
190/4:
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
190/5:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
190/6:
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
190/7:
import geopandas
cases = geopandas.read_file("cholera_cases.gpkg")
190/8:
import geopandas
cases = geopandas.read_file("cholera_cases.gpkg")
190/9: case.head()
190/10: cases.head()
190/11: stn.head()
190/12: df2.head()
190/13: df2['Count']=[int(i) for i in df2.COUNTYSN]
190/14: df2.head()
190/15: cases=df2[:]
190/16: coordinates = np.column_stack((cases.geometry.x, cases.geometry.y))
190/17:
coordinates = np.column_stack((cases.geometry.x, cases.geometry.y))
cells, generators = voronoi_frames(coordinates, clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/18: type(coordinates)
190/19: coordinates[:5]
190/20:
coordinates = np.column_stack((cases.geometry.x, cases.geometry.y))
cells, generators = voronoi_frames(coordinates, radius=None, clip='extent')#clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/21: %pip install --user pygeos
190/22: %pip install --user rtree
190/23:
coordinates = np.column_stack((cases.geometry.x, cases.geometry.y))
cells, generators = voronoi_frames(coordinates, radius=None, clip='extent')#clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/24:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
190/25:
coordinates = np.column_stack((cases.geometry.x, cases.geometry.y))
cells, generators = voronoi_frames(coordinates, radius=None, clip='extent')#clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/26:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
190/27:
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
190/28:
import geopandas
cases = geopandas.read_file("cholera_cases.gpkg")
190/29: df2['Count']=[int(i) for i in df2.COUNTYSN]
190/30: cases=df2[:]
190/31:
coordinates = np.column_stack((cases.geometry.x, cases.geometry.y))
cells, generators = voronoi_frames(coordinates, radius=None, clip='extent')#clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/32: %pip install --user rtree
190/33:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
190/34:
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
190/35:
import geopandas
cases = geopandas.read_file("cholera_cases.gpkg")
190/36: df2['Count']=[int(i) for i in df2.COUNTYSN]
190/37: cases=df2[:]
190/38:
coordinates = np.column_stack((cases.geometry.x, cases.geometry.y))
cells, generators = voronoi_frames(coordinates, radius=None, clip='extent')#clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/39: %pip install --user geopandas
190/40:
import geopandas
import rtree
cases = geopandas.read_file("cholera_cases.gpkg")
190/41:
coordinates = np.column_stack((cases.geometry.x, cases.geometry.y))
cells, generators = voronoi_frames(coordinates, radius=None, clip='extent')#clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/42:
import os
os.environ["USE_PYGEOS"] = "1"
import geopandas
import rtree
cases = geopandas.read_file("cholera_cases.gpkg")
190/43: df2['Count']=[int(i) for i in df2.COUNTYSN]
190/44: cases=df2[:]
190/45:
coordinates = np.column_stack((cases.geometry.x, cases.geometry.y))
cells, generators = voronoi_frames(coordinates, radius=None, clip='extent')#clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/46: %pip uninstall --user geopandas
190/47: %pip uninstall geopandas
190/48: %pip uninstall -y geopandas
190/49: %pip install --user geopandas
190/50:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
190/51:
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
190/52:
import os
os.environ["USE_PYGEOS"] = "1"
import geopandas
import rtree
cases = geopandas.read_file("cholera_cases.gpkg")
190/53: df2['Count']=[int(i) for i in df2.COUNTYSN]
190/54: cases=df2[:]
190/55:
coordinates = np.column_stack((cases.geometry.x, cases.geometry.y))
cells, generators = voronoi_frames(coordinates, radius=None, clip='extent')#clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/56:
coordinates = np.column_stack((df2.geometry.x, df2.geometry.y))
cells, generators = voronoi_frames(coordinates, radius=None, clip='extent')#clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/57:
coordinates = np.column_stack((df2.geometry.x, df2.geometry.y))
cells, generators = voronoi_frames(coordinates, clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/58:
%pip uninstall rtree
!sudo install -y libspatialindex-dev
%pip install --user rtree
190/59:
%pip uninstall -y rtree
!sudo install -y libspatialindex-dev
%pip install --user rtree
190/60:
%pip uninstall -y rtree
!sudo yum install -y libspatialindex-dev
%pip install --user rtree
190/61:
import os
import pygeos
import geopandas
geopandas.options.use_pygeos = True
cases = geopandas.read_file("cholera_cases.gpkg")
190/62: %pip install --user pygeos
190/63:
import os
import pygeos
import geopandas
geopandas.options.use_pygeos = True
cases = geopandas.read_file("cholera_cases.gpkg")
190/64:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
190/65:
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
190/66:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
import pygeos
190/67:
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
190/68:
%pip uninstall geopandas
%pip install --user geopandas
190/69:
%pip uninstall -y geopandas
%pip install --user geopandas
190/70:
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
190/71:
gpd.options.use_pygeos = False
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
190/72:
import os
import pygeos
import geopandas
geopandas.options.use_pygeos = True
cases = geopandas.read_file("cholera_cases.gpkg")
190/73:
import os
import rtree
import pygeos
import geopandas
geopandas.options.use_pygeos = True
cases = geopandas.read_file("cholera_cases.gpkg")
190/74:
import os
import rtree
import pygeos
import geopandas
gpd.options.use_pygeos = True
cases = geopandas.read_file("cholera_cases.gpkg")
190/75:
import os
import rtree
import pygeos
import geopandas
gpd.options.use_pygeos = True
cases = gpd.read_file("cholera_cases.gpkg")
190/76:
gpd.show_versions()
print(gpd.options.use_pygeos)
190/77:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
import pygeos
190/78:
gpd.options.use_pygeos = False
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df1.to_file('stn.shp',mode='w')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
190/79:
import os
import rtree
import pygeos
import geopandas
gpd.options.use_pygeos = True
cases = gpd.read_file("cholera_cases.gpkg")
190/80:
import os
import rtree
import pygeos
import geopandas
gpd.options.use_pygeos = False
cases = gpd.read_file("cholera_cases.gpkg")
190/81:
gpd.show_versions()
print(gpd.options.use_pygeos)
190/82:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
import pygeos
190/83:
gpd.show_versions()
print(gpd.options.use_pygeos)
190/84:
gpd.show_versions()
print(gpd.options.use_pygeos)
190/85:
gpd.show_versions()
print(gpd.options.use_pygeos)
190/86:
gpd.show_versions()
print(gpd.options.use_pygeos)
190/87:
gpd.show_versions()
print(gpd.options.use_pygeos)
190/88:
import netCDF4
root=/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/LGHAP.PM25.D001
fname=root/tempTW.nc
nc = netCDF4.Dataset(fname,'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
x,y=pnyc(df2.geometry.x,df2.geometry.y, inverse=False)
190/89:
import netCDF4
root='/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/LGHAP.PM25.D001'
fname=root+'/tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
x,y=pnyc(df2.geometry.x,df2.geometry.y, inverse=False)
190/90:
import netCDF4
from poyproj import Proj
root='/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/LGHAP.PM25.D001'
fname=root+'/tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
x,y=pnyc(df2.geometry.x,df2.geometry.y, inverse=False)
190/91: %pip install --user pyproj
190/92:
import netCDF4
from poyproj import Proj
root='/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/LGHAP.PM25.D001'
fname=root+'/tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
x,y=pnyc(df2.geometry.x,df2.geometry.y, inverse=False)
190/93:
import netCDF4
from pyproj import Proj
root='/nas2/cmaqruns/2019TZPP/output/Annual/aTZPP/LGHAP.PM25.D001'
fname=root+'/tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET, lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
x,y=pnyc(df2.geometry.x,df2.geometry.y, inverse=False)
190/94:
coordinates = np.column_stack(x,y) #(df2.geometry.x, df2.geometry.y))
cells, generators = voronoi_frames(coordinates, clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/95:
coordinates = np.column_stack((x,y)) #(df2.geometry.x, df2.geometry.y))
cells, generators = voronoi_frames(coordinates, clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
190/96:
import os
import rtree
import pygeos
import geopandas
gpd.options.use_pygeos = False
cases = gpd.read_file("cholera_cases.gpkg")
190/97:
coordinates = np.column_stack((x,y)) #(df2.geometry.x, df2.geometry.y))
cells, generators = voronoi_frames(coordinates, clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
194/1:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
import pygeos
195/1:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
import pygeos
195/2:
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
gpd.options.use_pygeos = False
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1=gpd.read_file('mainisland.shp')
df2=df0.loc[22:]
df2.to_file('stn.shp',mode='w')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
196/1:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
import pygeos
196/2:
df1=gpd.read_file('mainisland.shp')
df2 = gpd.read_file('stn.shp')
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
197/1:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
import pygeos
197/2:
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)

CNTYNAM=set(gdf.COUNTYNAME)-{'金門縣','澎湖縣','連江縣'}
ifirst=1
for c in list(CNTYNAM)[:]:
    a=gdf.loc[gdf.COUNTYNAME==c].reset_index(drop=True)
    area=[i.area for i in a.geometry]
    imax=area.index(max(area))
    if len(a)==1:
        b=a.to_crs(epsg=4326)
    else:
        b=a.loc[a.index==imax].reset_index(drop=True).to_crs(epsg=4326)
    if ifirst==1:
        df0=b.to_crs(epsg=4326)
        ifirst=0
    else:
        df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))
stn=pd.read_csv('/nas1/cmaqruns/2016base/data/sites/sta_ll.csv')
stnpnt=[Point(i,j) for i,j in zip(stn.lon,stn.lat)]

for i in range(len(stn)):
    b=gpd.GeoDataFrame({'COUNTYSN':stn.loc[i,'ID'] ,'COUNTYNAME':stn.loc[i,'New'],'geometry':[stnpnt[i]]})
    df0=gpd.GeoDataFrame(pd.concat([df0,b],ignore_index=True))

df1=df0.loc[:21]
df1.to_file('mainisland.shp',mode='w')
df2=df0.loc[22:]
df2.to_file('stn.shp',mode='w')        
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
coords = points_to_coords(gdf_proj.geometry)
region_polys, region_pts = voronoi_regions_from_coords(coords, boundary_shape)
boundary = gpd.read_file("mainisland.shp")
fig, ax = subplot_for_map(figsize=(12, 10))
plot_voronoi_polys_with_points_in_area(ax, boundary_shape, region_polys, coords, region_pts)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.tight_layout()
plt.show()
198/1:
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
       
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)
198/2:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
import pygeos
198/3:
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
       
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
boundary_shape = unary_union(boundary.geometry)
199/1:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
import pygeos
199/2:
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
       
boundary = gpd.read_file("mainisland.shp")
boundary = boundary.to_crs(epsg=4326)
199/3: boundary_shape = unary_union(boundary.geometry)
201/1:
import numpy as np
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
boundary = gpd.read_file("mainisland.shp")
df2=gpd.read_file('stn.shp')
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")

boundary.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
202/1:
from libpysal import weights, examples
from libpysal.cg import voronoi_frames
from contextily import add_basemap
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import geopandas as gpd
import pandas as pd
202/2:
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
df1=gpd.read_file(root+'boundary_shape.shp')
df2 = gpd.read_file(root+'stn.shp')
boundary = df1.to_crs(epsg=4326)
boundary_shape = df1.geometry[1]

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
202/3:
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
df2 = gpd.read_file(root+'stn.shp')
#boundary = df1.to_crs(epsg=4326)
boundary_shape = df1.geometry[1]

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
202/4:
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
df2 = gpd.read_file(root+'stn.shp')
#boundary = df1.to_crs(epsg=4326)
boundary_shape = boundary.geometry[1]

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
202/5:
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
df2 = gpd.read_file(root+'stn.shp')
#boundary = df1.to_crs(epsg=4326)
boundary_shape = boundary.geometry[0]

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs)
202/6:
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
df2 = gpd.read_file(root+'stn.shp')
#boundary = df1.to_crs(epsg=4326)
boundary_shape = boundary.geometry[0]

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(boundary.crs(epsg=4326))
202/7:
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
df2 = gpd.read_file(root+'stn.shp')
#boundary = df1.to_crs(epsg=4326)
boundary_shape = boundary.geometry[0]

df2=df2.reset_index(drop=True)
for i in range(len(df2)):
    p=df2.loc[i,'geometry']
    if not p.within(boundary_shape): # or boundary_shape.exterior.distance(p) < 0.01:
        df2=df2.drop(i)
df2=df2.reset_index(drop=True)
gdf_proj = df2.to_crs(epsg=4326)
202/8:
coordinates = np.column_stack((df2.geometry.x, df2.geometry.y))
cells, generators = voronoi_frames(coordinates, clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2)
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
202/9:
coordinates = np.column_stack((df2.geometry.x, df2.geometry.y))
cells, generators = voronoi_frames(coordinates, clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2,figsize=(12, 10))
add_basemap(ax)
ax.axis("off")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
202/10:
coordinates = np.column_stack((df2.geometry.x, df2.geometry.y))
cells, generators = voronoi_frames(coordinates, clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2,figsize=(12, 10))
add_basemap(ax)
ax.axis("off")
boundary.plot(ax=ax, color="gray")
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
202/11:
coordinates = np.column_stack((df2.geometry.x, df2.geometry.y))
cells, generators = voronoi_frames(coordinates, clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2,figsize=(12, 10))
add_basemap(ax)
ax.axis("off")
boundary.plot(ax=ax, color="gray", alpha=0.50)
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
202/12:
coordinates = np.column_stack((df2.geometry.x, df2.geometry.y))
cells, generators = voronoi_frames(coordinates, clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2,figsize=(12, 10))
add_basemap(ax)
#ax.axis("off")
boundary.plot(ax=ax, color="gray", alpha=0.50)
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
plt.show()
202/13:
gpd.show_versions()
print(gpd.options.use_pygeos)
202/14:
coordinates = np.column_stack((df2.geometry.x, df2.geometry.y))
cells, generators = voronoi_frames(coordinates, clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2,figsize=(12, 10))
add_basemap(ax)
#ax.axis("off")
boundary.plot(ax=ax, color="gray", alpha=0.50)
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
ax.set_title('Voronoi regions of Air Quality Station Networks')
plt.show()
202/15:
coordinates = np.column_stack((df2.geometry.x, df2.geometry.y))
cells, generators = voronoi_frames(coordinates, clip="convex hull")
delaunay = weights.Rook.from_dataframe(cells)
delaunay_graph = delaunay.to_networkx()
positions = dict(zip(delaunay_graph.nodes, coordinates))
ax = cells.plot(facecolor="lightblue", alpha=0.50, edgecolor="cornsilk", linewidth=2,figsize=(12, 10))
add_basemap(ax)
#ax.axis("off")
boundary.plot(ax=ax, color="gray", alpha=0.50)
nx.draw(
    delaunay_graph,
    positions,
    ax=ax,
    node_size=2,
    node_color="k",
    edge_color="k",
    alpha=0.8,
)
ax.set_title('Voronoi and Delaunay links of Taiwan Air Quality Station Networks')
plt.show()
204/1: a=5
204/2: print(a)
204/3: !python -V
204/4: cd example_data/
204/5: ls
204/6: import jsnon
204/7: %pip install json
204/8: !pip install json
204/9: !pip install --user json
204/10: qiot
205/1: import json
205/2: %history -g -f his.txt
205/3: !grep json his.txt|T
205/4: ls
205/5: prefix='ppi'
205/6: G_data = json.load(open(prefix + "-G.json"))
205/7: !grep G_data his.txt|M
205/8: !vi G_data his.txt
205/9: G_data.keys()
205/10: i=u'links'
205/11: type(G_data[i])
205/12: len(G_data[i])
205/13: i=u'nodes'
205/14: len(G_data[i])
205/15: G_data[i][5].keys()
205/16: j=u'feature'
205/17: len(G_data[i][5][j])
205/18: [len(G_data[i][5][j]) for j in G_data[i][5].keys()]
205/19: [len(G_data[i][5][j]) for j in G_data[i][5].keys() if type(G_data[i][5][j])==list]
205/20: [j for j in G_data[i][5].keys() if type(G_data[i][5][j])==list]
205/21: [(type(j),j) for j in G_data[i][5].keys()]
205/22: [(type(G_data[i][5][j]),j) for j in G_data[i][5].keys()]
205/23: j= u'feature'
205/24: set(G_data[i][5][j])
205/25: set([set(G_data[i][k][j]) for k in range(14000)])
205/26: set([set(G_data[i][k][j]) for k in range(1,14000)])
205/27: k
205/28: s=set()
205/29:
for k in range(1,14000):
    s+=set(G_data[i][k][j])
205/30:
for k in range(1,14000):
    ss=s|set(G_data[i][k][j])
    s=ss
205/31: s
205/32: j
205/33: i
205/34: i='links'
205/35: G_data[i][5].keys()
205/36: [(type(G_data[i][5][j]),j) for j in G_data[i][5].keys()]
205/37: j=u'target'
205/38: G_data[i][:5][j]
205/39: G_data[i][5][j]
205/40: [(k,G_data[i][k][j]),j) for k in range(5)]
205/41: [(k,G_data[i][k][j],j) for k in range(5)]
205/42: j=u'source'
205/43: [(k,G_data[i][k][j],j) for k in range(5)]
205/44: src=list(set([G_data[i][k][j] for k in range(140000)]))
205/45: src.sort()
205/46: src[-5:]
205/47: src[:5]
205/48: j=u'target'
205/49: tgt=list(set([G_data[i][k][j] for k in range(140000)]))
205/50: tgt.sort()
205/51: tgt[-5:]
205/52: i=u'nodes'
205/53: j='id'
205/54: ids=list(set([G_data[i][k][j] for k in range(140000)]))
205/55: i='links'
205/56: j=u'target'
205/57: tgt=list(set([G_data[i][k][j] for k in range(228000)]))
205/58: tgt.sort()
205/59: tgt[-5:]
205/60: len(G_data[i][5][j])
205/61: i=u'nodes'
205/62: j='id'
205/63: len(G_data[i][5][j])
205/64: j= u'feature'
205/65: len(G_data[i][5][j])
205/66: len(G_data[i][j])
205/67: len(G_data[i])
205/68: i='links'
205/69: len(G_data[i])
205/70: n=len(G_data[i])
205/71: i='links'
205/72: j=u'source'
205/73: src=list(set([G_data[i][k][j] for k in range(n)]))
205/74: src.sort()
205/75: src[-5:]
207/1: ls
207/2: pwd
208/1: cd ../
208/2: mkdir exam2
208/3: cd  exam2
208/4:
import torch
import torch.nn.functional as F
from torch_geometric_temporal.nn.recurrent import GConvGRU

class RecurrentGCN(torch.nn.Module):

    def __init__(self, node_features, num_classes):
        super(RecurrentGCN, self).__init__()
        self.recurrent_1 = GConvGRU(node_features, 32, 5)
        self.recurrent_2 = GConvGRU(32, 16, 5)
        self.linear = torch.nn.Linear(16, num_classes)

    def forward(self, x, edge_index, edge_weight):
        x = self.recurrent_1(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.recurrent_2(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.linear(x)
        return F.log_softmax(x, dim=1)
208/5: !pip install --user torch
208/6:
import torch
import torch.nn.functional as F
from torch_geometric_temporal.nn.recurrent import GConvGRU

class RecurrentGCN(torch.nn.Module):

    def __init__(self, node_features, num_classes):
        super(RecurrentGCN, self).__init__()
        self.recurrent_1 = GConvGRU(node_features, 32, 5)
        self.recurrent_2 = GConvGRU(32, 16, 5)
        self.linear = torch.nn.Linear(16, num_classes)

    def forward(self, x, edge_index, edge_weight):
        x = self.recurrent_1(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.recurrent_2(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.linear(x)
        return F.log_softmax(x, dim=1)
209/1:
import torch
import torch.nn.functional as F
from torch_geometric_temporal.nn.recurrent import GConvGRU

class RecurrentGCN(torch.nn.Module):

    def __init__(self, node_features, num_classes):
        super(RecurrentGCN, self).__init__()
        self.recurrent_1 = GConvGRU(node_features, 32, 5)
        self.recurrent_2 = GConvGRU(32, 16, 5)
        self.linear = torch.nn.Linear(16, num_classes)

    def forward(self, x, edge_index, edge_weight):
        x = self.recurrent_1(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.recurrent_2(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.linear(x)
        return F.log_softmax(x, dim=1)
210/1:
import torch
import torch.nn.functional as F
from torch_geometric_temporal.nn.recurrent import GConvGRU

class RecurrentGCN(torch.nn.Module):

    def __init__(self, node_features, num_classes):
        super(RecurrentGCN, self).__init__()
        self.recurrent_1 = GConvGRU(node_features, 32, 5)
        self.recurrent_2 = GConvGRU(32, 16, 5)
        self.linear = torch.nn.Linear(16, num_classes)

    def forward(self, x, edge_index, edge_weight):
        x = self.recurrent_1(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.recurrent_2(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.linear(x)
        return F.log_softmax(x, dim=1)
211/1:
import torch
import torch.nn.functional as F
from torch_geometric_temporal.nn.recurrent import GConvGRU

class RecurrentGCN(torch.nn.Module):

    def __init__(self, node_features, num_classes):
        super(RecurrentGCN, self).__init__()
        self.recurrent_1 = GConvGRU(node_features, 32, 5)
        self.recurrent_2 = GConvGRU(32, 16, 5)
        self.linear = torch.nn.Linear(16, num_classes)

    def forward(self, x, edge_index, edge_weight):
        x = self.recurrent_1(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.recurrent_2(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.linear(x)
        return F.log_softmax(x, dim=1)
212/1:
import torch
import torch.nn.functional as F
from torch_geometric_temporal.nn.recurrent import GConvGRU

class RecurrentGCN(torch.nn.Module):

    def __init__(self, node_features, num_classes):
        super(RecurrentGCN, self).__init__()
        self.recurrent_1 = GConvGRU(node_features, 32, 5)
        self.recurrent_2 = GConvGRU(32, 16, 5)
        self.linear = torch.nn.Linear(16, num_classes)

    def forward(self, x, edge_index, edge_weight):
        x = self.recurrent_1(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.recurrent_2(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.linear(x)
        return F.log_softmax(x, dim=1)
212/2: qiot
213/1:
import torch
import torch.nn.functional as F
from torch_geometric_temporal.nn.recurrent import GConvGRU

class RecurrentGCN(torch.nn.Module):

    def __init__(self, node_features, num_classes):
        super(RecurrentGCN, self).__init__()
        self.recurrent_1 = GConvGRU(node_features, 32, 5)
        self.recurrent_2 = GConvGRU(32, 16, 5)
        self.linear = torch.nn.Linear(16, num_classes)

    def forward(self, x, edge_index, edge_weight):
        x = self.recurrent_1(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.recurrent_2(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.linear(x)
        return F.log_softmax(x, dim=1)
213/2: !python -m pytest test
214/1:
import torch
import torch.nn.functional as F
from torch_geometric_temporal.nn.recurrent import DCRNN

class RecurrentGCN(torch.nn.Module):
    def __init__(self, node_features):
        super(RecurrentGCN, self).__init__()
        self.recurrent = DCRNN(node_features, 32, 1)
        self.linear = torch.nn.Linear(32, 1)

    def forward(self, x, edge_index, edge_weight):
        h = self.recurrent(x, edge_index, edge_weight)
        h = F.relu(h)
        h = self.linear(h)
        return h
214/2:
from tqdm import tqdm

model = RecurrentGCN(node_features = 4)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

model.train()

for epoch in tqdm(range(200)):
    cost = 0
    for time, snapshot in enumerate(train_dataset):
        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
        cost = cost + torch.mean((y_hat-snapshot.y)**2)
    cost = cost / (time+1)
    cost.backward()
    optimizer.step()
    optimizer.zero_grad()
214/3:
from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader
from torch_geometric_temporal.signal import temporal_signal_split

loader = ChickenpoxDatasetLoader()

dataset = loader.get_dataset()

train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)
214/4:
from tqdm import tqdm

model = RecurrentGCN(node_features = 4)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

model.train()

for epoch in tqdm(range(200)):
    cost = 0
    for time, snapshot in enumerate(train_dataset):
        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
        cost = cost + torch.mean((y_hat-snapshot.y)**2)
    cost = cost / (time+1)
    cost.backward()
    optimizer.step()
    optimizer.zero_grad()
214/5:
model.eval()
cost = 0
for time, snapshot in enumerate(test_dataset):
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
214/6:
from torch_geometric_temporal.dataset import WikiMathsDatasetLoader
from torch_geometric_temporal.signal import temporal_signal_split

loader = WikiMathsDatasetLoader()

dataset = loader.get_dataset(lags=14)

train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)
214/7:
import torch
import torch.nn.functional as F
from torch_geometric_temporal.nn.recurrent import GConvGRU

class RecurrentGCN(torch.nn.Module):
    def __init__(self, node_features, filters):
        super(RecurrentGCN, self).__init__()
        self.recurrent = GConvGRU(node_features, filters, 2)
        self.linear = torch.nn.Linear(filters, 1)

    def forward(self, x, edge_index, edge_weight):
        h = self.recurrent(x, edge_index, edge_weight)
        h = F.relu(h)
        h = self.linear(h)
        return h
214/8:
from tqdm import tqdm

model = RecurrentGCN(node_features=14, filters=32)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

model.train()

for epoch in tqdm(range(50)):
    for time, snapshot in enumerate(train_dataset):
        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
        cost = torch.mean((y_hat-snapshot.y)**2)
        cost.backward()
        optimizer.step()
        optimizer.zero_grad()
215/1:
import json
def read_json_file(filename):
    with open(filename) as f:
        js_graph = json.load(f)
    return json_graph.node_link_graph(js_graph)
215/2:
fname='dataset/chickenpox.json'
g=read_json_file(fname)
215/3:
import json
from networkx.readwrite import json_graph
def read_json_file(filename):
    with open(filename) as f:
        js_graph = json.load(f)
    return json_graph.node_link_graph(js_graph)
215/4:
fname='dataset/chickenpox.json'
g=read_json_file(fname)
215/5: from .chickenpox import ChickenpoxDatasetLoader
215/6: from dataset.chickenpox import ChickenpoxDatasetLoader
215/7: from chickenpox import ChickenpoxDatasetLoader
215/8: from torch_geometric_temporal.dataset.chickenpox import ChickenpoxDatasetLoader
215/9: loader = ChickenpoxDatasetLoader()
215/10: dataset = loader.get_dataset()
215/11: import networkx as nx
215/12: dir(dataset)
215/13: type(dataset.snapshot_count)
215/14: dataset.snapshot_count
215/15: dataset.targets
215/16: type(dataset.targets)
215/17: len(dataset.targets)
215/18: dataset.targets[:5]
215/19: type(dataset.features)
215/20: len(dataset.features)
215/21: len(dataset.features[0])
215/22:
from torch_geometric.datasets import Planetoid
dataset = Planetoid(root='/tmp/Cora', name='Cora')
print(len(dataset))
print(dataset.num_classes)
print(dataset.num_node_features)
215/23:
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

class GCN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GCNConv(dataset.num_node_features, 16)
        self.conv2 = GCNConv(16, dataset.num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)

        return F.log_softmax(x, dim=1)
    
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GCN().to(device)
data = dataset[0].to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

model.train()
for epoch in range(200):
    optimizer.zero_grad()
    out = model(data)
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()
215/24:
model.eval()
pred = model(data).argmax(dim=1)
correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()
acc = int(correct) / int(data.test_mask.sum())
print(f'Accuracy: {acc:.4f}')
216/1: !python torch_geometric_temporal/dataset/montevideo_bus.py
216/2:
from torch_geometric_temporal.dataset import (
    TwitterTennisDatasetLoader,
    MontevideoBusDatasetLoader,
    MTMDatasetLoader,
216/3:
from torch_geometric_temporal.dataset import (
    TwitterTennisDatasetLoader,
    MontevideoBusDatasetLoader,
    MTMDatasetLoader,)
216/4:
from torch_geometric_temporal.dataset import (
    TwitterTennisDatasetLoader,
    MontevideoBusDatasetLoader,
    MTMDatasetLoader,)
def test_montevideobus():
    loader = MontevideoBusDatasetLoader()
    dataset = loader.get_dataset()
    for epoch in range(1):
        for snapshot in dataset:
            assert snapshot.edge_index.shape == (2, 690)
            assert snapshot.edge_attr.shape == (690,)
            assert snapshot.x.shape == (675, 4)
            assert snapshot.y.shape == (675,)
216/5:
from torch_geometric_temporal.dataset import (
    TwitterTennisDatasetLoader,
    MontevideoBusDatasetLoader,
    MTMDatasetLoader,)

def test_montevideobus():
    loader = MontevideoBusDatasetLoader()
    dataset = loader.get_dataset()
    for epoch in range(1):
        for snapshot in dataset:
            assert snapshot.edge_index.shape == (2, 690)
            assert snapshot.edge_attr.shape == (690,)
            assert snapshot.x.shape == (675, 4)
            assert snapshot.y.shape == (675,)
from torch_geometric_temporal.dataset.montevideo_bus import MontevideoBusDatasetLoader
216/6:
import torch
import numpy as np
from typing import Sequence, Union
from torch_geometric.data import Data


Edge_Indices = Sequence[Union[np.ndarray, None]]
Edge_Weights = Sequence[Union[np.ndarray, None]]
Node_Feature = Union[np.ndarray, None]
Targets = Sequence[Union[np.ndarray, None]]
Additional_Features = Sequence[np.ndarray]
216/7: run ./torch_geometric_temporal/signal/dynamic_graph_static_signal.py
216/8: type(dataset)
216/9: loader=MontevideoBusDatasetLoader()
216/10: type(dataset)
216/11: type(loader)
216/12:
dataset = loader.get_dataset()
for epoch in range(1):
    for snapshot in dataset:
        assert snapshot.edge_index.shape == (2, 690)
        assert snapshot.edge_attr.shape == (690,)
        assert snapshot.x.shape == (675, 4)
        assert snapshot.y.shape == (675,)
type(dataset)
216/13:
dataset = loader.get_dataset()
for epoch in range(1):
    for snapshot in dataset:
        assert snapshot.edge_index.shape == (2, 690)
        assert snapshot.edge_attr.shape == (690,)
        assert snapshot.x.shape == (675, 4)
        assert snapshot.y.shape == (675,)
dir(dataset)
216/14:
print(dataset.t)
print(data.edge_weight)
216/15:
print(dataset.t)
print(dataset.snapshot_count)
print(dataset.edge_weight)
216/16: len(dataset.features)
216/17:
print(dataset.t)
print(dataset.snapshot_count)
print(dataset.edge_index)
216/18:
print(dataset.t)
print(dataset.snapshot_count)
print(len(dataset.edge_index)
216/19:
print(dataset.t)
print(dataset.snapshot_count)
print(len(dataset.edge_index))
216/20: [len(set(dataset.edge_index[i])) for in in range(2)]
216/21: [len(set(dataset.edge_index[i])) for i in range(2)]
216/22: [len(set(dataset.edge_index[:][i])) for i in range(2)]
216/23: [(dataset.edge_index[0][i],dataset.edge_index[1][i]) for i in range(2)]
216/24: [(dataset.features[i],dataset.targets[i]) for i in range(2)]
216/25: [(dataset.edge_index[0][i],dataset.edge_index[1][i]) for i in range(2)]
216/26: [(dataset.edge_weight[0][i],dataset.edge_weight[1][i]) for i in range(2)]
216/27: [(dataset.edge_weight[i]) for i in range(2)]
216/28: print(dataset.edge_weight.max,dataset.edge_weight.min)
216/29: print(dataset.edge_weight.max(),dataset.edge_weight.min())
216/30: print(type(dataset.features),type(dataset.targets),)
216/31: print(type(dataset.features),type(dataset.targets),len(dataset.features),len(dataset.targets),)
216/32:
print(dataset.t)
print(dataset.snapshot_count)
print(len(dataset.edge_index))
print(len(dataset.edge_weight))
216/33:
print(dataset.t)
print(dataset.snapshot_count)
print(len(dataset.edge_index),len(dataset.edge_index[0]))
print(len(dataset.edge_weight))
216/34:
print(dataset.edge_weight.max(),dataset.edge_weight.min())
print(dataset.features.max(),dataset.features.min())
print(dataset.targets.max(),dataset.targets.min())
216/35:
print(dataset.edge_weight.max(),dataset.edge_weight.min())
print(max(dataset.features),min(dataset.features.min))
print(dataset.targets.max(),dataset.targets.min())
216/36:
print(dataset.edge_weight.max(),dataset.edge_weight.min())
print(max(dataset.features[:]),min(dataset.features[:]))
print(dataset.targets.max(),dataset.targets.min())
216/37:
print(dataset.edge_weight.max(),dataset.edge_weight.min())
print(max(dataset.features.all()),min(dataset.features.all()))
print(dataset.targets.max(),dataset.targets.min())
216/38:
print(dataset.edge_weight.max(),dataset.edge_weight.min())
print(np.max(dataset.features),np.min(dataset.features))
print(dataset.targets.max(),dataset.targets.min())
216/39:
print(dataset.edge_weight.max(),dataset.edge_weight.min())
print(np.max(dataset.features),np.min(dataset.features))
print(np.max(dataset.targets),np.min(dataset.targets))
216/40:
print(dataset.features[:5])
print(dataset.targets[:5])
216/41: print(dataset.features[0])
216/42: print(len(dataset.features[0]))
216/43: print(len(dataset.features[1]))
216/44: type(additional_feature_keys)
216/45: type(dataset.additional_feature_keys)
216/46: print(type(dataset.additional_feature_keys)),print(len(dataset.additional_feature_keys)),
216/47: print(type(dataset.additional_feature_keys)),print(len(dataset.additional_feature_keys))
216/48: print(len(dataset.features[10]))
216/49: print((dataset.features[10]))
216/50: print((dataset.features[0]))
216/51: print((dataset.features[9]))
216/52: print((dataset.targets[9]))
216/53: (dataset.features[9]==dataset.targets[9]).all()
216/54: dataset.features[9]==dataset.targets[9]
216/55:
i=0
for snapshot in dataset:
    print(i,type(snapshot))
    i+=1
216/56: print(len(dataset.features[9]),len(dataset.targets[9]))
216/57:
print(len(dataset.features[9]),len(dataset.targets[9]))
print(len(dataset.features[9][0]),len(dataset.targets[9][0]))
216/58:
print(len(dataset.features[9]),len(dataset.targets[9]))
print(len(dataset.features[9][0]))
print(len(dataset.targets[9][0]))
216/59: print(dataset.features[9][0][:])
216/60:
print(dataset.features[9][0][:])
print(dataset.features[10][0][:])
216/61: 1.15019097/.41997799
216/62:
print(dataset.features[9][0][:],dataset.targets[9][0])
print(dataset.features[10][0][:],dataset.targets[10][0])
216/63: y
216/64: x
216/65: dataset.edge_index.index((0, 1))
216/66: list(dataset.edge_index).index((0, 1))
216/67: len('0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0'.split())
216/68: len('0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0'.split())
216/69: len(set(dataset.edge_index))
216/70: len(set(dataset.edge_index[:]))
216/71: len(set(dataset.edge_index[:][0]))
216/72: len(set(dataset.edge_index[:][1]))
217/1:
from stellargraph.mapper import (
    CorruptedGenerator,
    FullBatchNodeGenerator,
    GraphSAGENodeGenerator,
    HinSAGENodeGenerator,
    ClusterNodeGenerator,
)
from stellargraph import StellarGraph
from stellargraph.layer import GCN, DeepGraphInfomax, GraphSAGE, GAT, APPNP, HinSAGE

from stellargraph import datasets
from stellargraph.utils import plot_history

import pandas as pd
from matplotlib import pyplot as plt
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.manifold import TSNE
from IPython.display import display, HTML

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
from tensorflow.keras import Model
217/2: %pip install --user stellargraph
217/3:
from stellargraph.mapper import (
    CorruptedGenerator,
    FullBatchNodeGenerator,
    GraphSAGENodeGenerator,
    HinSAGENodeGenerator,
    ClusterNodeGenerator,
)
from stellargraph import StellarGraph
from stellargraph.layer import GCN, DeepGraphInfomax, GraphSAGE, GAT, APPNP, HinSAGE

from stellargraph import datasets
from stellargraph.utils import plot_history

import pandas as pd
from matplotlib import pyplot as plt
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.manifold import TSNE
from IPython.display import display, HTML

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
from tensorflow.keras import Model
217/4: %pip install --user chardet
217/5:
from stellargraph.mapper import (
    CorruptedGenerator,
    FullBatchNodeGenerator,
    GraphSAGENodeGenerator,
    HinSAGENodeGenerator,
    ClusterNodeGenerator,
)
from stellargraph import StellarGraph
from stellargraph.layer import GCN, DeepGraphInfomax, GraphSAGE, GAT, APPNP, HinSAGE

from stellargraph import datasets
from stellargraph.utils import plot_history

import pandas as pd
from matplotlib import pyplot as plt
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.manifold import TSNE
from IPython.display import display, HTML

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
from tensorflow.keras import Model
217/6:
dataset = datasets.Cora()
display(HTML(dataset.description))
G, node_subjects = dataset.load()
217/7:
fullbatch_generator = FullBatchNodeGenerator(G, sparse=False)
gcn_model = GCN(layer_sizes=[128], activations=["relu"], generator=fullbatch_generator)

corrupted_generator = CorruptedGenerator(fullbatch_generator)
gen = corrupted_generator.flow(G.nodes())
217/8:
infomax = DeepGraphInfomax(gcn_model, corrupted_generator)
x_in, x_out = infomax.in_out_tensors()

model = Model(inputs=x_in, outputs=x_out)
model.compile(loss=tf.nn.sigmoid_cross_entropy_with_logits, optimizer=Adam(lr=1e-3))
217/9:
epochs = 100
es = EarlyStopping(monitor="loss", min_delta=0, patience=20)
history = model.fit(gen, epochs=epochs, verbose=0, callbacks=[es])
plot_history(history)
217/10:
x_emb_in, x_emb_out = gcn_model.in_out_tensors()

# for full batch models, squeeze out the batch dim (which is 1)
x_out = tf.squeeze(x_emb_out, axis=0)
emb_model = Model(inputs=x_emb_in, outputs=x_out)
train_subjects, test_subjects = model_selection.train_test_split(
    node_subjects, train_size=0.1, test_size=None, stratify=node_subjects
)

test_gen = fullbatch_generator.flow(test_subjects.index)
train_gen = fullbatch_generator.flow(train_subjects.index)

test_embeddings = emb_model.predict(test_gen)
train_embeddings = emb_model.predict(train_gen)

lr = LogisticRegression(multi_class="auto", solver="lbfgs")
lr.fit(train_embeddings, train_subjects)

y_pred = lr.predict(test_embeddings)
gcn_acc = (y_pred == test_subjects).mean()
print(f"Test classification accuracy: {gcn_acc}")
217/11:
x_emb_in, x_emb_out = gcn_model.in_out_tensors()

# for full batch models, squeeze out the batch dim (which is 1)
x_out = tf.squeeze(x_emb_out, axis=0)
emb_model = Model(inputs=x_emb_in, outputs=x_out)
train_subjects, test_subjects = model_selection.train_test_split(
    node_subjects, train_size=0.1, test_size=None, stratify=node_subjects
)

test_gen = fullbatch_generator.flow(test_subjects.index)
train_gen = fullbatch_generator.flow(train_subjects.index)

test_embeddings = emb_model.predict(test_gen)
train_embeddings = emb_model.predict(train_gen)

lr = LogisticRegression(multi_class="auto", solver="lbfgs")
lr.fit(train_embeddings, train_subjects)

y_pred = lr.predict(test_embeddings)
gcn_acc = (y_pred == test_subjects).mean()
print(f"Test classification accuracy: {gcn_acc}")
217/12:
epochs =200
es = EarlyStopping(monitor="loss", min_delta=0, patience=20)
history = model.fit(gen, epochs=epochs, verbose=0, callbacks=[es])
plot_history(history)
217/13:
x_emb_in, x_emb_out = gcn_model.in_out_tensors()

# for full batch models, squeeze out the batch dim (which is 1)
x_out = tf.squeeze(x_emb_out, axis=0)
emb_model = Model(inputs=x_emb_in, outputs=x_out)
train_subjects, test_subjects = model_selection.train_test_split(
    node_subjects, train_size=0.1, test_size=None, stratify=node_subjects
)

test_gen = fullbatch_generator.flow(test_subjects.index)
train_gen = fullbatch_generator.flow(train_subjects.index)

test_embeddings = emb_model.predict(test_gen)
train_embeddings = emb_model.predict(train_gen)

lr = LogisticRegression(multi_class="auto", solver="lbfgs")
lr.fit(train_embeddings, train_subjects)

y_pred = lr.predict(test_embeddings)
gcn_acc = (y_pred == test_subjects).mean()
print(f"Test classification accuracy: {gcn_acc}")
217/14:
x_emb_in, x_emb_out = gcn_model.in_out_tensors()

# for full batch models, squeeze out the batch dim (which is 1)
x_out = tf.squeeze(x_emb_out, axis=0)
emb_model = Model(inputs=x_emb_in, outputs=x_out)
train_subjects, test_subjects = model_selection.train_test_split(
    node_subjects, train_size=0.1, test_size=None, stratify=node_subjects
)

test_gen = fullbatch_generator.flow(test_subjects.index)
train_gen = fullbatch_generator.flow(train_subjects.index)

test_embeddings = emb_model.predict(test_gen)
train_embeddings = emb_model.predict(train_gen)

lr = LogisticRegression(multi_class="auto", solver="lbfgs")
lr.fit(train_embeddings, train_subjects)

y_pred = lr.predict(test_embeddings)
gcn_acc = (y_pred == test_subjects).mean()
print(f"Test classification accuracy: {gcn_acc}")
217/15:
epochs =80
es = EarlyStopping(monitor="loss", min_delta=0, patience=20)
history = model.fit(gen, epochs=epochs, verbose=0, callbacks=[es])
plot_history(history)
217/16:
epochs =100
es = EarlyStopping(monitor="loss", min_delta=0, patience=20)
history = model.fit(gen, epochs=epochs, verbose=0, callbacks=[es])
plot_history(history)
217/17:
dataset = datasets.Cora()
display(HTML(dataset.description))
G, node_subjects = dataset.load()
217/18:
fullbatch_generator = FullBatchNodeGenerator(G, sparse=False)
gcn_model = GCN(layer_sizes=[128], activations=["relu"], generator=fullbatch_generator)

corrupted_generator = CorruptedGenerator(fullbatch_generator)
gen = corrupted_generator.flow(G.nodes())
217/19:
infomax = DeepGraphInfomax(gcn_model, corrupted_generator)
x_in, x_out = infomax.in_out_tensors()

model = Model(inputs=x_in, outputs=x_out)
model.compile(loss=tf.nn.sigmoid_cross_entropy_with_logits, optimizer=Adam(learning_rate=1e-3))
217/20:
epochs =120
es = EarlyStopping(monitor="loss", min_delta=0, patience=20)
history = model.fit(gen, epochs=epochs, verbose=0, callbacks=[es])
plot_history(history)
217/21:
x_emb_in, x_emb_out = gcn_model.in_out_tensors()

# for full batch models, squeeze out the batch dim (which is 1)
x_out = tf.squeeze(x_emb_out, axis=0)
emb_model = Model(inputs=x_emb_in, outputs=x_out)
train_subjects, test_subjects = model_selection.train_test_split(
    node_subjects, train_size=0.1, test_size=None, stratify=node_subjects
)

test_gen = fullbatch_generator.flow(test_subjects.index)
train_gen = fullbatch_generator.flow(train_subjects.index)

test_embeddings = emb_model.predict(test_gen)
train_embeddings = emb_model.predict(train_gen)

lr = LogisticRegression(multi_class="auto", solver="lbfgs")
lr.fit(train_embeddings, train_subjects)

y_pred = lr.predict(test_embeddings)
gcn_acc = (y_pred == test_subjects).mean()
print(f"Test classification accuracy: {gcn_acc}")
217/22:
all_embeddings = emb_model.predict(fullbatch_generator.flow(G.nodes()))

y = node_subjects.astype("category")
trans = TSNE(n_components=2)
emb_transformed = pd.DataFrame(trans.fit_transform(all_embeddings), index=G.nodes())
emb_transformed["label"] = y
alpha = 0.7

fig, ax = plt.subplots(figsize=(7, 7))
ax.scatter(
    emb_transformed[0],
    emb_transformed[1],
    c=emb_transformed["label"].cat.codes,
    cmap="jet",
    alpha=alpha,
)
ax.set(aspect="equal", xlabel="$X_1$", ylabel="$X_2$")
plt.title("TSNE visualization of GCN embeddings for cora dataset")
plt.show()
217/23:
def run_deep_graph_infomax(
    base_model, generator, epochs, reorder=lambda sequence, subjects: subjects
):
    corrupted_generator = CorruptedGenerator(generator)
    gen = corrupted_generator.flow(G.nodes())
    infomax = DeepGraphInfomax(base_model, corrupted_generator)

    x_in, x_out = infomax.in_out_tensors()

    model = Model(inputs=x_in, outputs=x_out)
    model.compile(loss=tf.nn.sigmoid_cross_entropy_with_logits, optimizer=Adam(lr=1e-3))
    history = model.fit(gen, epochs=epochs, verbose=0, callbacks=[es])

    x_emb_in, x_emb_out = base_model.in_out_tensors()
    # for full batch models, squeeze out the batch dim (which is 1)
    if generator.num_batch_dims() == 2:
        x_emb_out = tf.squeeze(x_emb_out, axis=0)

    emb_model = Model(inputs=x_emb_in, outputs=x_emb_out)

    test_gen = generator.flow(test_subjects.index)
    train_gen = generator.flow(train_subjects.index)

    test_embeddings = emb_model.predict(test_gen)
    train_embeddings = emb_model.predict(train_gen)

    # some generators yield predictions in a different order to the .flow argument,
    # so we need to get everything lined up correctly
    ordered_test_subjects = reorder(test_gen, test_subjects)
    ordered_train_subjects = reorder(train_gen, train_subjects)

    lr = LogisticRegression(multi_class="auto", solver="lbfgs")
    lr.fit(train_embeddings, ordered_train_subjects)

    y_pred = lr.predict(test_embeddings)
    acc = (y_pred == ordered_test_subjects).mean()

    return acc
217/24:
cluster_generator = ClusterNodeGenerator(G, clusters=12, q=4)
cluster_gcn_model = GCN(
    layer_sizes=[128], activations=["relu"], generator=cluster_generator
)


def cluster_reorder(sequence, subjects):
    # shuffle the subjects into the same order as the sequence yield
    return subjects[sequence.node_order]


cluster_gcn_acc = run_deep_graph_infomax(
    cluster_gcn_model, cluster_generator, epochs=epochs, reorder=cluster_reorder
)
print(f"Test classification accuracy: {cluster_gcn_acc}")
217/25:
gat_model = GAT(
    layer_sizes=[128], activations=["relu"], generator=fullbatch_generator, attn_heads=8,
)
gat_acc = run_deep_graph_infomax(gat_model, fullbatch_generator, epochs=epochs)

gat_acc
print(f"Test classification accuracy: {gat_acc}")
217/26:
appnp_model = APPNP(
    layer_sizes=[128], activations=["relu"], generator=fullbatch_generator
)
appnp_acc = run_deep_graph_infomax(appnp_model, fullbatch_generator, epochs=epochs)

graphsage_generator = GraphSAGENodeGenerator(G, batch_size=1000, num_samples=[5])

graphsage_model = GraphSAGE(
    layer_sizes=[128], activations=["relu"], generator=graphsage_generator
)
graphsage_acc = run_deep_graph_infomax(
    graphsage_model, graphsage_generator, epochs=epochs
)

hinsage_generator = HinSAGENodeGenerator(
    G, batch_size=1000, num_samples=[5], head_node_type="paper"
)

hinsage_model = HinSAGE(
    layer_sizes=[128], activations=["relu"], generator=hinsage_generator
)
hinsage_acc = run_deep_graph_infomax(hinsage_model, hinsage_generator, epochs=epochs)

#RGCN
from stellargraph.mapper import RelationalFullBatchNodeGenerator
from stellargraph.layer import RGCN

rgcn_generator = RelationalFullBatchNodeGenerator(G)

rgcn_model = RGCN(layer_sizes=[128], activations=["relu"], generator=rgcn_generator)

rgcn_acc = run_deep_graph_infomax(rgcn_model, rgcn_generator, epochs=epochs)

pd.DataFrame(
    [gat_acc, gcn_acc, cluster_gcn_acc, appnp_acc, graphsage_acc, hinsage_acc, rgcn_acc],
    index=["GAT", "GCN", "Cluster-GCN", "APPNP", "GraphSAGE", "HinSAGE", "RGCN"],
    columns=["Accuracy"],
)
217/27: dir(dataset)
217/28: dir(G)
217/29:
gfcn=['check_graph_for_ml',
 'connected_components',
 'create_graph_schema',
 'edge_arrays',
 'edge_feature_shapes',
 'edge_feature_sizes',
 'edge_features',
 'edge_type_ilocs_to_names',
 'edge_type_names_to_ilocs',
 'edge_types',
 'edges',
 'from_networkx',
 'has_node',
 'in_node_arrays',
 'in_nodes',
 'info',
 'is_directed',
 'neighbor_arrays',
 'neighbors',
 'node_degrees',
 'node_feature_shapes',
 'node_feature_sizes',
 'node_features',
 'node_ids_to_ilocs',
 'node_ilocs_to_ids',
 'node_type',
 'node_type_ilocs_to_names',
 'node_type_names_to_ilocs',
 'node_types',
 'nodes',
 'nodes_of_type',
 'number_of_edges',
 'number_of_nodes',
 'out_node_arrays',
 'out_nodes',
 'subgraph',
 'to_adjacency_matrix',
 'to_networkx',
 'unique_edge_type',
 'unique_node_type']
217/30:
for i in gfcn:
    exec('print ('+i+'G.'+i+')')
217/31:
for i in gfcn:
    exec('print ('+i+',G.'+i+')')
217/32: G.check_graph_for_ml
217/33:
for i in gfcn:
    exec('G.'+i)
217/34:
for i in gfcn:
    exec('G.'+i)
217/35:
for i in gfcn:
    eval('G.'+i)
217/36: G.nodes
217/37:
for i in gfcn:
    print(i,type(exec('G.'+i)))
217/38:
for i in gfcn:
    print(i,type(exec('G.'+i+'()')))
217/39:
for i in gfcn:
    try:
        print(i,type(exec('G.'+i+'()')))
    except:
        continue
217/40:
j=0
for i in G.nodes():
    print(i)
    j+=1
    if j ==5:break
217/41:
j=0
for i in G.info():
    print(i)
    j+=1
    if j ==5:break
217/42:
j=0
for i in G.check_graph_for_ml():
    print(i)
    j+=1
    if j ==5:break
217/43: G.to_adjacency_matrix
217/44:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
from stellargraph.mapper import (
    CorruptedGenerator,
    FullBatchNodeGenerator,
    GraphSAGENodeGenerator,
    HinSAGENodeGenerator,
    ClusterNodeGenerator,
)
from stellargraph import StellarGraph
from stellargraph.layer import GCN, DeepGraphInfomax, GraphSAGE, GAT, APPNP, HinSAGE

from stellargraph import datasets
from stellargraph.utils import plot_history

import pandas as pd
from matplotlib import pyplot as plt
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.manifold import TSNE
from IPython.display import display, HTML

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
from tensorflow.keras import Model
217/45:
import networkx as nx
nx.write_gexf(G, "/nas2/cmaqruns/2022fcst/grid45/cctm.fcst/daily/test.gexf")
217/46: %pip install --users netCDF4
217/47: %pip install --user netCDF4
217/48: ls *nc
217/49: pwd
217/50:
import xarray as xr

ds = xr.open_dataset('NO2.nc')
df = ds.to_dataframe()
217/51:
import netCDF4
nc = netCDF4.Dataset("NO2.nc",'r')
217/52: %pip install --user pyproj
217/53:
import netCDF4
from pyproj import Proj
nc = netCDF4.Dataset("NO2.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
217/54:
import netCDF4
from pyproj import Proj
nc = netCDF4.Dataset("NO2.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
217/55:
import netCDF4
from pyproj import Proj
import numpy as np
nc = netCDF4.Dataset("NO2.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
217/56:
X1,Y1,X2,Y2=X.flatten(),Y.flatten(),X.flatten(),Y.flatten()
DIST=np.zeros(ncol*nrow,ncol*nrow)
for j in range(ncol*nrow):
    for i in range(ncol*nrow):
        DIST[j,i]=(X1[j]-X2[i])**2+(Y1[j]-Y2[i])**2
for j in range(ncol*nrow):
    DIST[j,j]=1
DIST=1/DIST
s=np.sum(DIST)
DIST=DIST/s
217/57:
X1,Y1,X2,Y2=X.flatten(),Y.flatten(),X.flatten(),Y.flatten()
DIST=np.zeros(shape=(ncol*nrow,ncol*nrow))
for j in range(ncol*nrow):
    for i in range(ncol*nrow):
        DIST[j,i]=(X1[j]-X2[i])**2+(Y1[j]-Y2[i])**2
for j in range(ncol*nrow):
    DIST[j,j]=1
DIST=1/DIST
s=np.sum(DIST)
DIST=DIST/s
217/58: %pip install --user geopy
217/59: %pip install --user shapely
217/60:
from shapely.geometry import Point
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons.flatten(),lats.flatten())]
nodes=pd.DataFrame({'geometry':pnts})
217/61: nodes.head()
217/62:
from geopy.distance import geodesic as GD
d=np.zeros(shape=(ncol*nrow,ncol*nrow))
for j in range(ncol*nrow):
    for i in range(j,ncol*nrow):
        d[j,i]=GD(nodes.pnts[j],nodes.pnts[i])
217/63:
from geopy.distance import geodesic as GD
d=np.zeros(shape=(ncol*nrow,ncol*nrow))
for j in range(ncol*nrow):
    for i in range(j,ncol*nrow):
        d[j,i]=GD(nodes.geometry[j],nodes.geometry[i])
217/64: nodes.geometry[j].data
217/65: nodes.geometry[j].value
217/66: dir(nodes.geometry[j])
217/67:
from geopy.distance import geodesic as GD
d=np.zeros(shape=(ncol*nrow,ncol*nrow))
for j in range(ncol*nrow):
    for i in range(j,ncol*nrow):
        d[j,i]=nodes.geometry[j].distance(nodes.geometry[i])
217/68:
for j in range(ncol*nrow):
    for i in range(0,j):
        d[j,i]=d[i,j]
217/69: d[58,85],d[85,58]
217/70:
for j in range(ncol*nrow):
    d[j,j]=1
d=1/d
s2=np.sum(d)
d=d/s2
217/71: d==DIST
217/72: s,s2
217/73: nname[:5]
217/74:
NM=nrow*ncol
nname=[str(i) for i in range(NM)]
dfEdg=pd.DataFrame({'source':np.array([[nname[j] for i in range(nm)] for j in range(NM)]).flatten(),
                   'D-1':d,'D2-1':DIST})
217/75:
NM=nrow*ncol
nname=[str(i) for i in range(NM)]
dfEdg=pd.DataFrame({'source':np.array([[nname[j] for i in range(NM)] for j in range(NM)]).flatten(),
                   'D-1':d,'D2-1':DIST})
217/76:
NM=nrow*ncol
nname=[str(i) for i in range(NM)]
dfEdg=pd.DataFrame({'source':np.array([[nname[j] for i in range(NM)] for j in range(NM)]).flatten(),
                    'target':np.array([[nname[i] for i in range(NM)] for j in range(NM)]).flatten(),
                   'D-1':d,'D2-1':DIST})
217/77:
NM=nrow*ncol
nname=[str(i) for i in range(NM)]
dfEdg=pd.DataFrame({'source':np.array([[nname[j] for i in range(NM)] for j in range(NM)]).flatten(),
                    'target':np.array([[nname[i] for i in range(NM)] for j in range(NM)]).flatten(),
                   'D-1':d.flatten(),'D2-1':DIST.flatten()})
217/78: dfEdg.head(),dfEdg.tail()
217/79: dfEdg.head(),dfEdg.tail(),NM
217/80:
dfNod=node[:]
dfNod['name']=nname
217/81:
dfNod=nodes[:]
dfNod['name']=nname
217/82:
dfNod=nodes
dfNod['name']=nname
217/83: %pip install --user geopandas
217/84: import geopandas as gpd
217/85:
import geopandas as gpd
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]
idx=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape))].index
217/86:
import geopandas as gpd
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]
name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
217/87: len(name_inside)
217/88:
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in name_inside)].reset_index(drop=True)
dfEdg2=dfEdg.loc[dfEdg.source.map(lambda x:x in name_inside)].reset_index(drop=True)
dfEdg3=dfEdg2.loc[dfEdg2.target.map(lambda x:x in name_inside)].reset_index(drop=True)
217/89: len(dfEdg3),dfEdg3.head(),dfEdg3.tail()
217/90: len(dfNod2),dfEdg3.head(),dfEdg3.tail()
217/91: name_inside.head()
217/92:
s=set(name_inside)
list(s)[:5]
217/93: s=set(name_inside)
217/94:
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
dfEdg2=dfEdg.loc[dfEdg.source.map(lambda x:x in s)].reset_index(drop=True)
dfEdg3=dfEdg2.loc[dfEdg2.target.map(lambda x:x in s)].reset_index(drop=True)
217/95: dfNod2.head(),dfNod2.tail()
217/96: dfEdg3.head(),dfEdg3.tail()
217/97: 3373*3373
217/98: dfNod['NO2']=nc['NO2'][0,0,:,:].flatten()
217/99:
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
dfNod2.head()
217/100:
names=["target", "source"]
srctar=dfEdg3[names]
srctar.head()
217/101: len(set(srctar.target)),len(set(srctar.source)),
218/1:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

dataset = sg.datasets.METR_LA()
speed_data, sensor_dist_adj = dataset.load()
num_nodes, time_len = speed_data.shape
print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
print(speed_data.head())
218/2:
train_rate = 0.8
train_data, test_data = train_test_split(speed_data, train_rate)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)

def scale_data(train_data, test_data):
    max_speed = train_data.max()
    min_speed = train_data.min()
    train_scaled = (train_data - min_speed) / (max_speed - min_speed)
    test_scaled = (test_data - min_speed) / (max_speed - min_speed)
    return train_scaled, test_scaled

train_scaled, test_scaled = scale_data(train_data, test_data)
218/3:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

dataset = sg.datasets.METR_LA()
speed_data, sensor_dist_adj = dataset.load()
num_nodes, time_len = speed_data.shape
print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
print(speed_data.head())
218/4:
seq_len = 10
pre_len = 12
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
from stellargraph.layer import GCN_LSTM
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=sensor_dist_adj,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/5:
ythat = model.predict(trainX)
yhat = model.predict(testX)
218/6:
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -1
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/7:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 100]
a_true = test_rescref[:, 100]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/8:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point

nc = netCDF4.Dataset("NO2.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons.flatten(),lats.flatten())]
dfNod=pd.DataFrame({'geometry':pnts})
import geopandas as gpd
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]
name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
print(dfNod.head())
218/9:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd

nc = netCDF4.Dataset("NO2.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons.flatten(),lats.flatten())]
dfNod=pd.DataFrame({'geometry':pnts})
import geopandas as gpd
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]
name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
print(dfNod.head())
218/10:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd

nc = netCDF4.Dataset("NO2.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons.flatten(),lats.flatten())]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
import geopandas as gpd
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]
name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
print(dfNod.head())
218/11:
train_data, test_data = train_test_split(speed_data, train_rate)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/12:
train_data, test_data = train_test_split(dfNod, train_rate)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/13:
s=set(name_inside)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
train_data, test_data = train_test_split(dfNod2, train_rate)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/14: train_scaled, test_scaled = scale_data(train_data, test_data)
218/15:
s=set(name_inside)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
deldfNod2['geometry']
train_data, test_data = train_test_split(dfNod2, train_rate)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/16:
s=set(name_inside)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
del dfNod2['geometry']
train_data, test_data = train_test_split(dfNod2, train_rate)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/17:
seq_len = 10
pre_len = 12
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
218/18:
s=set(name_inside)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
del dfNod2['geometry']
train_scaled, test_scaled = scale_data(train_data, test_data)
train_data, test_data = train_test_split(dfNod2, train_rate)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/19:
s=set(name_inside)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
del dfNod2['geometry']
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/20:
s=set(name_inside)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
del dfNod2['geometry','name']
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/21:
s=set(name_inside)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
del dfNod2[['geometry','name']]
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/22:
s=set(name_inside)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
for i in ['geometry','name']:
    if i in dfNo2.columns: del dfNod2[i]
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/23:
s=set(name_inside)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/24:
seq_len = 10
pre_len = 12
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
218/25:
cn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=sensor_dist_adj,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/26: print(sensor_dist_adj)
218/27: type(sensor_dist_adj)
218/28: sensor_dist_adj.shape
218/29:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
d=np.zeros(shape=(n,n))
for j in range(n):
    for i in range(j,n):
        d[j,i]=dfNod2.geometry[j].distance(dfNod2.geometry[i])
for j in range(n):
    for i in range(0,j):
        d[j,i]=d[i,j]       
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/30: d[:5,:5]
218/31:
cn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/32:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/33:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -1
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/34:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 100]
a_true = test_rescref[:, 100]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/35: test_rescpred.shape
218/36:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 500]
a_true = test_rescref[:, 500]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/37:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

#dataset = sg.datasets.METR_LA()
#speed_data, sensor_dist_adj = dataset.load()
#num_nodes, time_len = speed_data.shape
#print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
#print(speed_data.head())
218/38:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd


nc = netCDF4.Dataset("NO2.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons.flatten(),lats.flatten())]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]

name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
print(dfNod.head())
218/39:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
d=np.zeros(shape=(n,n))
for j in range(n):
    for i in range(j,n):
        d[j,i]=dfNod2.geometry[j].distance(dfNod2.geometry[i])
for j in range(n):
    for i in range(0,j):
        d[j,i]=d[i,j]       
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/40:
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/41:
seq_len = 10
pre_len = 12
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
218/42: d[:5,:5]
218/43:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/44:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -1
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/45:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 1500]
a_true = test_rescref[:, 1500]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/46:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/47:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 1500]
a_true = test_rescref[:, 1500]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/48:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[2000, 2000],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/49:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/50:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 1500]
a_true = test_rescref[:, 1500]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/51:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[240, 1],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/52:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/53:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 1500]
a_true = test_rescref[:, 1500]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/54:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[20, 20],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/55:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/56:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 1500]
a_true = test_rescref[:, 1500]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/57:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 2000]
a_true = test_rescref[:, 2000]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/58:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

dataset = sg.datasets.METR_LA()
speed_data, sensor_dist_adj = dataset.load()
#num_nodes, time_len = speed_data.shape
#print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
#print(speed_data.head())
218/59: d[:5,:5],sensor_dist_adj[:5,:5]
218/60: d[:5,:5].max(),sensor_dist_adj.max()
218/61: d.max(),sensor_dist_adj.max()
218/62: d=d/d.max()
218/63:
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/64:
seq_len = 10
pre_len = 12
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
218/65: d[:5,:5]
218/66:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[20, 20],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/67:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/68:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 2000]
a_true = test_rescref[:, 2000]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/69: nc = netCDF4.Dataset("2019.nc",'r')
218/70:
nc = netCDF4.Dataset("2019.nc",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
218/71:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
218/72:
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/73:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/74:
seq_len = 10
pre_len = 12
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
218/75:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/76:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/77:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 2000]
a_true = test_rescref[:, 2000]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/78:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 1000]
a_true = test_rescref[:, 1000]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/79:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 300]
a_true = test_rescref[:, 300]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/80:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:200, 300]
a_true = test_rescref[:200, 300]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/81:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[::12, 300]
a_true = test_rescref[::12, 300]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/82:
seq_len = 72
pre_len = 24
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
218/83:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/84:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/85:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[::12, 300]
a_true = test_rescref[::12, 300]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/86:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[::2, 300]
a_true = test_rescref[::2, 300]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/87:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:100:1, 300]
a_true = test_rescref[:100:1, 300]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/88:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:100:1, 350]
a_true = test_rescref[:100:1, 350]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/89: d.max()
218/90: d.min()
218/91:
seq_len = 72
pre_len = 24
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
218/92:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[5, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[2000, 2],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=10,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/93:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/94:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:100:1, 350]
a_true = test_rescref[:100:1, 350]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/95:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=10,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/96:
seq_len = 72
pre_len = 24
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
218/97:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=10,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/98:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/99:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:100:1, 350]
a_true = test_rescref[:100:1, 350]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/100:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/101:
seq_len = 72
pre_len = 24
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
218/102:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=200,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/103:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/104:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:100:1, 350]
a_true = test_rescref[:100:1, 350]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/105:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:100:1, 350]
a_true = test_rescref[:100:1, 350]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/106:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:1000:1, 350]
a_true = test_rescref[:1000:1, 350]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/107:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:1000:1, 3050]
a_true = test_rescref[:1000:1, 3050]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/108:
nc = netCDF4.Dataset("2019.nc",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
218/109:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
218/110: dfNod2.head()
218/111: d[:5,:5]
218/112: d2=d[:]
218/113:
d=np.zeros(shape=(n,n))
for j in range(n):
    for i in range(j,n):
        d[j,i]=dfNod2.geometry[j].distance(dfNod2.geometry[i])
for j in range(n):
    for i in range(0,j):
        d[j,i]=d[i,j]
d=d/d.max()
218/114: d==d2
218/115: (d==d2).all()
218/116: dfNod2.shape[1]
218/117:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/118:
seq_len = 72
pre_len = 24
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
218/119:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=50,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
218/120:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
218/121:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:1000:1, 3050]
a_true = test_rescref[:1000:1, 3050]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/122:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[1000:2000:1, 3050]
a_true = test_rescref[10000:2000:1, 3050]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/123:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[1000:2000:1, 3050]
a_true = test_rescref[1000:2000:1, 3050]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
219/1:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

#dataset = sg.datasets.METR_LA()
#speed_data, sensor_dist_adj = dataset.load()
#num_nodes, time_len = speed_data.shape
#print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
#print(speed_data.head())
219/2:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.ncD",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
219/3:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.ncD",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons.flatten(),lats.flatten())]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})

for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
219/4:
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]
name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
219/5:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
219/6: dfNod2.head()
219/7:
d=np.zeros(shape=(n,n))
for j in range(n):
    for i in range(j,n):
        d[j,i]=dfNod2.geometry[j].distance(dfNod2.geometry[i])
for j in range(n):
    for i in range(0,j):
        d[j,i]=d[i,j]
d=d/d.max()
219/8:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
219/9:
def scale_data(train_data, test_data):
    max_speed = train_data.max()
    min_speed = train_data.min()
    train_scaled = (train_data - min_speed) / (max_speed - min_speed)
    test_scaled = (test_data - min_speed) / (max_speed - min_speed)
    return train_scaled, test_scaled
219/10:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
219/11:
seq_len = 14
pre_len = 10
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
219/12:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=50,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
219/13: from stellargraph.layer import GCN_LSTM
219/14:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=50,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
219/15:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
219/16:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:1000:1, 3050]
a_true = test_rescref[:1000:1, 3050]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
219/17:
seq_len = 5
pre_len = 4
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
219/18:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
219/19:
seq_len = 5
pre_len = 4
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
219/20:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=50,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
219/21:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
219/22:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:1000:1, 3050]
a_true = test_rescref[:1000:1, 3050]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
219/23:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[20, 20],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=50,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
219/24:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
219/25:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:1000:1, 3050]
a_true = test_rescref[:1000:1, 3050]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
219/26:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[300, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
219/27:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
219/28:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:1000:1, 3050]
a_true = test_rescref[:1000:1, 3050]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
219/29:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:1000:1, 50]
a_true = test_rescref[:1000:1, 50]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
219/30: dfNod2.head()
219/31:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
219/32: name_inside
219/33: name_inside.head()
219/34: name_inside.column
219/35: name_inside.columns
219/36: name_inside.column()
219/37: type(name_inside)
219/38: name_inside[0]
219/39: df=pd.DataFrame({'name':name_inside})
219/40: df
219/41:
df['ix']=[int(int(i)%ncol) for i in n]
df['iy']=[int(int(i)/ncol) for i in n]
219/42:
df['ix']=[int(int(i)%ncol) for i in df.name]
df['iy']=[int(int(i)/ncol) for i in df.name]
219/43: df
219/44: d.shape
219/45: len(d)
219/46: len(df)
219/47: n
219/48: %pip install --user FortranFile
219/49:
from scipy.io import FortranFile
fname=str(n)+'x'+str(n).bin
with FortranFile(fname, 'w') as f:
    f.write_record(d)
219/50:
from scipy.io import FortranFile
fname=str(n)+'x'+str(n)+'.bin'
with FortranFile(fname, 'w') as f:
    f.write_record(d)
219/51:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 50]
a_true = train_rescref[:1000:1, 50]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
218/124: dfNod2.head()
218/125:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.9
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
218/126:
seq_len = 144
pre_len = 28
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
218/127:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[100, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=50,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
221/1:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg
from stellargraph.layer import GCN_LSTM

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

def scale_data(train_data, test_data):
    max_speed = train_data.max()
    min_speed = train_data.min()
    train_scaled = (train_data - min_speed) / (max_speed - min_speed)
    test_scaled = (test_data - min_speed) / (max_speed - min_speed)
    return train_scaled, test_scaled

#dataset = sg.datasets.METR_LA()
#speed_data, sensor_dist_adj = dataset.load()
#num_nodes, time_len = speed_data.shape
#print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
#print(speed_data.head())
221/2:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.nc",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons.flatten(),lats.flatten())]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})

for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
221/3:
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]
name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
221/4:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
221/5: dfNod2.head()
221/6:
from scipy.io import FortranFile
fname=str(n)+'x'+str(n)+'.bin'
with FortranFile(fname, 'r') as f:
    d=f.read_record(dtype=np.float64)
d=d.reshape(n,n)    
#with FortranFile(fname, 'w') as f:
#    f.write_record(d)
221/7:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
221/8:
seq_len = 72
pre_len = 24
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
221/9:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 1],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
222/1: from pandas import *
222/2: df=read_csv('gridLL.csv')
222/3:
import netCDF4
nc = netCDF4.Dataset("2019.nc",'r')
222/4:
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
222/5: V[3]
222/6:
import numpy as np
for v in V[3]:
    if v in ['PM25_NH4','PM25_NO3','PM25_SO4',]:continue
    exec(v+'=np.array(nc["'+v+'"][:,0,:,:])')
222/7: pwd
222/8: from dtconvertor import dt2jul, jul2dt
222/9: ls *.py
222/10: pwd
222/11: from dtconvertor import dt2jul, jul2dt
222/12: ls *py
222/13: ls *py
222/14: from dtconvertor import dt2jul, jul2dt
222/15: dts=[jul2dt(nc.variables[v][t,0,:] for t in range(nt))]
222/16: dts=[jul2dt(nc.variables[v][t,0,:]) for t in range(nt)]
222/17: t
222/18: v='TFLAG'
222/19: dts=[jul2dt(nc.variables[v][t,0,:]) for t in range(nt)]
222/20: dts[:5]
222/21: flags=[dts[t].strftime("%Y%m%d%H") for t in range(nt)]
222/22: df.head()
222/23: dfvar=DataFrame()
222/24: [v for v in V[3] if v not in ['PM25_NH4','PM25_NO3','PM25_SO4',]]
222/25: col=[v for v in V[3] if v not in ['PM25_NH4','PM25_NO3','PM25_SO4',]]
222/26:
var=[]
for v in col:
    exec('var.append('+v+')')
222/27:
var={}
for v in col:
    exec('var.update({"'+v+'":'+v+'})')
222/28:
for t in range(nt):
    df0=DataFrame({v:var[v][t,:,:].flatten() for v in col})
    df0['tflag']=tflags
    dfvar.append(df0,inplace=True,ignore_index=True)
222/29:
for t in range(nt):
    df0=DataFrame({v:var[v][t,:,:].flatten() for v in col})
    df0['tflag']=flags
    dfvar=dfvar.append(df0,ignore_index=True)
222/30: t
222/31:
for t in range(nt):
    df0=DataFrame({v:var[v][t,:,:].flatten() for v in col})
    df0['tflag']=flags[t]
    dfvar=dfvar.append(df0,ignore_index=True)
222/32: df.head()
222/33: dfvar=DataFrame()
222/34:
for t in range(5):
    for v in col:
      df[v]=var[v][t,:,:].flatten()
    df['tflag']=flags[t]
    df0=pivot_table(df,index='TOWNCODE',values=['tflag']+col,aggfunc=np.mean).reset_index()
    dfvar=dfvar.append(df0,ignore_index=True)
222/35: dfvar=DataFrame()
222/36:
for t in range(5):
    for v in col:
      df[v]=var[v][t,:,:].flatten()
    df['tflag']=flags[t]
    df0=pivot_table(df,index='TOWNCODE',values=['tflag']+col,aggfunc=np.mean).reset_index()
    dfvar=dfvar.append(df0,ignore_index=True)
222/37: t
222/38: len(df)
222/39: v
222/40: var[v][0,:,:].shape
221/10:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
222/41: df=read_csv('gridLL.csv')
222/42: len(df)
222/43: dfvar=DataFrame()
222/44:
for t in range(5):
    for v in col:
      df[v]=var[v][t,:,:].flatten()
    df['tflag']=flags[t]
    df0=pivot_table(df,index='TOWNCODE',values=['tflag']+col,aggfunc=np.mean).reset_index()
    dfvar=dfvar.append(df0,ignore_index=True)
222/45: dfvar.head()
222/46: dfvar.tail()
222/47: df.head()
222/48: dfvar.tail()
222/49: dfvar=DataFrame()
222/50:
for t in range(nt):
    for v in col:
      df[v]=var[v][t,:,:].flatten()
    df['tflag']=int(flags[t])
    df0=pivot_table(df,index='TOWNCODE',values=['tflag']+col,aggfunc=np.mean).reset_index()
    dfvar=dfvar.append(df0,ignore_index=True)
221/11:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 50]
a_true = train_rescref[:1000:1, 50]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
221/12:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[1, 20],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
222/51: len(dfvar)
222/52: dfvar.tail()
222/53: n=3739
222/54:
from scipy.io import FortranFile
fname=str(n)+'x'+str(n)+'.bin'
with FortranFile(fname, 'r') as f:
    d=f.read_record(dtype=np.float64)
d=d.reshape(n,n)
222/55: import geopandas as gpd
222/56:
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]
222/57: from shapely.geometry import Point
222/58: df.head()
222/59: pnts=[Point(lon,lat) for lon,lat in zip(df.LON,df.LAT)]
222/60: pnts[:5]
222/61: df.columns
222/62: df['ix']=np.array([[i for i in range(ncol)] for j in range(nrow)]).flatten()
222/63: df['iy']=np.array([[j for i in range(ncol)] for j in range(nrow)]).flatten()
222/64: c=['ix','iy']
222/65: df[c].tail()
222/66: df[c].head
222/67: name_inside=df.loc[df.geometry.map(lambda p:p.within(boundary_shape))]
222/68: df['geometry']=pnts
222/69: name_inside=df.loc[df.geometry.map(lambda p:p.within(boundary_shape))]
222/70: len(name_inside)
222/71: n
222/72:
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
222/73: from pyproj import Proj
222/74: pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
222/75:
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
222/76: df.LAT=lats.flatten()
222/77: df.LON=lons.flatten()
222/78: pnts=[Point(lon,lat) for lon,lat in zip(df.LON,df.LAT)]
222/79: df['geometry']=pnts
222/80: name_inside=df.loc[df.geometry.map(lambda p:p.within(boundary_shape))]
222/81: len(name_inside)
222/82: n
222/83: name_inside.head()
222/84: d[:5,:5]
222/85: d.shape
222/86: df['IYIX']=[j*100+i for i,j in zip(df.ix,df.iy)]
222/87: name_inside['IYIX']=[j*100+i for i,j in zip(name_inside.ix,name_inside.iy)]
222/88: name_inside=name_inside.reset_index(drop=True)
222/89: name_inside['IYIX']=[j*100+i for i,j in zip(name_inside.ix,name_inside.iy)]
222/90: name_inside.head()
222/91: df.columns
222/92: '0' in set(name_inside.TOWNCODE)
222/93: df.TOWNCODE[:5]
222/94: df.TOWNCODE[-5:]
222/95: df.TOWNCODE.max()
222/96: 0 in set(name_inside.TOWNCODE)
222/97: 68000130 in set(name_inside.TOWNCODE)
222/98: len(set(name_inside.TOWNCODE))
222/99: len(set(df.TOWNCODE))
222/100: %history -f py.txt
222/101: !cat py.txt
221/13:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
221/14:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 50]
a_true = train_rescref[:1000:1, 50]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
221/15:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 150]
a_true = train_rescref[:1000:1, 150]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
221/16:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 10]
a_true = train_rescref[:1000:1, 10]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
221/17:
#all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 10]
a_true = train_rescref[:1000:1, 10]
plt.plot(a_pred,a_true "r-", label="prediction")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
221/18:
#all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 10]
a_true = train_rescref[:1000:1, 10]
plt.plot(a_pred,a_true , label="prediction")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
221/19:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:500:1, 10]
a_true = train_rescref[:500:1, 10]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
222/102: !df
222/103: pwd
222/104: !python -V
220/1:
d=np.zeros(shape=(n,n))
for j in range(n):
    for i in range(j,n):
        d[j,i]=dfNod2.geometry[j].distance(dfNod2.geometry[i])        
for j in range(n):
    for i in range(0,j):
        d[j,i]=d[i,j]
    d[j,j]=1
d=1/d
d=d/d.max()
220/2:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

dataset = sg.datasets.METR_LA()
speed_data, sensor_dist_adj = dataset.load()
#num_nodes, time_len = speed_data.shape
#print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
#print(speed_data.head())
220/3:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd


nc = netCDF4.Dataset("2019.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons.flatten(),lats.flatten())]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]

name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
print(dfNod.head())
220/4:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
220/5: dfNod2.head()
220/6:
d=np.zeros(shape=(n,n))
for j in range(n):
    for i in range(j,n):
        d[j,i]=dfNod2.geometry[j].distance(dfNod2.geometry[i])        
for j in range(n):
    for i in range(0,j):
        d[j,i]=d[i,j]
    d[j,j]=1
d=1/d
d=d/d.max()
220/7: d[:5,:5]
220/8:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.9
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
220/9:
train_rate = 0.8
train_data, test_data = train_test_split(speed_data, train_rate)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)

def scale_data(train_data, test_data):
    max_speed = train_data.max()
    min_speed = train_data.min()
    train_scaled = (train_data - min_speed) / (max_speed - min_speed)
    test_scaled = (test_data - min_speed) / (max_speed - min_speed)
    return train_scaled, test_scaled

train_scaled, test_scaled = scale_data(train_data, test_data)
220/10:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.9
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
220/11:
seq_len = 144
pre_len = 28
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
220/12:
seq_len = 144
pre_len = 28
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
220/13:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[100, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=50,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
220/14:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg
from stellargraph.layer import GCN_LSTM

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

dataset = sg.datasets.METR_LA()
speed_data, sensor_dist_adj = dataset.load()
#num_nodes, time_len = speed_data.shape
#print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
#print(speed_data.head())
220/15:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[100, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=50,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
222/105: 68000130 in set(name_inside.TOWNCODE)
222/106: d.shape
222/107: d[:5,:5]
222/108: n
222/109: d=1/d
222/110: d[:5,:5]
222/111:
for j in range(n):
    d[j,j]=1
222/112: d[:5,:5]
222/113: dfNod2.head()
222/114: dfNod.head()
220/16: dfNod2.geometry[:5].distance(dfNod2.geometry[:5])
220/17:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
220/18: dfNod2.head()
220/19: dfNod2.geometry[:5].distance(dfNod2.geometry[:5])
220/20:
d=np.zeros(shape=(n,n))
for j in range(n):
    for i in range(j,n):
        d[j,i]=dfNod2.geometry[j].distance(dfNod2.geometry[i])        
for j in range(n):
    for i in range(0,j):
        d[j,i]=d[i,j]
d=1/d
for j in range(n):
    d[j,j]=1
d=d/d.max()
222/115: d.shape
222/116: len(df)
222/117: df.head()
222/118: !vi py.txt
222/119: name_inside.head()
222/120: name_inside.column()
222/121: name_inside.columns
222/122: n=len(name_inside)
222/123: n
220/21: d[:5,:5]
220/22:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.9
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
220/23:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.9
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
220/24:
seq_len = 144
pre_len = 28
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
220/25:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[100, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=50,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
222/124: town_name=pivot_tables(name_inside,index='TOWNCODE',values='TOWNNAME',aggfunc=count).reset_index()
222/125: town_name=pd.pivot_tables(name_inside,index='TOWNCODE',values='TOWNNAME',aggfunc=count).reset_index()
222/126: from pandas import *
222/127: town_name=pd.pivot_tables(name_inside,index='TOWNCODE',values='TOWNNAME',aggfunc=count).reset_index()
222/128: town_name=pivot_tables(name_inside,index='TOWNCODE',values='TOWNNAME',aggfunc=count).reset_index()
222/129: town_name=pivot_table(name_inside,index='TOWNCODE',values='TOWNNAME',aggfunc=count).reset_index()
222/130: town_name=pivot_table(name_inside,index='TOWNCODE',values='TOWNNAME',aggfunc="count").reset_index()
222/131: town_name.head()
222/132: town=set(name_inside.TOWNCODE[:])
222/133: town=list(set(name_inside.TOWNCODE[:]))
222/134: town[:5]
222/135: town_name={i:name_inside.loc[name_inside.TOWNCODE==i,['TOWNNAME']].reset_index(drop=True)[0] for i in town}
222/136: town_name={i:j for i,j in zip(name_inside.TOWNCODE[:],name_inside.TOWNNAME)}
222/137: town_name
222/138: name_inside.columns
222/139: mm=len(town)
222/140: m=len(town)
222/141: dd=np.zeros(shape=(m,m))
222/142: m
222/143: IXIY=list(name_inside.IXIY)
222/144: IYIX=list(name_inside.IYIX)
222/145: len(name_inside)
222/146: n
222/147: df_dd=DataFrame({'T1':np.array([[j for i in range(n)] for j in range(n)]).flatten(),'T2':np.array([[i for i in range(n)] for j in range(n)]).flatten(),'dd':d.flatten()})
222/148: df_dd.head()
222/149: df_dd[n:n+5]
222/150: df_dd[n-5:n+5]
222/151: df_dd['TOWNCODE1']=[name_inside.TOWNCODE[i] for i in df_dd[T1]]
222/152: df_dd['TOWNCODE1']=[name_inside.TOWNCODE[i] for i in df_dd.T1]
222/153: TCD={name_inside.TOWNCODE[i] for i in name_inside.index}
222/154: df_dd['TOWNCODE2']=[TCD[i] for i in df_dd.T2]
222/155: TCD={i:name_inside.TOWNCODE[i] for i in name_inside.index}
222/156: df_dd['TOWNCODE2']=[TCD[i] for i in df_dd.T2]
222/157: df_dd.head()
222/158: type(dd)
222/159: dd[:5,:5]
222/160: m
222/161: pvdd=pivot_table(df_dd,index=['TOWNCODE1','TOWNCODE2'],value='dd',aggfunc=np.mean).reset_index()
222/162: pvdd=pivot_table(df_dd,index=['TOWNCODE1','TOWNCODE2'],values='dd',aggfunc=np.mean).reset_index()
222/163: pvdd.head()
222/164: len(pvdd)
222/165: 337*337
222/166: dd=pvdd.dd.reshape(m,m)
222/167: dd=pvdd.dd[:].reshape(m,m)
222/168: dd=np.array(pvdd.dd[:]).reshape(m,m)
222/169: dd[:5,:5]
222/170: dd=1/dd
222/171: dd=dd/dd.max()
222/172: dd[:5,:5]
222/173:
from scipy.io import FortranFile
fname='twn_DisInv'str(m)+'x'+str(m)+'.bin'
with FortranFile(fname, 'w') as f:
    f.write_record(dd)
222/174:
from scipy.io import FortranFile
fname='twn_DisInv'+str(m)+'x'+str(m)+'.bin'
with FortranFile(fname, 'w') as f:
    f.write_record(dd)
224/1:
from scipy.io import FortranFile
import geopandas as gpd
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]
from shapely.geometry import Point

df=read_csv('gridLL.csv')
nc = netCDF4.Dataset("2019.nc",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
col=[v for v in V[3] if v not in ['PM25_NH4','PM25_NO3','PM25_SO4',]]
v='TFLAG'
dts=[jul2dt(nc.variables[v][t,0,:]) for t in range(nt)]
flags=[dts[t].strftime("%Y%m%d%H") for t in range(nt)]

dfvar=DataFrame()
for t in range(nt):
    for v in col:
      df[v]=nc[v][t,0,:,:].flatten()
    df['tflag']=int(flags[t])
    df0=pivot_table(df,index='TOWNCODE',values=['tflag']+col,aggfunc=np.mean).reset_index()
    dfvar=dfvar.append(df0,ignore_index=True)
224/2:
from pandas import *
import netCDF4
import numpy as np
from dtconvertor import dt2jul, jul2dt
from scipy.io import FortranFile
import geopandas as gpd
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]
from shapely.geometry import Point
from pyproj import Proj

df=read_csv('gridLL.csv')
224/3:
nc = netCDF4.Dataset("2019.nc",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
col=[v for v in V[3] if v not in ['PM25_NH4','PM25_NO3','PM25_SO4',]]
v='TFLAG'
dts=[jul2dt(nc.variables[v][t,0,:]) for t in range(nt)]
flags=[dts[t].strftime("%Y%m%d%H") for t in range(nt)]

dfvar=DataFrame()
for t in range(nt):
    for v in col:
      df[v]=nc[v][t,0,:,:].flatten()
    df['tflag']=int(flags[t])
    df0=pivot_table(df,index='TOWNCODE',values=['tflag']+col,aggfunc=np.mean).reset_index()
    dfvar=dfvar.append(df0,ignore_index=True)
224/4: dfvar.head()
224/5: dfvar.set_index('tflag').to_csv('dfvar.csv')
224/6: %history -f py2.txt
224/7: !vi py2.txt
224/8: %history -g -f py2.txt
224/9: !vi py2.txt
224/10:
n=3739
fname=str(n)+'x'+str(n)+'.bin'
with FortranFile(fname, 'r') as f:
    d=f.read_record(dtype=np.float64)
d=d.reshape(n,n)
224/11: d[:5,:5]
224/12: ls *bin
224/13: mv 3739x3739.bin pnt_Dis3739x3739.bin
224/14: df.head()
224/15:
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
df.LAT=lats.flatten()
df.LON=lons.flatten()
df.Point=[Point(lon,lat) for lon,lat in zip(df.LON,df.LAT)]
name_inside=df.loc[df.Point.map(lambda p:p.within(boundary_shape))].reset_index(drop=True)
224/16: len(name_inside)
224/17: n
224/18: !vi py2.txt
224/19: d[:5,:5]
224/20: d=1/d
224/21:
for j in range(n):
    d[j,j]=d.max()
224/22:
fname='pnt_Dis'+str(n)+'x'+str(n)+'.bin'
with FortranFile(fname, 'r') as f:
    d=f.read_record(dtype=np.float64)
d=d.reshape(n,n)
224/23: d=1/d
224/24:
for j in range(n):
    d[j,j]=1.
224/25: d[:5,:5]
224/26: dmax=d.max()
224/27:
for j in range(n):
    d[j,j]=dmax.
224/28:
for j in range(n):
    d[j,j]=dmax
224/29: d=d/d.max()
224/30: d[:5,:5]
224/31: !vi py2.txt
224/32:
town=list(set(name_inside.TOWNCODE[:]))
m=len(town)
224/33: fname='twn_DisInv'str(m)+'x'+str(m)+'.bin'
224/34: fname='twn_DisInv'+str(m)+'x'+str(m)+'.bin'
224/35:
with FortranFile(fname, 'r') as f:
  d2=f.read_record(dtype=np.float64)
d2=d2.reshape(n,n)
224/36:
with FortranFile(fname, 'r') as f:
  d2=f.read_record(dtype=np.float64)
d2=d2.reshape(m,m)
224/37:
df_dd=DataFrame({'T1':np.array([[j for i in range(n)] for j in range(n)]).flatten(),'T2':np.array([[i for i in range(n)] for j in range(n)]).flatten(),'dd':d.flatten()})
TCD={i:name_inside.TOWNCODE[i] for i in name_inside.index}
df_dd['TOWNCODE1']=[TCD[i] for i in df_dd.T1]
df_dd['TOWNCODE2']=[TCD[i] for i in df_dd.T2]
pvdd=pivot_table(df_dd,index=['TOWNCODE1','TOWNCODE2'],values='dd',aggfunc=np.mean).reset_index()
dd=np.array(pvdd.dd[:]).reshape(m,m)
224/38: dd[:5,:5]
224/39: d2[:5,:5]
224/40: d[:5,:5]
224/41: pvdd.head()
224/42: len(town)
224/43: m
224/44: pvdd[m-1:m+5]
224/45: pvdd[2*m-5:2*m+5]
224/46:
fname='twn_DisInv'+str(m)+'x'+str(m)+'.bin'
with FortranFile(fname, 'w') as f:
    f.write_record(dd)
224/47: !lst
224/48: dfvar.head()
225/1:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg
from stellargraph.layer import GCN_LSTM

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

def scale_data(train_data, test_data):
    max_speed = train_data.max()
    min_speed = train_data.min()
    train_scaled = (train_data - min_speed) / (max_speed - min_speed)
    test_scaled = (test_data - min_speed) / (max_speed - min_speed)
    return train_scaled, test_scaled

#dataset = sg.datasets.METR_LA()
#speed_data, sensor_dist_adj = dataset.load()
#num_nodes, time_len = speed_data.shape
#print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
#print(speed_data.head())
224/49: name_inside.columns
224/50: len(name_inside)
224/51: !vi py2.txt
224/52: df_twnNode=pvdd[:m]
224/53: df_twnNode.tail()
224/54: df_twnNode.head()
224/55: m==len(df_twnNode)
224/56:
TownNode=pvdd[:m]
TwonNode.dd=[town_name[i] for i in TownNode.TOWNCODE2]
224/57: town_name={i:j for i,j in zip(name_inside.TOWNCODE[:],name_inside.TOWNNAME)}
224/58:
TownNode=pvdd[:m]
TwonNode.dd=[town_name[i] for i in TownNode.TOWNCODE2]
224/59: TownNode.dd=[town_name[i] for i in TownNode.TOWNCODE2]
224/60: TownNode.head()
224/61: TownNode.tail()
224/62: TownNode.columns
224/63:
del TownNode['TOWNCODE1']
TownNode.columns=['TOWNCODE2', 'TOWNNAME']
224/64: TownNode.columns=['TOWNCODE', 'TOWNNAME']
224/65: TownNode.head()
224/66: TownNode.tail()
224/67: !lst
225/2:
import pandas as pd
dfNod=pd.read_csv('dfvar.csv')
225/3: dfNod.head()
225/4:
n=len(set(dfNod.TOWNCODE))
from scipy.io import FortranFile
fname='twn_DisInv'+str(n)+'x'+str(n)+'.bin'
with FortranFile(fname, 'r') as f:
    d=f.read_record(dtype=np.float64)
d=d.reshape(n,n)    
#with FortranFile(fname, 'w') as f:
#    f.write_record(d)
224/68: !lst
224/69: len(set(dfvar.TOWNCODE))
224/70: 0 in set(dfvar.TOWNCODE)
224/71: set(town)-set(dfvar.TOWNCODE)
224/72: set(town)==set(dfvar.TOWNCODE)
224/73: len(town)
224/74: set(dfvar.TOWNCODE)-set(town)
224/75: !lst
224/76: TownNode.set_index('TOWNCODE').to_csv('TownNode.csv')
225/5: TownNode=pd.read_csv('TownNode.csv')
225/6:
TownNode=pd.read_csv('TownNode.csv')
n=len(set(TownNode.TOWNCODE))
town=set(TownNode.TOWNCODE)
dfNod2=dfNod.loc[dfNod.TOWNCODE.map(t:t in town)].reset_index(drop=True)
225/7:
TownNode=pd.read_csv('TownNode.csv')
n=len(set(TownNode.TOWNCODE))
town=set(TownNode.TOWNCODE)
dfNod2=dfNod.loc[dfNod.TOWNCODE.map(lambda t:t in town)].reset_index(drop=True)
225/8:
n=len(set(dfNod2.TOWNCODE))
from scipy.io import FortranFile
fname='twn_DisInv'+str(n)+'x'+str(n)+'.bin'
with FortranFile(fname, 'r') as f:
    d=f.read_record(dtype=np.float64)
d=d.reshape(n,n)    
#with FortranFile(fname, 'w') as f:
#    f.write_record(d)
226/1:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg
from stellargraph.layer import GCN_LSTM

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

dataset = sg.datasets.METR_LA()
speed_data, sensor_dist_adj = dataset.load()
num_nodes, time_len = speed_data.shape
print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
print(speed_data.head())
224/77: dfvar.head()
224/78: dfNod2=dfvar.loc[dfvar.TOWNCODE.map(lambda t:t in town)].reset_index(drop=True)
224/79: len(set(dfNod2.TOWNCODE))==n
224/80: len(set(dfNod2.TOWNCODE))
224/81: n
224/82: m
224/83: tflag=dfNod2.tflag[0::m]
224/84: len(tflag)
224/85: len(set(tflag))
224/86: tflag[:5]
224/87:
for t in range(len(dfNod2)//m):
    TownNode[int(t)]=dfNod2.loc[dfNod2.tflag==tflag[t],'NO2']
224/88: tflag=list(dfNod2.tflag[0::m])
224/89: tflag[:5]
224/90:
for t in range(len(dfNod2)//m):
    TownNode[int(t)]=dfNod2.loc[dfNod2.tflag==tflag[t],'NO2']
224/91: TownNode.head()
224/92: dfNod2.head()
224/93:
for t in range(len(dfNod2)//m):
    TownNode[int(t)]=list(dfNod2.loc[dfNod2.tflag==tflag[t],'NO2'])
224/94: TownNode.head()
224/95: TownNode.tail()
225/9:
tflag=list(dfNod2.tflag[0::n])
for t in range(len(dfNod2)//n):
    TownNode[int(t)]=dfNod2.loc[dfNod2.tflag==tflag[t],'NO2']
for c in ['TOWNCODE','TOWNNAME']:
    if c in TownNode.columns:del TownNode[c]
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
224/96: !top
225/10: len(tflag)
225/11: len(dfNod2)//n
225/12: len(dfNod2)//n
225/13: len(TownNode)
225/14:
train_data, test_data = train_test_split(TownNode, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
225/15:
seq_len = 72
pre_len = 24
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
225/16:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
225/17:
seq_len = 24
pre_len = 12
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
225/18:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
225/19:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[10, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
225/20:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[10, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
225/21:
print(d[:5,:5])
print(d.max())
225/22: TownNode
225/23:
TownNode=pd.read_csv('TownNode.csv')
tflag=list(dfNod2.tflag[0::n])
nt=len(tflag)
for t in range(nt):
    TownNode[int(t)]=dfNod2.loc[dfNod2.tflag==tflag[t],'NO2']
for c in ['TOWNCODE','TOWNNAME']:
    if c in TownNode.columns:del TownNode[c]
225/24: TownNode
225/25: dfNod2
225/26:
TownNode=pd.read_csv('TownNode.csv')
tflag=list(dfNod2.tflag[0::n])
nt=len(tflag)
no2=np.array(dfNod2.NO2).reshape(n,nt)
for t in range(nt):
    TownNode[int(t)]=no2[:,t]
for c in ['TOWNCODE','TOWNNAME']:
    if c in TownNode.columns:del TownNode[c]
225/27: TownNode
225/28:
train_rate = 0.8
train_data, test_data = train_test_split(TownNode, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
225/29:
seq_len = 72
pre_len = 24
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
225/30:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[10, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
225/31:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[10, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=10,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
225/32:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[10, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=10,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
) 
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
225/33:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
225/34:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 50]
a_true = train_rescref[:1000:1, 50]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/35:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[10, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
) 
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
225/36:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
225/37:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 50]
a_true = train_rescref[:1000:1, 50]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/38:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -5]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
225/39:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 50]
a_true = train_rescref[:1000:1, 50]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/40:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:100:1, 50]
a_true = train_rescref[:100:1, 50]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/41:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:100:1, 150]
a_true = train_rescref[:100:1, 150]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/42:
train_rate = 0.8
train_data, test_data = train_test_split(TownNode, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
225/43:
seq_len = 24
pre_len = 12
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
225/44:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[2, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
) 
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
225/45:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[2, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[20, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
) 
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
225/46:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -5]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * (max_speed-min_speed))+min_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
225/48:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -5]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * (max_speed-min_speed))+min_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))

print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
225/50:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -1
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
225/51:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:100:1, 150]
a_true = train_rescref[:100:1, 150]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/52:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 150]
a_true = train_rescref[:1000:1, 150]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/53:
seq_len = 72
pre_len = 72
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
225/54:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[20, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[20, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
) 
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
225/55:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 150]
a_true = TownNode.iloc[0,:]#train_rescref[:1000:1, 150]
#plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/56:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 150]
a_true = TownNode.iloc[10,:]#train_rescref[:1000:1, 150]
#plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/57:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 150]
a_true = TownNode.iloc[50,:]#train_rescref[:1000:1, 150]
#plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/58:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 150]
a_true = TownNode.iloc[90,:]#train_rescref[:1000:1, 150]
#plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/59:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 150]
a_true = TownNode.iloc[90,:1000]#train_rescref[:1000:1, 150]
#plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/60:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 150]
a_true = TownNode.iloc[90,:100]#train_rescref[:1000:1, 150]
#plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
224/97: dfvar.head()
224/98: df0
225/61: nc = netCDF4.Dataset("2019.nc",'r')
225/62:
import netCDF4
nc = netCDF4.Dataset("2019.nc",'r')
225/63:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 150]
a_true = nc['NO2'][:,0,nrow//2,ncol//2]#TownNode.iloc[90,:100]#train_rescref[:1000:1, 150]
#plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/64:
import netCDF4
nc = netCDF4.Dataset("2019.nc",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
225/65:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 150]
a_true = nc['NO2'][:,0,nrow//2,ncol//2]#TownNode.iloc[90,:100]#train_rescref[:1000:1, 150]
#plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/66:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[:1000:1, 150]
a_true = nc['NO2'][:100,0,nrow//2,ncol//2]#TownNode.iloc[90,:100]#train_rescref[:1000:1, 150]
#plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
225/67:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 150]
a_true = nc['NO2'][:1000,0,nrow//2,ncol//2]#TownNode.iloc[90,:100]#train_rescref[:1000:1, 150]
#plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
224/99: col
224/100:
dfvar=DataFrame()
for t in range(nt):
  for v in col:
    df[v]=np.array(nc.variables[v][t,0,:,:]).flatten()
  df['tflag']=int(flags[t])
  df0=pivot_table(df,index='TOWNCODE',values=['tflag']+col,aggfunc=np.mean).reset_index()
  dfvar=dfvar.append(df0,ignore_index=True)
dfvar.set_index('tflag').to_csv('dfvar2.csv')
224/101: !head dfvar.csv
224/102: !head dfvar2.csv
224/103: !tail dfvar2.csv
224/104: !tail dfvar.csv
224/105: !diff dfvar2.csv dfvar.csv
226/2:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg
from stellargraph.layer import GCN_LSTM

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

dataset = sg.datasets.METR_LA()
speed_data, sensor_dist_adj = dataset.load()
num_nodes, time_len = speed_data.shape
print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
print(speed_data.head())

def scale_data(train_data, test_data):
    max_speed = train_data.max()
    min_speed = train_data.min()
    train_scaled = (train_data - min_speed) / (max_speed - min_speed)
    test_scaled = (test_data - min_speed) / (max_speed - min_speed)
    return train_scaled, test_scaled
226/3:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd


nc = netCDF4.Dataset("2019.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons.flatten(),lats.flatten())]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]

name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
print(dfNod.head())
226/4:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
226/5: dfNod2.head()
224/106: !lst
224/107: ls -lh *bin
226/6:
n=3739
fname='pnt_Dis'+str(n)+'x'+str(n)+'.bin'
with FortranFile(fname, 'r') as f:
  d=f.read_record(dtype=np.float64)
d=d.reshape(n,n)
d=1/d
for j in range(n):
  d[j,j]=1.
dmax=d.max()
for j in range(n):
  d[j,j]=dmax
d=d/d.max()
226/7:
from scipy.io import FortranFile
n=3739
fname='pnt_Dis'+str(n)+'x'+str(n)+'.bin'
with FortranFile(fname, 'r') as f:
  d=f.read_record(dtype=np.float64)
d=d.reshape(n,n)
d=1/d
for j in range(n):
  d[j,j]=1.
dmax=d.max()
for j in range(n):
  d[j,j]=dmax
d=d/d.max()
226/8: d[:5,:5]
226/9:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.9
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
226/10:
seq_len = 72
pre_len = 24
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
226/11:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[100, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=50,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
226/12:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
226/13:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[1000:2000:1, 3050]
a_true = test_rescref[1000:2000:1, 3050]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
226/14:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 3]
a_true = test_rescref[:, 3]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
226/15:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 35]
a_true = test_rescref[:, 35]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
224/108: !lst
224/109: cp Forecasting\ using\ spatio-temporal\ data\ with\ combined\ Graph\ Convolution_LSTM\ model.ipynb byGridHr.ipynb
226/16:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd


nc = netCDF4.Dataset("2019.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons.flatten(),lats.flatten())]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]

name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
print(dfNod.head())
226/17:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd


nc = netCDF4.Dataset("2019.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons.flatten(),lats.flatten())]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]

#name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
print(dfNod.head())
226/18:
s=set(name_inside)
n=NM #len(s)
dfNod2=dfNod #.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
226/19: dfNod2.head()
226/20:
d=np.zeros(shape=(n,n))
for j in range(n):
    for i in range(j,n):
        d[j,i]=dfNod2.geometry[j].distance(dfNod2.geometry[i])        
for j in range(n):
    for i in range(0,j):
        d[j,i]=d[i,j]
d=1/d
for j in range(n):
    d[j,j]=1
d=d/d.max()
226/21:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d
for j in range(n):
    d[j,j]=1
d=d/d.max()
226/22:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd


nc = netCDF4.Dataset("2019.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]

#name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
print(dfNod.head())
226/23:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d
for j in range(n):
    d[j,j]=1
d=d/d.max()
226/24: d[:5,:5]
226/25: d[NM//2,NM//2]
226/26: d[NM//2-5:NM//2+5,NM//2-5:NM//2+5]
226/27: d[:5,:5]==d[NM//2-5:NM//2+5,NM//2-5:NM//2+5]
226/28: d[:5,:5]==d[-5:,-5:] #d[NM//2-5:NM//2+5,NM//2-5:NM//2+5]
226/29:
from scipy.io import FortranFile
n=NM
fname='pnt_InvDis'+str(n)+'x'+str(n)+'.bin'
with FortranFile(fname, 'w') as f:
    f.write(d)
226/30:
from scipy.io import FortranFile
n=NM
fname='pnt_InvDis'+str(n)+'x'+str(n)+'.bin'
with FortranFile(fname, 'w') as f:
    f.write_record(d)
226/31:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.9
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
226/32:
seq_len = 72
pre_len = 24
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
226/33:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[100, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=50,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
224/110: !top
228/1:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd


nc = netCDF4.Dataset("2019.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]

#name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
print(dfNod.head())
228/2:
s=set(name_inside)
n=NM #len(s)
dfNod2=dfNod #.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
228/3:
#s=set(name_inside)
n=NM #len(s)
dfNod2=dfNod #.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
228/4:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d
for j in range(n):
    d[j,j]=1
d=d/d.max()
228/5: d[:5,:5]==d[-5:,-5:] #d[NM//2-5:NM//2+5,NM//2-5:NM//2+5]
228/6:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.9
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
228/7:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg
from stellargraph.layer import GCN_LSTM

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

dataset = sg.datasets.METR_LA()
speed_data, sensor_dist_adj = dataset.load()
num_nodes, time_len = speed_data.shape
print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
print(speed_data.head())

def scale_data(train_data, test_data):
    max_speed = train_data.max()
    min_speed = train_data.min()
    train_scaled = (train_data - min_speed) / (max_speed - min_speed)
    test_scaled = (test_data - min_speed) / (max_speed - min_speed)
    return train_scaled, test_scaled
228/8:
for i in ['geometry','name']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.9
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
228/9:
seq_len = 72
pre_len = 24
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
228/10:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 72],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[20, 72],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=50,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
228/11:
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
228/12:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 72],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[20, 72],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=5,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
229/1:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg
from stellargraph.layer import GCN_LSTM

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

def scale_data(train_data, test_data):
    max_speed = train_data.max()
    min_speed = train_data.min()
    train_scaled = (train_data - min_speed) / (max_speed - min_speed)
    test_scaled = (test_data - min_speed) / (max_speed - min_speed)
    return train_scaled, test_scaled

#dataset = sg.datasets.METR_LA()
#speed_data, sensor_dist_adj = dataset.load()
#num_nodes, time_len = speed_data.shape
#print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
#print(speed_data.head())
229/2:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.ncD",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
dfNod.Point=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame('name':[str(i) for i in range(NM)]})

for t in range(nt):
    dfNod[str(t)]=np.array(nc['NO2'][t,0,:,:]).flatten()
229/3:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.ncD",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
dfNod.Point=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'name':[str(i) for i in range(NM)]})

for t in range(nt):
    dfNod[str(t)]=np.array(nc['NO2'][t,0,:,:]).flatten()
229/4:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.ncD",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})

for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
229/5:
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]
name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
229/6:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
229/7: dfNod2.head()
229/8:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d
dmax=d.max()
for j in range(n):
    d[j,j]=dmax
d=d/dmax
229/9:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.ncD",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})

for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
dfNod['X']=X
dfNod['Y']=Y
229/10:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
229/11: dfNod2.head()
229/12:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=dfNod2.X,dfNod2.Y
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d
dmax=d.max()
for j in range(n):
    d[j,j]=dmax
d=d/dmax
229/13:
for i in ['geometry','name','X','Y']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.5
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
229/14:
seq_len = 10
pre_len = 5
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
229/15:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[1, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[10, 10],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
229/16: d[:5,:5]
229/17: d[:5,:5],dmax
229/18:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=dfNod2.X,dfNod2.Y
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
for j in range(n):
    d[j,j]=1
d=1/d
dmax=d.max()
d=d/dmax
for j in range(n):
    d[j,j]=1
229/19:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
229/20:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=dfNod2.X,dfNod2.Y
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
for j in range(n):
    d[j,j]=1
d=1/d
dmax=d.max()
d=d/dmax
for j in range(n):
    d[j,j]=1
229/21: d[:5,:5],dmax
229/22:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=dfNod2.X/1000.,dfNod2.Y/1000.
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
for j in range(n):
    d[j,j]=1
d=1/d
dmax=d.max()
d=d/dmax
for j in range(n):
    d[j,j]=1
229/23: d[:5,:5],dmax
229/24:
for i in ['geometry','name','X','Y']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.5
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
229/25:
seq_len = 10
pre_len = 5
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
229/26:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[1, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[10, 10],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
229/27:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
229/28:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
a_true = train_rescref[::1, 10]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/29:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
#plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/30:
seq_len = 5
pre_len = 2
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
229/31:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[1, 2],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[10, 10],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
229/32:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
229/33:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 10]

#plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/34:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 10]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/35:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[12, 2],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[10, 10],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
229/36:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
229/37:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 10]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/38:
seq_len = 5
pre_len = 3
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
229/39:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[12, 5],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[20, 10],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
229/40:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 10]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/41:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 10]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/42:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 100]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 100]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/43:
seq_len = 3
pre_len = 1
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
229/44:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[12, 5],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[20, 10],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
229/45:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 10]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/46:
seq_len = 5
pre_len = 1
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
229/47:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[120, 5],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=6,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
229/48:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 10]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/49:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 100]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 100]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/50:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[::1, 100]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = test_rescref[::1, 100]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/51:
seq_len = 20
pre_len = 3
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
229/52:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[120, 20],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=6,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
229/53:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 10]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/54:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[::1, 100]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = test_rescref[::1, 100]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/55:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.ncD",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})

col2=['NO2','CO','VOC']
nt0=0
for v in col2
    vmax=np.array(nc[v][:,0,:,:]).max()
    for t in range(nt0,nt0+nt):
        dfNod[str(t)]=nc[v][t,0,:,:].flatten()/vmax
    nt0+=nt
dfNod['X']=X
dfNod['Y']=Y
229/56:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.ncD",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})

col2=['NO2','CO','VOC']
nt0=0
for v in col2:
    vmax=np.array(nc[v][:,0,:,:]).max()
    for t in range(nt0,nt0+nt):
        dfNod[str(t)]=nc[v][t,0,:,:].flatten()/vmax
    nt0+=nt
dfNod['X']=X
dfNod['Y']=Y
229/57:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.ncD",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})

col2=['NO2','CO','VOC']
nt0=0
for v in col2:
    vmax=np.array(nc[v][:,0,:,:]).max()
    for t in range(nt0,nt0+nt):
        dfNod[str(t)]=nc[v][t-nt0,0,:,:].flatten()/vmax
    nt0+=nt
dfNod['X']=X
dfNod['Y']=Y
229/58:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
229/59: dfNod2.head()
229/60: dfNod2
229/61:
for i in ['geometry','name','X','Y']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
229/62:
seq_len = 20
pre_len = 3
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
229/63:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[120, 20],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=6,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
230/1:
import netCDF4
nc = netCDF4.Dataset("2019.ncD",'r')
230/2:
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
229/64:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 10]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/65:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[::1, 100]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = test_rescref[::1, 100]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/66:
import scipy as sp
for v in col2:
    shape, loc, scale = sp.stats.lognorm.fit(nc[v][:], floc=0)
    print v,shape, loc, scale
229/67:
import scipy as sp
for v in col2:
    shape, loc, scale = sp.stats.lognorm.fit(nc[v][:], floc=0)
    print (v,shape, loc, scale)
229/68:
def log_normalize(a, axis=None):
    """
    Normalizes the input array so that ``sum(exp(a)) == 1``.

    Parameters
    ----------
    a : array
        Non-normalized input data.

    axis : int
        Dimension along which normalization is performed.

    Notes
    -----
    Modifies the input **inplace**.
    """
    if axis is not None and a.shape[axis] == 1:
        # Handle single-state GMMHMM in the degenerate case normalizing a
        # single -inf to zero.
        a[:] = 0
    else:
        with np.errstate(under="ignore"):
            a_lse = logsumexp(a, axis, keepdims=True)
        a -= a_lse
229/69:
for v in col2:
    var=nc[v][:].flatten()
    print(v,sp.stats.lognorm.fit(var, floc=0))
    a=log_normalize(var)
    print(v,sp.stats.lognorm.fit(var, floc=0))
229/70:
from scipy.special import logsumexp
for v in col2:
    var=nc[v][:].flatten()
    print(v,sp.stats.lognorm.fit(var, floc=0))
    a=log_normalize(var)
    print(v,sp.stats.lognorm.fit(var, floc=0))
229/71:
from scipy.special import logsumexp
for v in col2:
    var=np.array(nc[v][:].flatten())
    print(v,sp.stats.lognorm.fit(var, floc=0))
    a=log_normalize(var)
    print(v,sp.stats.lognorm.fit(var, floc=0))
229/72:
from scipy.special import logsumexp
for v in col2:
    var=np.array(nc[v][:].flatten())
    shape, loc, scale = sp.stats.lognorm.fit(nc[v][:], floc=0)
    print(v,hape, loc, scale)
    a=log_normalize(var)
    var2=(exp(var*shape+np.log(scale))))
    print(v,sp.stats.lognorm.fit(var, floc=0))
229/73:
from scipy.special import logsumexp
for v in col2:
    var=np.array(nc[v][:].flatten())
    shape, loc, scale = sp.stats.lognorm.fit(nc[v][:], floc=0)
    print(v,hape, loc, scale)
    a=log_normalize(var)
    var2=(exp(var*shape+np.log(scale)))
    print(v,sp.stats.lognorm.fit(var, floc=0))
229/74:
from scipy.special import logsumexp
for v in col2:
    var=np.array(nc[v][:].flatten())
    shape, loc, scale = sp.stats.lognorm.fit(nc[v][:], floc=0)
    print(v,shape, loc, scale)
    a=log_normalize(var)
    var2=(exp(var*shape+np.log(scale)))
    print(v,sp.stats.lognorm.fit(var, floc=0))
229/75:
from scipy.special import logsumexp
for v in col2:
    var=np.array(nc[v][:].flatten())
    shape, loc, scale = sp.stats.lognorm.fit(nc[v][:], floc=0)
    print(v,shape, loc, scale)
    a=log_normalize(var)
    var2=(np.exp(var*shape+np.log(scale)))
    print(v,sp.stats.lognorm.fit(var2, floc=0))
229/76:
from scipy.special import logsumexp
for v in col2:
    var=np.array(nc[v][:].flatten())
    shape, loc, scale = sp.stats.lognorm.fit(nc[v][:], floc=0)
    print(v,shape, loc, scale)
    print(v,var.max(),var.min())
    a=log_normalize(var)
    print(v,var.max(),var.min())
    var2=(np.exp(var*shape+np.log(scale)))
    print(v,sp.stats.lognorm.fit(var2, floc=0))
229/77:
from scipy.special import logsumexp
for v in col2:
    var=np.array(nc[v][:].flatten())
    shape, loc, scale = sp.stats.lognorm.fit(nc[v][:], floc=0)
    print(v,shape, loc, scale)
    print(v,var.max(),var.min())
    a=log_normalize(var)
    print(np.sum(np.exp(var)))
229/78:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.ncD",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})

col2=['NO2','CO','VOC']
nt0=0
for v in col2:
    vmax=np.array(nc[v][:,0,:,:]).max()
    var=nc[v][:,0,:,:].flatten()
    a=log_normalize(var)
    var=var.reshape(nt,nrow,ncol)
    for t in range(nt0,nt0+nt):
        dfNod[str(t)]=var[t-nt0,:,:].flatten()
    nt0+=nt
dfNod['X']=X
dfNod['Y']=Y
229/79:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.ncD",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})

col2=['NO2','CO','VOC']
nt0=0
for v in col2:
    vmax=np.array(nc[v][:,0,:,:]).max()
    var=np.array(nc[v][:,0,:,:]).flatten()
    a=log_normalize(var)
    var=var.reshape(nt,nrow,ncol)
    for t in range(nt0,nt0+nt):
        dfNod[str(t)]=var[t-nt0,:,:].flatten()
    nt0+=nt
dfNod['X']=X
dfNod['Y']=Y
229/80:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
229/81: dfNod2
229/82:
for i in ['geometry','name','X','Y']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
229/83: dfNod2
229/84:
seq_len = 20
pre_len = 3
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
229/85: trainX[:5,:5,:5]
229/86:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[120, 20],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=6,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
229/87:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 10]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/88:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[::1, 100]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = test_rescref[::1, 100]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/89:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd

nc = netCDF4.Dataset("2019.ncD",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})

col2=['NO2','CO','VOC']
nt0=0
for v in col2:
    vmax=np.array(nc[v][:,0,:,:]).max()
    var=np.array(nc[v][:,0,:,:]).flatten()/vmax
#    a=log_normalize(var)
    var=var.reshape(nt,nrow,ncol)
    for t in range(nt0,nt0+nt):
        dfNod[str(t)]=var[t-nt0,:,:].flatten()
    nt0+=nt
dfNod['X']=X
dfNod['Y']=Y
229/90:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
229/91: dfNod2
229/92:
for i in ['geometry','name','X','Y']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.8
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
229/93:
seq_len = 20
pre_len = 3
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
229/94:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[120, 20],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=6,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
233/1:
import numpy as np
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
gdf.head()
boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
gdf.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
233/2:
import numpy as np
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
gdf.head()
#boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
gdf.plot(ax=ax, color="gray")
df1.plot(ax=ax, markersize=3.5)#, color="brown")
df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
233/3:
import numpy as np
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
gdf.head()
#boundary = gpd.read_file("mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
gdf.plot(ax=ax, color="gray")
#df1.plot(ax=ax, markersize=3.5)#, color="brown")
#df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
233/4: gdf
233/5:
shp_fname="/home/kuang/bin/TWN_TOWN.shp"
gdf = gpd.read_file(shp_fname)
233/6:
shp_fname="/home/kuang/bin/GIS/TOWN_MOI_1090727.shp"
gdf = gpd.read_file(shp_fname)
233/7:
ig, ax = plt.subplots(figsize=(12, 10))
gdf.plot(ax=ax, color="gray")
#df1.plot(ax=ax, markersize=3.5)#, color="brown")
#df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
233/8:
ig, ax = plt.subplots(figsize=(22, 20))
gdf.plot(ax=ax, color="gray")
#df1.plot(ax=ax, markersize=3.5)#, color="brown")
#df2.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
233/9: gdf
229/95:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 10]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/96:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[::1, 100]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = test_rescref[::1, 100]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/97: trainX[:5,:5,:5]
229/98:
seq_len = 10
pre_len = 3
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
229/99:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[120, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[100, 100],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=6,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
229/100:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[:, :, -1]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = train_rescpred[::1, 10]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = train_rescref[::1, 10]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/101:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[::1, 100]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = test_rescref[::1, 100]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
229/102:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[::1, 1000]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = test_rescref[::1, 1000]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
234/1:
from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader
from torch_geometric_temporal.signal import temporal_signal_split

loader = ChickenpoxDatasetLoader()

dataset = loader.get_dataset()

train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)
234/2:
import torch
import torch.nn.functional as F
from torch_geometric_temporal.nn.recurrent import DCRNN

class RecurrentGCN(torch.nn.Module):
    def __init__(self, node_features):
        super(RecurrentGCN, self).__init__()
        self.recurrent = DCRNN(node_features, 32, 1)
        self.linear = torch.nn.Linear(32, 1)

    def forward(self, x, edge_index, edge_weight):
        h = self.recurrent(x, edge_index, edge_weight)
        h = F.relu(h)
        h = self.linear(h)
        return h
234/3:
from tqdm import tqdm

model = RecurrentGCN(node_features = 4)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

model.train()

for epoch in tqdm(range(200)):
    cost = 0
    for time, snapshot in enumerate(train_dataset):
        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
        cost = cost + torch.mean((y_hat-snapshot.y)**2)
    cost = cost / (time+1)
    cost.backward()
    optimizer.step()
    optimizer.zero_grad()
234/4:
model.eval()
cost = 0
for time, snapshot in enumerate(test_dataset):
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
>>> MSE: 1.0232
234/5:
model.eval()
cost = 0
for time, snapshot in enumerate(test_dataset):
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/6: type(train_dataset)
234/7:

for time, snapshot in enumerate(train_dataset):
        print(time)
234/8:
i=1
for time, snapshot in enumerate(train_dataset):
    print(time)
    print(type(snapshot))
    if i==1:break
234/9:
i=1
for time, snapshot in enumerate(train_dataset):
    print(time)
    print(dir(snapshot))
    if i==1:break
234/10: snapshot.x
234/11: snapshot.x.shape
234/12: snapshot.y.shape
234/13: snapshot.size
234/14:
i=1
for time, snapshot in enumerate(train_dataset):
    print(time)
#    if i==1:break
234/15:
from tqdm import tqdm

model = RecurrentGCN(node_features = 4)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

model.train()

for epoch in tqdm(range(200)):
    cost = 0
    if epoch==199:print(epoch)
    for time, snapshot in enumerate(train_dataset):
        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
        cost = cost + torch.mean((y_hat-snapshot.y)**2)
    cost = cost / (time+1)
    cost.backward()
    optimizer.step()
    optimizer.zero_grad()
234/16:
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(np.mean(snapshot.y))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append(np.mean(y_hat))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/17:
import numpy as np
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(np.mean(snapshot.y))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append(np.mean(y_hat))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/18: type(snapshot.y)
234/19: type(np.array(snapshot.y))
234/20: np.mean(np.array(snapshot.y))
234/21:
import numpy as np
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(np.mean(np.array(snapshot.y)))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append(np.mean(np.array(y_hat)))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/22:
import numpy as np
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(np.mean(np.array(snapshot.y)))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append(np.mean(tensor.detach().numpy.array((y_hat))))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/23: type(y_hat)
234/24: torch.mean(y_hat)
234/25: torch.mean(y_hat).value
234/26: dir(torch.mean(y_hat))
234/27: torch.mean(y_hat).values
234/28: torch.mean(y_hat).values()
234/29: print(torch.mean(y_hat).values)
234/30: torch.mean(y_hat)
234/31: tensor.mean(y_hat)
234/32: torch.mean(y_hat)
234/33: torch.mean(y_hat).data
234/34: torch.mean(y_hat).data.values
234/35: torch.mean(y_hat).data[0]
234/36: torch.mean(y_hat).data
234/37: np.array(torch.mean(y_hat).data)
234/38: np.array(torch.mean(y_hat).data)[0]
234/39: print(np.array(torch.mean(y_hat).data))
234/40:
import numpy as np
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(np.mean(np.array(snapshot.y)))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append(np.array(torch.mean(y_hat).data))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/41:
import matplotlib.pyplot as plt

## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = y1
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = y0

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
234/42: len(y0)
234/43: test_dataset.size
234/44: snapshot.size
234/45: y_hat.shape
234/46: type(dataset)
234/47:
from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader
from torch_geometric_temporal.signal import temporal_signal_split

loader = ChickenpoxDatasetLoader()

dataset = loader.get_dataset()

train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)
234/48:
from tqdm import tqdm

model = RecurrentGCN(node_features = 4)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

model.train()

for epoch in tqdm(range(200)):
    cost = 0
    if epoch==199:print(epoch)
    for time, snapshot in enumerate(train_dataset):
        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
        cost = cost + torch.mean((y_hat-snapshot.y)**2)
    cost = cost / (time+1)
    cost.backward()
    optimizer.step()
    optimizer.zero_grad()
234/49:
import numpy as np
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(np.mean(np.array(snapshot.y)))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append(np.array(torch.mean(y_hat).data))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/50:
import matplotlib.pyplot as plt

## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = y1
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = y0

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
234/51:
import numpy as np
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(snapshot.y[0]) #np.mean(np.array(snapshot.y)))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append(y_hat[0,0]) #np.array(torch.mean(y_hat).data))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/52:
import matplotlib.pyplot as plt

## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = y1
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = y0

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
234/53: len(y0)
234/54: len(y1)
234/55:
import matplotlib.pyplot as plt

## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = y1[:]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = y0[:]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
234/56: y1[:5]
234/57: print(np.array(torch(y_hat).data))
234/58: print(y_hat.item)
234/59: type(y_hat)
234/60: y_hat.item()
234/61: y_hat[0].item()
234/62:
import numpy as np
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(snapshot.y[0]) #np.mean(np.array(snapshot.y)))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append(y_hat[0].item()) #np.array(torch.mean(y_hat).data))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/63:
import matplotlib.pyplot as plt

## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = y1[:]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = y0[:]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
234/64: dir(dataset)
234/65: node_features, snapshot.size
234/66:
from tqdm import tqdm

model = RecurrentGCN(node_features = 4)

optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

model.train()

for epoch in tqdm(range(200)):
    cost = 0
    if epoch==199:print(epoch)
    for time, snapshot in enumerate(train_dataset):
        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
        cost = cost + torch.mean((y_hat-snapshot.y)**2)
    cost = cost / (time+1)
    cost.backward()
    optimizer.step()
    optimizer.zero_grad()
234/67:
import numpy as np
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(snapshot.y[0]) #np.mean(np.array(snapshot.y)))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append(y_hat[0].item()) #np.array(torch.mean(y_hat).data))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/68:
import matplotlib.pyplot as plt

## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = y1[:]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = y0[:]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
234/69:
import numpy as np
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(snapshot.y[10]) #np.mean(np.array(snapshot.y)))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append(y_hat[10].item()) #np.array(torch.mean(y_hat).data))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/70:
import matplotlib.pyplot as plt

## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = y1[:]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = y0[:]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
234/71:
import numpy as np
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(snapshot.y[:]) #np.mean(np.array(snapshot.y)))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append(y_hat[:].item()) #np.array(torch.mean(y_hat).data))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/72:
import numpy as np
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(snapshot.y[:]) #np.mean(np.array(snapshot.y)))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append([i.item() for i in y_hat]) #np.array(torch.mean(y_hat).data))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/73: snapshot.size
234/74: np.array(y0).shape
234/75:
import matplotlib.pyplot as plt
y0,y1=np.array(y0).flatten().reshape(259,20),np.array(y1).flatten().reshape(259,20)
## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = y1[0,:]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = y0[0,:]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
234/76: y0
234/77:
import numpy as np
model.eval()
cost = 0
y0=[]
y1=[]
for time, snapshot in enumerate(test_dataset):
    y0.append(np.array([i.item() for i in snapshot.y[:]]))#np.mean(np.array(snapshot.y)))
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
    y1.append(np.array([i.item() for i in y_hat])) #np.array(torch.mean(y_hat).data))
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
234/78:
import matplotlib.pyplot as plt
y0,y1=np.array(y0).flatten().reshape(259,20),np.array(y1).flatten().reshape(259,20)
## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = y1[0,:]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
a_true = y0[0,:]

plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
234/79:
import matplotlib.pyplot as plt
y0,y1=np.array(y0).flatten().reshape(259,20),np.array(y1).flatten().reshape(259,20)
## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
for i in range(5):
    a_pred = y1[i,:]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
    a_true = y0[i,:]

    plt.plot(a_pred, "r-", label="prediction")
    plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
234/80:
import matplotlib.pyplot as plt
y0,y1=np.array(y0).flatten().reshape(259,20),np.array(y1).flatten().reshape(259,20)
## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
for i in range(5):
    a_pred = y1[:,i]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
    a_true = y0[:,i]

    plt.plot(a_pred, "r-", label="prediction")
    plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
228/13:
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' #added by kuang
import sys
import urllib.request

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import stellargraph as sg
from stellargraph.layer import GCN_LSTM

def train_test_split(data, train_portion):
    time_len = data.shape[1]
    train_size = int(time_len * train_portion)
    train_data = np.array(data.iloc[:, :train_size])
    test_data = np.array(data.iloc[:, train_size:])
    return train_data, test_data

def sequence_data_preparation(seq_len, pre_len, train_data, test_data):
    trainX, trainY, testX, testY = [], [], [], []

    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):
        a = train_data[:, i : i + seq_len + pre_len]
        trainX.append(a[:, :seq_len])
        trainY.append(a[:, -1])

    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):
        b = test_data[:, i : i + seq_len + pre_len]
        testX.append(b[:, :seq_len])
        testY.append(b[:, -1])

    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)

    return trainX, trainY, testX, testY

dataset = sg.datasets.METR_LA()
speed_data, sensor_dist_adj = dataset.load()
num_nodes, time_len = speed_data.shape
print("No. of sensors:", num_nodes, "\nNo of timesteps:", time_len)
print(speed_data.head())

def scale_data(train_data, test_data):
    max_speed = train_data.max()
    min_speed = train_data.min()
    train_scaled = (train_data - min_speed) / (max_speed - min_speed)
    test_scaled = (test_data - min_speed) / (max_speed - min_speed)
    return train_scaled, test_scaled
228/14:
train_rate = 0.2
train_data, test_data = train_test_split(speed_data, train_rate)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)

def scale_data(train_data, test_data):
    max_speed = train_data.max()
    min_speed = train_data.min()
    train_scaled = (train_data - min_speed) / (max_speed - min_speed)
    test_scaled = (test_data - min_speed) / (max_speed - min_speed)
    return train_scaled, test_scaled

train_scaled, test_scaled = scale_data(train_data, test_data)
228/15:
ythat = model.predict(trainX)
yhat = model.predict(testX)
228/16:
seq_len = 10
pre_len = 12
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
from stellargraph.layer import GCN_LSTM
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=sensor_dist_adj,
    gc_layer_sizes=[16, 10],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[200, 200],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=100,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
228/17:
ythat = model.predict(trainX)
yhat = model.predict(testX)
228/18:
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * (max_speed-min_speed))+min_speed
test_rescref = np.array(testY * (max_speed-min_speed))+min_speed
## Rescale model predicted values
train_rescpred = np.array((ythat) * (max_speed-min_speed))+min_speed
test_rescpred = np.array((yhat) * (max_speed-min_speed))+min_speed
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -1
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
228/19:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 100]
a_true = test_rescref[:, 100]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
228/20:
nc = netCDF4.Dataset("2019.nc",'r')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
228/21:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
228/22:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd


nc = netCDF4.Dataset("2019.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]

name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
print(dfNod.head())
228/23:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
228/24: dfNod2.head()
228/25:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d
dmax=d.max()
for j in range(n):
    d[j,j]=dmax
d=np.sqrt(d/dmax)
228/26:
import netCDF4
from pyproj import Proj
import numpy as np
from shapely.geometry import Point
import pandas as pd
import geopandas as gpd


nc = netCDF4.Dataset("2019.nc",'r')
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=nc.variables[V[3][0]].shape
NM=nrow*ncol
x1d=[nc.XORIG+nc.XCELL*(i+0.5) for i in range(ncol)]
y1d=[nc.YORIG+nc.YCELL*(i+0.5) for i in range(nrow)]
X,Y=np.meshgrid(x1d,y1d)
X,Y=X.flatten(),Y.flatten()
lons, lats= pnyc(X,Y, inverse=True)
pnts=[Point(lon,lat) for lon,lat in zip(lons,lats)]
dfNod=pd.DataFrame({'geometry':pnts,'name':[str(i) for i in range(NM)]})
root='/nas2/cmaqruns/2022fcst/fusion/Voronoi/'
boundary=gpd.read_file(root+'boundary_shape.shp')
boundary_shape = boundary.geometry[0]

name_inside=dfNod.loc[dfNod.geometry.map(lambda p:p.within(boundary_shape)),'name']
for t in range(nt):
    dfNod[str(t)]=nc['NO2'][t,0,:,:].flatten()
dfNod['X']=X
dfNod['Y']=Y
print(dfNod.head())
228/27:
s=set(name_inside)
n=len(s)
dfNod2=dfNod.loc[dfNod.name.map(lambda x:x in s)].reset_index(drop=True)
228/28:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d
dmax=d.max()
for j in range(n):
    d[j,j]=dmax
d=np.sqrt(d/dmax)
228/29: dfNod2.head()
228/30:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=dfNod2.X,dfNod2.Y
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d
dmax=d.max()
for j in range(n):
    d[j,j]=dmax
d=np.sqrt(d/dmax)
228/31: d[:5,:5]#==d[-5:,-5:] #d[NM//2-5:NM//2+5,NM//2-5:NM//2+5]
228/32:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=np.array(dfNod2.X),np.array(dfNod2.Y)
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d
dmax=d.max()
for j in range(n):
    d[j,j]=dmax
d=np.sqrt(d/dmax)
228/33: d[:5,:5]#==d[-5:,-5:] #d[NM//2-5:NM//2+5,NM//2-5:NM//2+5]
228/34:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=np.array(dfNod2.X),np.array(dfNod2.Y)
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d
dmax=d.max()
for j in range(n):
    d[j,j]=dmax
d=d/dmax
228/35: d[:5,:5]#==d[-5:,-5:] #d[NM//2-5:NM//2+5,NM//2-5:NM//2+5]
228/36:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=np.array(dfNod2.X),np.array(dfNod2.Y)
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d
for j in range(n):
    d[j,j]=1.
dmax=d.max    
for j in range(n):
    d[j,j]=dmax
d=d/dmax
228/37:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=np.array(dfNod2.X),np.array(dfNod2.Y)
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d
for j in range(n):
    d[j,j]=1.
dmax=d.max()   
for j in range(n):
    d[j,j]=dmax
d=d/dmax
228/38: d[:5,:5]#==d[-5:,-5:] #d[NM//2-5:NM//2+5,NM//2-5:NM//2+5]
228/39:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=np.array(dfNod2.X),np.array(dfNod2.Y)
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d*1000.
for j in range(n):
    d[j,j]=1.
dmax=d.max()   
for j in range(n):
    d[j,j]=dmax
d=d/dmax
228/40: d[:5,:5]#==d[-5:,-5:] #d[NM//2-5:NM//2+5,NM//2-5:NM//2+5]
228/41:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=np.array(dfNod2.X),np.array(dfNod2.Y)
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d*1000.
for j in range(n):
    d[j,j]=1.
dmax=d.max()   
for j in range(n):
    d[j,j]=dmax
d=np.sqrt(d/dmax)
228/42: d[:5,:5]#==d[-5:,-5:] #d[NM//2-5:NM//2+5,NM//2-5:NM//2+5]
228/43:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=np.array(dfNod2.X),np.array(dfNod2.Y)
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d*1000.
for j in range(n):
    d[j,j]=1.
dmax=d.max()   
for j in range(n):
    d[j,j]=dmax
d=d/dmax
228/44: d[:5,:5]#==d[-5:,-5:] #d[NM//2-5:NM//2+5,NM//2-5:NM//2+5]
228/45:
d=np.zeros(shape=(n,n))
X1=np.zeros(shape=(n,n))
Y1=np.zeros(shape=(n,n))
X2=np.zeros(shape=(n,n))
Y2=np.zeros(shape=(n,n))
X,Y=np.array(dfNod2.X),np.array(dfNod2.Y)
X1[:,:],X2[:,:]=X[:,None],X[None,:]
Y1[:,:],Y2[:,:]=Y[:,None],Y[None,:]
d[:,:]=np.sqrt((X1-X2)**2+(Y1-Y2)**2) #dfNod2.geometry[j].distance(dfNod2.geometry[i])        
#for j in range(n):
#    for i in range(0,j):
#        d[j,i]=d[i,j]
d=1/d*1000.
for j in range(n):
    d[j,j]=1.
dmax=d.max()   
for j in range(n):
    d[j,j]=dmax
d=np.sqrt(np.sqrt(d/dmax))
228/46: d[:5,:5]#==d[-5:,-5:] #d[NM//2-5:NM//2+5,NM//2-5:NM//2+5]
228/47:
for i in ['geometry','name','X','Y']:
    if i in dfNod2.columns: del dfNod2[i]
train_rate = 0.5
train_data, test_data = train_test_split(dfNod2, train_rate)
train_scaled, test_scaled = scale_data(train_data, test_data)
print("Train data: ", train_data.shape)
print("Test data: ", test_data.shape)
228/48:
seq_len = 72
pre_len = 24
trainX, trainY, testX, testY = sequence_data_preparation(
    seq_len, pre_len, train_scaled, test_scaled
)
print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)
228/49:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 72],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[20, 72],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=5,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
228/50:
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
228/51:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
228/52:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 35]
a_true = test_rescref[:, 35]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
228/53:
gcn_lstm = GCN_LSTM(
    seq_len=seq_len,
    adj=d,
    gc_layer_sizes=[16, 72],
    gc_activations=["relu", "relu"],
    lstm_layer_sizes=[20, 72],
    lstm_activations=["tanh", "tanh"],)
x_input, x_output = gcn_lstm.in_out_tensors()
model = Model(inputs=x_input, outputs=x_output)
model.compile(optimizer="adam", loss="mae", metrics=["mse"])
history = model.fit(
    trainX,
    trainY,
    epochs=200,
    batch_size=60,
    shuffle=True,
    verbose=0,
    validation_data=[testX, testY],
)
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
234/81:
import matplotlib.pyplot as plt
y0,y1=np.array(y0).flatten().reshape(259,20),np.array(y1).flatten().reshape(259,20)
## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
for i in range(5):
    a_pred = y1[:100,i]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
    a_true = y0[:100,i]

    plt.plot(a_pred, "r-", label="prediction")
    plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
234/82:
import matplotlib.pyplot as plt
y0,y1=np.array(y0).flatten().reshape(259,20),np.array(y1).flatten().reshape(259,20)
## Rescale values
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
for i in range(1):
    a_pred = y1[:100,i]
#a_true = nc['NO2'][:,0,nrow//2,ncol//2]#train_rescref[::1, 10]
    a_true = y0[:100,i]

    plt.plot(a_pred, "r-", label="prediction")
    plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
230/3: 365*4
230/4: cd /nas2/cmaqruns/2019TZPP/output/Annual/met_em
230/5: fname='met_em.d01.2010-01-01_00:00:00.nc'
230/6: nc = netCDF4.Dataset(fname,'r')
230/7: v='PRES'
230/8: nc[v][0,:5,:5,:5]
230/9: nc[v][0,0,:5,:5]
230/10: nc[v]
230/11: nc[v].shape
230/12: nc[v][0,:,5,5]
230/13: np.array(nc[v][0,:,5,5]/1000)
230/14:
import numpy as np
for v in V[3]:
    if v in ['PM25_NH4','PM25_NO3','PM25_SO4',]:continue
    exec(v+'=np.array(nc["'+v+'"][:,0,:,:])')
230/15: np.array(nc[v][0,:,5,5]/1000)
230/16: v='PRES'
230/17: np.array(nc[v][0,:,5,5]/1000)
230/18: np.array(nc[v][0,:,15,15]/1000)
230/19: np.array(nc[v][0,:,45,45]/1000)
230/20: set(np.array(nc[v][0,3,:,:]/1000))
230/21: set(list(np.array(nc[v][0,3,:,:]/1000)))
230/22: set(list(np.array(nc[v][0,3,:,:])/1000))
230/23: set(list(np.array(nc[v][0,3,:,:]).flatten()/1000))
228/54:
print(model.summary())
print(
    "Train loss: ",
    history.history["loss"][-1],
    "\nTest loss:",
    history.history["val_loss"][-1],)
sg.utils.plot_history(history)
228/55:
ythat = model.predict(trainX)
yhat = model.predict(testX)
## Rescale values
max_speed = train_data.max()
min_speed = train_data.min()

## actual train and test values
train_rescref = np.array(trainY * max_speed)
test_rescref = np.array(testY * max_speed)
## Rescale model predicted values
train_rescpred = np.array((ythat) * max_speed)
test_rescpred = np.array((yhat) * max_speed)
## Naive prediction benchmark (using previous observed value)

testnpred = np.array(testX)[
    :, :, -5
]  # picking the last speed of the 10 sequence for each segment in each sample
testnpredc = (testnpred) * max_speed
## Performance measures

seg_mael = []
seg_masel = []
seg_nmael = []

for j in range(testX.shape[-1]):

    seg_mael.append(
        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))
    )  # Mean Absolute Error for NN
    seg_nmael.append(
        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))
    )  # Mean Absolute Error for naive prediction
    if seg_nmael[-1] != 0:
        seg_masel.append(
            seg_mael[-1] / seg_nmael[-1]
        )  # Ratio of the two: Mean Absolute Scaled Error
    else:
        seg_masel.append(np.NaN)

print("Total (ave) MAE for NN: " + str(np.mean(np.array(seg_mael))))
print("Total (ave) MAE for naive prediction: " + str(np.mean(np.array(seg_nmael))))
print(
    "Total (ave) MASE for per-segment NN/naive MAE: "
    + str(np.nanmean(np.array(seg_masel)))
)
print(
    "...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction."
)
# plot violin plot of MAE for naive and NN predictions
fig, ax = plt.subplots()
# xl = minsl

ax.violinplot(
    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

ax.violinplot(
    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0
)

line1 = mlines.Line2D([], [], label="NN")
line2 = mlines.Line2D([], [], color="C1", label="Instantaneous")

ax.set_xlabel("Scaled distribution amplitude (after Gaussian convolution)")
ax.set_ylabel("Mean Absolute Error")
ax.set_title("Distribution over segments: NN pred (blue) and naive pred (orange)")
plt.legend(handles=(line1, line2), title="Prediction Model", loc=2)
plt.show()
228/56:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:, 35]
a_true = test_rescref[:, 35]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
228/57:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:100, 35]
a_true = test_rescref[:100, 35]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
228/58:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[:300, 35]
a_true = test_rescref[:300, 35]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
228/59:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[100:500, 35]
a_true = test_rescref[100:300, 35]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
228/60:
##all test result visualization
fig1 = plt.figure(figsize=(15, 8))
#    ax1 = fig1.add_subplot(1,1,1)
a_pred = test_rescpred[100:500, 35]
a_true = test_rescref[100:500, 35]
plt.plot(a_pred, "r-", label="prediction")
plt.plot(a_true, "b-", label="true")
plt.xlabel("time")
plt.ylabel("speed")
plt.legend(loc="best", fontsize=10)
plt.show()
230/24: pwd
230/25: pwd
230/26: cd ~/MyPrograms/Taiwan-Python-Map
230/27: run render_script.py
230/28: %pip install seaborn
230/29: run render_script.py
230/30: !fc-list :lang=zh family
230/31: plt.rcParams['font.sans-serif']
230/32: plt.rcParams['font.family']
230/33: !fc-list |grep 'WenQuanYi Zen Hei Sharp'
230/34: !which python
230/35:
import matplotlib.font_manager
a = sorted([f.name for f in matplotlib.font_manager.fontManager.ttflist])
for i in a:
  print(i)
230/36: run render_script.py
230/37: pwd
230/38: run render_script.py
230/39: sf.shapeRecords
230/40: print(sf.shapeRecords)
230/41: print(sf.shapeRecords())
230/42: dir(sf.shapeRecords)
230/43:
id=0
for shape in sf.shapeRecords():
    print(shape.record[:])
230/44: run render_script.py
230/45: run render_script.py
230/46: run render_script.py
230/47: run render_script.py
230/48: run render_script.py
230/49: run render_script.py
230/50: run render_script.py
230/51: run render_script.py
230/52: run render_script.py
230/53: run render_script.py
230/54: run render_script.py
235/1: import geopandas as gpd
235/2: import geoplot as gplt
235/3: %pip install geoplot
235/4: %pip install Proj==8.0.0
235/5: %pip install Proj
235/6: %pip install geoplot
235/7: %conda install geoplot
235/8: %conda install geoplot -c conda-forge
230/55: !fc-list :lang=zh family
235/9: %pip3 install geoplot
235/10: !pip3 install geoplot
235/11: !python -V
236/1: import geopandas as gpd
236/2:
import geopandas as gpd
import geoplot as gplt
236/3: !pip3 install gplt
236/4: !pip3 install geoplot
236/5: !pip3 install geoplot
236/6:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt
236/7: %pip install geoplot
237/1:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt
237/2: !pip3 install--user geoplot
237/3: !pip install--user geoplot
238/1:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt
238/2: !which python
239/1:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt
241/1:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt
241/2: %pip3 install cartopy
241/3: %pip install cartopy
241/4: !locate libgeos_c.so.1
241/5: !which python
241/6: %pip install geos
241/7: ls -lh /home/kuang/.conda/envs/py39/lib/ligb*
241/8: ln -s /home/kuang/.conda/envs/pyn_env/lib/libgeos_c.so.1.17.1 /home/kuang/.conda/envs/py39/lib/libgeos_c.so.1
241/9: !ln -s /home/kuang/.conda/envs/pyn_env/lib/libgeos_c.so.1.17.1 /home/kuang/.conda/envs/py39/lib/libgeos_c.so.1
241/10:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt
241/11: !locate libgeos.so.3.11.1
243/1: !python -V
243/2:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt
244/1:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt
244/2:
nyc_boroughs = gpd.read_file(gplt.datasets.get_path('nyc_boroughs'))
tickets = gpd.read_file(gplt.datasets.get_path('nyc_parking_tickets'))

proj = gcrs.AlbersEqualArea(central_latitude=40.7128, central_longitude=-74.0059)
244/3:
def plot_state_to_ax(state, ax):
    gplt.choropleth(
        tickets.set_index('id').loc[:, [state, 'geometry']],
        hue=state, cmap='Blues',
        linewidth=0.0, ax=ax
    )
    gplt.polyplot(
        nyc_boroughs, edgecolor='black', linewidth=0.5, ax=ax
    )


f, axarr = plt.subplots(2, 2, figsize=(12, 13), subplot_kw={'projection': proj})

plt.suptitle('Parking Tickets Issued to State by Precinct, 2016', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('ny', axarr[0][0])
axarr[0][0].set_title('New York (n=6,679,268)')

plot_state_to_ax('nj', axarr[0][1])
axarr[0][1].set_title('New Jersey (n=854,647)')

plot_state_to_ax('pa', axarr[1][0])
axarr[1][0].set_title('Pennsylvania (n=215,065)')

plot_state_to_ax('ct', axarr[1][1])
axarr[1][1].set_title('Connecticut (n=126,661)')
230/56: pwd
230/57: !tail render_script.py
245/1:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt
245/2:
def plot_state_to_ax(state, ax):
    gplt.choropleth(
        tickets.set_index('id').loc[:, [state, 'geometry']],
        hue=state, cmap='Blues',
        linewidth=0.0, ax=ax
    )
    gplt.polyplot(
        nyc_boroughs, edgecolor='black', linewidth=0.5, ax=ax
    )


f, axarr = plt.subplots(2, 2, figsize=(12, 13), subplot_kw={'projection': proj})

plt.suptitle('Parking Tickets Issued to State by Precinct, 2016', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('ny', axarr[0][0])
axarr[0][0].set_title('New York (n=6,679,268)')

plot_state_to_ax('nj', axarr[0][1])
axarr[0][1].set_title('New Jersey (n=854,647)')

plot_state_to_ax('pa', axarr[1][0])
axarr[1][0].set_title('Pennsylvania (n=215,065)')

plot_state_to_ax('ct', axarr[1][1])
axarr[1][1].set_title('Connecticut (n=126,661)')
245/3:
def plot_state_to_ax(state, ax):
    gplt.choropleth(
        tickets.set_index('id').loc[:, [state, 'geometry']],
        hue=state, cmap='Blues',
        linewidth=0.0, ax=ax
    )
    gplt.polyplot(
        nyc_boroughs, edgecolor='black', linewidth=0.5, ax=ax
    )

# load the data
nyc_boroughs = gpd.read_file(gplt.datasets.get_path('nyc_boroughs'))
tickets = gpd.read_file(gplt.datasets.get_path('nyc_parking_tickets'))

proj = gcrs.AlbersEqualArea(central_latitude=40.7128, central_longitude=-74.0059)    

f, axarr = plt.subplots(2, 2, figsize=(12, 13), subplot_kw={'projection': proj})

plt.suptitle('Parking Tickets Issued to State by Precinct, 2016', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('ny', axarr[0][0])
axarr[0][0].set_title('New York (n=6,679,268)')

plot_state_to_ax('nj', axarr[0][1])
axarr[0][1].set_title('New Jersey (n=854,647)')

plot_state_to_ax('pa', axarr[1][0])
axarr[1][0].set_title('Pennsylvania (n=215,065)')

plot_state_to_ax('ct', axarr[1][1])
axarr[1][1].set_title('Connecticut (n=126,661)')
245/4: df=gpd.read_file(vor_stn.shp)
245/5: df=gpd.read_file("vor_stn.shp")
245/6: df
245/7: tickets
245/8:
df['sox']=df.COUNTYSN**2
df['nox']=np.sqrt(df.COUNTYSN)
245/9:
df['sox']=np.array(df.COUNTYSN)**2
df['nox']=np.sqrt(np.array(df.COUNTYSN))
245/10:
import numpy as np
df['sox']=np.array(df.COUNTYSN)**2
df['nox']=np.sqrt(np.array(df.COUNTYSN))
245/11:
import numpy as np
df['sox']=(np.array(df.COUNTYSN))**2
df['nox']=np.sqrt(np.array(df.COUNTYSN))
245/12: df.COUNTYSN[0]
245/13:
import numpy as np
df['sox']=(np.array(df.COUNTYSN),dtype=float)**2
df['nox']=np.sqrt(np.array(df.COUNTYSN),dtype=float)
245/14:
import numpy as np
df['sox']=(np.array(df.COUNTYSN,dtype=float))**2
df['nox']=np.sqrt(np.array(df.COUNTYSN,dtype=float))
245/15: df
245/16:
def plot_state_to_ax(pollutant, ax):
    gplt.choropleth(
        df.set_index('id').loc[:, [pollutant, 'geometry']],
        hue=state, cmap='Blues',
        linewidth=0.0, ax=ax
    )
    gplt.polyplot(
        nyc_boroughs, edgecolor='black', linewidth=0.5, ax=ax
    )
245/17:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, axarr = plt.subplots(1, 2, figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('sox', axarr[0][0])
axarr[0][0].set_title('right figure')

plot_state_to_ax('nox', axarr[0][1])
axarr[0][1].set_title('New Jersey (n=854,647)')
245/18:
def plot_state_to_ax(pollutant, ax):
    gplt.choropleth(
        df.loc[:, [pollutant, 'geometry']],
        hue=state, cmap='Blues',
        linewidth=0.0, ax=ax
    )
    gplt.polyplot(
        nyc_boroughs, edgecolor='black', linewidth=0.5, ax=ax
    )
245/19:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, axarr = plt.subplots(1, 2, figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('sox', axarr[0][0])
axarr[0][0].set_title('right figure')

plot_state_to_ax('nox', axarr[0][1])
axarr[0][1].set_title('New Jersey (n=854,647)')
245/20: axarr.shap
245/21: axarr.shape
245/22:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, axarr = plt.subplots(1, 2, figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('sox', axarr[0])
axarr[0][0].set_title('right figure')

plot_state_to_ax('nox', axarr[1])
axarr[0][1].set_title('New Jersey (n=854,647)')
245/23:
def plot_state_to_ax(pollutant, ax):
    gplt.choropleth(
        df.loc[:, [pollutant, 'geometry']],
        hue=pollutant, cmap='Blues',
        linewidth=0.0, ax=ax
    )
    gplt.polyplot(
        nyc_boroughs, edgecolor='black', linewidth=0.5, ax=ax
    )
245/24:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, axarr = plt.subplots(1, 2, figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('sox', axarr[0])
axarr[0][0].set_title('right figure')

plot_state_to_ax('nox', axarr[1])
axarr[0][1].set_title('New Jersey (n=854,647)')
245/25:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, axarr = plt.subplots(1, 2, figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('sox', axarr[0])
axarr[0].set_title('right figure')

plot_state_to_ax('nox', axarr[1])
axarr[1].set_title('New Jersey (n=854,647)')
245/26: nyc_boroughs
245/27:
def plot_state_to_ax(pollutant, ax):
    gplt.choropleth(
        df.loc[:, [pollutant, 'geometry']],
        hue=pollutant, cmap='Blues',
        linewidth=0.0, ax=ax
    )
    gplt.polyplot(
        df, edgecolor='black', linewidth=0.5, ax=ax
    )
245/28:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, axarr = plt.subplots(1, 2, figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('sox', axarr[0])
axarr[0].set_title('right figure')

plot_state_to_ax('nox', axarr[1])
axarr[1].set_title('New Jersey (n=854,647)')
245/29:
shp_path = '/nas1/Data/GIS/TWN_town/TOWN_MOI_1120317.shp'
twn=gpd.read_file(shp_path)
twn
245/30:
import numpy as np
twn['sox']=(np.array(twn.COUNTYSN,dtype=float))**2
twn['nox']=np.sqrt(np.array(twn.COUNTYSN,dtype=float))
245/31:
import numpy as np
twn['sox']=(np.array(twn.index,dtype=float))**2
twn['nox']=np.sqrt(np.array(twn.index,dtype=float))
245/32:
def plot_state_to_ax(pollutant, ax):
    gplt.choropleth(
        twn.loc[:, [pollutant, 'geometry']],
        hue=pollutant, cmap='Blues',
        linewidth=0.0, ax=ax
    )
    gplt.polyplot(
        twn, edgecolor='black', linewidth=0.5, ax=ax
    )
245/33:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, axarr = plt.subplots(1, 2, figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('sox', axarr[0])
axarr[0].set_title('sox')

plot_state_to_ax('nox', axarr[1])
axarr[1].set_title('nox')
245/34:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, ax = plt.subplots(figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('sox',ax)
245/35:
import shapefile as shp
sf = shp.Reader(shp_path)
id=0
for shape in sf.shapeRecords():
    if id==0:
        print(shape)
        break
    x = [i[0] for i in shape.shape.points[:]]
    y = [i[1] for i in shape.shape.points[:]]
    df=pd.DataFrame({'x':x,'y':y})
    boo=(df.x>119.9) & (df.x<123) & (df.y<25.5)
    df=df.loc[boo].reset_index(drop=True)
    if len(df)==0:continue
    x,y=list(df.x),list(df.y)
    points = list(zip(x,y))
    visited = set()
    xx = []
    yy = []
    for p in points :
        if p in visited :
            plt.plot(xx, yy, 'k')
            xx = []
            yy = []
        else :
            xx.append(p[0])
            yy.append(p[1])
            visited.add(p)
245/36:
import shapefile as shp
sf = shp.Reader(shp_path)
id=0
for shape in sf.shapeRecords():
    if id==0:
        print(type(shape))
        break
    x = [i[0] for i in shape.shape.points[:]]
    y = [i[1] for i in shape.shape.points[:]]
    df=pd.DataFrame({'x':x,'y':y})
    boo=(df.x>119.9) & (df.x<123) & (df.y<25.5)
    df=df.loc[boo].reset_index(drop=True)
    if len(df)==0:continue
    x,y=list(df.x),list(df.y)
    points = list(zip(x,y))
    visited = set()
    xx = []
    yy = []
    for p in points :
        if p in visited :
            plt.plot(xx, yy, 'k')
            xx = []
            yy = []
        else :
            xx.append(p[0])
            yy.append(p[1])
            visited.add(p)
245/37: dir(shape)
245/38: dir(shape.shape)
245/39: print(shape.shape.shapeType)
245/40: print(shape.shape.shapeTypeName)
245/41: twn.geometry[0]
245/42: dir(twn.geometry[0])
245/43: dir(twn.geometry[0].coords)
245/44: dir(twn.geometry[0].coords())
245/45: twn.geometry[0].coords()
245/46: twn.geometry[0]
245/47: twn.geometry[0].coords
245/48: dir(twn.geometry[0])
245/49:
a=twn.geometry[0]
a.coords()
245/50:
a=twn.geometry[0]
x,y=a.coords()
245/51:
a=twn.geometry[0]
xx, yy = a.exterior.coords.xy
245/52:
a=twn.geometry[0]
xx, yy = a.exterior.coords.xy
print(xx,yy)
245/53:
twn['mxX']=[np.max(a.exterior.coords.xy[0]) for a in twn.geometry]
twn['mxY']=[np.max(a.exterior.coords.xy[1]) for a in twn.geometry]
twn['mnX']=[np.min(a.exterior.coords.xy[0]) for a in twn.geometry]
boo=(twn.mnX>119.9) & (twn.mxX<123) & (twn.mxY<25.5)
twn=twn.loc[twn.geometry.map(lambda a:)]
245/54:
twn['mxX']=[np.max(a.exterior.coords.xy[0]) for a in twn.geometry]
twn['mxY']=[np.max(a.exterior.coords.xy[1]) for a in twn.geometry]
twn['mnX']=[np.min(a.exterior.coords.xy[0]) for a in twn.geometry]
boo=(twn.mnX>119.9) & (twn.mxX<123) & (twn.mxY<25.5)
twn=twn.loc[boo].index_reset(drop=True)
245/55: twn1=twn.loc[twn.geometry.shapeTypeName=='POLYGON']
245/56: dir(twn.geometry)
245/57: twn.geometry.geometry
245/58: type(twn.geometry.geometry[4])
245/59:
twn0=twn.loc[twn.geometry.geometry==shapely.geometry.multipolygon.Polygon].reset_index(drop=True)
twn1=twn.loc[twn.geometry.geometry==shapely.geometry.multipolygon.MultiPolygon].reset_index(drop=True)
245/60:
import shapely
twn0=twn.loc[twn.geometry.geometry==shapely.geometry.multipolygon.Polygon].reset_index(drop=True)
twn1=twn.loc[twn.geometry.geometry==shapely.geometry.multipolygon.MultiPolygon].reset_index(drop=True)
245/61:
import shapely
twn0=twn.loc[twn.geometry.geometry==shapely.geometry.polygon.Polygon].reset_index(drop=True)
twn1=twn.loc[twn.geometry.geometry==shapely.geometry.multipolygon.MultiPolygon].reset_index(drop=True)
245/62: twn0
245/63: twn1
245/64: twn.geometry.geometry
245/65:
import shapely
twn0=twn.loc[twn.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)].reset_index(drop=True)
twn1=twn.loc[twn.geometry.geometry==shapely.geometry.multipolygon.MultiPolygon].reset_index(drop=True)
245/66: twn0
245/67:
import shapely
twn0=twn.loc[twn.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)].reset_index(drop=True)
twn1=twn.loc[twn.geometry.geometry.map(lambda p:type(p)==shapely.geometry.multipolygon.MultiPolygon)].reset_index(drop=True)
245/68: len(twn)==len(twn0)+len(twn1)
245/69:
twn0['mxX']=[np.max(a.exterior.coords.xy[0]) for a in twn0.geometry]
twn0['mxY']=[np.max(a.exterior.coords.xy[1]) for a in twn0.geometry]
twn0['mnX']=[np.min(a.exterior.coords.xy[0]) for a in twn0.geometry]
boo=(twn0.mnX>119.9) & (twn0.mxX<123) & (twn0.mxY<25.5)
twn0=twn0.loc[boo].index_reset(drop=True)
245/70:
import shapely
twn0=twn.loc[twn.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)].reset_index(drop=True)
twn1=twn.loc[twn.geometry.geometry.map(lambda p:type(p)==shapely.geometry.multipolygon.MultiPolygon)].reset_index(drop=True)
245/71:
twn0['mxX']=[np.max(a.exterior.coords.xy[0]) for a in twn0.geometry]
twn0['mxY']=[np.max(a.exterior.coords.xy[1]) for a in twn0.geometry]
twn0['mnX']=[np.min(a.exterior.coords.xy[0]) for a in twn0.geometry]
boo=(twn0.mnX>119.9) & (twn0.mxX<123) & (twn0.mxY<25.5)
twn0=twn0.loc[boo]
245/72: len(twn)==len(twn0)+len(twn1)
245/73: dir(twn1.geometry)
245/74: twn1.geometry[0].x
245/75: dir(twn1.geometry[0])
245/76: type(twn1.geometry[0])
245/77: dir(twn1.geometry[0])
245/78:
p1=shapely.geometry.Point(119.9,21.5)
p2=shapely.geometry.Point(122.4,21.5)
p3=shapely.geometry.Point(122.4,25.5)
p4=shapely.geometry.Point(119.9,25.5)
Frame=shapely.geometry.Polygon(p1,p2,p3,p4)
245/79:
p1=shapely.geometry.Point(119.9,21.5)
p2=shapely.geometry.Point(122.4,21.5)
p3=shapely.geometry.Point(122.4,25.5)
p4=shapely.geometry.Point(119.9,25.5)
Frame=shapely.geometry.Polygon([p1,p2,p3,p4])
245/80: Frame
245/81: Frame.within(twn.geometry[0])
245/82: twn1=twn.loc[twn.geometry.map(lambda p:Frame.with(p))
245/83: twn1=twn.loc[twn.geometry.map(lambda p:Frame.within(p))]
245/84: twn1
245/85: twn
245/86: p
245/87: len(twn)
245/88: Frame.within(twn.geometry[200])
245/89: twn.geometry[200].within(Frame)
245/90: twn1=twn.loc[twn.geometry.map(lambda p:p.within(Frame))]
245/91: len(twn),len(twn1)
245/92:
shp_path = '/nas1/Data/GIS/TWN_town/TOWN_MOI_1120317.shp'
twn=gpd.read_file(shp_path)
p1=shapely.geometry.Point(119.9,21.5)
p2=shapely.geometry.Point(122.4,21.5)
p3=shapely.geometry.Point(122.4,25.5)
p4=shapely.geometry.Point(119.9,25.5)
Frame=shapely.geometry.Polygon([p1,p2,p3,p4])
twn=twn.loc[twn.geometry.map(lambda p:p.within(Frame))]
twn
245/93:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, ax = plt.subplots(figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('sox',ax)
245/94:
import numpy as np
twn['sox']=(np.array(twn.index,dtype=float))**2
twn['nox']=np.sqrt(np.array(twn.index,dtype=float))
245/95:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, ax = plt.subplots(figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
plt.subplots_adjust(top=0.95)

plot_state_to_ax('sox',ax)
245/96:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, ax = plt.subplots(figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
#plt.subplots_adjust(top=0.95)

plot_state_to_ax('sox',ax)
245/97:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, ax = plt.subplots(figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
plt.subplots_adjust(top=0.9)

plot_state_to_ax('sox',ax)
245/98:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, ax = plt.subplots(figsize=(12, 13), subplot_kw={'projection': proj})
plt.suptitle('something', fontsize=16)
plt.subplots_adjust(top=0.8)

plot_state_to_ax('sox',ax)
245/99:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, ax = plt.plots(figsize=(12, 13), subplot_kw={'projection': proj})
plt.title('something', fontsize=16)
plt.subplots_adjust(top=0.8)

plot_state_to_ax('sox',ax)
245/100:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f, ax = plt.plot(figsize=(12, 13), subplot_kw={'projection': proj})
plt.title('something', fontsize=16)
plt.subplots_adjust(top=0.8)

plot_state_to_ax('sox',ax)
245/101:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f=plt.figure(figsize=(12, 13))
ax=plt.axes(projection=proj)
plt.title('something', fontsize=16)
plt.subplots_adjust(top=0.8)

plot_state_to_ax('sox',ax)
245/102:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f=plt.figure(figsize=(13, 13))
ax=plt.axes(projection=proj)
plt.title('something', fontsize=16)
plot_state_to_ax('sox',ax)
245/103:
shp_path = '/nas1/Data/GIS/TWN_town/TOWN_MOI_1120317.shp'
twn=gpd.read_file(shp_path)
pnd_file='/nas2/cmaqruns/2022fcst/fusion/Voronoi/boundary_shape.shp'
Frame=gpd.read_file(shp_path).geometry[0]
twn=twn.loc[twn.geometry.map(lambda p:p.within(Frame))]
len(twn)
245/104: Frame
245/105:
shp_path = '/nas1/Data/GIS/TWN_town/TOWN_MOI_1120317.shp'
twn=gpd.read_file(shp_path)
bnd_file='/nas2/cmaqruns/2022fcst/fusion/Voronoi/boundary_shape.shp'
Frame=gpd.read_file(bnd_file).geometry[0]
twn=twn.loc[twn.geometry.map(lambda p:p.within(Frame))]
len(twn)
245/106: Frame
245/107:
import numpy as np
twn['sox']=(np.array(twn.index,dtype=float))**2
twn['nox']=np.sqrt(np.array(twn.index,dtype=float))
245/108:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f=plt.figure(figsize=(13, 13))
ax=plt.axes(projection=proj)
plt.title('something', fontsize=16)
plot_state_to_ax('sox',ax)
245/109:
shp_path = '/nas1/Data/GIS/TWN_town/TOWN_MOI_1120317.shp'
twn=gpd.read_file(shp_path)
#bnd_file='/nas2/cmaqruns/2022fcst/fusion/Voronoi/boundary_shape.shp'
#Frame=gpd.read_file(bnd_file).geometry[0]
p1=shapely.geometry.Point(119.9,21.5)
p2=shapely.geometry.Point(122.4,21.5)
p3=shapely.geometry.Point(122.4,25.5)
p4=shapely.geometry.Point(119.9,25.5)
Frame=shapely.geometry.Polygon([p1,p2,p3,p4])
twn=twn.loc[twn.geometry.map(lambda p:p.within(Frame))]
len(twn)
245/110:
import numpy as np
twn['sox']=(np.array(twn.index,dtype=float))**2
twn['nox']=np.sqrt(np.array(twn.index,dtype=float))
245/111:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f=plt.figure(figsize=(13, 13))
ax=plt.axes(projection=proj)
plt.title('something', fontsize=16)
plot_state_to_ax('sox',ax)
245/112:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
plt.title('something', fontsize=16)
plot_state_to_ax('sox',ax)
245/113:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty.plot(ax=ax)
245/114:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
245/115: cty
245/116:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
fig, ax = plt.subplots(figsize=(12, 10))
cty.plot(ax=ax, markersize=3.5, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
245/117:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
fig, ax = plt.subplots(figsize=(12, 10))
cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/118:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty=cty.loc[twn.geometry.map(lambda p:p.within(Frame))]
fig, ax = plt.subplots(figsize=(12, 10))
cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/119:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty=cty.loc[cty.geometry.map(lambda p:p.within(Frame))]
fig, ax = plt.subplots(figsize=(12, 10))
cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/120:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty0=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)]
cty1=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.multipolygon.MultiPolygon)]
cty1
245/121:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty0=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)]
cty1=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.multipolygon.MultiPolygon)]
dir(cty1.geometry[0])
245/122:
for p in cty1.geometry[0]:
    print(type(p))
245/123:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty0=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)]
cty0=cty0.loc[cty0.geometry.map(lambda p:p.within(Frame))]
cty1=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.multipolygon.MultiPolygon)]
df=gpd.DataFrame({i:[] for i in cty1.columns})
col=[i for i in cty1.columns if i !='geometry']
j=0
for i in range(len(cty1)):
    mp=cty1.geometry[i]
    for p  in mp.geoms:
        if p.within(Frame):
            for c in col:
                df.loc[j,c]=cty1.loc[i,c]
            df.loc[j,geometry]=p
cty=cty1.append(df)
fig, ax = plt.subplots(figsize=(12, 10))
cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/124:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty0=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)]
cty0=cty0.loc[cty0.geometry.map(lambda p:p.within(Frame))]
cty1=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.multipolygon.MultiPolygon)]
df=gpd.GeoDataFrame({i:[] for i in cty1.columns})
col=[i for i in cty1.columns if i !='geometry']
j=0
for i in range(len(cty1)):
    mp=cty1.geometry[i]
    for p  in mp.geoms:
        if p.within(Frame):
            for c in col:
                df.loc[j,c]=cty1.loc[i,c]
            df.loc[j,geometry]=p
cty=cty1.append(df)
fig, ax = plt.subplots(figsize=(12, 10))
cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/125:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty0=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)]
cty0=cty0.loc[cty0.geometry.map(lambda p:p.within(Frame))]
cty1=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.multipolygon.MultiPolygon)]
df=gpd.GeoDataFrame({i:[] for i in cty1.columns})
col=[i for i in cty1.columns if i !='geometry']
j=0
for i in range(len(cty1)):
    mp=cty1.geometry[i]
    for p  in mp.geoms:
        if p.within(Frame):
            for c in col:
                df.loc[j,c]=cty1.loc[i,c]
            df.loc[j,'geometry']=p
cty=cty1.append(df)
fig, ax = plt.subplots(figsize=(12, 10))
cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/126: cty1
245/127:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty0=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)]
cty0=cty0.loc[cty0.geometry.map(lambda p:p.within(Frame))]
cty1=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.multipolygon.MultiPolygon)]
df=gpd.GeoDataFrame({i:[] for i in cty1.columns})
col=[i for i in cty1.columns if i !='geometry']
j=0
for i in cty1.index:
    mp=cty1.geometry[i]
    for p  in mp.geoms:
        if p.within(Frame):
            for c in col:
                df.loc[j,c]=cty1.loc[i,c]
            df.loc[j,'geometry']=p
            j+=1
cty=cty1.append(df)
fig, ax = plt.subplots(figsize=(12, 10))
cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/128:
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty0=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)]
cty0=cty0.loc[cty0.geometry.map(lambda p:p.within(Frame))]
cty1=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.multipolygon.MultiPolygon)]
df=gpd.GeoDataFrame({i:[] for i in cty1.columns})
col=[i for i in cty1.columns if i !='geometry']
j=0
for i in cty1.index:
    mp=cty1.geometry[i]
    for p  in mp.geoms:
        if p.within(Frame):
            for c in col:
                df.loc[j,c]=cty1.loc[i,c]
            df.loc[j,'geometry']=p
            j+=1
cty=pd.concat(cty1,df)
fig, ax = plt.subplots(figsize=(12, 10))
cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/129:
import pandas as pd
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty0=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)]
cty0=cty0.loc[cty0.geometry.map(lambda p:p.within(Frame))]
cty1=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.multipolygon.MultiPolygon)]
df=gpd.GeoDataFrame({i:[] for i in cty1.columns})
col=[i for i in cty1.columns if i !='geometry']
j=0
for i in cty1.index:
    mp=cty1.geometry[i]
    for p  in mp.geoms:
        if p.within(Frame):
            for c in col:
                df.loc[j,c]=cty1.loc[i,c]
            df.loc[j,'geometry']=p
            j+=1
cty=pd.concat(cty1,df)
fig, ax = plt.subplots(figsize=(12, 10))
cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/130:
import pandas as pd
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty0=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)]
cty0=cty0.loc[cty0.geometry.map(lambda p:p.within(Frame))]
cty1=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.multipolygon.MultiPolygon)]
df=gpd.GeoDataFrame({i:[] for i in cty1.columns})
col=[i for i in cty1.columns if i !='geometry']
j=0
for i in cty1.index:
    mp=cty1.geometry[i]
    for p  in mp.geoms:
        if p.within(Frame):
            for c in col:
                df.loc[j,c]=cty1.loc[i,c]
            df.loc[j,'geometry']=p
            j+=1
cty=pd.concat([cty1,df])
fig, ax = plt.subplots(figsize=(12, 10))
cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/131: Frame
245/132:
import pandas as pd
cty_path = '/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp'
cty=gpd.read_file(cty_path)
cty0=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.polygon.Polygon)]
cty0=cty0.loc[cty0.geometry.map(lambda p:p.within(Frame))]
cty1=cty.loc[cty.geometry.geometry.map(lambda p:type(p)==shapely.geometry.multipolygon.MultiPolygon)]
df=gpd.GeoDataFrame({i:[] for i in cty1.columns})
col=[i for i in cty1.columns if i !='geometry']
j=0
for i in cty1.index:
    mp=cty1.geometry[i]
    for p  in mp.geoms:
        if p.within(Frame):
            for c in col:
                df.loc[j,c]=cty1.loc[i,c]
            df.loc[j,'geometry']=p
            j+=1
cty=pd.concat([cty0,df])
fig, ax = plt.subplots(figsize=(12, 10))
cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/133:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
plt.title('something', fontsize=16)
plot_state_to_ax('sox',ax)
cty.plot(ax=ax, markersize=3.5, color="none")
245/134:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
plt.title('something', fontsize=16)
plot_state_to_ax('sox',ax)
cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/135:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
cty.plot(ax=ax, markersize=3.5, color="none")
plt.title('something', fontsize=16)
plot_state_to_ax('sox',ax)

ax.axis("off")
plt.axis("equal")
plt.show()
245/136:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
plt.title('something', fontsize=16)
plot_state_to_ax('sox',ax)
#cty.plot(ax=ax, markersize=3.5, color="none")
ax.axis("off")
plt.axis("equal")
plt.show()
245/137:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
plt.title('something', fontsize=16)
plot_state_to_ax('sox',ax)
#cty.plot(ax=ax, markersize=3.5, color="none")
plt.show()
245/138:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
plt.title('something', fontsize=16)
plot_state_to_ax('sox',ax)
cty.plot(ax=ax, markersize=3.5, color="none")
plt.show()
246/1:
import geopandas as gpd
import pandas as pd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt
import numpy as np
import sys, os
246/2: shp_path = '/nas1/Data/GIS/TWN_town/TOWN_MOI_1120317.shp'
246/3: !lst
246/4: !vi geoplot.ipyn
246/5: !vi geoplot.ipynb
246/6: !cat geoplot.ipynb
246/7: ls ~/bin
246/8: ls ~/bin/*2md.py
246/9: !vi geoplot.ipynb
246/10: !vi geoplot.py
246/11:
shp_path = '/nas1/Data/GIS/TWN_town/TOWN_MOI_1120317.shp'
twn=gpd.read_file(shp_path)
246/12:
x,y=[119.9,122.4,122.4,119.9,],[21.5,21.5,25.5,25.5,]
Frame=shapely.geometry.Polygon([shapely.geometry.Point(i,j) for i,j in zip(x,y)])
246/13: import shapely
246/14:
x,y=[119.9,122.4,122.4,119.9,],[21.5,21.5,25.5,25.5,]
Frame=shapely.geometry.Polygon([shapely.geometry.Point(i,j) for i,j in zip(x,y)])
246/15: twn=twn.loc[twn.geometry.map(lambda p:p.within(Frame))]
246/16: df=twn
246/17: col=[i for i in df.columns]
246/18: col
246/19: df.head()
246/20: type(df.TOWNCODE[0])
246/21: sTOWNCODE=set(twn.TOWNCODE)
246/22: len(twn)
246/23: len(sTOWNCODE)
246/24: '10014020' in twn.TOWNCODE
246/25: '10014020' in sTOWNCODE
246/26: fname='/nas2/cmaqruns/2019TZPP/output/Annual/LGHAP.PM25.D001/LGHAP2000.csv'
246/27: df=pd.read_csv(fname)
246/28: df.head()
246/29: type(df.TOWNCODE[0])
246/30: type(df.TOWNCODE[1])
246/31: '9020060' in sTOWNCODE
246/32: '09020060' in sTOWNCODE
246/33: [i for i in sTOWNCODE if '9020060' in i]
246/34:
twn=gpd.read_file(shp_path)
sTOWNCODE=set(twn.TOWNCODE)
246/35: '09020060' in sTOWNCODE
246/36: type(df.TOWNCODE[0]) != str
246/37:
if type(df.TOWNCODE[0]) != str:
  df.TOWNCODE=[str(i) for i in df.TOWNCODE]
  df.TOWNCODE=[(8-len(i))*'0'+str(i) for i in df.TOWNCODE]
246/38: df.head()
246/39: s1=sTOWNCODE-set(df.TOWNCODE)
246/40: len(s1)
246/41: s1
246/42: s1=sTOWNCODE && set(df.TOWNCODE)
246/43: s1=sTOWNCODE & set(df.TOWNCODE)
246/44: s1
246/45: len(s1)
246/46: len(sTOWNCODE)
246/47: len(df)
246/48: len(set(df.TOWNCODE))
246/49:
lastTwn=sTOWNCODE & set(df.TOWNCODE)
df=df.loc[df.TOWNCODE.map(lambda s:s in lastTwn)].reset_index(drop=True)
246/50:
if len(df)>len(lastTwn):
  df=df[:len(lastTwn)]
246/51: df.last()
246/52: df.head()
246/53: df.tail()
246/54: df=pd.read_csv(fname)
246/55:
if type(df.TOWNCODE[0]) != str:
  df.TOWNCODE=[str(i) for i in df.TOWNCODE]
  df.TOWNCODE=[(8-len(i))*'0'+str(i) for i in df.TOWNCODE]
246/56: lastTwn=sTOWNCODE & set(df.TOWNCODE)
246/57: df1=df.loc[df.TOWNCODE.map(lambda s:s in lastTwn)].reset_index(drop=True)
246/58: df=df.loc[df.TOWNCODE.map(lambda s:s in lastTwn)].reset_index(drop=True)
246/59: df1=df[:len(lastTwn)]
246/60: df.tail()
246/61: df1.tail()
246/62: df[357:366]
246/63:
if len(df)>len(lastTwn):
  df=df[:len(lastTwn)]
246/64: df.TOWNCODE==twn.TOWNCODE
246/65: list(df.TOWNCODE)==(twn.TOWNCODE)
246/66: list(df.TOWNCODE)==list(twn.TOWNCODE)
246/67: df=df.sort_values('TOWNCODE')
246/68: twn=twn.sort_values('TOWNCODE')
246/69: list(df.TOWNCODE)==list(twn.TOWNCODE)
246/70: set(df.TOWNCODE) - set(twn.TOWNCODE)
246/71: set(twn.TOWNCODE) - set(df.TOWNCODE)
246/72: ls *csv
246/73: ll=pd.read_csv('gridLLvor.csv')
246/74: ll.head()
246/75: 9007010 in set(ll.TOWNCODE)
246/76:
twn_geom={i:j for i,j in zip(list(twn.TOWNCODE),list(twn.geometry))}
df['geometry']=[twn_geom[i] for i in df.TOWNCODE]
246/77:
proj = gcrs.AlbersEqualArea(central_latitude=24.5, central_longitude=120)
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
plt.title(itm_nam in fname, fontsize=16)
246/78: df.head()
246/79:
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
plt.title(itm_nam+' in '+fname, fontsize=16)
gplt.choropleth(
        df.loc[:, [itm_nam, 'geometry']],
        hue=pollutant, cmap='Blues',
        linewidth=0.0, ax=ax
    )
    gplt.polyplot(
        df, edgecolor='black', linewidth=0.5, ax=ax
    )
plt.show()
246/80:
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
plt.title(itm_nam+' in '+fname, fontsize=16)
gplt.choropleth(
        df.loc[:, [itm_nam, 'geometry']],
        hue=pollutant, cmap='Blues',
        linewidth=0.0, ax=ax
    )
gplt.polyplot(
        df, edgecolor='black', linewidth=0.5, ax=ax
    )
plt.show()
246/81: itm_nam='PM25'
246/82:
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
plt.title(itm_nam+' in '+fname, fontsize=16)
gplt.choropleth(
        df.loc[:, [itm_nam, 'geometry']],
        hue=pollutant, cmap='Blues',
        linewidth=0.0, ax=ax
    )
gplt.polyplot(
        df, edgecolor='black', linewidth=0.5, ax=ax
    )
plt.show()
246/83:
gplt.choropleth(
        df.loc[:, [itm_nam, 'geometry']],
        hue=itm_nam, cmap='Blues',
        linewidth=0.0, ax=ax
    )
gplt.polyplot(
        df, edgecolor='black', linewidth=0.5, ax=ax
    )
plt.show()
246/84: len(twn)
246/85: twn=twn.loc[twn.TOWNCODE.map(lambda s:s in lastTwn)]
246/86: len(twn)
246/87: df_itm={i:j for i,j in zip(list(df.TOWNCODE),list(df[itm_nam]))}
246/88: twn[itm_nam]=[df_geom[i] for i in twn.TOWNCODE]
246/89: twn[itm_nam]=[df_itm[i] for i in twn.TOWNCODE]
246/90:
gplt.choropleth(
        twn.loc[:, [itm_nam, 'geometry']],
        hue=itm_nam, cmap='Blues',
        linewidth=0.0, ax=ax
    )
gplt.polyplot(
        twn, edgecolor='black', linewidth=0.5, ax=ax
    )
plt.show()
246/91:
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
plt.title(itm_nam+' in '+fname, fontsize=16)
gplt.choropleth(
        twn.loc[:, [itm_nam, 'geometry']],
        hue=itm_nam, cmap='Blues',
        linewidth=0.0, ax=ax
    )
gplt.polyplot(
        twn, edgecolor='black', linewidth=0.5, ax=ax
    )
plt.show()
246/92:
f=plt.figure(figsize=(15, 13))
ax=plt.axes(projection=proj)
if '/' in fname:
  fname=fname.split('/')[-1]
plt.title(itm_nam+' in '+fname, fontsize=16)
gplt.choropleth(
        twn.loc[:, [itm_nam, 'geometry']],
        hue=itm_nam, cmap='Blues',
        linewidth=0.0, ax=ax
    )
gplt.polyplot(
        twn, edgecolor='black', linewidth=0.5, ax=ax
    )
plt.show()
246/93: !lst
246/94: fname
246/95: fname='/nas2/cmaqruns/2019TZPP/output/Annual/LGHAP.PM25.D001/LGHAP2000.csv'
246/96: run csv_choro.py PM25 /nas2/cmaqruns/2019TZPP/output/Annual/LGHAP.PM25.D001/LGHAP2000.csv
246/97: run csv_choro.py PM25 /nas2/cmaqruns/2019TZPP/output/Annual/LGHAP.PM25.D001/LGHAP2000.csv
246/98: run csv_choro.py PM25 /nas2/cmaqruns/2019TZPP/output/Annual/LGHAP.PM25.D001/LGHAP2000.csv
246/99: run csv_choro.py PM25 /nas2/cmaqruns/2019TZPP/output/Annual/LGHAP.PM25.D001/LGHAP2000.csv
246/100: run csv_choro.py PM25 /nas2/cmaqruns/2019TZPP/output/Annual/LGHAP.PM25.D001/LGHAP2000.csv
246/101: !head /nas2/cmaqruns/2019TZPP/output/Annual/LGHAP.PM25.D001/LGHAP2000.csv
246/102: !python -V
246/103:
df_itm={i:0 for i in twn.TOWNCODE}
df_itm.update({i:j for i,j in zip(list(df.TOWNCODE),list(df[itm_nam]))})
246/104: twn[itm_nam]=[df_itm[i] for i in twn.TOWNCODE]
246/105: df.head()
246/106: df['TOWNCODE2']=[i:[-3:] for i in df.TOWNCODE ]
246/107: df['TOWNCODE2']=[i[-3:] for i in df.TOWNCODE ]
246/108: df.head()
246/109: col=['COUNTYCODE','TOWNCODE2','PM25']
246/110: df[col].set_index('COUNTYCODE').to_csv('df.csv',header=None)
246/111: !head df.csv
246/112: fname='/nas2/cmaqruns/2019TZPP/output/Annual/LGHAP.PM25.D001/LGHAP2000.csv'
246/113: fname='/home/kuang/AQMP/202106/town_aqstEnew.csv'
246/114: df=pd.read_csv(fname)
246/115: fname='/nas1/CAM-chem/Annuals/town_aqstEnew.csv'
246/116: df=pd.read_csv(fname)
246/117: df.head()
246/118: df['stns']=[pd.Series([int(i) for i in j.split(';')]) for j in df.aq_st]
246/119: df.loc[df.aq_st==';']
246/120: df.head()
246/121: df.tail()
246/122: j
246/123: j=df.aq_st[0]
246/124: j.split(';')
246/125: df['stns']=[pd.Series([int(i) for i in j[:-1].split(';')]) for j in df.aq_st]
246/126: df.head()
246/127: df.stns[0]
246/128: df.stns[1]
246/129: df.stns[-1]
246/130: df.stns[100]
246/131: df.loc[df.aq_st=='0;']
246/132: len(df.loc[df.aq_st=='0;'])
246/133: df=df.loc[df.aq_st=='0;'].reset_index(drop=True)
246/134: df['stns']=[pd.Series([int(i) for i in j[:-1].split(';')]) for j in df.aq_st]
246/135: len(df)
246/136: df=pd.read_csv(fname)
246/137: df=df.loc[df.aq_st!='0;'].reset_index(drop=True)
246/138: len(df)
246/139: df.head()
246/140: df.tail()
246/141: df['stns']=[pd.Series([int(i) for i in j[:-1].split(';')]) for j in df.aq_st]
246/142: fname='/nas1/CAM-chem/Annuals/town_aqstEnew2.csv'
246/143: df.set_index('code').to_csv(fname)
248/1:
import numpy as np
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
248/2: !which python
248/3: !pip install geovoronoi
248/4:
import numpy as np
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
249/1:
import numpy as np
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
from shapely.ops import cascaded_union, unary_union
from shapely.geometry import Point
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area
from geovoronoi import voronoi_regions_from_coords, points_to_coords
249/2:
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
gdf.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
249/3:
fig, ax = plt.subplots(figsize=(12, 10))
gdf.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
249/4:
shp_fname="/nas1/Data/GIS/TWEPA_AQSTA/10704.shp"
gdf = gpd.read_file(shp_fname)
fig, ax = plt.subplots(figsize=(12, 10))
gdf.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
249/5:
shp_fname="/nas1/Data/GIS/TWEPA_AQSTA/10704.shp"
gdf = gpd.read_file(shp_fname)
boundary = gpd.read_file("/nas2/cmaqruns/2022fcst/fusion/Voronoi/mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
gdf.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
249/6:
shp_fname="/nas1/Data/GIS/TWEPA_AQSTA/10704.shp"
gdf = gpd.read_file(shp_fname)
#boundary = gpd.read_file("/nas2/cmaqruns/2022fcst/fusion/Voronoi/mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
gdf.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
249/7:
shp_fname="/nas1/Data/GIS/TWEPA_AQSTA/10704.shp"
gdf = gpd.read_file(shp_fname)
#boundary = gpd.read_file("/nas2/cmaqruns/2022fcst/fusion/Voronoi/mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
#boundary.plot(ax=ax, color="gray")
gdf.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
249/8: gdf.crs
249/9: boundry.crs
249/10: boundary.crs
249/11: gdf
249/12:
gdf2=gdf
gdf2.geometry=[Point(i,j) for i,j in zip(list(gdf2.TWD97Lon),list(gdf2.TWD97Lat))]
249/13: gdf2.crs
249/14: gdf2.crs=boundary.crs
249/15:
shp_fname="/nas1/Data/GIS/TWEPA_AQSTA/10704.shp"
gdf = gpd.read_file(shp_fname)
#boundary = gpd.read_file("/nas2/cmaqruns/2022fcst/fusion/Voronoi/mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
#boundary.plot(ax=ax, color="gray")
gdf2.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
249/16:
shp_fname="/nas1/Data/GIS/TWEPA_AQSTA/10704.shp"
gdf = gpd.read_file(shp_fname)
#boundary = gpd.read_file("/nas2/cmaqruns/2022fcst/fusion/Voronoi/mainisland.shp")
fig, ax = plt.subplots(figsize=(12, 10))
boundary.plot(ax=ax, color="gray")
gdf2.plot(ax=ax, markersize=3.5)#, color="brown")
ax.axis("off")
plt.axis("equal")
plt.show()
249/17: gdf2.crs
249/18: run voronoi_epaShp.py
249/19: run voronoi_epaShp.py
249/20: run voronoi_epaShp.py
249/21: stn
249/22: stn2=gpd.read_file('stn.shp')
249/23: len(stn2)
250/1: from bs4 import BeautifulSoup
250/2:
from bs4 import BeautifulSoup
fn=open('segments.html','r')
soup = BeautifulSoup(fn,'html.parser')
250/3: soup.find_all('value')
250/4: soup.find_all('freeway_from')
250/5: print(soup)
250/6: print(soup.find_all('select'))
250/7: print(soup.find_all('option'))
250/8: dir(soup.find_all('option'))
250/9: print(soup.find_all('option'))
250/10:
opt=soup.find_all('option')
values=[i.split('value="')[1].split('"')[0] for i in opt]
250/11: opt=soup.find_all('option')
250/12: type(opt)
250/13: opt=list(soup.find_all('option'))
250/14: type(opt)
250/15: values=[i.split('value="')[1].split('"')[0] for i in opt]
250/16: opt[:5]
250/17: values=[i.split('=')[1].split('>')[0] for i in opt]
250/18: values=[i.split('\=')[1].split('\>')[0] for i in opt]
250/19:
i=opt[0]
print(i)
250/20:
i=opt[0]
i.split('\=')[1]
250/21:
i=opt[0]
i.split('=')[1]
250/22:
i=opt[0]
type(i)
250/23:
i=opt[0]
dir(i)
250/24:
i=opt[0]
i.name
250/25:
i=opt[0]
dir(i)
250/26: i.namespace
250/27: i.namespace
250/28: i.prefix
250/29: i
250/30: i.get_text
250/31: i.get_text()
250/32: i.get_attr()
250/33: i.get_attr
250/34: i.get_attribute_list
250/35: i.find_all
250/36: i.find_all()
250/37: i.find_all('value')
250/38: opt[1].find_all('value')
250/39: opt[1].find_all
250/40: opt[1].contents
250/41: opt[1].sttrs
250/42: opt[1].value
250/43: opt[1].values
250/44: opt[1].values()
250/45: opt[1].text
250/46: texts=[i.text for i in opt]
250/47: opt[1].string
250/48: opt[1].strings
250/49: opt[1].strings()
250/50: opt[1].prefix
250/51: opt[1].prefix()
250/52: opt[1].prettify
250/53: opt[1].tag
250/54: opt[1].find('value')
250/55: (str(opt[1]))
250/56: values=[str(i).split('"')[1] for i in opt]
250/57:
opt=list(soup.find_all('option'))
values=[str(i).split('"')[1] for i in opt]
texts=[i.text for i in opt]
v2t={i:j for i,j in zip(values,texts)}
250/58: v2t
250/59:
import json
with open("segments.json",'w') as f
   v2t.dump(f)
250/60:
import json
with open("segments.json",'w') as f:
   v2t.dump(f)
250/61:
import json
with open("segments.json",'w') as f:
   json.dump(v2t,f)
250/62: v2t={i:j.encode('utf8') for i,j in zip(values,texts)}
250/63:
import json
with open("segments.json",'w') as f:
   json.dump(v2t,f)
250/64: v2t
250/65: v2t={i:j for i,j in zip(values,texts)}
250/66: v2t
250/67:
import json
with open("segments.json",'w',coding='utf8') as f:
   json.dump(v2t,f)
250/68:
import json
with open("segments.json",'w',encoding='utf8') as f:
   json.dump(v2t,f)
250/69:
import json
with open("segments.json",'w',encoding='big5') as f:
   json.dump(v2t,f)
250/70:
#! coding="utf8"
import json
with open("segments.json",'w',encoding='big5') as f:
   json.dump(v2t,f)
250/71: v2t={i:j.decoding(''utf8') for i,j in zip(values,texts)}
250/72: v2t={i:j.decoding('utf8') for i,j in zip(values,texts)}
250/73: v2t={i:j.encoding('utf8').decoding('big5') for i,j in zip(values,texts)}
250/74: v2t={i:j.encode('utf8').decoding('big5') for i,j in zip(values,texts)}
250/75: v2t={i:j.decode('utf8').decoding('big5') for i,j in zip(values,texts)}
250/76: v2t={i:j for i,j in zip(values,texts)}
250/77:
#! coding="utf8"
import json
with open("segments.json",'w',encoding='big5') as f:
   json.dump(v2t,f)
250/78:
with open("segments.json",'r',encoding='big5') as f:
   a=json.read(f)
a
250/79:
with open("segments.json",'r',encoding='big5') as f:
   a=json.load(f)
a
250/80:
with open("segments.json",'r') as f:
   a=json.load(f)
a
250/81:
with open("segments.json",'r',encoding='big5') as f:
   a=json.load(f)
250/82:
fn=open('results.html','r')
soup = BeautifulSoup(fn,'html.parser')
250/83:
span=list(soup.find_all('span'))
span[0]
250/84:
fn=open('results.html','r')
soup = BeautifulSoup(fn,'html.parser')
250/85:
span=list(soup.find_all('span'))
span[0]
250/86: len(span)
250/87: span
250/88: span[-1].split('約')[1].split('分')[0]
250/89: str(span[-1]).split('約')[1].split('分')[0]
250/90: lags=[str(span[i]).split('約')[1].split('分')[0] for i in range(-7,0)]
250/91:
lags=[str(span[i]).split('約')[1].split('分')[0] for i in range(-7,0)]
lags
250/92: from selenium import webdriver
250/93: %pip install selenium
250/94: from selenium import webdriver
250/95:
from selenium import webdriver
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
250/96:
from selenium import webdriver
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
250/97:
from selenium.webdriver.support.ui import Select

def clkid(ID):                                  #click the id
    button_element = driver.find_element_by_id(ID)
    button_element.click()
    return
def clkpath(ID):                                        #click the xpath (no use)
    button_element = driver.find_element_by_xpath(ID)
    button_element.click()
    return
def SelectByIDnValue(ID,v):                     #click and select by value
    select = Select(driver.find_element_by_id(ID))
    select.select_by_value(v)
    return
250/98:
clkid("freeway_from")
SelectByIDnValue("freeway_from","1_0")
clkid("section_from")
SelectByIDnValue("section_from","0038")
250/99: dir(driver)
250/100: driver.find_element("section_from")
250/101: a=driver.find_elements()
250/102: a
250/103: a=driver.find_elements("section_from")
250/104: a=driver.find_elements("id")
250/105: a
250/106: a=driver.find_elements("select")
250/107: a=driver.find_elements("section_from")
250/108: a=driver.find_elements("tag name")
250/109: a=driver.find_elements("value")
250/110:
clkid("tripFuturePickTime")
SelectByIDnValue("freeway_from","00:00")
clkid("freeway_from")
SelectByIDnValue("freeway_from","1_0")
clkid("section_from")
SelectByIDnValue("section_from","0038")
clkid("freeway_from")
SelectByIDnValue("freeway_from","1_0")
clkid("section_from")
SelectByIDnValue("section_from","0038")
250/111: dir(Select)
250/112:
from selenium.webdriver.support.ui import Select

def clkid(ID):                                  #click the id
    button_element = driver.find_element("select",ID)
    button_element.click()
    return
def clkpath(ID):                                        #click the xpath (no use)
    button_element = driver.find_element_by_xpath(ID)
    button_element.click()
    return
def SelectByIDnValue(ID,v):                     #click and select by value
    select = Select(driver.find_element("option",v))
    select.select_by_value(v)
    return
250/113:
clkid("tripFuturePickTime")
SelectByIDnValue("freeway_from","00:00")
clkid("freeway_from")
SelectByIDnValue("freeway_from","1_0")
clkid("section_from")
SelectByIDnValue("section_from","0038")
clkid("freeway_from")
SelectByIDnValue("freeway_from","1_0")
clkid("section_from")
SelectByIDnValue("section_from","0038")
250/114: driver.find_element("select", "tripFuturePickTime")
250/115: driver.find_element("name", "tripFuturePickTime")
250/116: driver.find_element("id", "tripFuturePickTime")
250/117:
from selenium.webdriver.support.ui import Select

def clkid(ID):                                  #click the id
    button_element = driver.find_element("id",ID)
    button_element.click()
    return
def clkpath(ID):                                        #click the xpath (no use)
    button_element = driver.find_element_by_xpath(ID)
    button_element.click()
    return
def SelectByIDnValue(ID,v):                     #click and select by value
    select = Select(driver.find_element("value",v))
    select.select_by_value(v)
    return
250/118:
clkid("tripFuturePickTime")
SelectByIDnValue("freeway_from","00:00")
clkid("freeway_from")
SelectByIDnValue("freeway_from","1_0")
clkid("section_from")
SelectByIDnValue("section_from","0038")
clkid("freeway_from")
SelectByIDnValue("freeway_from","1_0")
clkid("section_from")
SelectByIDnValue("section_from","0038")
250/119: driver.find_element("value", "00:00")
250/120: driver.find_element("id", "tripFuturePickTime")
250/121: driver.find_element("value","00:00")
250/122: driver.find_element("class","myselect sel_plan_gotime")
250/123: driver.find_element("link text","myselect sel_plan_gotime")
250/124: driver.find_element("id","myselect sel_plan_gotime")
250/125: driver.find_element("name","myselect sel_plan_gotime")
250/126: driver.find_element("div","myselect sel_plan_gotime")
250/127: driver.find_element(By.ID,"myselect sel_plan_gotime")
250/128:
from selenium.webdriver.support.ui import Select
from selenium.webdriver.common.by import By

def clkid(ID):                                  #click the id
    button_element = driver.find_element("id",ID)
    button_element.click()
    return
def clkpath(ID):                                        #click the xpath (no use)
    button_element = driver.find_element_by_xpath(ID)
    button_element.click()
    return
def SelectByIDnValue(ID,v):                     #click and select by value
    select = Select(driver.find_element(By.ID,ID))
    select.select_by_value(v)
    return
250/129:
clkid("tripFuturePickTime")
SelectByIDnValue("freeway_from","00:00")
clkid("freeway_from")
SelectByIDnValue("freeway_from","1_0")
clkid("section_from")
SelectByIDnValue("section_from","0038")
clkid("freeway_from")
SelectByIDnValue("freeway_from","1_0")
clkid("section_from")
SelectByIDnValue("section_from","0038")
250/130:
clkid("tripFuturePickTime")
SelectByIDnValue("tripFuturePickTime","00:00")
clkid("freeway_from")
SelectByIDnValue("freeway_from","1_0")
clkid("section_from")
SelectByIDnValue("section_from","0038")
clkid("freeway_from")
SelectByIDnValue("freeway_from","1_0")
clkid("section_from")
SelectByIDnValue("section_from","0038")
250/131: select
250/132:
ID=tripFuturePickTime
select = Select(driver.find_element(By.ID,ID))
250/133:
ID=tripFuturePickTime
select = Select(driver.find_element(By.ID,ID))
select
250/134:
ID="tripFuturePickTime"
select = Select(driver.find_element(By.ID,ID))
select
250/135: dir(select)
250/136: select.options
250/137: select.options()
250/138: dir(select)
250/139: dir(select.select_by_value)
250/140:
ID="00:00"
button_element = driver.find_element(By.value,ID)
button_element.click()
250/141: dir(By)
250/142:
ID="00:00"
button_element = driver.find_element(By.CSS_SELECTOR,ID)
button_element.click()
250/143:
ID="00:00"
button_element = driver.find_element(By.NAME,ID)
button_element.click()
250/144: select.select_by_visible_text('00:00')
250/145:
ID="tripFuturePickTime"
select = Select(driver.find_element(By.ID,ID))
select.select_by_visible_text('00:00')
250/146:
ID="tripFuturePickTime"
driver.find_element(By.XPATH,"//select[@name='"+ID+"']/option[text()='00:00']").click()
250/147:
ID="tripFuturePickTime"
driver.find_element(By.XPATH,"//select[@id='"+ID+"']/option[value()='00:00']").click()
250/148:
ID="tripFuturePickTime"
select = Select(driver.find_element(By.ID,ID))
select.select_by_index(0)
250/149:
ID="tripFuturePickTime"
select = Select(driver.find_element(By.ID,ID))
select.select_by_index(10)
250/150:
ID="tripFuturePickTime"
select = Select(driver.find_element(By.ID,ID))
select.select_by_index(10).click()
250/151:
clkid("tripFuturePickTime")
SelectByIDnValue("tripFuturePickTime",10)
clkid("freeway_from")
SelectByIDnValue("freeway_from",0)
clkid("section_from")
SelectByIDnValue("section_from",3)
clkid("freeway_from")
SelectByIDnValue("freeway_end",0)
clkid("section_from")
SelectByIDnValue("section_end",20)
clkid("tp_node_btn button_primary btn_func_reverse")
250/152:
from selenium.webdriver.support.ui import Select
from selenium.webdriver.common.by import By

def clkid(ID):                                  #click the id
    button_element = driver.find_element("id",ID)
    button_element.click()
    return
def clkpath(ID):                                        #click the xpath (no use)
    button_element = driver.find_element_by_xpath(ID)
    button_element.click()
    return
def SelectByIDnValue(ID,v):                     #click and select by value
    select = Select(driver.find_element(By.ID,ID))
    select.select_by_index(v)
    return
250/153:
clkid("tripFuturePickTime")
SelectByIDnValue("tripFuturePickTime",10)
clkid("freeway_from")
SelectByIDnValue("freeway_from",0)
clkid("section_from")
SelectByIDnValue("section_from",3)
clkid("freeway_from")
SelectByIDnValue("freeway_end",0)
clkid("section_from")
SelectByIDnValue("section_end",20)
clkid("tp_node_btn button_primary btn_func_reverse")
250/154:
clkid("tripFuturePickTime")
SelectByIDnValue("tripFuturePickTime",10)
clkid("freeway_from")
SelectByIDnValue("freeway_from",1)
clkid("section_from")
SelectByIDnValue("section_from",3)
clkid("freeway_from")
SelectByIDnValue("freeway_end",0)
clkid("section_from")
SelectByIDnValue("section_end",20)
clkid("tp_node_btn button_primary btn_func_reverse")
250/155:
clkid("tripFuturePickTime")
SelectByIDnValue("tripFuturePickTime",10)
clkid("freeway_from")
SelectByIDnValue("freeway_from",1)
clkid("section_from")
SelectByIDnValue("section_from",3)
clkid("freeway_from")
SelectByIDnValue("freeway_end",1)
clkid("section_from")
SelectByIDnValue("section_end",20)
clkid("tp_node_btn button_primary btn_func_reverse")
250/156:
clkid("tripFuturePickTime")
SelectByIDnValue("tripFuturePickTime",10)
clkid("freeway_from")
SelectByIDnValue("freeway_from",1)
clkid("section_from")
SelectByIDnValue("section_from",3)
clkid("freeway_from")
SelectByIDnValue("freeway_end",1)
clkid("section_from")
SelectByIDnValue("section_end",20)
clkpath("tp_node_btn button_primary btn_func_reverse")
250/157:
from selenium.webdriver.support.ui import Select
from selenium.webdriver.common.by import By

def clkid(ID):                                  #click the id
    button_element = driver.find_element("id",ID)
    button_element.click()
    return
def clkpath(ID):                                        #click the xpath (no use)
    button_element = driver.find_element(By.XPATH,ID)
    button_element.click()
    return
def SelectByIDnValue(ID,v):                     #click and select by value
    select = Select(driver.find_element(By.ID,ID))
    select.select_by_index(v)
    return
250/158:
clkid("tripFuturePickTime")
SelectByIDnValue("tripFuturePickTime",10)
clkid("freeway_from")
SelectByIDnValue("freeway_from",1)
clkid("section_from")
SelectByIDnValue("section_from",3)
clkid("freeway_from")
SelectByIDnValue("freeway_end",1)
clkid("section_from")
SelectByIDnValue("section_end",20)
clkpath(//button[@class="tp_node_btn button_primary btn_func_reverse"])
250/159:
clkid("tripFuturePickTime")
SelectByIDnValue("tripFuturePickTime",10)
clkid("freeway_from")
SelectByIDnValue("freeway_from",1)
clkid("section_from")
SelectByIDnValue("section_from",3)
clkid("freeway_from")
SelectByIDnValue("freeway_end",1)
clkid("section_from")
SelectByIDnValue("section_end",20)
clkpath('//button[@class="tp_node_btn button_primary btn_func_reverse"]')
250/160:
from selenium import webdriver
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
250/161:
clkid("tripFuturePickTime")
SelectByIDnValue("tripFuturePickTime",10)
clkid("freeway_from")
SelectByIDnValue("freeway_from",1)
clkid("section_from")
SelectByIDnValue("section_from",3)
clkid("freeway_from")
SelectByIDnValue("freeway_end",1)
clkid("section_from")
SelectByIDnValue("section_end",20)
clkpath('//button[@class="tp_node_btn button_primary btn_func_reverse"]')
250/162:
clkid("tripFuturePickTime")
SelectByIDnValue("tripFuturePickTime",10)
clkid("freeway_from")
SelectByIDnValue("freeway_from",1)
clkid("section_from")
SelectByIDnValue("section_from",3)
clkid("freeway_from")
SelectByIDnValue("freeway_end",1)
clkid("section_from")
SelectByIDnValue("section_end",20)
clkpath('//button[@class="button_primary tripplan_btn"]')
250/163: clkpath('//button[@class="button_primary tripplan_btn"]')
250/164: driver.close()
250/165: import codecs
250/166:
from selenium import webdriver
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
250/167:
clkid("tripFuturePickTime")
SelectByIDnValue("tripFuturePickTime",10)
clkid("freeway_from")
SelectByIDnValue("freeway_from",1)
clkid("section_from")
SelectByIDnValue("section_from",3)
clkid("freeway_from")
SelectByIDnValue("freeway_end",1)
clkid("section_from")
SelectByIDnValue("section_end",20)
clkpath('//button[@class="button_primary tripplan_btn"]')
n = os.path.join(".", "results.html")
f = codecs.open(n, "w", "utf−8")
h = driver.page_source
f.write(h)
250/168:
fn=open('results.html','r')
soup = BeautifulSoup(fn,'html.parser')
span=list(soup.find_all('span'))
lags=[str(span[i]).split('約')[1].split('分')[0] for i in range(-7,0)]
print(lags)
250/169: import os
250/170:
n = os.path.join("/nas2/kuang/tp_future", "results.html")
f = codecs.open(n, "w", "utf−8")
h = driver.page_source
f.write(h)
250/171:
fn=open('results.html','r')
soup = BeautifulSoup(fn,'html.parser')
span=list(soup.find_all('span'))
lags=[str(span[i]).split('約')[1].split('分')[0] for i in range(-7,0)]
print(lags)
250/172: span
250/173:
fn=open('results.html','r')
soup = BeautifulSoup(fn,'html.parser')
span=list(soup.find_all('span'))
lags=[str(i).split('約')[1].split('分')[0] for i in span]
print(lags)
250/174:
fn=open('results.html','r')
soup = BeautifulSoup(fn,'html.parser')
span=list(soup.find_all('span'))
lags=[str(i).split('約')[1].split('分')[0] for i in span if '約' in i]
print(lags)
250/175:
fn=open('results.html','r')
soup = BeautifulSoup(fn,'html.parser')
span=[str(i) for i in list(soup.find_all('span'))]
lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
print(lags)
250/176:
for d in range(1):
  for t in range(1):
    for w in range(1):
      for s in range(1)
fn=open('results.html','r')
soup = BeautifulSoup(fn,'html.parser')
span=[str(i) for i in list(soup.find_all('span'))]
lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
print(lags)
250/177: len(v2t)
250/178: v2t
250/179: lag
250/180: lags
250/181:
from pandas import DataFrame
SegOfWay={1:5}
col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
i=0
250/182: df.loc[0,'date']='20230607'
250/183: df.iloc[0,:3]=[1,2,3]
250/184: df
250/185:
SegOfWay={1:5}
col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
i=0
for d in range(1):
  for t in range(1):
    for w in range(1):
      for s1 in range(SegOfWay[w]):
        for s2 in range(SegOfWay[w]):
          if s1==s2:continue
            fn=open('results.html','r')
            soup = BeautifulSoup(fn,'html.parser')
            span=[str(i) for i in list(soup.find_all('span'))]
            lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
            df.iloc[i,:]=[d,t,w,s1,s2]+lags
            i+=1
250/186:
SegOfWay={1:5}
col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
i=0
for d in range(1):
  for t in range(1):
    for w in range(1):
      for s1 in range(SegOfWay[w]):
        for s2 in range(SegOfWay[w]):
          if s1==s2:continue
          fn=open('results.html','r')
          soup = BeautifulSoup(fn,'html.parser')
          span=[str(i) for i in list(soup.find_all('span'))]
          lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
          df.iloc[i,:]=[d,t,w,s1,s2]+lags
          i+=1
250/187:
SegOfWay={1:5}
col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
i=0
for d in range(1):
  for t in range(1):
    for w in range(1,2):
      for s1 in range(SegOfWay[w]):
        for s2 in range(SegOfWay[w]):
          if s1==s2:continue
          fn=open('results.html','r')
          soup = BeautifulSoup(fn,'html.parser')
          span=[str(i) for i in list(soup.find_all('span'))]
          lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
          df.iloc[i,:]=[d,t,w,s1,s2]+lags
          i+=1
250/188:
SegOfWay={1:5}
col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
i=0
for d in range(1):
  for t in range(1):
    for w in range(1,2):
      for s1 in range(SegOfWay[w]):
        for s2 in range(SegOfWay[w]):
          if s1==s2:continue
          fn=open('results.html','r')
          soup = BeautifulSoup(fn,'html.parser')
          span=[str(i) for i in list(soup.find_all('span'))]
          lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
          df.iloc[0,:]=[d,t,w,s1,s2]+lags
          if i==0:
            dfa=df
          else:
            dfa=dfa.append(df,ignore_index=True)
          i+=1
250/189: df
250/190: [d,t,w,s1,s2]+lags
250/191:
SegOfWay={1:5}
col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
i=0
for d in range(1):
  for t in range(1):
    for w in range(1,2):
      for s1 in range(SegOfWay[w]):
        for s2 in range(SegOfWay[w]):
          if s1==s2:continue
          fn=open('results.html','r')
          soup = BeautifulSoup(fn,'html.parser')
          span=[str(i) for i in list(soup.find_all('span'))]
          lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
          df.loc[i,'date']=d
          df.iloc[i,1:]=[t,w,s1,s2]+lags
          if i==0:
            dfa=df
#          else:
#            dfa=dfa.append(df,ignore_index=True)
          i+=1
250/192: df
250/193:
def click_run_save(d,t,w,s1,s2):
    clkid("tripFuturePickTime")
    SelectByIDnValue("tripFuturePickTime",t)
    clkid("freeway_from")
    SelectByIDnValue("freeway_from",w)
    clkid("section_from")
    SelectByIDnValue("section_from",s1)
    clkid("freeway_from")
    SelectByIDnValue("freeway_end",w)
    clkid("section_from")
    SelectByIDnValue("section_end",s2)
    clkpath('//button[@class="button_primary tripplan_btn"]')
    n = os.path.join("/nas2/kuang/tp_future", "results.html")
    f = codecs.open(n, "w", "utf−8")
    h = driver.page_source
    f.write(h)
    return
250/194:
SegOfWay={1:5}
col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
i=0
for d in range(1,2):
  for t in range(1,2):
    for w in range(1,2):
      for s1 in range(SegOfWay[w]):
        for s2 in range(SegOfWay[w]):
          if s1==s2:continue
          click_run_save(d,t,w,s1,s2)
          fn=open('results.html','r')
          soup = BeautifulSoup(fn,'html.parser')
          span=[str(i) for i in list(soup.find_all('span'))]
          lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
          df.loc[i,'date']=d
          df.iloc[i,1:]=[t,w,s1,s2]+lags
          i+=1
250/195:
SegOfWay={1:5}
col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
i=0
for d in range(1,2):
  for t in range(10,11):
    for w in range(1,2):
      for s1 in range(SegOfWay[w]):
        for s2 in range(SegOfWay[w]):
          if s1==s2:continue
          click_run_save(d,t,w,s1,s2)
          fn=open('results.html','r')
          soup = BeautifulSoup(fn,'html.parser')
          span=[str(i) for i in list(soup.find_all('span'))]
          lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
          df.loc[i,'date']=d
          df.iloc[i,1:]=[t,w,s1,s2]+lags
          i+=1
250/196: df
250/197: driver.quit()
250/198:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
250/199:
SegOfWay={1:5}
col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
i=0
for d in range(1,2):
  for t in range(10,11):
    for w in range(1,2):
      for s1 in range(SegOfWay[w]-1):
        s2=s1+1
        lags=click_run_save(d,t,w,s1,s2)
        df.loc[i,'date']=d
        df.iloc[i,1:]=[t,w,s1,s2]+lags
        i+=1
        sleep 1
        lags=click_run_save(d,t,w,s2,s1)
        df.loc[i,'date']=d
        df.iloc[i,1:]=[t,w,s1,s2]+lags
        i+=1
        sleep 1
250/200:
import time
time.sleep(delayInSeconds)
250/201:
import time
time.sleep(1)
250/202:
SegOfWay={1:5}
col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
i=0
for d in range(1,2):
  for t in range(10,11):
    for w in range(1,2):
      for s1 in range(SegOfWay[w]-1):
        s2=s1+1
        lags=click_run_save(d,t,w,s1,s2)
        df.loc[i,'date']=d
        df.iloc[i,1:]=[t,w,s1,s2]+lags
        i+=1
        time.sleep(1)
        lags=click_run_save(d,t,w,s2,s1)
        df.loc[i,'date']=d
        df.iloc[i,1:]=[t,w,s1,s2]+lags
        i+=1
        time.sleep(1)
250/203: lags
250/204: df
250/205: d,t,w,s1,s2
250/206:
def click_run_save(d,t,w,s1,s2):
    clkid("tripFuturePickTime")
    SelectByIDnValue("tripFuturePickTime",t)
    clkid("freeway_from")
    SelectByIDnValue("freeway_from",w)
    clkid("section_from")
    SelectByIDnValue("section_from",s1)
    clkid("freeway_from")
    SelectByIDnValue("freeway_end",w)
    clkid("section_from")
    SelectByIDnValue("section_end",s2)
    clkpath('//button[@class="button_primary tripplan_btn"]')
    n = os.path.join("/nas2/kuang/tp_future", "results.html")
    f = codecs.open(n, "w", "utf−8")
    h = driver.page_source
    f.write(h)
    fn=open('results.html','r')
    soup = BeautifulSoup(fn,'html.parser')
    span=[str(i) for i in list(soup.find_all('span'))]
    lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
    return lags
250/207:
SegOfWay={1:5}
col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
i=0
for d in range(1,2):
  for t in range(10,11):
    for w in range(1,2):
      for s1 in range(SegOfWay[w]-1):
        s2=s1+1
        lags=click_run_save(d,t,w,s1,s2)
        df.loc[i,'date']=d
        df.iloc[i,1:]=[t,w,s1,s2]+lags
        i+=1
        time.sleep(1)
        lags=click_run_save(d,t,w,s2,s1)
        df.loc[i,'date']=d
        df.iloc[i,1:]=[t,w,s1,s2]+lags
        i+=1
        time.sleep(1)
250/208: df
250/209:
import datetime
print(datetime.date.today())
250/210:
import datetime
print(datetime.date.today().strptime("%Y/%m/%d"))
250/211:
import datetime
print(datetime.date.today().strftime("%Y/%m/%d"))
250/212:
input_1=driver.find_element(By.ID,"tripFuturePickDate")
input_1.send_keys(datetime.date.today().strftime("%Y/%m/%d"))
250/213:
from bs4 import BeautifulSoup
import os
import json
from pandas import DataFrame
from selenium import webdriver

fn=open('segments.html','r')
soup = BeautifulSoup(fn,'html.parser')
250/214:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
250/215:
import datetime
print(datetime.date.today().strftime("%Y/%m/%d"))
250/216:
input_1=driver.find_element(By.ID,"tripFuturePickDate").clear()
input_1.send_keys(datetime.date.today().strftime("%Y/%m/%d"))
250/217:
driver.find_element(By.ID,"tripFuturePickDate").clear()
input_1=driver.find_element(By.ID,"tripFuturePickDate")
input_1.send_keys(datetime.date.today().strftime("%Y/%m/%d"))
250/218:
d=0
day=datetime.date.today()+datetime.timedelta(days=d)
250/219:
d=0
day=datetime.date.today()+datetime.timedelta(days=d)
day
250/220:
d=1
day=datetime.date.today()+datetime.timedelta(days=d)
day
250/221:
def click_run_save(d,t,w,s1,s2):
    day=datetime.date.today()+datetime.timedelta(days=d)
    clkid("tripFuturePickDate")
    driver.find_element(By.ID,"tripFuturePickDate").clear()
    input_1=driver.find_element(By.ID,"tripFuturePickDate")
    input_1.send_keys(day.strftime("%Y/%m/%d"))
    clkid("tripFuturePickTime")
    SelectByIDnValue("tripFuturePickTime",t)
    clkid("freeway_from")
    SelectByIDnValue("freeway_from",w)
    clkid("section_from")
    SelectByIDnValue("section_from",s1)
    clkid("freeway_from")
    SelectByIDnValue("freeway_end",w)
    clkid("section_from")
    SelectByIDnValue("section_end",s2)
    clkpath('//button[@class="button_primary tripplan_btn"]')
    n = os.path.join("/nas2/kuang/tp_future", "results.html")
    f = codecs.open(n, "w", "utf−8")
    h = driver.page_source
    f.write(h)
    fn=open('results.html','r')
    soup = BeautifulSoup(fn,'html.parser')
    span=[str(i) for i in list(soup.find_all('span'))]
    lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
    return lags
250/222: click_run_save(1,68,1,1,2)
250/223: click_run_save(1,34,1,1,2)
250/224: click_run_save(3,34,1,1,2)
250/225: click_run_save(3,34,1,1,90)
250/226: click_run_save(3,34,1,1,20)
250/227: click_run_save(3,34,1,19,20)
250/228: len(v2t)
250/229: v2t
250/230: len(v2t)
250/231: len(v2t)-11
250/232: click_run_save(3,34,1,1,85)
250/233:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
250/234: click_run_save(3,34,1,1,85)
250/235: click_run_save(3,34,1,1,84)
250/236: driver.quit()
250/237:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
250/238:
def click_run_save(d,t,w,s1,s2):
    day=datetime.date.today()+datetime.timedelta(days=d)
    clkid("tripFuturePickDate")
    driver.find_element(By.ID,"tripFuturePickDate").clear()
    input_1=driver.find_element(By.ID,"tripFuturePickDate")
    input_1.send_keys(day.strftime("%Y/%m/%d"))
    clkid("tripFuturePickTime")
    SelectByIDnValue("tripFuturePickTime",t)
    clkid("freeway_from")
    SelectByIDnValue("freeway_from",w)
    clkid("section_from")
    SelectByIDnValue("section_from",s1)
    clkid("freeway_from")
    SelectByIDnValue("freeway_end",w)
    clkid("section_from")
    SelectByIDnValue("section_end",s2)
    clkpath('//button[@class="button_primary tripplan_btn"]')
    if abs(s1-s2) >10:time.sleep(5)
    n = os.path.join("/nas2/kuang/tp_future", "results.html")
    f = codecs.open(n, "w", "utf−8")
    h = driver.page_source
    f.write(h)
    fn=open('results.html','r')
    soup = BeautifulSoup(fn,'html.parser')
    span=[str(i) for i in list(soup.find_all('span'))]
    lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
    return lags
250/239: click_run_save(3,34,1,1,84)
250/240:
len(v2t)-11
v2t
250/241: text
250/242: texts
250/243: driver.quit()
250/244:
from bs4 import BeautifulSoup
import os
import json
from pandas import DataFrame
from selenium import webdriver

fn=open('segment3.html','r')
soup = BeautifulSoup(fn,'html.parser')
250/245:
opt=list(soup.find_all('option'))
values=[str(i).split('"')[1] for i in opt]
texts=[i.text for i in opt]
v2t={i:j for i,j in zip(values,texts)}
250/246: len(texts)
250/247: texts[:5],text[-5:]
250/248: texts[:5],texts[-5:]
250/249:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
250/250:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
250/251:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
251/1:
from bs4 import BeautifulSoup
import os
import json
from pandas import DataFrame
from selenium import webdriver

fn=open('segment3.html','r')
soup = BeautifulSoup(fn,'html.parser')
252/1:
from bs4 import BeautifulSoup
import os
import json
from pandas import DataFrame
from selenium import webdriver

fn=open('segment3.html','r')
soup = BeautifulSoup(fn,'html.parser')
252/2:
from selenium.webdriver.support.ui import Select
from selenium.webdriver.common.by import By

def clkid(ID):                                  #click the id
    button_element = driver.find_element("id",ID)
    button_element.click()
    return
def clkpath(ID):                                        #click the xpath (no use)
    button_element = driver.find_element(By.XPATH,ID)
    button_element.click()
    return
def SelectByIDnValue(ID,v):                     #click and select by value
    select = Select(driver.find_element(By.ID,ID))
    select.select_by_index(v)
    return
252/3:
def click_run_save(d,t,w,s1,s2):
    day=datetime.date.today()+datetime.timedelta(days=d)
    clkid("tripFuturePickDate")
    driver.find_element(By.ID,"tripFuturePickDate").clear()
    input_1=driver.find_element(By.ID,"tripFuturePickDate")
    input_1.send_keys(day.strftime("%Y/%m/%d"))
    clkid("tripFuturePickTime")
    SelectByIDnValue("tripFuturePickTime",t)
    clkid("freeway_from")
    SelectByIDnValue("freeway_from",w)
    clkid("section_from")
    SelectByIDnValue("section_from",s1)
    clkid("freeway_from")
    SelectByIDnValue("freeway_end",w)
    clkid("section_from")
    SelectByIDnValue("section_end",s2)
    clkpath('//button[@class="button_primary tripplan_btn"]')
    if abs(s1-s2) >10:time.sleep(5)
    n = os.path.join("/nas2/kuang/tp_future", "results.html")
    f = codecs.open(n, "w", "utf−8")
    h = driver.page_source
    f.write(h)
    fn=open('results.html','r')
    soup = BeautifulSoup(fn,'html.parser')
    span=[str(i) for i in list(soup.find_all('span'))]
    lags=[i.split('約')[1].split('分')[0] for i in span if '約' in i]
    return lags
252/4:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
252/5: driver.quit()
252/6:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
252/7: click_run_save(3,34,1,1,84)
252/8:
from bs4 import BeautifulSoup
import os
import json
from pandas import DataFrame
from selenium import webdriver
import datetime

fn=open('segment3.html','r')
soup = BeautifulSoup(fn,'html.parser')
252/9: click_run_save(3,34,1,1,84)
252/10:
from bs4 import BeautifulSoup
import os
import json
from pandas import DataFrame
from selenium import webdriver
import datetime
import time
fn=open('segment3.html','r')
soup = BeautifulSoup(fn,'html.parser')
252/11: click_run_save(3,34,1,1,84)
252/12: click_run_save(4,34,1,1,84)
252/13:
from bs4 import BeautifulSoup
import os
import json
from pandas import DataFrame
from selenium import webdriver
import datetime
import time
import codecs
fn=open('segment3.html','r')
soup = BeautifulSoup(fn,'html.parser')
252/14:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
252/15: click_run_save(4,35,1,1,84)
252/16: driver.quit()
252/17: import numpy as np
252/18: np.random.randint(low=4,high=10,size=10)
252/19: np.random.randint(low=4,high=10)
252/20:
def click_run_save(d,t,w,s1,s2):
    day=datetime.date.today()+datetime.timedelta(days=d)
    clkid("tripFuturePickDate")
    driver.find_element(By.ID,"tripFuturePickDate").clear()
    input_1=driver.find_element(By.ID,"tripFuturePickDate")
    input_1.send_keys(day.strftime("%Y/%m/%d"))
    clkid("tripFuturePickTime")
    SelectByIDnValue("tripFuturePickTime",t)
    clkid("freeway_from")
    SelectByIDnValue("freeway_from",w)
    clkid("section_from")
    SelectByIDnValue("section_from",s1)
    clkid("freeway_from")
    SelectByIDnValue("freeway_end",w)
    clkid("section_from")
    SelectByIDnValue("section_end",s2)
    clkpath('//button[@class="button_primary tripplan_btn"]')
    if abs(s1-s2) >10:time.sleep(5)
    n = os.path.join("/nas2/kuang/tp_future", "results.html")
    f = codecs.open(n, "w", "utf−8")
    h = driver.page_source
    f.write(h)
    fn=open('results.html','r')
    soup = BeautifulSoup(fn,'html.parser')
    span=[str(i) for i in list(soup.find_all('span'))]
    lags=[i.split('約')[1].split('分')[0]+'分' for i in span if '約' in i]
    return lags
252/21:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
252/22: driver.quit()
252/23: driver.quit()
252/24:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
252/25:
SegOfWay={1:84,2:12,3:7,4:84,5:4,6:8,7:7,8:8,9:6,10:8}
#'1_0': '國道1號', 'N1H_0': '國1高架', '2_0': '國道2號',
# '3_0': '國道3號', 'N3A_0': '國道3甲', '4_0': '國道4號',
# '5_0': '國道5號', '6_0': '國道6號', '8_0': '國道8號', '10_0': '國道10號',

col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
d0=DataFrame({i:[0] for i in col})
i=0
for d in range(1,2):
    for t in range(1,49):
        for w in range(1,11):
            for s1 in range(SegOfWay[w]-1):
                s2=s1+1
                lags=click_run_save(d,t,w,s1,s2)
                df.loc[i,'date']=d
                df.iloc[i,1:]=[t,w,s1,s2]+lags
                d0.iloc[0,:]=[d,t,w,s1,s2]+lags
                i+=1
                time.sleep()

                lags=click_run_save(d,t,w,s2,s1)
                df.loc[i,'date']=d
                df.iloc[i,1:]=[t,w,s2,s1]+lags
                d0.iloc[0,:]=[d,t,w,s2,s1]+lags
                i+=1
                d0.set_index('date').to_csv('tp_future.csv',mode='a')
                
                time.sleep(np.random.randint(low=5,high=30))
252/26:
SegOfWay={1:84,2:12,3:7,4:84,5:4,6:8,7:7,8:8,9:6,10:8}
#'1_0': '國道1號', 'N1H_0': '國1高架', '2_0': '國道2號',
# '3_0': '國道3號', 'N3A_0': '國道3甲', '4_0': '國道4號',
# '5_0': '國道5號', '6_0': '國道6號', '8_0': '國道8號', '10_0': '國道10號',

col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
d0=DataFrame({i:[0] for i in col})
i=0
for d in range(1,2):
    for t in range(1,49):
        for w in range(1,11):
            for s1 in range(SegOfWay[w]-1):
                s2=s1+1
                lags=click_run_save(d,t,w,s1,s2)
                df.loc[i,'date']=d
                df.iloc[i,1:]=[t,w,s1,s2]+lags
                d0.iloc[0,:]=[d,t,w,s1,s2]+lags
                i+=1
                time.sleep(1)

                lags=click_run_save(d,t,w,s2,s1)
                df.loc[i,'date']=d
                df.iloc[i,1:]=[t,w,s2,s1]+lags
                d0.iloc[0,:]=[d,t,w,s2,s1]+lags
                i+=1
                d0.set_index('date').to_csv('tp_future.csv',mode='a')
                
                time.sleep(np.random.randint(low=5,high=30))
252/27: driver.quit()
252/28:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
252/29:
SegOfWay={1:84,2:12,3:7,4:84,5:4,6:8,7:7,8:8,9:6,10:8}
#'1_0': '國道1號', 'N1H_0': '國1高架', '2_0': '國道2號',
# '3_0': '國道3號', 'N3A_0': '國道3甲', '4_0': '國道4號',
# '5_0': '國道5號', '6_0': '國道6號', '8_0': '國道8號', '10_0': '國道10號',

col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
d0=DataFrame({i:[0] for i in col})
i=0
for d in range(1,2):
    for t in range(1,49):
        for w in range(1,11):
            for s1 in range(SegOfWay[w]-1):
                s2=s1+1
                lags=click_run_save(d,t,w,s1,s2)
                df.loc[i,'date']=d
                df.iloc[i,1:]=[t,w,s1,s2]+lags
                d0.iloc[0,:]=[d,t,w,s1,s2]+lags
                i+=1
                time.sleep(1)

                lags=click_run_save(d,t,w,s2,s1)
                df.loc[i,'date']=d
                df.iloc[i,1:]=[t,w,s2,s1]+lags
                d0.iloc[0,:]=[d,t,w,s2,s1]+lags
                i+=1
                d0.set_index('date').to_csv('tp_future.csv',mode='a')
                
                time.sleep(np.random.randint(low=5,high=30))
252/30: driver.quit()
252/31: driver.quit()
252/32:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
252/33:
SegOfWay={1:84,2:12,3:7,4:84,5:4,6:8,7:7,8:8,9:6,10:8}
#'1_0': '國道1號', 'N1H_0': '國1高架', '2_0': '國道2號',
# '3_0': '國道3號', 'N3A_0': '國道3甲', '4_0': '國道4號',
# '5_0': '國道5號', '6_0': '國道6號', '8_0': '國道8號', '10_0': '國道10號',

col=['date','hr','highway','segment1','segment2']
col+=['t'+str(i) for i in range(1,8)]
df=DataFrame({i:[] for i in col})
d0=DataFrame({i:[0] for i in col})
d0.set_index('date').to_csv('tp_future.csv')
i=0
for d in range(1,2):
    for t in range(1,49):
        for w in range(1,11):
            for s1 in range(SegOfWay[w]-1):
                s2=s1+1
                lags=click_run_save(d,t,w,s1,s2)
                df.loc[i,'date']=d
                df.iloc[i,1:]=[t,w,s1,s2]+lags
                d0.iloc[0,:]=[d,t,w,s1,s2]+lags
                i+=1
                time.sleep(1)

                lags=click_run_save(d,t,w,s2,s1)
                df.loc[i,'date']=d
                df.iloc[i,1:]=[t,w,s2,s1]+lags
                d0.iloc[0,:]=[d,t,w,s2,s1]+lags
                i+=1
                d0.set_index('date').to_csv('tp_future.csv',header=None,mode='a')
                time.sleep(np.random.randint(low=5,high=30))
253/1:
from bs4 import BeautifulSoup
import os
import json
from pandas import DataFrame
from selenium import webdriver
import datetime
import time
import codecs
import numpy as np

fn=open('segment3.html','r')
soup = BeautifulSoup(fn,'html.parser')
253/2:
from selenium.webdriver.support.ui import Select
from selenium.webdriver.common.by import By

def clkid(ID):                                  #click the id
    button_element = driver.find_element("id",ID)
    button_element.click()
    return
def clkpath(ID):                                        #click the xpath (no use)
    button_element = driver.find_element(By.XPATH,ID)
    button_element.click()
    return
def SelectByIDnValue(ID,v):                     #click and select by value
    select = Select(driver.find_element(By.ID,ID))
    select.select_by_index(v)
    return
253/3:
def click_run_save(d,t,w,s1,s2):
    day=datetime.date.today()+datetime.timedelta(days=d)
    clkid("tripFuturePickDate")
    driver.find_element(By.ID,"tripFuturePickDate").clear()
    input_1=driver.find_element(By.ID,"tripFuturePickDate")
    input_1.send_keys(day.strftime("%Y/%m/%d"))
    clkid("tripFuturePickTime")
    SelectByIDnValue("tripFuturePickTime",t)
    clkid("freeway_from")
    SelectByIDnValue("freeway_from",w)
    clkid("section_from")
    SelectByIDnValue("section_from",s1)
    clkid("freeway_from")
    SelectByIDnValue("freeway_end",w)
    clkid("section_from")
    SelectByIDnValue("section_end",s2)
    clkpath('//button[@class="button_primary tripplan_btn"]')
    if abs(s1-s2) >10:time.sleep(5)
    n = os.path.join("/nas2/kuang/tp_future", "results.html")
    f = codecs.open(n, "w", "utf−8")
    h = driver.page_source
    f.write(h)
    fn=open('results.html','r')
    soup = BeautifulSoup(fn,'html.parser')
    span=[str(i) for i in list(soup.find_all('span'))]
    lags=[i.split('約')[1].split('分')[0]+'分' for i in span if '約' in i]
    return lags
253/4:
driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
driver.get("https://1968.freeway.gov.tw/tp_future")
253/5: click_run_save(4,35,2,1,12)
253/6: driver.quit()
255/1: s='1小時02分'
255/2: int(s.split('小時')[0])*60+int(s[-4:-2])
255/3: int(s.split('小時')[0])*60+int(s[-3:-1])
255/4: s='02分'
255/5: int(s.split('小時')[0])*60+int(s[-3:-1])
255/6: !lst
255/7: cat tp_future.csv >> tp_future.csv1
255/8: from pandas import *
255/9: !vi tp_future.csv1
255/10: df=read_csv('tp_future.csv1')
255/11: df.head()
255/12: idx=df.loc[df.t1.map(lambda t: if '小時' in t),'t1'].index
255/13: idx=df.loc[df.t1.map(lambda t: '小時' in t),'t1'].index
255/14: ll=[int(s.split('小時')[0])*60+int(s[-3:-1]) for s in df.loc[idx,"t1"]]
255/15: df.loc[df.t1.map(lambda t: if '小時' in t),'t1']=ll
255/16: df.loc[idx,'t1']=ll
255/17: df.head()
255/18:
for c in [t+str(i) for i in range{2,8)]:
    idx=df.loc[df.t1.map(lambda t: '小時' in t),c].index
255/19:
for c in [t+str(i) for i in range(2,8)]:
    idx=df.loc[df.t1.map(lambda t: '小時' in t),c].index
    ll=[int(s.split('小時')[0])*60+int(s[-3:-1]) for s in df.loc[idx,c]]
    df.loc[idx,c]=ll
255/20:
for c in ["t"+str(i) for i in range(2,8)]:
    idx=df.loc[df.t1.map(lambda t: '小時' in t),c].index
    ll=[int(s.split('小時')[0])*60+int(s[-3:-1]) for s in df.loc[idx,c]]
    df.loc[idx,c]=ll
255/21: c
255/22:
for c in ["t"+str(i) for i in range(2,8)]:
    idx=df.loc[df[c].map(lambda t: '小時' in t),c].index
    ll=[int(s.split('小時')[0])*60+int(s[-3:-1]) for s in df.loc[idx,c]]
    df.loc[idx,c]=ll
255/23: df.head()
255/24: df.tail()
255/25:
for c in ["t"+str(i) for i in range(1,8)]:
    idx=df.loc[df[c].map(lambda t: '分' in t),c].index
    ll=[int(s.split('分')[0]) for s in df.loc[idx,c]]
    df.loc[idx,c]=ll
255/26: c
255/27: idx=df.loc[df[c].map(lambda t: '分' in t)].index
255/28: t
255/29: df[c][:5]
255/30: df[c][-5:]
255/31:
for c in ["t"+str(i) for i in range(1,8)]:
    idx=df.loc[df[c].map(lambda t: '分' in str(t))].index
    ll=[int(s.split('分')[0]) for s in df.loc[idx,c]]
    df.loc[idx,c]=ll
255/32: df.head()
255/33: df.tail()
255/34: df=df.loc[df.t1>0].reset_index(drop=True)
255/35: df=df.loc[df.t1!='0'].reset_index(drop=True)
255/36: df.tail()
255/37: df.set_index('date').to_csv('tp_future.csv')
255/38: history
255/39: !vi str_min.py
255/40: df.head()
255/41: df.tail()
255/42: df.head(20)
255/43: df.loc[20:40]
255/44: df0=df.loc[df.segment1=='0'].reset_index(drop=True)
255/45: df0.head()
255/46: df0=df.loc[df.segment1==0].reset_index(drop=True)
255/47: df0.head()
255/48: df0=df0.drop_duplicate
255/49: df0=df0.drop_duplicate()
255/50: df0=df0.drop_duplicates
255/51: df0.head()
255/52: df0=df.loc[df.segment1==0].reset_index(drop=True)
255/53: df0=df0.drop_duplicates()
255/54: df0.head()
255/55: df0=df.loc[df.segment1==0].reset_index(drop=True)
255/56: df0=df0.drop_duplicates().reset_index(drop=True)
255/57: len(set(df.hr))
255/58: len(df0)
255/59: len(df0)/7
255/60: pivot_table(df0,index='hiway',values='segment1',aggfunc='count').reset_index()
255/61: pivot_table(df0,index='highway',values='segment1',aggfunc='count').reset_index()
255/62: df0.loc[(df0.date==1) & (df0.highway==6)]
255/63: df0.loc[(df0.date==1) & (df0.highway==7)]
255/64: pivot_table(df0,index=['date','highway'],values='segment1',aggfunc='count').reset_index()
255/65: a=pivot_table(df0,index=['date','highway'],values='segment1',aggfunc='count').reset_index()
255/66: a.loc[a.segment1!=7]
255/67: df0.loc[df0.date==10]
255/68: set(df0.loc[df0.date==10,'hr'])
255/69: a=df0.loc[df0.date==10]
255/70: pivot_table(a,index=['date','highway'],values='segment1',aggfunc='count').reset_index()
255/71: df0.loc[(df0.date==10) & (df0.highway==6)]
255/72: df0.loc[(df0.date==10) & (df0.highway==7)]
255/73: df0.loc[(df0.date==9) & (df0.highway==7)]
255/74: df0.loc[(df0.date==9) & (df0.highway==6)]
255/75: df0.loc[(df0.date==10) & (df0.highway==6)]
255/76:
i=len(df0)
for hr in [29,36,43]:
    for hw in range(1,7):
        df0.loc[i,'date']=10
        idx=df0.loc[(df0.date==9)&(df0.hr==hr)&(df0.segment2==hw)].index
        df0.iloc[i,:]=df0.iloc[idx,:]
        i+=1
255/77: idx
255/78: df0.loc[(df0.date==9)&(df0.hr==hr)&(df0.segment2==hw)].index
255/79: hr,hw
255/80: df0.loc[(df0.date==9)&(df0.segment2==hw)].index
255/81: df0.loc[(df0.date==9)].index
255/82: df0.loc[df0.segment2==hw].index
255/83: df0.loc[df0.segment2==str(hw)].index
255/84:
i=len(df0)
for hr in [29,36,43]:
    for hw in range(1,7):
        df0.loc[i,'date']=10
        idx=df0.loc[(df0.date==9)&(df0.hr==hr)&(df0.highway==hw)].index
        df0.iloc[i,:]=df0.iloc[idx,:]
        i+=1
255/85: len(df0)
255/86: df0.tail()
255/87: df0=df.loc[df.segment1==0].reset_index(drop=True)
255/88: df0=df0.drop_duplicates().reset_index(drop=True)
255/89: len(df0)
255/90: df0.loc[df0.hiway==str(hw)].index
255/91: df0.loc[df0.highway==str(hw)].index
255/92: hw=1
255/93: df0.loc[df0.highway==str(hw)].index
255/94: hw
255/95: df0.head()
255/96: df0.loc[df0.highway==1].index
255/97: df0.loc[df0.highway==1].index()
255/98: idx=df0.loc[(df0.date==9)&(df0.hr==hr)&(df0.highway==hw)].index
255/99: idx
255/100: df0.iloc[idx,:]
255/101: list(df0.iloc[idx,:])
255/102: df0.iloc[idx,:].value
255/103: df0.iloc[idx,:].values
255/104:
i=len(df0)
for hr in [29,36,43]:
    for hw in range(1,7):
        df0.loc[i,'date']=10
        idx=df0.loc[(df0.date==9)&(df0.hr==hr)&(df0.highway==hw)].index
        df0.iloc[i,:]=df0.iloc[idx,:].values
        i+=1
255/105: df0.tail()
255/106: df0=df.loc[df.segment1==0].reset_index(drop=True)
255/107: df0=df0.drop_duplicates().reset_index(drop=True)
255/108:
i=len(df0)
for hr in [29,36,43]:
    for hw in range(1,7):
        df0.loc[i,'date']=10
        idx=df0.loc[(df0.date==9)&(df0.hr==hr)&(df0.highway==hw)].index
        df0.iloc[i,:]=list(df0.iloc[idx,:].values)
        i+=1
255/109: list(df0.iloc[idx,:].values)
255/110: list(df0.iloc[idx,:].values[:])
255/111: df0.iloc[idx,:].values[:]
255/112: df0.iloc[idx,:].values
255/113: df0.iloc[idx,:].data
255/114: df0.iloc[idx,:].data()
255/115: df0.iloc[idx,:]
255/116: df0.iloc[idx,:].value[0]
255/117: df0.iloc[idx,:].values[0]
255/118: list(df0.iloc[idx,:].values[0])
255/119: df0=df0.drop_duplicates().reset_index(drop=True)
255/120: df0=df.loc[df.segment1==0].reset_index(drop=True)
255/121: df0=df0.drop_duplicates().reset_index(drop=True)
255/122:
i=len(df0)
for hr in [29,36,43]:
    for hw in range(1,7):
        df0.loc[i,'date']=10
        idx=df0.loc[(df0.date==9)&(df0.hr==hr)&(df0.highway==hw)].index
        df0.iloc[i,:]=list(df0.iloc[idx,:].values[0])
        i+=1
255/123: df0.tail()
255/124: df0=df.loc[df.segment1==0].reset_index(drop=True)
255/125: df0=df0.drop_duplicates().reset_index(drop=True)
255/126:
i=len(df0)
for hr in [29,36,43]:
    for hw in range(1,7):
        df0.loc[i,'date']=10
        idx=df0.loc[(df0.date==9)&(df0.hr==hr)&(df0.highway==hw)].index
        df0.iloc[i,:]=[int(j) for j in list(df0.iloc[idx,:].values[0])]
        i+=1
255/127: df0.tail()
255/128: for c in 'date    hr  highway  segment1  segment2'.split()
255/129:
for c in 'date    hr  highway  segment1  segment2'.split():
    df0[c]=[int(i) for i in df0[c]]
255/130: df0.tail()
255/131: df0=df.loc[df.segment1==0].reset_index(drop=True)
255/132: df0=df0.drop_duplicates().reset_index(drop=True)
255/133:
i=len(df0)
for hr in [29,36,43]:
    for hw in range(1,7):
        df0.loc[i,'date']=10
        idx=df0.loc[(df0.date==9)&(df0.hr==hr)&(df0.highway==hw)].index
        df0.iloc[i,1:]=list(df0.iloc[idx,:].values[0])[1:]
        i+=1
255/134: df0.tail()
255/135:
for c in 'date    hr  highway  segment1  segment2'.split():
    df0[c]=[int(i) for i in df0[c]]
255/136: df0.set_index('date').to_csv('tp_futureGo.csv')
255/137: df1=df.loc[df.segment1!=0].reset_index(drop=True)
255/138: df1=df1.drop_duplicates().reset_index(drop=True)
255/139: df0=df.loc[df.segment1!=0].reset_index(drop=True)
255/140: df0=df0.drop_duplicates().reset_index(drop=True)
255/141:
i=len(df0)
for hr in [29,36,43]:
    for hw in range(1,7):
        df0.loc[i,'date']=10
        idx=df0.loc[(df0.date==9)&(df0.hr==hr)&(df0.highway==hw)].index
        df0.iloc[i,1:]=list(df0.iloc[idx,:].values[0])[1:]
        i+=1
255/142:
for c in 'date    hr  highway  segment1  segment2'.split():
    df0[c]=[int(i) for i in df0[c]]
255/143: df0.tail()
255/144: df0.set_index('date').to_csv('tp_futureFrom.csv')
255/145: col='t1   t2   t3   t4   t5   t6   t7'.split()
255/146: colv='t1   t2   t3   t4   t5   t6   t7'.split()
255/147: coli='date  hr  highway'.split()
255/148: a=pivot_table(df0,index=coli,values=colv,aggfunc=np.mean).reset_index()
255/149: import numpy as np
255/150: a=pivot_table(df0,index=coli,values=colv,aggfunc=np.mean).reset_index()
255/151: a.head()
255/152:
i=len(df)
for hr in [29,36,43]:
    for hw in range(1,7):
        df.loc[i,'date']=10
        idx=df.loc[(df.date==9)&(df.hr==hr)&(df.highway==hw)].index
        df.iloc[i,1:]=list(df.iloc[idx,:].values[0])[1:]
        i+=
255/153: len(df)
255/154: len(a)
255/155: pivot_table(a,index=['date','highway'],values='segment1',aggfunc='count').reset_index()
255/156: pivot_table(a,index=['date','highway'],values='t1',aggfunc='count').reset_index()
255/157: pivot_table(a.loc[a.date=10],index=['highway'],values='t1',aggfunc='count').reset_index()
255/158: pivot_table(a.loc[a.date==10],index=['highway'],values='t1',aggfunc='count').reset_index()
255/159: len(a)
255/160: a.set_index('date').to_csv('tp_future.csv')
255/161: a.tail()
255/162: dates=np.range(1,11)
255/163: dates=np.arange(1,11)
255/164: dates
255/165: hrs=np.arange(1,50)
255/166: hws=np.arange(1,11)
255/167: one=np.ones(shape=(50*10),dtype=int)
255/168: dd=np.outer(dates,one)
255/169: dd=np.outer(dates,one).flatten()
255/170: one=np.ones(shape=(10),dtype=int)
255/171: h1=np.outer(one,hrs).flatten()
255/172: hh=np.outer(h1,one).flatten()
255/173: len(dd),len(hh)
255/174: one=np.ones(shape=(49*10),dtype=int)
255/175: dd=np.outer(dates,one).flatten()
255/176: len(dd),len(hh)
255/177: ww=np.outer(one,hws).flatten()
255/178: len(dd),len(hh),len(ww)
255/179: df=DataFrame({'date':dd,"hr":hh,"highway",ww})
255/180: df=DataFrame({'date':dd,"hr":hh,"highway":ww})
255/181: df.head()
255/182: a.head()
255/183: np.arange(1,50,7)
255/184: set(a.hr)
255/185: a=a.sort_values(coli).reset_index(drop=True)
255/186: len(a)
255/187: len(df)
255/188:
for h in np.arange(1,50,7):
    for i in range(7)
255/189:
df['mins']=0
for h in np.arange(1,50,7):
    idx=a.loc[a.hr==h].index
    for i in range(7):
        t=h+i
        c='t'+str(i+1)
        df.loc[df.hr==t,'mins']=a.loc[idx,c]
255/190: df.head(30)
255/191: df.tail()
255/192: c
255/193:
df['mins']=0
for h in range(1,50,7):
    idx=a.loc[a.hr==h].index
    for i in range(7):
        t=h+i
        c='t'+str(i+1)
        df.loc[df.hr==t,'mins']=a.loc[idx,c]
255/194: df.head(30)
255/195: a.loc[idx,c].head(30)
255/196: list(a.loc[idx,c])[:30]
255/197:
df['mins']=0
for h in range(1,50,7):
    idx=a.loc[a.hr==h].index
    for i in range(7):
        t=h+i
        c='t'+str(i+1)
        df.loc[df.hr==t,'mins']=list(a.loc[idx,c])
255/198: df.head(30)
255/199: df.tail(30)
255/200: df.set_index('date').to_csv('tp_future4900.csv')
255/201: !cat tp_future.py
255/202: history
255/203: !lst
255/204: !cat str_min.py
256/1: fname='/nas2/cmaqruns/2022fcst/grid45/cctm.fcst/daily/CCTM_ACONC_v532_intel_CWBWRF_45k_20230615.nc'
256/2: import netCDF
256/3: import netCDF4
256/4: nc = netCDF4.Dataset(fname,'r+')
256/5: nc.SDATE-=1
256/6: v='TFLAG'
256/7: nc[v][:,:,0]=nc.SDATE
256/8: nc.close()
259/1:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt

boston_airbnb_listings = gpd.read_file(gplt.datasets.get_path('boston_airbnb_listings'))

ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='viridis', projection=gcrs.WebMercator(), figsize=(12, 12),
    shade=True
)
gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/2:
ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='viridis', projection=gcrs.WebMercator(), figsize=(12, 12),
    fill=True
)
gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/3:
ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(12, 12),
    fill=True
)
gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/4:
ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(12, 12),
    fill=True,
    legend=True
)
gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/5:
ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(15, 15),
    fill=True,
    legend=True
)
gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/6:
ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,
    legend=True
)
gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/7:
ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,
    legend=True
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/8:
ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=false,
    legend=True,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/9:
ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=False,
    legend=True,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/10:
ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.05,
    legend=True,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/11:
ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.1,
    legend=True,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/12:
ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.01,
    legend=True,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/13:
    ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.01,
    cbar=True,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/14:
    ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.01,
    cbar=True,alpha=0.5,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/15:
    ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/16:
    ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.legend(ncol=2, loc="lower right", frameon=True)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/17:
    ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.legend(loc="lower right", frameon=True)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/18:
    ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    legend=True,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.legend(loc="lower right", frameon=True)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/19:
    ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    legend=True,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/20: boston_airbnb_listings
259/21:
import netCDF
from pyproj import Proj
import shapely
import pandas as pd
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
v='PM25_TOT'
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=pd.DataFrame({v:var,"geometry":geo})
259/22:
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
v='PM25_TOT'
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=pd.DataFrame({v:var,"geometry":geo})
259/23:
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v=V[3][0]
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=pd.DataFrame({v:var,"geometry":geo})
259/24:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v=V[3][0]
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=pd.DataFrame({v:var,"geometry":geo})
259/25: df
259/26: max(var)
259/27:

ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    legend=True,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
#gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/28:

ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    legend=True,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/29:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v=V[3][0]
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.DataFrame({v:var,"geometry":geo})
259/30:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v=V[3][0]
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.GeoDataFrame({v:var,"geometry":geo})
259/31: df
259/32:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v=V[3][0]
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.GeoDataFrame({v:var,"geometry":geo}).to_crs(epsg=4326)
259/33:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v=V[3][0]
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.GeoDataFrame({v:var,"geometry":geo})
df=df.to_crs(epsg=4326)
259/34:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v=V[3][0]
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.GeoDataFrame({v:var,"geometry":geo})
df=df.to_crs(epsg=4326)
df[v]=var
df["geometry"]=geo
259/35:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v=V[3][0]
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.GeoDataFrame({v:[],"geometry":[]}).to_crs(epsg=4326)
df[v]=var
df["geometry"]=geo
259/36:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v=V[3][0]
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.GeoDataFrame().to_crs(epsg=4326)
df[v]=var
df["geometry"]=geo
259/37:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='tempTW.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v=V[3][0]
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.GeoDataFrame()#.to_crs(epsg=4326)
df[v]=var
df["geometry"]=geo
259/38:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    legend=True,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/39:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='/nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/CCTM_ACONC_v532_intel_TWEPA_3k_20230625.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v='PM25_TOT'
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.GeoDataFrame({v:var,"geometry":geo})#.to_crs(epsg=4326)
259/40:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='/nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/CCTM_ACONC_v532_intel_TWEPA_3k_20230625.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v='O3'
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.GeoDataFrame({v:var,"geometry":geo})#.to_crs(epsg=4326)
259/41:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    legend=True,
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/42:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    bw_adjust=0,
    
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/43:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    bw_adjust=0.1,
    
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/44:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    bw_adjust=0.5,
    
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/45:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    bw_adjust=0.2,
    
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/46:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    bw_adjust=10.2,
    
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/47:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    bw_adjust=1.2,
    
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/48:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.0,
    cbar=True,alpha=0.5,
    bw_method='scott', bw_adjust=1,
    
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/49:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.005,
    cbar=True,alpha=0.5,
    bw_method='scott', bw_adjust=1,
    
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/50:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.05,
    cbar=True,alpha=0.5,
    bw_method='scott', bw_adjust=1,
    
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/51:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.05,
    cbar=True,alpha=0.5,
    bw_method='scott', bw_adjust=0.1,
    
)
#gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/52: df
259/53:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.05,
    cbar=True,alpha=0.5,
    bw_method='scott', bw_adjust=0.1,
    
)
gplt.pointplot(df, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/54:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.LambertConformal(), figsize=(5, 5),
    fill=True,thresh=0.05,
    cbar=True,alpha=0.5,
    bw_method='scott', bw_adjust=0.1,
    
)
#gplt.pointplot(df, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/55:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.05,
    cbar=True,alpha=0.5,
    bw_method='scott', bw_adjust=0.1,
    
)
#gplt.pointplot(df, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/56: dir(boston_airbnb_listings)
259/57: boston_airbnb_listings.geom_type(),df.geom_type()
259/58: boston_airbnb_listings.geom_type,df.geom_type()
259/59: boston_airbnb_listings.geom_type,df.geom_type
259/60: boston_airbnb_listings.head()
259/61: df.head()
259/62:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='/nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/CCTM_ACONC_v532_intel_TWEPA_3k_20230625.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v='NO2'
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.GeoDataFrame({v:var,"geometry":geo})#.to_crs(epsg=4326)
259/63:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.05,
    cbar=True,alpha=0.5,
    bw_method='scott', bw_adjust=0.1,
    
)
#gplt.pointplot(df, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/64: df.head()
259/65: df.max
259/66: df.max()
259/67: max(df["NO2"])
259/68:

ax = gplt.kdeplot(
    df["NO2"], cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.05,
    cbar=True,alpha=0.5,
    bw_method='scott', bw_adjust=0.1,
    
)
#gplt.pointplot(df, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
259/69:

ax = gplt.kdeplot(
    df, cmap='rainbow', projection=gcrs.WebMercator(), figsize=(5, 5),
    fill=True,thresh=0.05,
    cbar=True,alpha=0.5,
    bw_method='scott', bw_adjust=0.1,
    
)
#gplt.pointplot(df, s=1, color='black', ax=ax)
ax.set(xlim=(0, 24), ylabel="",
       xlabel="Automobile collisions per billion miles")
gplt.webmap(df, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
260/1:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='/nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/CCTM_ACONC_v532_intel_TWEPA_3k_20230625.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v='NO2'
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.GeoDataFrame({v:var,"geometry":geo})#.to_crs(epsg=4326)
260/2:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt

boston_airbnb_listings = gpd.read_file(gplt.datasets.get_path('boston_airbnb_listings'))

ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='viridis', projection=gcrs.WebMercator(), figsize=(12, 12),
    shade=True
)
gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
261/1:
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
fig, ax = plt.subplots(figsize=(12, 10))
gdf.plot(ax=ax, color="gray")
261/2:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt

boston_airbnb_listings = gpd.read_file(gplt.datasets.get_path('boston_airbnb_listings'))

ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='viridis', projection=gcrs.WebMercator(), figsize=(12, 12),
    shade=True
)
gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
262/1:
import geopandas as gpd
import geoplot as gplt
import geoplot.crs as gcrs
import matplotlib.pyplot as plt

boston_airbnb_listings = gpd.read_file(gplt.datasets.get_path('boston_airbnb_listings'))

ax = gplt.kdeplot(
    boston_airbnb_listings, cmap='viridis', projection=gcrs.WebMercator(), figsize=(12, 12),
    shade=True
)
gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)
gplt.webmap(boston_airbnb_listings, ax=ax)
plt.title('Boston AirBnB Locations, 2016', fontsize=18)
263/1:
import geopandas as gpd
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
#fig, ax = plt.subplots(figsize=(12, 10))
#gdf.plot(ax=ax, color="gray")
264/1: gdf
264/2:
import geopandas as gpd
shp_fname="/home/kuang/bin/TWN_COUNTY.shp"
gdf = gpd.read_file(shp_fname)
#fig, ax = plt.subplots(figsize=(12, 10))
#gdf.plot(ax=ax, color="gray")
265/1:
import geopandas as gpd
shp_fname="/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp"
gdf = gpd.read_file(shp_fname)
#fig, ax = plt.subplots(figsize=(12, 10))
#gdf.plot(ax=ax, color="gray")
265/2: gdf
265/3:
fig, ax = plt.subplots(figsize=(12, 10))
gdf.plot(ax=ax, color="gray")
265/4:
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(12, 10))
gdf.plot(ax=ax, color="gray")
265/5:
import geopandas as gpd
shp_fname="/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp"
gdf = gpd.read_file(shp_fname)
x,y=[119.9,122.4,122.4,119.9,],[21.5,21.5,25.5,25.5,]
Frame=shapely.geometry.Polygon([shapely.geometry.Point(i,j) for i,j in zip(x,y)])
gdf=gdf.loc[twn.geometry.map(lambda p:p.within(Frame))]
265/6:
import geopandas as gpd
import shapely
shp_fname="/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp"
gdf = gpd.read_file(shp_fname)
x,y=[119.9,122.4,122.4,119.9,],[21.5,21.5,25.5,25.5,]
Frame=shapely.geometry.Polygon([shapely.geometry.Point(i,j) for i,j in zip(x,y)])
gdf=gdf.loc[twn.geometry.map(lambda p:p.within(Frame))]
265/7:
import geopandas as gpd
import shapely
shp_fname="/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp"
gdf = gpd.read_file(shp_fname)
x,y=[119.9,122.4,122.4,119.9,],[21.5,21.5,25.5,25.5,]
Frame=shapely.geometry.Polygon([shapely.geometry.Point(i,j) for i,j in zip(x,y)])
gdf=gdf.loc[gdf.geometry.map(lambda p:p.within(Frame))]
265/8:
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(12, 10))
gdf.plot(ax=ax, color="gray")
265/9:
import geopandas as gpd
import shapely
shp_fname="/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp"
gdf = gpd.read_file(shp_fname)
x,y=[119.9,122.4,122.4,119.9,],[21.5,21.5,25.5,25.5,]
Frame=shapely.geometry.Polygon([shapely.geometry.Point(i,j) for i,j in zip(x,y)])
shp_path = '/nas1/Data/GIS/TWN_town/TOWN_MOI_1120317.shp'
twn=gpd.read_file(shp_path)
twn=twn.loc[twn.geometry.map(lambda p:p.within(Frame))]
265/10:
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(12, 10))
twn.plot(ax=ax, color="gray")
265/11:
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(12, 10))
twn.plot(ax=ax, color="none")
265/12:
import numpy as np
import netCDF4
from pyproj import Proj
import shapely
import pandas as pd
fname='/nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily/CCTM_ACONC_v532_intel_TWEPA_3k_20230625.nc'
nc = netCDF4.Dataset(fname,'r')
V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
v='NO2'
var=nc[v][0,0,:,:].flatten()
pnyc = Proj(proj='lcc', datum='NAD83', lat_1=nc.P_ALP, lat_2=nc.P_BET,lat_0=nc.YCENT, lon_0=nc.XCENT, x_0=0, y_0=0.0)
X=[nc.XORIG+nc.XCELL*i for i in range(ncol)]
Y=[nc.YORIG+nc.YCELL*i for i in range(nrow)]
x,y=np.meshgrid(X, Y)
x=x.flatten();y=y.flatten()
lon, lat = pnyc(x, y, inverse=True)
lon,lat=lon.flatten(),lat.flatten()
geo=[shapely.geometry.Point(i,j) for i,j in zip(lon,lat)]
df=gpd.GeoDataFrame({v:var,"geometry":geo})#.to_crs(epsg=4326)
265/13: lat.min(),lat.max()
265/14: lon.min(),lon.max()
265/15:
import geopandas as gpd
import shapely
shp_fname="/nas1/Data/GIS/twn_county/COUNTY_MOI_1090820.shp"
gdf = gpd.read_file(shp_fname)
x,y=[119.2,122.4,122.4,119.2,],[21.5,21.5,25.5,25.5,]
Frame=shapely.geometry.Polygon([shapely.geometry.Point(i,j) for i,j in zip(x,y)])
shp_path = '/nas1/Data/GIS/TWN_town/TOWN_MOI_1120317.shp'
twn=gpd.read_file(shp_path)
twn=twn.loc[twn.geometry.map(lambda p:p.within(Frame))]
265/16:
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(12, 10))
twn.plot(ax=ax, color="none")
266/1:
import numpy as np
import netCDF4
import os, sys, subprocess, datetime
from dtconvertor import dt2jul, jul2dt
from calendar import monthrange
from dateutil.relativedelta import relativedelta

def rel_mon(dt,im):
  a=dt+relativedelta(months=im)
  return a.year,a.month
266/2: pwd
266/3: cd ../../../smoke
266/4: cd ../../smoke
266/5:
import numpy as np
import netCDF4
import os, sys, subprocess, datetime
from dtconvertor import dt2jul, jul2dt
from calendar import monthrange
from dateutil.relativedelta import relativedelta

def rel_mon(dt,im):
  a=dt+relativedelta(months=im)
  return a.year,a.month
266/6: ls dt*
266/7: !ln -s /home/kuang/bin/dtconvertor.py .
266/8:
import numpy as np
import netCDF4
import os, sys, subprocess, datetime
from dtconvertor import dt2jul, jul2dt
from calendar import monthrange
from dateutil.relativedelta import relativedelta

def rel_mon(dt,im):
  a=dt+relativedelta(months=im)
  return a.year,a.month
266/9:
tar =subprocess.check_output('which tar' ,shell=True).decode('utf8').strip('\n')
ncks=subprocess.check_output('which ncks',shell=True).decode('utf8').strip('\n')
pwd =subprocess.check_output('pwd',shell=True).decode('utf8').strip('\n').split('/')[4]
D={'grid45':'7','grid09':'8','grid03':'3'}
nf={'grid45':1,'grid09':1,'grid03':3}
if pwd not in D:sys.exit('pwd not right!')
D=D[pwd]
nf=nf[pwd]
266/10: tdy='2023-07-30'
266/11:

nd=12
if len(sys.argv)==3:nd=int(sys.argv[2])
nhr=nd*24+1
266/12:
  yr=tdy.split('-')[0]
  mm=tdy.split('-')[1]
  with open('fnames.txt','r') as f:
    bdate7s=[i.split('.')[1] for i in f]
266/13: !scp dev2:/u01/cmaqruns/2022fcst/grid03/smoke/fnames.txt .
266/14:
  yr=tdy.split('-')[0]
  mm=tdy.split('-')[1]
  with open('fnames.txt','r') as f:
    bdate7s=[i.split('.')[1] for i in f]
266/15: bdate7s
266/16:
  yr=tdy.split('-')[0]
  mm=tdy.split('-')[1]
  with open('fnames.txt','r') as f:
    bdate7s=[datetime.datetime.strptime(i.split('.')[1],"%Y%m%d") for i in f]
266/17: bdate7s
266/18: bdate7s[0].week
266/19: bdate7s[0].week_days()
266/20: dir(bdates7)
266/21: dir(bdate7s)
266/22: bdate7s[0].weekday()
266/23: bdate7s[0]
266/24: datetime.datetime(2023,1,25).weekday()
266/25: deld=bdate7s[0].weekday()-datetime.datetime(2023, bdate7s[0].month, bdate7s[0].days)
266/26: deld=bdate7s[0].weekday()-datetime.datetime(2023, bdate7s[0].month, bdate7s[0].day)
266/27: deld=bdate7s[0].weekday()-datetime.datetime(2023, bdate7s[0].month, bdate7s[0].day).weekday()
266/28: deld
266/29: len(bdate7s)
266/30:
  yr=tdy.split('-')[0]
  mm=tdy.split('-')[1]
  with open('fnames.txt','r') as f:
    bdate7s=[datetime.datetime.strptime(i.split('.')[1],"%Y%m%d") for i in f]
266/31: deld=bdate7s[0].weekday()-datetime.datetime(2023, bdate7s[0].month, bdate7s[0].day).weekday()
266/32: deld
266/33: bdate7s
266/34: !scp dev2:/u01/cmaqruns/2022fcst/grid03/smoke/fnames.txt .
266/35: deld=bdate7s[-1].weekday()-datetime.datetime(2023, bdate7s[-1].month, bdate7s[-1].day).weekday()
266/36: deld
266/37: bdate7m=[i-datetime.timedelta(days=deld) for i in f]
266/38: bdate7m=[i-datetime.timedelta(days=deld) for i in bdate7s]
266/39: bdate7m
266/40: bdate7s
266/41: bdate7m=[datetime.datetime(int(yr), i.month, i.day) for i in bdate7m]
266/42: bdate7m
266/43: tdy
266/44: bdate7m[i-1]<=bdate<bdate7m[i]
266/45: i=0
266/46: bdate7m[i-1]<=bdate<bdate7m[i]
266/47: bdate=datetime.datetime.strptime(tdy,"%Y-%m-%d")
266/48: bdate7m[i-1]<=bdate<bdate7m[i]
266/49:
for i in range(1,12):
    print(bdate7m[i-1]<=bdate<bdate7m[i])
266/50:
for i in range(1,12):
    print(i,bdate7m[i-1]<=bdate<bdate7m[i])
266/51:
  yr=tdy.split('-')[0]
  mm=tdy.split('-')[1]
  with open('fnames.txt','r') as f:
    bdate7s=[datetime.datetime.strptime(i.split('.')[1],"%Y%m%d") for i in f]
266/52: len(bdate7s)
266/53: bdate7m=[i-datetime.timedelta(days=deld) for i in bdate7s]
266/54: len(bdate7m)
266/55: bdate7m=[datetime.datetime(int(yr), i.month, i.day) for i in bdate7m]
266/56: i=11
266/57: bdate7m[i]
266/58: bdate7m[i-1]<=bdate<bdate7m[i]
266/59:
for i in range(1,12):
    print(i,bdate7m[i-1]<=bdate<bdate7m[i])
266/60:
  m=0
  for i in range(11,-1,-1)
    if bdate>bdate7m[i]: m=i+1
266/61:
  m=0
  for i in range(11,-1,-1):
    if bdate>bdate7m[i]: m=i+1
266/62: m
266/63:
  m=0
  for i in range(12):
    if bdate>bdate7m[i]: m=i+1
266/64: m
266/65: bdate7m[0]=[datetime.datetime(int(yr-1), i.month, i.day) for i in bdate7m[:1]]
266/66: bdate7m[0]=[datetime.datetime(int(yr)-1, i.month, i.day) for i in bdate7m[:1]]
266/67: bdate7m
266/68: ':2d'.format(bdate7m[m].month)
266/69: '%:2d'.format(bdate7m[m].month)
266/70: ':2d%'.format(bdate7m[m].month)
266/71: '{:2d}'.format(bdate7m[m].month)
266/72: '{:02d}'.format(bdate7m[m].month)
266/73:
  with open('fnames.txt','r') as f:
    fns=[i.split('.')[1] for i in f]
266/74: fns
266/75: bdate7s
266/76: bdate7s=[datetime.datetime.strptime(i.split('.')[1],"%Y%m%d") for i in fns]
266/77: bdate7s=[datetime.datetime.strptime(i,"%Y%m%d") for i in fns]
266/78: bdate7s
266/79: fns
266/80: bdate7s[m]-bdate7m[m]
266/81:
  with open('fnames.txt','r') as f:
    fns=[i.strip('\n') for i in f]
266/82: fns
266/83: fns[0].split('b3g')[0]
266/84: fns[0].split(bdates[0])[1]
266/85: fns[0].split(bdate7s[0])[1]
266/86: sdates=[i.split('.') for i in fns]
266/87: sdates
266/88: sdates=[i.split('.')[1] for i in fns]
266/89: sdates
266/90: fns[0].split(sdates[0])[1]
266/91: fns[0].split('b3g')[0]
267/1: import wandb
267/2: wandb
267/3: wandb login
267/4: !wandb login
267/5: @wandb login
267/6: run wandb login
268/1: from flamingo_mini import FlamingoConfig, FlamingoModel, FlamingoProcessor
269/1: from flamingo_mini import FlamingoConfig, FlamingoModel, FlamingoProcessor
269/2: !which pip
270/1: import torch
271/1: !pip install transformers
271/2:
from transformers import pipeline

classifier = pipeline("sentiment-analysis") #使用情感分析
classifier(
    [
        "寶寶覺得苦，但寶寶不說",
        "我愛寶寶"
    ]
)
272/1:
from transformers import pipeline

classifier = pipeline("sentiment-analysis") #使用情感分析
classifier(
    [
        "寶寶覺得苦，但寶寶不說",
        "我愛寶寶"
    ]
)
273/1:
from transformers import pipeline

classifier = pipeline("sentiment-analysis") #使用情感分析
classifier(
    [
        "寶寶覺得苦，但寶寶不說",
        "我愛寶寶"
    ]
)
273/2: !pip install datasets
273/3:
from datasets import load_dataset_builder
ds_builder = load_dataset_builder("poem_sentiment")
273/4:
print(ds_builder.info.description)
print(ds_builder.info.features)
273/5:
from datasets import load_dataset
sentiment = load_dataset("poem_sentiment")
273/6: sentiment
273/7:
train_ds = sentiment["train"]
valid_ds = sentiment["validation"]
test_ds = sentiment["test"]
273/8: dataset_train = load_dataset("rotten_tomatoes", split="train")
273/9:
import pandas as pd
sentiment.set_format(type="pandas")
df = sentiment["train"][:]
df.head(10)
273/10:
import matplotlib.pyplot as plt

df["label_name"].value_counts().plot.barh()
plt.title("Poem Classes")
plt.show()
273/11:
import matplotlib.pyplot as plt

df["label"].value_counts().plot.barh()
plt.title("Poem Classes")
plt.show()
273/12: sentiment_train = sentiment["train"].shuffle(seed=5566).select(range(100))
273/13:
sentiment_filtered = sentiment.filter(lambda x: len(x["verse_text"]) > 30)
sentiment_filtered
273/14:
new_dataset = sentiment.map(
    lambda x: {"verse_text": [ len(o) for o in x["verse_text"] ] }, batched=True
)
new_dataset['test'][:3]
273/15:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
nih_dataset = load_dataset("csv", data_files=url, split= "train")
273/16: !pip install zstandard
273/17:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
nih_dataset = load_dataset("csv", data_files=url, split= "train")
274/1:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
nih_dataset = load_dataset("csv", data_files=url, split= "train")
274/2:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
url="https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/data_0_time1625801885_default.jsonl.zst"
nih_dataset = load_dataset("csv", data_files=url, split= "train")
274/3:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
url="https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/data_0_time1625801885_default.jsonl.zst"
url='.data_0_time1625801885_default.jsonl.zst'
nih_dataset = load_dataset("csv", data_files=url, split= "train")
274/4:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
url="https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/data_0_time1625801885_default.jsonl.zst"
url='.data_0_time1625801885_default.jsonl.zst'
nih_dataset = load_dataset("csv", data_files=url)
274/5:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
url="https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/data_0_time1625801885_default.jsonl.zst"
url='./data_0_time1625801885_default.jsonl.zst'
nih_dataset = load_dataset("csv", data_files=url)
274/6:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
url="https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/data_0_time1625801885_default.jsonl.zst"
url='./data_0_time1625801885_default.jsonl.zst'
nih_dataset = load_dataset("csv", data_files=url, split= "train")
274/7: nih_dataset
274/8:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
url="https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/data_0_time1625801885_default.jsonl.zst"
url='./data_0_time1625801885_default.jsonl'
nih_dataset = load_dataset("csv", data_files=url)
274/9:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
url="https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/data_0_time1625801885_default.jsonl.zst"
url='./data_0_time1625801885_default.jsonl'
nih_dataset = load_dataset("jsonl", data_files=url)
274/10: dir(load_dataset)
274/11:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
url="https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/data_0_time1625801885_default.jsonl.zst"
url='./data_0_time1625801885_default.jsonl'
nih_dataset = load_dataset("json", data_files=url)
274/12:
import psutil

print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")

print(f"Number of files in dataset : {nih_dataset.dataset_size}")

size_gb = nih_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
274/13:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
url="https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/data_0_time1625801885_default.jsonl.zst"
url='./data_0_time1625801885_default.jsonl'
nih_dataset = load_dataset("json", data_files=url, split= "train")
274/14:
from datasets import load_dataset
url = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst"
url="https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/data_0_time1625801885_default.jsonl.zst"
url='./data_0_time1625801885_default.jsonl'
nih_dataset = load_dataset("json", data_files=url, split= "train")
274/15:
import psutil

print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")

print(f"Number of files in dataset : {nih_dataset.dataset_size}")

size_gb = nih_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
274/16: !pip install psutil
274/17:
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(nih_dataset), batch_size):
    _ = nih_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(nih_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)
274/18:
nih_dataset_streamed = load_dataset(
    "json", data_files=url, split="train", streaming=True
)
274/19: nih_dataset_streamed
274/20: next(iter(nih_dataset_streamed))
274/21:
shuffled_dataset = nih_dataset_streamed.shuffle(buffer_size=10_000, seed=5566)
next(iter(shuffled_dataset))
274/22:
string = "Only those who will risk going too far can possibly find out how far one can go."
tokenized_str = list(string)
print(tokenized_str)
274/23:
token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_str)))}
print(token2idx)
input_ids = [token2idx[token] for token in tokenized_str]
print(input_ids)
274/24:
from transformers import AutoTokenizer

string = "Only those who will risk going too far can possibly find out how far one can go."

model_name = "distilbert-base-uncased-finetuned-sst-2-english" #直接叫model名字
tokenizer = AutoTokenizer.from_pretrained(model_name)
274/25:
encoded_str = tokenizer(string, padding=True, truncation=True) 
encoded_str
274/26: len(set(encoded_str['input_ids']))
274/27: len(string.split())
274/28:
tokens = tokenizer.convert_ids_to_tokens(encoded_str.input_ids)
tokens
274/29:
from datasets import load_dataset
sentiment = load_dataset("poem_sentiment")
def tokenize(batch):
    return tokenizer(batch["verse_text"], padding=True, truncation=True)
print(tokenize(sentiment["train"][:3]))
274/30:
sentiment_encoded = sentiment.map(tokenize, batched=True, batch_size=None)
print(sentiment_encoded["train"].column_names)
276/1:
from transformers import AutoTokenizer

tokenizer_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
string_arr = [
    "Only those who will risk going too far can possibly find out how far one can go.",
    "Baby shark, doo doo doo doo doo doo, Baby shark!"
]
inputs = tokenizer(string_arr, padding=True, truncation=True, return_tensors="pt")
print(inputs)
276/2:
from transformers import AutoModelForSequenceClassification

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
outputs = model(**inputs)
print(outputs.logits)
276/3:
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
276/4:
string_arr = [
    "Only those who will risk going too far can definitely find out how far one can go.",
    "Baby shark, doo doo doo doo doo doo, Baby shark!"
]
inputs = tokenizer(string_arr, padding=True, truncation=True, return_tensors="pt")
276/5:
from transformers import AutoModelForSequenceClassification

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
outputs = model(**inputs)
print(outputs.logits)
276/6:
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
276/7:
from transformers import AutoTokenizer

tokenizer_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
string_arr = [
    "Only those who will risk going too far can definitely find out how far one can go.",
    "Baby shark, doo doo doo doo doo doo, Baby shark!"
]
inputs = tokenizer(string_arr, padding=True, truncation=True, return_tensors="pt")
print(inputs)
276/8:
from transformers import AutoModelForSequenceClassification

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
outputs = model(**inputs)
print(outputs.logits)
276/9:
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
276/10:
from transformers import AutoTokenizer

tokenizer_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
string_arr = [
    "Only those who will risk going too far can definitely find out how far one can go.",
    "Baby shark, doo doo doo doo doo doo, Baby shark!"
]
inputs = tokenizer(string_arr, padding=True, truncation=True, return_tensors="pt")
print(inputs)
276/11:
from transformers import AutoModelForSequenceClassification

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
outputs = model(**inputs)
print(outputs.logits)
276/12:
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
276/13:
from transformers import AutoTokenizer

tokenizer_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
string_arr = [
    "Only those who will risk going too far can possibly find out how far one can go.",
    "Baby shark, doo doo doo doo doo doo, Baby shark!"
]
inputs = tokenizer(string_arr, padding=True, truncation=True, return_tensors="pt")
print(inputs)
276/14:
from transformers import AutoModelForSequenceClassification

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
outputs = model(**inputs)
print(outputs.logits)
276/15:
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
276/16:
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "Only those who will risk going too far can definitely find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
276/17:
from transformers import TFAutoModel
tf_model = TFAutoModel.from_pretrained("xlm-roberta-base", from_pt=True)
276/18: !pip install tensorflow
276/19:
from transformers import TFAutoModel
tf_model = TFAutoModel.from_pretrained("xlm-roberta-base", from_pt=True)
276/20: !pip install tensorflow
276/21:
from transformers import TFAutoModel
tf_model = TFAutoModel.from_pretrained("xlm-roberta-base", from_pt=True)
277/1:
from transformers import TFAutoModel
tf_model = TFAutoModel.from_pretrained("xlm-roberta-base", from_pt=True)
277/2:
from datasets import load_dataset
sentiment = load_dataset("poem_sentiment")
sentiment
277/3:
import pandas as pd

sentiment.set_format(type="pandas")
df = sentiment["train"][:]
df.head()
277/4:
def label_int2str(row):
    return sentiment["train"].features["label"].int2str(row)

df["label_name"] = df["label"].apply(label_int2str)
df.head()
277/5:
labels = sentiment["train"].features["label"].names
print(labels)
277/6:
import matplotlib.pyplot as plt

df["label_name"].value_counts(ascending=True).plot.barh()
plt.title("Number of labels")
plt.show()
277/7:
sentiment.reset_format()
from transformers import AutoTokenizer

model_name = "distilbert-base-uncased" # 第三天預設的distilbert-base-uncased-finetuned-sst-2-english用這個
tokenizer = AutoTokenizer.from_pretrained(model_name)
def tokenize(batch):
    return tokenizer(batch["verse_text"], padding=True, truncation=True)
sentiment_encoded = sentiment.map(tokenize, batched=True, batch_size=None)
next(iter(sentiment_encoded["train"])) #忘記這裡為什麼要用 next(iter())才能看到印出來的資料，可以回去看載入極巨大資料篇
277/8:
valid_ds = sentiment["validation"]
valid_ds["label"][:]
277/9:
from transformers import AutoModelForSequenceClassification
import torch

num_labels = 4
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = (AutoModelForSequenceClassification
        .from_pretrained(model_name, num_labels=num_labels
        ,id2label={"0": "negative",
                    "1": "positive",
                    "2": "no_impact",
                    "3": "mixed"}
        ,label2id={"negative": "0",
                    "positive": "1",
                    "no_impact": "2",
                    "mixed": "3"})
         .to(device))
277/10:
from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(sentiment_encoded["train"]) // batch_size
model_name = "poem_model"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=40,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  label_names= labels,
                                  report_to = "azure_ml",
                                  logging_steps=logging_steps)
277/11: !pip install transformers[torch]
277/12:
from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(sentiment_encoded["train"]) // batch_size
model_name = "poem_model"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=40,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  label_names= labels,
                                  report_to = "azure_ml",
                                  logging_steps=logging_steps)
277/13: !pip install accelerate -U
277/14:
from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(sentiment_encoded["train"]) // batch_size
model_name = "poem_model"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=40,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  label_names= labels,
                                  report_to = "azure_ml",
                                  logging_steps=logging_steps)
278/1:
from transformers import AutoModelForSequenceClassification
import torch

num_labels = 4
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = (AutoModelForSequenceClassification
        .from_pretrained(model_name, num_labels=num_labels
        ,id2label={"0": "negative",
                    "1": "positive",
                    "2": "no_impact",
                    "3": "mixed"}
        ,label2id={"negative": "0",
                    "positive": "1",
                    "no_impact": "2",
                    "mixed": "3"})
         .to(device))
278/2:
from transformers import AutoModelForSequenceClassification
import torch
model_name = "poem_model"
num_labels = 4
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = (AutoModelForSequenceClassification
        .from_pretrained(model_name, num_labels=num_labels
        ,id2label={"0": "negative",
                    "1": "positive",
                    "2": "no_impact",
                    "3": "mixed"}
        ,label2id={"negative": "0",
                    "positive": "1",
                    "no_impact": "2",
                    "mixed": "3"})
         .to(device))
278/3:
from transformers import AutoModelForSequenceClassification
import torch
model_name = "distilbert-base-uncased" # 第三天預設的distilbert-base-uncased-finetuned-sst-2-english用這個
num_labels = 4
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = (AutoModelForSequenceClassification
        .from_pretrained(model_name, num_labels=num_labels
        ,id2label={"0": "negative",
                    "1": "positive",
                    "2": "no_impact",
                    "3": "mixed"}
        ,label2id={"negative": "0",
                    "positive": "1",
                    "no_impact": "2",
                    "mixed": "3"})
         .to(device))
278/4:
from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(sentiment_encoded["train"]) // batch_size
model_name = "poem_model"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=40,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  label_names= labels,
                                  report_to = "azure_ml",
                                  logging_steps=logging_steps)
278/5:
sentiment.reset_format()
from transformers import AutoTokenizer

model_name = "distilbert-base-uncased" # 第三天預設的distilbert-base-uncased-finetuned-sst-2-english用這個
tokenizer = AutoTokenizer.from_pretrained(model_name)
def tokenize(batch):
    return tokenizer(batch["verse_text"], padding=True, truncation=True)
sentiment_encoded = sentiment.map(tokenize, batched=True, batch_size=None)
next(iter(sentiment_encoded["train"])) #忘記這裡為什麼要用 next(iter())才能看到印出來的資料，可以回去看載入極巨大資料篇
278/6:
from datasets import load_dataset
sentiment = load_dataset("poem_sentiment")
sentiment
278/7:
sentiment.reset_format()
from transformers import AutoTokenizer

model_name = "distilbert-base-uncased" # 第三天預設的distilbert-base-uncased-finetuned-sst-2-english用這個
tokenizer = AutoTokenizer.from_pretrained(model_name)
def tokenize(batch):
    return tokenizer(batch["verse_text"], padding=True, truncation=True)
sentiment_encoded = sentiment.map(tokenize, batched=True, batch_size=None)
next(iter(sentiment_encoded["train"])) #忘記這裡為什麼要用 next(iter())才能看到印出來的資料，可以回去看載入極巨大資料篇
278/8:
from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(sentiment_encoded["train"]) // batch_size
model_name = "poem_model"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=40,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  label_names= labels,
                                  report_to = "azure_ml",
                                  logging_steps=logging_steps)
278/9:
import pandas as pd

sentiment.set_format(type="pandas")
df = sentiment["train"][:]
df.head()
278/10:
def label_int2str(row):
    return sentiment["train"].features["label"].int2str(row)

df["label_name"] = df["label"].apply(label_int2str)
df.head()
278/11:
labels = sentiment["train"].features["label"].names
print(labels)
278/12:
sentiment.reset_format()
from transformers import AutoTokenizer

model_name = "distilbert-base-uncased" # 第三天預設的distilbert-base-uncased-finetuned-sst-2-english用這個
tokenizer = AutoTokenizer.from_pretrained(model_name)
def tokenize(batch):
    return tokenizer(batch["verse_text"], padding=True, truncation=True)
sentiment_encoded = sentiment.map(tokenize, batched=True, batch_size=None)
next(iter(sentiment_encoded["train"])) #忘記這裡為什麼要用 next(iter())才能看到印出來的資料，可以回去看載入極巨大資料篇
278/13:
from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(sentiment_encoded["train"]) // batch_size
model_name = "poem_model"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=40,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  label_names= labels,
                                  report_to = "azure_ml",
                                  logging_steps=logging_steps)
278/14:
from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(sentiment_encoded["train"]) // batch_size
model_name = "poem_model"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=40,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  label_names= labels,
                                  report_to = "azure_ml",
                                  logging_steps=logging_steps)
278/15:
from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}
278/16:
from transformers import Trainer

trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=sentiment_encoded["train"],
                  eval_dataset=sentiment_encoded["validation"],
                  tokenizer=tokenizer)
trainer.train()
278/17: !pip install azureml-sdk
278/18:
from transformers import Trainer

trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=sentiment_encoded["train"],
                  eval_dataset=sentiment_encoded["validation"],
                  tokenizer=tokenizer)
trainer.train()
280/1:
from transformers import AutoModelForSequenceClassification
import torch
model_name = "distilbert-base-uncased" # 第三天預設的distilbert-base-uncased-finetuned-sst-2-english用這個
num_labels = 4
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = (AutoModelForSequenceClassification
        .from_pretrained(model_name, num_labels=num_labels
        ,id2label={"0": "negative",
                    "1": "positive",
                    "2": "no_impact",
                    "3": "mixed"}
        ,label2id={"negative": "0",
                    "positive": "1",
                    "no_impact": "2",
                    "mixed": "3"})
         .to(device))
280/2:
from datasets import load_dataset
sentiment = load_dataset("poem_sentiment")
sentiment
280/3:
import pandas as pd

sentiment.set_format(type="pandas")
df = sentiment["train"][:]
df.head()
280/4:
def label_int2str(row):
    return sentiment["train"].features["label"].int2str(row)

df["label_name"] = df["label"].apply(label_int2str)
df.head()
280/5:
labels = sentiment["train"].features["label"].names
print(labels)
280/6:
import matplotlib.pyplot as plt

df["label_name"].value_counts(ascending=True).plot.barh()
plt.title("Number of labels")
plt.show()
280/7:
sentiment.reset_format()
from transformers import AutoTokenizer

model_name = "distilbert-base-uncased" # 第三天預設的distilbert-base-uncased-finetuned-sst-2-english用這個
tokenizer = AutoTokenizer.from_pretrained(model_name)
def tokenize(batch):
    return tokenizer(batch["verse_text"], padding=True, truncation=True)
sentiment_encoded = sentiment.map(tokenize, batched=True, batch_size=None)
# next(iter(sentiment_encoded["train"])) #忘記這裡為什麼要用 next(iter())才能看到印出來的資料，可以回去看載入極巨大資料篇
280/8:
from transformers import AutoModelForSequenceClassification
import torch
model_name = "distilbert-base-uncased" # 第三天預設的distilbert-base-uncased-finetuned-sst-2-english用這個
num_labels = 4
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = (AutoModelForSequenceClassification
        .from_pretrained(model_name, num_labels=num_labels
        ,id2label={"0": "negative",
                    "1": "positive",
                    "2": "no_impact",
                    "3": "mixed"}
        ,label2id={"negative": "0",
                    "positive": "1",
                    "no_impact": "2",
                    "mixed": "3"})
         .to(device))
280/9:
from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(sentiment_encoded["train"]) // batch_size
model_name = "poem_model"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=40,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  label_names= labels,
                                  report_to = "azure_ml",
                                  logging_steps=logging_steps)
280/10:
from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}
280/11:
from transformers import Trainer

trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=sentiment_encoded["train"],
                  eval_dataset=sentiment_encoded["validation"],
                  tokenizer=tokenizer)
trainer.train()
281/1:
from transformers import TFAutoModel
tf_model = TFAutoModel.from_pretrained("xlm-roberta-base", from_pt=True)
281/2:
from datasets import load_dataset
sentiment = load_dataset("poem_sentiment")
sentiment
281/3:
import pandas as pd

sentiment.set_format(type="pandas")
df = sentiment["train"][:]
#df.head()
281/4:
def label_int2str(row):
    return sentiment["train"].features["label"].int2str(row)

df["label_name"] = df["label"].apply(label_int2str)
#df.head()
281/5:
labels = sentiment["train"].features["label"].names
print(labels)
281/6:
import matplotlib.pyplot as plt

df["label_name"].value_counts(ascending=True).plot.barh()
plt.title("Number of labels")
# plt.show()
281/7:
sentiment.reset_format()
from transformers import AutoTokenizer

model_name = "distilbert-base-uncased" # 第三天預設的distilbert-base-uncased-finetuned-sst-2-english用這個
tokenizer = AutoTokenizer.from_pretrained(model_name)
def tokenize(batch):
    return tokenizer(batch["verse_text"], padding=True, truncation=True)
sentiment_encoded = sentiment.map(tokenize, batched=True, batch_size=None)
# next(iter(sentiment_encoded["train"])) #忘記這裡為什麼要用 next(iter())才能看到印出來的資料，可以回去看載入極巨大資料篇
281/8:
from transformers import AutoModelForSequenceClassification
import torch
model_name = "distilbert-base-uncased" # 第三天預設的distilbert-base-uncased-finetuned-sst-2-english用這個
num_labels = 4
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = (AutoModelForSequenceClassification
        .from_pretrained(model_name, num_labels=num_labels
        ,id2label={"0": "negative",
                    "1": "positive",
                    "2": "no_impact",
                    "3": "mixed"}
        ,label2id={"negative": "0",
                    "positive": "1",
                    "no_impact": "2",
                    "mixed": "3"})
         .to(device))
281/9:
from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(sentiment_encoded["train"]) // batch_size
model_name = "poem_model"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=40,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  label_names= labels,
                                  report_to = "azure_ml",
                                  logging_steps=logging_steps)
281/10:
from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}
281/11:
from transformers import Trainer

trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=sentiment_encoded["train"],
                  eval_dataset=sentiment_encoded["validation"],
                  tokenizer=tokenizer)
trainer.train()
281/12:
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model= "poem_model")
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
281/13:
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model= model)
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
281/14:
from transformers import Trainer

trainer = Trainer(model="./poem_model", args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=sentiment_encoded["train"],
                  eval_dataset=sentiment_encoded["validation"],
                  tokenizer=tokenizer)
trainer.train()
281/15:
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model="./poem_model")
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
281/16:
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model="poem_model", , tokenizer=tokenizer)
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
281/17:
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model="poem_model", tokenizer=tokenizer)
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
281/18:
#use a local pre-trained model, must git add to remote firstly.
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model="poem_model", tokenizer=tokenizer)
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
281/19:
#use a local pre-trained model, must git add to remote firstly.
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model="sinotec2/poem_model", tokenizer=tokenizer)
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
281/20:
#use a local pre-trained model, must git add to remote firstly.
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model="sinotec2/poem_model", tokenizer=tokenizer)
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
281/21:
#use a local pre-trained model, must git add to remote firstly.
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model="sinotec2/poem_model")
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
281/22:
#use a local pre-trained model, must git add to remote firstly.
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model="sinotec2/poem_model/checkpoint-500")
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
281/23:
#use a local pre-trained model, must git add to remote firstly.
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model="sinotec2/poem_model/checkpoint-500")
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
281/24:
#use a local pre-trained model, must git add to remote firstly.
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model="sinotec2/poem_model")
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
281/25:
#to use a local pre-trained model, must git add to remote firstly.
#1. create a new repo in HF_hub (say poem_model) 
#2. git clone  https://huggingface.co/sinotec2/poem_model (an empty repo)
#3. mv the pre-trained model to poem_model/. (without sub-direc.)
#4. git add .;git commit "...";git push
#5. THEN you can use pipeline to download the model(under specific username)
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model="sinotec2/poem_model")
classifier(
    [
        "Only those who will risk going too far can possibly find out how far one can go.",
        "Baby shark, doo doo doo doo doo doo, Baby shark!"
    ]
)
282/1:
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "gpt2-xl"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
282/2:
input_txt = "I have a pen, I have an "
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
iterations = []
n_steps = 10
choices_per_step = 3

with torch.no_grad():
    for _ in range(n_steps):
        iteration = dict()
        iteration["Input"] = tokenizer.decode(input_ids[0])
        output = model(input_ids)
        # 選最後一個 token 然後過 softmax 後選出機率最大
        next_token_logits = output.logits[0, -1, :]
        next_token_probs = torch.softmax(next_token_logits, dim=-1)
        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)
        
        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)
        iterations.append(iteration)

print(iterations[-1])
282/3:
max_length = 64
input_txt = """I have a pen, I have an iphone, I have a laptop. Thus,"""
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_length=max_length)
print(tokenizer.decode(output[0]))
282/4:
max_length = 128

### 昨天的句子太簡單不好玩，今天改成比特幣創世區塊的新聞稿
input_txt = """
Alistair Darling has been forced to consider a second bailout for banks as the lending drought worsens. \n
The Cancellor will decide tithin weeks whether to pump billions more into the economy as evidence mounts that \
the 37 billion part-nationalisation last yearr has failed to keep credit flowing,
"""
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_length=max_length, num_beams=1,  do_sample=False)
print(tokenizer.decode(output[0]))
282/5:
do_sample=True;top_p=0.95 
temperature=1.5
num_beams=1
no_repeat_ngram_size=5
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_length=max_length)
print(tokenizer.decode(output[0]))
282/6:

input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_length=max_length,do_sample=True,top_p=0.95,temperature=1.5,num_beams=1,no_repeat_ngram_size=5)
print(tokenizer.decode(output[0]))
283/1:
from transformers import BertTokenizerFast,AutoModelForCausalLM

tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')
model = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese').to(device)
283/2:
from transformers import BertTokenizerFast,AutoModelForCausalLM

import torch
device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')
model = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese').to(device)
283/3:
max_length=256
input_txt = """
隨著貸款日益枯竭，Alistair Darling 被迫考慮對銀行進行第二次救助。 \
財政大臣將在幾週內決定是否向經濟中再注入數十億美元，因為有證據表明\
去年 370 億的部分國有化未能保持信貸流動，
"""
283/4:
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_length=max_length, num_beams=1,  do_sample=True, top_k=50)
print(tokenizer.decode(output[0]))
283/5:
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_length=max_length, num_beams=1,  do_sample=True, top_k=50)
print(tokenizer.decode(output[0].replace(' ','')))
283/6:
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_length=max_length, num_beams=1,  do_sample=True, top_k=50)
print(tokenizer.decode(str(output[0]).replace(' ','')))
283/7:
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_length=max_length, num_beams=1,  do_sample=True, top_k=50)
print(tokenizer.decode(output[0]))
283/8:
from transformers import pipeline

pipe = pipeline("summarization", model=model)
result = pipe(input_text)
result
283/9:
from transformers import pipeline

pipe = pipeline("summarization", model=model,tokenizer=tokenizer)
result = pipe(input_text)
result
283/10:
from transformers import pipeline

pipe = pipeline("summarization", model=model,tokenizer=tokenizer)
result = pipe(input_txt)
result
283/11:
from transformers import pipeline

pipe = pipeline("summarization", model=model,tokenizer=tokenizer)
result = pipe(input_txt)
result
283/12: print (pipe(tokenizer.decode(output[0])))
283/13:
pipe_pegasus = pipeline("summarization", model="google/pegasus-cnn_dailymail")
result_pegasus = pipe_pegasus(input_txt)
result_pegasus
283/14:
pipe_pegasus = pipeline("summarization", model="google/pegasus-cnn_dailymail",tokenizer=tokenizer)
result_pegasus = pipe_pegasus(input_txt)
result_pegasus
283/15:
input_text="""
Alistair Darling has been forced to consider a second bailout for banks as the lending drought worsens. 

The Cancellor will decide tithin weeks whether to pump billions more into the economy as evidence mounts that the 37 billion part-nationalisation last yearr has failed to keep credit flowing,

Mr Darling, the former Liberal Democrat chancellor, admitted that the situation had become critical but insisted that there was still time to turn things around. 

He told the BBC that the crisis in the banking sector was the most serious problem facing the economy but also highlighted other issues, such as the falling value of sterling and the threat of inflation. 

"The worst fears about the banking crisis seem not to be panning out," he said, adding that there had not been a single banker arrested or charged over the crash. 

"The economy, the economy"

Mr Darling said "there's been a very, very strong recovery" since the autumn of 2008.

"There are very big problems ahead of us, not least of which is inflation. It is likely to be a very high inflation rate. "

The economy is expected to grow by 0.3% in the quarter to the end of this year.
"""
283/16:
pipe_pegasus = pipeline("summarization", model="google/pegasus-cnn_dailymail")
result_pegasus = pipe_pegasus(input_txt)
result_pegasus
283/17:
pipe_pegasus = pipeline("summarization", model="google/pegasus-cnn_dailymail",tokenizer=tokenizer)
result_pegasus = pipe_pegasus(input_txt)
result_pegasus
283/18:
pipe_pegasus = pipeline("summarization", model="google/pegasus-cnn_dailymail",tokenizer=tokenizer)
result_pegasus = pipe_pegasus(input_text)
result_pegasus
283/19:
pipe_pegasus = pipeline("summarization", model="google/pegasus-cnn_dailymail")
result_pegasus = pipe_pegasus(input_text)
result_pegasus
283/20:
pipe_pegasus = pipeline("summarization", model="google/pegasus-cnn_dailymail",tokenizer=tokenizer)
result_pegasus = pipe_pegasus(input_text)
result_pegasus
283/21:
pipe_pegasus = pipeline("summarization", model="google/t5_11b_trueteacher_and_anli")
result_pegasus = pipe_pegasus(input_text)
result_pegasus
283/22:
pipe_pegasus = pipeline("summarization", model="google/flan-t5-base ")
result_pegasus = pipe_pegasus(input_text)
result_pegasus
283/23:
pipe_pegasus = pipeline("summarization", model="google/flan-t5-base")
result_pegasus = pipe_pegasus(input_text)
result_pegasus
283/24:
result = pipe(input_txt)
result
284/1: !pip install rouge_score
284/2:
from datasets import load_metric

rouge_metric = load_metric("rouge")
284/3: txt='隨 著 貸 款 日 益 枯 竭 ， 被 迫 考 慮 對 銀 行 進 行 第 二 次 救 助 。 財 政 大 臣 將 在 幾 週 內 決 定 是 否 向 經 濟 中 再 注 入 數 十 億 美 元 ， 因 為 有 證 據 表 明 去 年 370 億 的 部 分 國 有 化 未 能 保 持 信 貸 流 動 ， 此'
284/4:
input_text = """
隨著貸款日益枯竭，Alistair Darling 被迫考慮對銀行進行第二次救助。 \
財政大臣將在幾週內決定是否向經濟中再注入數十億美元，因為有證據表明\
去年 370 億的部分國有化未能保持信貸流動，
"""
paragraph_result_T5='隨 著 貸 款 日 益 枯 竭 ， 被 迫 考 慮 對 銀 行 進 行 第 二 次 救 助 。 財 政 大 臣 將 在 幾 週 內 決 定 是 否 向 經 濟 中 再 注 入 數 十 億 美 元 ， 因 為 有 證 據 表 明 去 年 370 億 的 部 分 國 有 化 未 能 保 持 信 貸 流 動 ， 此'
284/5:

scores = rouge_metric.compute(
    predictions=[paragraph_result_T5], references=[input_text]
)
print(scores)
284/6:
dataset_url = "https://huggingface.co/datasets/gopalkalpande/bbc-news-summary/raw/main/bbc-news-summary.csv"
from datasets import load_dataset
remote_dataset = load_dataset("csv", data_files=dataset_url)
import pandas as pd

remote_dataset.set_format(type="pandas")

df = remote_dataset["train"][:]

df.head(10)
284/7:
remote_dataset.reset_format()
train_dataset = remote_dataset.shuffle(seed=5566)
from datasets import DatasetDict

train_test_dataset = train_dataset['train'].train_test_split(test_size=0.1)

test_valid = train_test_dataset['test'].train_test_split(test_size=0.5)

train_test_valid_dataset = DatasetDict({
    'train': train_test_dataset['train'],
    'test': test_valid['test'],
    'valid': test_valid['train']})

train_test_valid_dataset
284/8:
from transformers import AutoModelForSeq2SeqLM,AutoTokenizer
import torch

model_name = "google/pegasus-cnn_dailymail"
device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
tokenizer = AutoTokenizer.from_pretrained(model_name)
284/9:
from transformers import AutoModelForSeq2SeqLM,AutoTokenizer
import torch

model_name = "google/flan-t5-base"
device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
tokenizer = AutoTokenizer.from_pretrained(model_name)
284/10:
from transformers import Seq2SeqTrainingArguments, trainer

model_saved_name = model_name.split("/")[-1] 

args = Seq2SeqTrainingArguments( 
    output_dir=f"{model_name}-finetuned", 
    num_train_epochs=1, 
    warmup_steps=100,
    per_device_train_batch_size=1, 
    per_device_eval_batch_size=1,
    weight_decay=0.01, 
    logging_steps=10,
    evaluation_strategy='steps',
    eval_steps=100, 
    save_steps=1e6,
    gradient_accumulation_steps=64,
    report_to="azure_ml"
)
284/11:
from transformers import DataCollatorForSeq2Seq

seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
284/12:
import nltk
from nltk.tokenize import sent_tokenize

nltk.download("punkt")
284/13:
from datasets import load_metric

rouge_metric = load_metric("rouge")
284/14:
import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # 這裡把 DataCollatorForSeq2Seq 會填入的 -100 排除掉
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]

    result = rouge_metric.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
284/15:
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset= dataset_pt["train"],
    eval_dataset = dataset_pt["valid"],
    data_collator=seq2seq_data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()
284/16:
def convert_dataset(dataset):
    input_encodings = tokenizer(dataset["Articles"], max_length=1024,
                                truncation=True)

    with tokenizer.as_target_tokenizer():
        target_encodings = tokenizer(dataset["Summaries"], max_length=128,
                                     truncation=True)

    return {"input_ids": input_encodings["input_ids"],
            "attention_mask": input_encodings["attention_mask"],
            "labels": target_encodings["input_ids"]}

dataset_pt = train_test_valid_dataset.map(convert_dataset,
                                       batched=True)
columns = ["input_ids", "labels", "attention_mask"]
dataset_pt.set_format(type="torch", columns=columns)
284/17:
from transformers import Seq2SeqTrainingArguments, trainer

model_saved_name = model_name.split("/")[-1] 

args = Seq2SeqTrainingArguments( 
    output_dir=f"{model_name}-finetuned", 
    num_train_epochs=1, 
    warmup_steps=100,
    per_device_train_batch_size=1, 
    per_device_eval_batch_size=1,
    weight_decay=0.01, 
    logging_steps=10,
    evaluation_strategy='steps',
    eval_steps=100, 
    save_steps=1e6,
    gradient_accumulation_steps=64,
    report_to="azure_ml"
)
284/18:
from transformers import DataCollatorForSeq2Seq

seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
284/19:
import nltk
from nltk.tokenize import sent_tokenize

nltk.download("punkt")
284/20:
from datasets import load_metric

rouge_metric = load_metric("rouge")
284/21:
import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # 這裡把 DataCollatorForSeq2Seq 會填入的 -100 排除掉
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]

    result = rouge_metric.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
284/22:
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset= dataset_pt["train"],
    eval_dataset = dataset_pt["valid"],
    data_collator=seq2seq_data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()
284/23: trainer.evaluate()
284/24:
from transformers import Seq2SeqTrainingArguments, trainer

model_saved_name = model_name.split("/")[-1] 

args = Seq2SeqTrainingArguments( 
    output_dir=f"{model_name}-finetuned", 
    num_train_epochs=1, 
    warmup_steps=100,
    per_device_train_batch_size=10, 
    per_device_eval_batch_size=10,
    weight_decay=0.01, 
    logging_steps=10,
    evaluation_strategy='steps',
    eval_steps=100, 
    save_steps=1e6,
    gradient_accumulation_steps=64,
    report_to="azure_ml"
)
284/25:
from transformers import DataCollatorForSeq2Seq

seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
284/26:
import nltk
from nltk.tokenize import sent_tokenize

nltk.download("punkt")
284/27:
from datasets import load_metric

rouge_metric = load_metric("rouge")
284/28:
import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # 這裡把 DataCollatorForSeq2Seq 會填入的 -100 排除掉
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]

    result = rouge_metric.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
284/29:
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset= dataset_pt["train"],
    eval_dataset = dataset_pt["valid"],
    data_collator=seq2seq_data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()
284/30: trainer.evaluate()
286/1: !pip install protobuf==3.20.0 transformers>=4.27.1 icetk cpm_kernels torch==2.0.1
287/1:
from transformers import AutoTokenizer, AutoModel, AutoConfig
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
config = AutoConfig.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True, pre_seq_len=256)
model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True).half().cuda()

model = model.quantize(bits=8, kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"))
new_prefix_state_dict = {}
287/2:
or k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
287/3:
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
287/4:
from transformers import AutoTokenizer, AutoModel, AutoConfig
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
config = AutoConfig.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True, pre_seq_len=256)
model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True).half()

model = model.quantize(bits=8, kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"))
new_prefix_state_dict = {}
289/1:
from datasets import load_dataset
lawdb=load_dataset("LawBERT-tw/LawBERT_data)
289/2:
from datasets import load_dataset
lawdb=load_dataset("LawBERT-tw/LawBERT_data")
289/3: lawdb
289/4:
train_ds = lawdb["train"]
valid_ds = lawdb["validation"]
test_ds = lawdb["test"]
289/5:
import pandas as pd
df=lawdb.set_format(type="pandas")
df.head(10)
289/6: df
289/7:
import pandas as pd
df=lawdb.set_format(type="pandas")
289/8: df
289/9: df[0]
289/10: lawdb['law']
289/11: lawdb['law'][0]
289/12:
from transformers import TFAutoModel
tf_model = TFAutoModel.from_pretrained("LawBERT-tw/LawBERT-s", from_pt=True)
289/13:
from transformers import pipeline
classifier = pipeline(task= "a test", model='LawBERT-tw/LawBERT-s')
289/14:
from transformers import pipeline
classifier = pipeline(task= 'question-answering', model='LawBERT-tw/LawBERT-s')
289/15:
classifier([
        "中華民國境內強姦殺人罪責為何?"
    ])
289/16:
question = "什么是合同法？"
context = "合同法是一门法律学科，它规定了合同的法律规则和合同各方的权利和义务。合同是一种法律上的协议，其中一方同意提供某种商品、服务或履行某项义务，而另一方同意支付报酬或履行相应的义务。"
classifier(question=question, context=context)
289/17:
with open('title.txt','r') as f:
    lines=[i.strip('\n') for i in f]
Documents=''
for i in lines:
    Documents+=i
289/18: Documents[:100]
289/19: qa_pipeline = pipeline(task= "document-question-answering", model='LawBERT-tw/LawBERT-s')
289/20:
question = "檢測計畫？"
classifier(question=question, context=Documents)
289/21:
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline

# 加载 'LawBERT-tw/LawBERT-s' 模型和分词器
model_name = 'LawBERT-tw/LawBERT-s'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

# 创建一个问题回答（Question Answering）流程
qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer)

print("LawBot: 你好！我是法律机器人 LawBot。我可以回答与法律相关的问题。输入 '退出' 来结束对话。")

while True:
    user_input = input("User: ")

    if user_input.lower() == "退出":
        print("LawBot: 再见！")
        break

    # 使用问题回答模型回答用户的问题
    answer = qa_pipeline(context="法律文本或法规等相关文本", question=user_input)
    
    print("LawBot:", answer["answer"])
289/22:
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline

# 加载 'LawBERT-tw/LawBERT-s' 模型和分词器
model_name = 'LawBERT-tw/LawBERT-s'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

# 创建一个问题回答（Question Answering）流程
qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer)

print("LawBot: 你好！我是法律机器人 LawBot。我可以回答与法律相关的问题。输入 '退出' 来结束对话。")

while True:
    user_input = input("User: ")

    if user_input.lower() == "退出":
        print("LawBot: 再见！")
        break

    # 使用问题回答模型回答用户的问题
    answer = qa_pipeline(context=Documents, question=user_input)
    
    print("LawBot:", answer["answer"])
289/23: classifier = pipeline(task= 'mask-generation', model='LawBERT-tw/LawBERT-s')
289/24: classifier = pipeline(model='LawBERT-tw/LawBERT-s')
289/25: classifier("Paris is the [MASK] of France.")
289/26: classifier("辦法依空氣污染防制法（以下簡稱本法）第[MASK]規定訂定之",context=Documents)
289/27: classifier("辦法依空氣污染防制法（以下簡稱本法）第[MASK]規定訂定之")
289/28: classifier("公私場所應依空氣污染物排放檢測計畫（以下簡稱檢測計畫）執行固定污染源定期檢測，其取得方式如下[MASK]")
289/29: classifier("公私場所應依空氣污染物排放檢測計畫（以下簡稱檢測計畫）執行固定污染源定期檢測，其取得方式如下:[MASK]")
289/30: classifier("公私場所應依空氣污染物排放檢測計畫（以下簡稱[MASK]）執行固定污染源定期檢測，其取得方式如下:")
290/1: run cli.py --model-path lmsys/vicuna-7b-v1.5-16k
290/2: args
290/3: args.device
290/4: args.device='cpu'
290/5: main(args)
291/1: fname='2021res.csv'
291/2: from pandas import *
291/3: df=read_csv(fname)
291/4: df.head()
291/5: df[0]
291/6: df.loc[0]
291/7: df.NMHC.max
291/8: df.NMHC.max()
291/9: df.AMB_TEMP.max()
291/10: df.DEW_POINT.max()
291/11: df.SHELT_TEMP.max()
291/12: df.PRESSURE.max()
290/6: main(args)
293/1: run cli.py --model-path lmsys/vicuna-7b-v1.5-16k
293/2: args.device='cpu'
293/3: main(args)
294/1: run model_worker.py --model-path lmsys/vicuna-7b-v1.5-16k
294/2: args.device
294/3: device
296/1:
from transformers import AutoTokenizer, AutoModel, AutoConfig
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
config = AutoConfig.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True, pre_seq_len=256)
model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True,device=cpu).half().cuda()
296/2: model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True,device="cpu").half().cuda()
296/3: model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True).half()
296/4: model = model.quantize(bits=8, kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
296/5: model = model.quantize(bits=8, device="cpu", kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
296/6: model = model.quantize(bits=8, device="cpu", kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
296/7:
model = model.quantize(bits=8, device="cpu", kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"))
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
296/8:
import torch
model = model.quantize(bits=8, device="cpu", kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"))
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
296/9:
import torch,os
model = model.quantize(bits=8, device="cpu", kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"))
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
296/10:
import torch,os
model = model.quantize(bits=8, device="cpu", kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"))
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
296/11:
import torch,os
model = model.quantize(bits=8, device="cpu", kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"), map_location=torch.device('cpu'))
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
296/12:
model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)
model.transformer.prefix_encoder.float()
model = model.eval()

response, history = model.chat(tokenizer, "你好", history=[])
print(response)
296/13: model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True).half().cuda()
296/14: model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True)
296/15: model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True).half()
296/16:
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"), map_location=torch.device('cpu'))
model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True).half()
296/17:
from transformers import AutoTokenizer, AutoModel, AutoConfig
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
config = AutoConfig.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True, pre_seq_len=256)
296/18:
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"), map_location=torch.device('cpu'))
model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True).half()
296/19:
import torch,os
model = model.quantize(bits=8, device="cpu", kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"), map_location=torch.device('cpu'))
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
296/20:
import torch,os
model = model.quantize(bits=8, kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"), map_location=torch.device('cpu'))
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
296/21:
model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)
model.transformer.prefix_encoder.float()
model = model.eval()

response, history = model.chat(tokenizer, "你好", history=[])
print(response)
296/22:
from transformers import AutoTokenizer, AutoModel, AutoConfig
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
config = AutoConfig.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True, pre_seq_len=256)
296/23:
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"), map_location=torch.device('cpu'))
model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True)
296/24:
import torch,os
model = model.quantize(bits=8, kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
#prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"), map_location=torch.device('cpu'))
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
296/25:
import torch,os
model = model.quantize(bits=8, device="cpu",kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
#prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"), map_location=torch.device('cpu'))
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
296/26:
model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)
model.transformer.prefix_encoder.float()
model = model.eval()

response, history = model.chat(tokenizer, "你好", history=[])
print(response)
296/27:
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"), map_location=torch.device('cpu'))
model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True).half()
296/28:
from transformers import AutoTokenizer, AutoModel, AutoConfig
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
config = AutoConfig.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True, pre_seq_len=256)
296/29:
prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"), map_location=torch.device('cpu'))
model = AutoModel.from_pretrained("THUDM/chatglm-6b", config=config, trust_remote_code=True).half()
296/30:
import torch,os
model = model.quantize(bits=8, device="cpu",kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
#prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"), map_location=torch.device('cpu'))
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
296/31:
import torch,os
model = model.quantize(bits=8, kernel_file="xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b/quantization_kernels.so")
#prefix_state_dict = torch.load(os.path.join("xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b", "pytorch_model.bin"), map_location=torch.device('cpu'))
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
296/32:
model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)
model.transformer.prefix_encoder.float()
model = model.eval()
296/33:
response, history = model.chat(tokenizer, "你好", history=[])
print(response)
298/1:
from datasets import load_dataset
lawdb=load_dataset("LawBERT-tw/LawBERT_data")
298/2: lawdb
298/3:
import pandas as pd
df=lawdb.set_format(type="pandas")
298/4: lawdb['law'][0]
298/5:
from transformers import TFAutoModel
tf_model = TFAutoModel.from_pretrained("LawBERT-tw/LawBERT-s", from_pt=True)
298/6:
from transformers import pipeline
classifier = pipeline(task= 'question-answering', model='LawBERT-tw/LawBERT-s')
298/7:
question = "什么是合同法？"
context = "合同法是一门法律学科，它规定了合同的法律规则和合同各方的权利和义务。合同是一种法律上的协议，其中一方同意提供某种商品、服务或履行某项义务，而另一方同意支付报酬或履行相应的义务。"
classifier(question=question, context=context)
298/8:
with open('title.txt','r') as f:
    lines=[i.strip('\n') for i in f]
Documents=''
for i in lines:
    Documents+=i
298/9: Documents[:100]
298/10:
question = "what is 檢測計畫?"
classifier(question=question, context=Documents)
298/11:
question = "固定污染源自行或委託檢測及申報管理辦法修正日期?"
classifier(question=question, context=Documents)
298/12:
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline

# 加载 'LawBERT-tw/LawBERT-s' 模型和分词器
model_name = 'LawBERT-tw/LawBERT-s'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

# 创建一个问题回答（Question Answering）流程
qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer)

print("LawBot: 你好！我是法律机器人 LawBot。我可以回答与法律相关的问题。输入 '退出' 来结束对话。")

while True:
    user_input = input("User: ")

    if user_input.lower() == "退出":
        print("LawBot: 再见！")
        break

    # 使用问题回答模型回答用户的问题
    answer = qa_pipeline(context=Documents, question=user_input)
    
    print("LawBot:", answer["answer"])
300/1:
import matplotlib.pyplot as plt

# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名')
plt.ylabel('PM2.5年均值 (μg/m³)')
plt.title('台灣各站點PM2.5年均值')
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
300/2:
#! coding="utf7"
import matplotlib.pyplot as plt

# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名')
plt.ylabel('PM2.5年均值 (μg/m³)')
plt.title('台灣各站點PM2.5年均值')
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
300/3: print(matplotlib.__file__)
300/4:
import matplotlib
print(matplotlib.__file__)
301/1:
import matplotlib.pyplot as plt

# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名')
plt.ylabel('PM2.5年均值 (μg/m³)')
plt.title('台灣各站點PM2.5年均值')
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
301/2:
import matplotlib.font_manager
 
a = sorted([f.name for f in matplotlib.font_manager.fontManager.ttflist])
 
for i in a:
    print(i)
301/3:
import matplotlib.font_manager
 
a = sorted([f.name for f in matplotlib.font_manager.fontManager.ttflist])
 
for i in a:
    print(i)
301/4:
import matplotlib.pyplot as plt

# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名')
plt.ylabel('PM2.5年均值 (μg/m³)')
plt.title('台灣各站點PM2.5年均值')
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
302/1:
import matplotlib.pyplot as plt

# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名')
plt.ylabel('PM2.5年均值 (μg/m³)')
plt.title('台灣各站點PM2.5年均值')
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
302/2:
import matplotlib
print(matplotlib.matplotlib_fname())
302/3:
import matplotlib.pyplot as plt

# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名')
plt.ylabel('PM2.5年均值 (μg/m³)')
plt.title('台灣各站點PM2.5年均值')
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
302/4:
import matplotlib.pyplot as plt

# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名')
plt.ylabel('PM2.5年均值 (μg/m³)')
plt.title('台灣各站點PM2.5年均值')
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
302/5:
import matplotlib.pyplot as plt

# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名')
plt.ylabel('PM2.5年均值 (μg/m³)')
plt.title('台灣各站點PM2.5年均值')
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
302/6:
import matplotlib
import os

cache_dir = matplotlib.get_cachedir()
font_list_path = os.path.join(cache_dir, 'fontList.json')

print(f'fontList.json 文件的位置：{font_list_path}')
302/7:
import matplotlib.font_manager
 
a = sorted([f.name for f in matplotlib.font_manager.fontManager.ttflist])
 
for i in a:
    print(i)
302/8:
import matplotlib.pyplot as plt

# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名')
plt.ylabel('PM2.5年均值 (μg/m³)')
plt.title('台灣各站點PM2.5年均值')
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
303/1:
import matplotlib.pyplot as plt

# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名')
plt.ylabel('PM2.5年均值 (μg/m³)')
plt.title('台灣各站點PM2.5年均值')
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
303/2:
import matplotlib.font_manager
 
a = sorted([f.name for f in matplotlib.font_manager.fontManager.ttflist])
 
for i in a:
    print(i)
303/3:
import matplotlib
import os

cache_dir = matplotlib.get_cachedir()
font_list_path = os.path.join(cache_dir, 'fontList.json')

print(f'fontList.json 文件的位置：{font_list_path}')
303/4:
import matplotlib.pyplot as plt

font1 = font(fname="~/.local/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/TaipeiSansTCBeta-Bold.ttf")
# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名',font=font1)
plt.ylabel('PM2.5年均值 (μg/m³)',font=font1)
plt.title('台灣各站點PM2.5年均值',font=font1)
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
303/5:
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties as font

font1 = font(fname="~/.local/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/TaipeiSansTCBeta-Bold.ttf")
# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名',font=font1)
plt.ylabel('PM2.5年均值 (μg/m³)',font=font1)
plt.title('台灣各站點PM2.5年均值',font=font1)
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
303/6:
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties as font

font1 = font(fname=".local/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/TaipeiSansTCBeta-Bold.ttf")
# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名',font=font1)
plt.ylabel('PM2.5年均值 (μg/m³)',font=font1)
plt.title('台灣各站點PM2.5年均值',font=font1)
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
303/7:
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties as font

font1 = font(fname=".local/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/TaipeiSansTCBeta-Bold.ttf")
# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue',font=font1)
plt.xlabel('站名',font=font1)
plt.ylabel('PM2.5年均值 (μg/m³)',font=font1)
plt.title('台灣各站點PM2.5年均值',font=font1)
plt.xticks(rotation=45)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
303/8:
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties as font

font1 = font(fname=".local/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/TaipeiSansTCBeta-Bold.ttf")
# 數據
stations = ["恆春", "潮州", "林園", "小港", "左營", "楠梓", "橋頭", "台南", "新營", "斗六", "二林", "彰化"]
pm25_values = [6.7, 9.3, 10.1, 11.4, 12.4, 12.8, 13.0, 13.1, 13.3, 14.0, 14.5, 15.5]

# 繪製長條圖
plt.figure(figsize=(10, 6))
plt.bar(stations, pm25_values, color='skyblue')
plt.xlabel('站名',font=font1)
plt.ylabel('PM2.5年均值 (μg/m³)',font=font1)
plt.title('台灣各站點PM2.5年均值',font=font1)
plt.xticks(rotation=45,font=font1)  # 如果站名太長，可以旋轉x軸標籤
plt.tight_layout()

# 顯示圖表
plt.show()
304/1: import netCDF4
304/2: fname='zero.nc'
304/3: nc = netCDF4.Dataset(fname,'r+')
304/4: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
304/5: V
304/6: nc.close()
304/7: nc = netCDF4.Dataset(fname,'r+')
304/8: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
304/9: V
304/10: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
304/11:  nt,nlay,nrow,ncol
304/12: cd /nas2/cmaqruns/2022fcst/grid03/cctm.fcst/daily
304/13: nc.close()
304/14: nc = netCDF4.Dataset(fname,'r+')
304/15: nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
304/16: nt,nlay,nrow,ncol
304/17: v='TFLAG'
304/18: nc[v].shape
304/19: v='PM10'
304/20: nc.close()
304/21: nc = netCDF4.Dataset(fname,'r+')
304/22: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
304/23: V
304/24: pwd
304/25: nc.close()
304/26: nc = netCDF4.Dataset(fname,'r+')
304/27: V
304/28: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
304/29: V
304/30: v='PM25_TOT'
304/31: nc[v][:]=0.
304/32: nc.close()
304/33: nc = netCDF4.Dataset(fname,'r+')
304/34: nc[v][:]=0.
304/35: nc.close()
304/36: nc = netCDF4.Dataset(fname,'r+')
304/37: nc[v][:]=0.001
304/38: nc.close()
304/39:
import numpy as np
import cv2
import os, sys
from PIL import Image
304/40: image=cv2.imread('output_AVPM25_20231001140000.png')
304/41: ny,nx,nz=image.shape
304/42: ny,nx,nz
304/43: !lst
304/44: imageFrame=cv2.imread('PM25_TOT_000.png')
304/45: imageFrame.shape
304/46:
from PIL import Image

# 打开PNG图像文件
image_path = 'PM25_TOT_000.png'  # 替换为您的PNG图像文件路径
image = Image.open(image_path)

# 将图像转换为RGB模式（如果不是的话）
image = image.convert('RGB')

# 获取图像的所有像素点
pixels = list(image.getdata())

# 创建一个空集合来存储颜色
color_set = set()

# 遍历每个像素，将颜色添加到集合中
for pixel in pixels:
    color_set.add(pixel)

# 打印颜色集合
for color in color_set:
    print(color)

# 打印颜色数量
print("颜色数量:", len(color_set))
304/47: pixels.shape
304/48: image.shape
304/49: image = Image.open(image_path)
304/50: image.shape
304/51: image = image.convert('RGB')
304/52: a=image.getdata()
304/53: a.shape
304/54: a=np.array(image.getdata())
304/55: a.shape
304/56: ny,nx,nz
304/57: 376*200
304/58: ny,nx,nz=imageFrame.shape
304/59: ny,nx,nz
304/60: 740*611
304/61: a=a.reshape(ny,nx,nz)
304/62: a.shape
304/63: pixels = list(a[:,100,3])
304/64: pixels = list(a[:,:100,:])
304/65:
color_set = set()

# 遍历每个像素，将颜色添加到集合中
for pixel in pixels:
    color_set.add(pixel)

# 打印颜色集合
for color in color_set:
    print(color)

# 打印颜色数量
print("颜色数量:", len(color_set))
这段代码将打开PNG图像文件，将其转换为RGB模式（如果不是的话），然后遍历图像的所有像素，将颜色添加到一个集合中。最后，它会打印颜色集合和颜色的数量。

请替换image_path变量为您要分析的PNG图像的文件路径。这样，您就可以找到PNG图像的颜色集合。
304/66:
color_set = set()

# 遍历每个像素，将颜色添加到集合中
for pixel in pixels:
    color_set.add(pixel)

# 打印颜色集合
for color in color_set:
    print(color)
304/67: pixels.shape
304/68: pixels[:5]
304/69: len(pixels)
304/70: pixels = list(a[:,:100,:].flatten().reshape(ny*100,3))
304/71: len(pixels)
304/72:
color_set = set()

# 遍历每个像素，将颜色添加到集合中
for pixel in pixels:
    color_set.add(pixel)

# 打印颜色集合
for color in color_set:
    print(color)
304/73: pixcel
304/74: pixcels[:5]
304/75: pixcels[:5]
304/76: pixels[:5]
304/77: pixels=[list(i) for i in pixels]
304/78:
color_set = set()

# 遍历每个像素，将颜色添加到集合中
for pixel in pixels:
    color_set.add(pixel)

# 打印颜色集合
for color in color_set:
    print(color)
304/79: pixels[:5]
304/80:
image = Image.open(image_path)

# 将图像转换为RGB模式（如果不是的话）
image = image.convert('RGB')

# 获取图像的所有像素点
pixels1 = list(image.getdata())
304/81: pixels1[:5]
304/82: pixels=[tuple(i) for i in pixels]
304/83:
color_set = set()

# 遍历每个像素，将颜色添加到集合中
for pixel in pixels:
    color_set.add(pixel)

# 打印颜色集合
for color in color_set:
    print(color)
304/84: nx
304/85: len(color_set)
304/86: pixels1[:5]
304/87: white=(255, 255, 255)
304/88: image.shape
304/89: image=cv2.imread('output_AVPM25_20231001140000.png')
304/90: ny2,nx2,nz2=image.shape
304/91:
from PIL import Image

# 打开PNG图像文件
image_path = 'PM25_TOT_000.png'  # 替换为您的PNG图像文件路径
image = Image.open(image_path)

# 将图像转换为RGB模式（如果不是的话）
image = image.convert('RGB')

# 获取图像的所有像素点
pixels = list(image.getdata())

# 创建一个空集合来存储颜色
color_set = set()

# 遍历每个像素，将颜色添加到集合中
for pixel in pixels:
    color_set.add(pixel)

# 打印颜色集合
for color in color_set:
    print(color)

# 打印颜色数量
print("颜色数量:", len(color_set))
304/92: pixels1[:5]
304/93: pixels[:5]
304/94: a=np.array(pixels).reshape(ny,nx)
304/95: len(pixels)
305/1:
from PIL import Image

# 打开PNG图像文件
image_path = 'PM25_TOT_000.png'  # 替换为您的PNG图像文件路径
image = Image.open(image_path)

# 将图像转换为RGB模式（如果不是的话）
image = image.convert('RGB')

# 获取图像的所有像素点
pixels = list(image.getdata())

# 创建一个空集合来存储颜色
color_set = set()

# 遍历每个像素，将颜色添加到集合中
for pixel in pixels:
    color_set.add(pixel)

# 打印颜色集合
for color in color_set:
    print(color)

# 打印颜色数量
print("颜色数量:", len(color_set))
305/2: len(pixels)
305/3:
import numpy as np
import cv2
import os, sys
from PIL import Image
305/4: imageFrame=cv2.imread('PM25_TOT_000.png')
305/5: image=cv2.imread('output_AVPM25_20231001140000.png')
305/6: pixels[:5]
305/7: idx=np.where(np.sum(imageFrame,axis=2)==255*3)
305/8: len(idx[0])
305/9: ny2,nx2,nz2=image.shape
305/10: iy2=idx[0]<ny2
305/11: ix2=idx[1]<nx2
305/12: imageFrame[iy2,ix2,:]=image[iy2,ix2,:]
305/13: iy2[:5]
305/14: idx[0][:5]
305/15: idx[1][:5]
305/16: imageFrame=cv2.imread('PM25_TOT_000.png')
305/17: ix2=[],iy2=[]
305/18: ix2=[];iy2=[]
305/19:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i])
        iy2.append(idx[0][i])
305/20: len(ix2)
305/21: ix2=np.array(ix2)
305/22: iy2=np.array(iy2)
305/23: imageFrame[iy2,ix2,:]=image[iy2,ix2,:]
305/24: cv2.imwrite("NCLonOTM.png",imageFrame)
305/25: pwd
305/26: ny,nx,nz=imageFrame.shape
305/27: ny,nx,nz
305/28: 740*10.2/14.4
305/29: 611*10.2/14.4
305/30: !lst
305/31: ls -lrth *pnt|tail
305/32: ls -lrth *png|tail
305/33: !convert PM25_TOT_000.png -resize 524X432 PM25_TOT_0001.png
305/34: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/35: idx=np.where(np.sum(imageFrame,axis=2)==255*3)
305/36: ix2=[];iy2=[]
305/37:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i])
        iy2.append(idx[0][i])
305/38: imageFrame[iy2,ix2,:]=image[iy2,ix2,:]
305/39: cv2.imwrite("NCLonOTM.png",imageFrame)
305/40: ny,nx,nz=imageFrame.shape
305/41: ny,nx,nz
305/42: !convert PM25_TOT_000.png -resize 432X524 PM25_TOT_0001.png
305/43: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/44: idx=np.where(np.sum(imageFrame,axis=2)==255*3)
305/45: ix2=[];iy2=[]
305/46:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i])
        iy2.append(idx[0][i])
305/47: imageFrame[iy2,ix2,:]=image[iy2,ix2,:]
305/48: cv2.imwrite("NCLonOTM.png",imageFrame)
305/49: idx2=np.where(np.sum(image,axis=2)==255*3)
305/50: image[idx2[0],idx2[1],:]=0
305/51: imageFrame[iy2,ix2,:]=image[iy2,ix2,:]
305/52: cv2.imwrite("NCLonOTM.png",imageFrame)
305/53: idx2=np.where(np.sum(image,axis=2)==0)
305/54: image=cv2.imread('output_AVPM25_20231001140000.png')
305/55: image[idx2[0],idx2[1],:]=255
305/56: imageFrame[iy2,ix2,:]=image[iy2,ix2,:]
305/57: cv2.imwrite("NCLonOTM.png",imageFrame)
305/58: ny,nx,nz=imageFrame.shape
305/59: ny,nx,nz
305/60: 432*40/119
305/61: 524*15/144
305/62: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/63: imageFrame[iy2+54,ix2+145,:]=image[iy2,ix2,:]
305/64: iy2=np.array(iy2)+54
305/65: ix2=np.array(ix2)+145
305/66: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/67: imageFrame[iy2,ix2,:]=image[iy2,ix2,:]
305/68: ix2=[];iy2=[]
305/69:
for i in range(len(idx[0])):
    if idx[0][i]+54<ny2 and idx[1][i]+145<nx2:
        ix2.append(idx[1][i]+145)
        iy2.append(idx[0][i]+54)
305/70: imageFrame[iy2,ix2,:]=image[iy2,ix2,:]
305/71: cv2.imwrite("NCLonOTM.png",imageFrame)
305/72: ix2=[];iy2=[];ix=[];iy=[]
305/73:
for i in range(len(idx[0])):
    if idx[0][i]+54<ny2 and idx[1][i]+145<nx2:
        ix2.append(idx[1][i]+145)
        iy2.append(idx[0][i]+54)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/74: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/75: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/76: cv2.imwrite("NCLonOTM.png",imageFrame)
305/77: ix2=[];iy2=[];ix=[];iy=[]
305/78:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i]+145)
        iy2.append(idx[0][i]+54)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/79: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/80: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/81: cv2.imwrite("NCLonOTM.png",imageFrame)
305/82: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/83: image=cv2.imread('output_AVPM25_20231001140000.png')
305/84: idx2=np.where(np.sum(image,axis=2)==0)
305/85: image[idx2[0],idx2[1],:]=255
305/86:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        if np.sum(image[idx[0][i],idx[1][i],:])==255*3:continue
        ix2.append(idx[1][i]+145)
        iy2.append(idx[0][i]+54)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/87: ix2=[];iy2=[];ix=[];iy=[]
305/88:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        if np.sum(image[idx[0][i],idx[1][i],:])==255*3:continue
        ix2.append(idx[1][i]+145)
        iy2.append(idx[0][i]+54)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/89: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/90: cv2.imwrite("NCLonOTM.png",imageFrame)
305/91: image=cv2.imread('output_AVPM25_20231001140000.png')
305/92: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/93: idx=np.where(np.sum(imageFrame[54:,145:,:],axis=2)==255*3)
305/94: ix2=[];iy2=[];ix=[];iy=[]
305/95:
for i in range(len(idx[0])):
    if idx[0][i]+54<ny2 and idx[1][i]+145<nx2:
        ix2.append(idx[1][i]+145)
        iy2.append(idx[0][i]+54)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/96: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/97: cv2.imwrite("NCLonOTM.png",imageFrame)
305/98: idx2=np.where(np.sum(image,axis=2)==0)
305/99: image[idx2[0],idx2[1],:]=255
305/100: ix2=[];iy2=[];ix=[];iy=[]
305/101:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i]+145)
        iy2.append(idx[0][i]+54)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/102: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/103: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/104: cv2.imwrite("NCLonOTM.png",imageFrame)
305/105: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/106: idx=np.where(np.sum(imageFrame[54:,141:,:],axis=2)==255*3)
305/107: ix2=[];iy2=[];ix=[];iy=[]
305/108:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i]+145)
        iy2.append(idx[0][i]+54)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/109: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/110: cv2.imwrite("NCLonOTM.png",imageFrame)
305/111: ix2=[];iy2=[];ix=[];iy=[]
305/112: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/113:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i]+141)
        iy2.append(idx[0][i]+54)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/114: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/115: cv2.imwrite("NCLonOTM.png",imageFrame)
305/116: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/117: ix2=[];iy2=[];ix=[];iy=[]
305/118: idx=np.where(np.sum(imageFrame[56:,141:,:],axis=2)==255*3)
305/119:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i]+141)
        iy2.append(idx[0][i]+56)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/120: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/121: cv2.imwrite("NCLonOTM.png",imageFrame)
305/122: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/123: ix2=[];iy2=[];ix=[];iy=[]
305/124: idx=np.where(np.sum(imageFrame[56:,143:,:],axis=2)==255*3)
305/125:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i]+143)
        iy2.append(idx[0][i]+56)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/126: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/127: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/128: idx=np.where(np.sum(imageFrame[58:,143:,:],axis=2)==255*3)
305/129: ix2=[];iy2=[];ix=[];iy=[]
305/130:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i]+143)
        iy2.append(idx[0][i]+58)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/131: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/132: cv2.imwrite("NCLonOTM.png",imageFrame)
305/133: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/134: idx=np.where(np.sum(imageFrame[57:,143:,:],axis=2)==255*3)
305/135: ix2=[];iy2=[];ix=[];iy=[]
305/136:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i]+143)
        iy2.append(idx[0][i]+57)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/137: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/138: cv2.imwrite("NCLonOTM.png",imageFrame)
305/139: history
305/140: !vi postEPA.py
305/141: !vi postEPA.py
305/142: !vi postEPA.py
305/143: !vi postEPA.py
305/144: idx=np.where(np.sum(imageFrame[57:,143:,:],axis=2)!=0)
305/145: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/146: ix2=[];iy2=[];ix=[];iy=[]
305/147:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i]+143)
        iy2.append(idx[0][i]+57)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/148: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/149: cv2.imwrite("NCLonOTM.png",imageFrame)
305/150: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/151: imageFrame[ny//2:ny//2+10,nx//2:xy//2+10;0]
305/152: imageFrame[ny//2:ny//2+10,nx//2:xy//2+10,0]
305/153: imageFrame[ny//2:ny//2+10,nx//2:nx//2+10,0]
305/154:
s=set()
for i in range(10):
    for j in range(10):
        s=s||set(tuple(imageFrame[ny//2:ny//2+i,nx//2:nx//2+j,:]))
305/155:
s=set()
for i in range(10):
    for j in range(10):
        s=s|set(tuple(imageFrame[ny//2:ny//2+i,nx//2:nx//2+j,:]))
305/156:
s=set()
for i in range(10):
    for j in range(10):
        s=s|set(tuple(imageFrame[ny//2+i,nx//2+j,:]))
305/157: s
305/158: s=set()
305/159:
s=set()
for i in range(10):
    for j in range(10):
        s.add(tuple(imageFrame[ny//2+i,nx//2+j,:]))
305/160: s
305/161:
s=set()
for i in range(50):
    for j in range(50):
        s.add(tuple(imageFrame[ny//2+i,nx//2+j,:]))
305/162: s
305/163: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/164: idx=np.where(np.sum(imageFrame[57:,143:,:],axis=2)>=200*3)
305/165: ix2=[];iy2=[];ix=[];iy=[]
305/166:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i]+143)
        iy2.append(idx[0][i]+57)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/167: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/168: cv2.imwrite("NCLonOTM.png",imageFrame)
305/169: idx3=np.where(np.sum(imageFrame,axis=2)<200*3)
305/170: ix2=[];iy2=[];ix=[];iy=[]
305/171:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        if np.sum(image[idx[0][i],idx[1][i],:])==255*3:continue
        ix2.append(idx[1][i]+143)
        iy2.append(idx[0][i]+57)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/172: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/173: cv2.imwrite("NCLonOTM.png",imageFrame)
305/174: ix2=[];iy2=[];ix=[];iy=[]
305/175:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        ix2.append(idx[1][i]+143)
        iy2.append(idx[0][i]+57)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/176: len(ix)
305/177: ix2=[];iy2=[];ix=[];iy=[]
305/178:
for i in range(len(idx[0])):
    if idx[0][i]<ny2 and idx[1][i]<nx2:
        if np.sum(image[idx[0][i],idx[1][i],:])==255*3:continue
        ix2.append(idx[1][i]+143)
        iy2.append(idx[0][i]+57)
        iy.append(idx[0][i])
        ix.append(idx[1][i])
305/179: len(ix)
305/180: imageFrame=cv2.imread('PM25_TOT_0001.png')
305/181: imageFrame[iy2,ix2,:]=image[iy,ix,:]
305/182: cv2.imwrite("NCLonOTM.png",imageFrame)
305/183: history
305/184: !vi postEPA.py
305/185: !vi postEPA.py
305/186: run postEPA.py
305/187: ny2,nx2,nz2=image.shape
305/188: !vi postEPA.py
305/189: run postEPA.py
305/190: !vi postEPA.py
305/191: !vi postEPA.py
305/192: exit5
306/1: import json
306/2:
import json
import codecs

# 讀取檔案
with open('2023-10-17-conv.json', encoding='big5') as f:
    data = json.load(f)

# 列印內容
for item ineler in data:
    print(item)
306/3: data
306/4:
with open('2023-10-17-conv.json', encoding='big5') as f:
    data = json.load(f)
306/5: import codecs
306/6: !cat 2023-10-17-conv.json
306/7:
with open('2023-10-17-conv.json') as f:
    data = json.load(f)
306/8:
with open('2023-10-17-conv.json','r') as f:
    data = json.load(f)
306/9:
with open('2023-10-17-conv.json','r') as f:
    data = [i for i in f]
306/10: d1=json.load(data[0])
306/11: data[0]
306/12: print data[0]
306/13: print(data[0])
306/14: print(data[0].encode('big5'))
306/15: print(data[0].decode('big5'))
306/16:
with open('2023-10-17-conv.json',encoding='big5') as f:
    data = [i for i in f]
306/17: data[0]
306/18:
with open('2023-10-17-conv.json',encoding='utf8') as f:
    data = [i for i in f]
306/19: data[0]
306/20: a="\\u4f60\\u6703\\u64b0\\u5beb\\u516c\\u53f8\\u5167\\u90e8\\u7c3d\\u5448\\u55ce?".encoding('big5')
306/21: a="\\u4f60\\u6703\\u64b0\\u5beb\\u516c\\u53f8\\u5167\\u90e8\\u7c3d\\u5448\\u55ce?".decoding('big5')
306/22: "\\u4f60\\u6703\\u64b0\\u5beb\\u516c\\u53f8\\u5167\\u90e8\\u7c3d\\u5448\\u55ce?".decoding('big5')
306/23: !cat 2023-10-17-conv.json
306/24: a="\u4f60\u6703\u64b0\u5beb\u516c\u53f8\u5167\u90e8\u7c3d\u5448\u55ce?"
306/25: print (a)
306/26: b="\u662f\u7684\uff0c\u6211\u53ef\u4ee5\u5e6b\u60a8\u64b0\u5beb\u516c\u53f8\u5167\u90e8\u7c3d\u5448\u3002\u8acb\u60a8\u63d0\u4f9b\u76f8\u95dc\u7684\u5167\u5bb9\u548c\u9700\u6c42\uff0c\u4ee5\u53ca\u60a8\u7684\u516c\u53f8\u7684\u7279\u5b9a\u8981\u6c42\u548c\u898f\u5b9a\uff0c\u6211\u5c07\u6839\u64da\u9019\u4e9b\u4fe1\u606f\u70ba\u60a8\u64b0\u5beb\u4e00\u4efd\u7b26\u5408\u516c\u53f8\u9700\u6c42\u7684\u5167\u90e8\u7c3d\u5448\u3002"
306/27: print(b)
306/28: c="\u662f\u7684\uff0c\u6211\u53ef\u4ee5\u5e6b\u60a8\u64b0\u5beb\u516c\u53f8\u5167\u90e8\u7c3d\u5448\u3002\u8acb\u60a8\u63d0\u4f9b\u76f8\u95dc\u7684\u5167\u5bb9\u548c\u9700\u6c42\uff0c\u4ee5\u53ca\u60a8\u7684\u516c\u53f8\u7684\u7279\u5b9a\u8981\u6c42\u548c\u898f\u5b9a\uff0c\u6211\u5c07\u6839\u64da\u9019\u4e9b\u4fe1\u606f\u70ba\u60a8\u64b0\u5beb\u4e00\u4efd\u7b26\u5408\u516c\u53f8\u9700\u6c42\u7684\u5167\u90e8\u7c3d\u5448\u3002"], ["USER", "6W\u7684\u5167\u5bb9\u3002\u4eba:\u884c\u653f\u90e8\u694a\u4f73\u7dba\u3001\u9b4f\u5609\u52a9\u3001\u66e0\u6c38\u9293\u3001\u4ee5\u53ca\u6703\u8a08\u63a1\u8cfc\u4eba\u54e1\u7b49\uff0c\u4e8b\uff1a\u8fa6\u7406\u7db2\u9801\u4f3a\u670d\u5668\u8edf\u9ad4\u66f4\u65b0\uff0c\u820a\u7684\u7248\u672c\u5df2\u7d93\u4e0d\u518d\u652f\u63f4\uff0c\u7db2\u9801\u8207\u5c0d\u61c9\u7a0b\u5f0f\u90fd\u9700\u8981\u6539\u5beb\u3001\u642c\u79fb\uff0c\u6642\u9593\uff1a\u642c\u79fb\u671f\u9593\u7d04\u97002\u500b\u6708\u9032\u884c\u6e2c\u8a66\u3001\u6539\u5beb\uff0c\u5730\u9ede\uff1a\u672c\u516c\u53f8\u5167\u4f5c\u696d\uff0c\u7269\uff1a\u5982\u65b0\u7248\u672c\u78ba\u8a8d\u53ef\u884c\uff0c\u5c07\u53e6\u884c\u63a1\u8cfc\u8edf\u9ad4\u6388\u6b0a\uff0c\u7d04\u9700\u6bcf\u5e7420\u842c\u5143\u3002\u8acb\u5206\u6210\u4e3b\u65e8\u3001\u8aaa\u660e\u3001\u64ec\u8fa6\u7b493\u500b\u6bb5\u843d\u64b0\u5beb\u3002\u5b57\u6578\u9650\u5236\u5728200\u5b57\u5167\u3002"], ["ASSISTANT", "\u4e3b\u65e8\uff1a\n----\n\n\u672c\u5167\u90e8\u7c3d\u5448\u70ba\u516c\u53f8\u8fa6\u7406\u7db2\u9801\u4f3a\u670d\u5668\u8edf\u9ad4\u66f4\u65b0\u63d0\u51fa\uff0c\u4ee5\u78ba\u4fdd\u516c\u53f8\u7db2\u9801\u7cfb\u7d71\u7684\u6b63\u5e38\u904b\u4f5c\u3002\u820a\u7248\u672c\u5df2\u7d93\u4e0d\u518d\u652f\u63f4\uff0c\u56e0\u6b64\u9700\u8981\u9032\u884c\u6539\u5beb\u548c\u642c\u79fb\u3002\u642c\u79fb\u671f\u9593\u7d04\u97002\u500b\u6708\u9032\u884c\u6e2c\u8a66\u548c\u6539\u5beb\uff0c\u4e26\u4e14\u53ef\u80fd\u9700\u8981\u6bcf\u5e74\u63a1\u8cfc\u8edf\u9ad4\u6388\u6b0a\uff0c\u7d04\u9700\u6bcf\u5e7420\u842c\u5143\u3002\n\n\u8aaa\u660e\uff1a\n----\n\n\u8fd1\u671f\u767c\u73fe\u7db2\u9801\u4f3a\u670d\u5668\u8edf\u9ad4\u5df2\u7d93\u4e0d\u518d\u652f\u63f4\uff0c\u56e0\u6b64\u9700\u8981\u9032\u884c\u820a\u7248\u672c\u7684\u6539\u5beb\u548c\u642c\u79fb\u3002\u9019\u5c07\u5c0d\u516c\u53f8\u7db2\u9801\u7cfb\u7d71\u9020\u6210\u5f71\u97ff\uff0c\u56e0\u6b64\u9700\u8981\u8fa6\u7406\u7db2\u9801\u4f3a\u670d\u5668\u8edf\u9ad4\u66f4\u65b0\u3002\u5728\u9032\u884c\u66f4\u65b0\u524d\uff0c\u9700\u8981\u9032\u884c2\u500b\u6708\u7684\u6e2c\u8a66\u548c\u6539\u5beb\uff0c\u4ee5\u78ba\u4fdd\u7cfb\u7d71\u7684\u6b63\u5e38\u904b\u4f5c\u3002\u53e6\u5916\uff0c\u5982\u679c\u65b0\u7248\u672c\u78ba\u8a8d\u53ef\u884c\uff0c\u5c07\u53e6\u884c\u63a1\u8cfc\u8edf\u9ad4\u6388\u6b0a\uff0c\u7d04\u9700\u6bcf\u5e7420\u842c\u5143\u3002\n\n\u64ec\u8fa6\uff1a\n----\n\n1. \u64b0\u5beb\u66f4\u65b0\u8a08\u756b\u66f8\uff0c\u5305\u62ec\u66f4\u65b0\u7684\u76ee\u7684\u3001\u6b65\u9a5f\u548c\u6642\u9593\u8868\u7b49\u3002\n2. \u8fa6\u7406\u65b0\u7248\u672c\u7684\u8cfc\u8cb7\u548c\u6388\u6b0a\u3002\n3. \u9032\u884c\u66f4\u65b0\u7684\u6e2c\u8a66\u548c\u6539\u5beb\uff0c\u78ba\u4fdd\u7cfb\u7d71"
306/29: c= "\u4f60\u6703\u64b0\u5beb\u516c\u53f8\u5167\u90e8\u7c3d\u5448\u55ce?"
306/30: print(c)
306/31: c="6W\u7684\u5167\u5bb9\u3002\u4eba:\u884c\u653f\u90e8\u694a\u4f73\u7dba\u3001\u9b4f\u5609\u52a9\u3001\u66e0\u6c38\u9293\u3001\u4ee5\u53ca\u6703\u8a08\u63a1\u8cfc\u4eba\u54e1\u7b49\uff0c\u4e8b\uff1a\u8fa6\u7406\u7db2\u9801\u4f3a\u670d\u5668\u8edf\u9ad4\u66f4\u65b0\uff0c\u820a\u7684\u7248\u672c\u5df2\u7d93\u4e0d\u518d\u652f\u63f4\uff0c\u7db2\u9801\u8207\u5c0d\u61c9\u7a0b\u5f0f\u90fd\u9700\u8981\u6539\u5beb\u3001\u642c\u79fb\uff0c\u6642\u9593\uff1a\u642c\u79fb\u671f\u9593\u7d04\u97002\u500b\u6708\u9032\u884c\u6e2c\u8a66\u3001\u6539\u5beb\uff0c\u5730\u9ede\uff1a\u672c\u516c\u53f8\u5167\u4f5c\u696d\uff0c\u7269\uff1a\u5982\u65b0\u7248\u672c\u78ba\u8a8d\u53ef\u884c\uff0c\u5c07\u53e6\u884c\u63a1\u8cfc\u8edf\u9ad4\u6388\u6b0a\uff0c\u7d04\u9700\u6bcf\u5e7420\u842c\u5143\u3002\u8acb\u5206\u6210\u4e3b\u65e8\u3001\u8aaa\u660e\u3001\u64ec\u8fa6\u7b493\u500b\u6bb5\u843d\u64b0\u5beb\u3002\u5b57\u6578\u9650\u5236\u5728200\u5b57\u5167\u3002"
306/32: print(c)
306/33: d="\u4e3b\u65e8\uff1a\n----\n\n\u672c\u5167\u90e8\u7c3d\u5448\u70ba\u516c\u53f8\u8fa6\u7406\u7db2\u9801\u4f3a\u670d\u5668\u8edf\u9ad4\u66f4\u65b0\u63d0\u51fa\uff0c\u4ee5\u78ba\u4fdd\u516c\u53f8\u7db2\u9801\u7cfb\u7d71\u7684\u6b63\u5e38\u904b\u4f5c\u3002\u820a\u7248\u672c\u5df2\u7d93\u4e0d\u518d\u652f\u63f4\uff0c\u56e0\u6b64\u9700\u8981\u9032\u884c\u6539\u5beb\u548c\u642c\u79fb\u3002\u642c\u79fb\u671f\u9593\u7d04\u97002\u500b\u6708\u9032\u884c\u6e2c\u8a66\u548c\u6539\u5beb\uff0c\u4e26\u4e14\u53ef\u80fd\u9700\u8981\u6bcf\u5e74\u63a1\u8cfc\u8edf\u9ad4\u6388\u6b0a\uff0c\u7d04\u9700\u6bcf\u5e7420\u842c\u5143\u3002\n\n\u8aaa\u660e\uff1a\n----\n\n\u8fd1\u671f\u767c\u73fe\u7db2\u9801\u4f3a\u670d\u5668\u8edf\u9ad4\u5df2\u7d93\u4e0d\u518d\u652f\u63f4\uff0c\u56e0\u6b64\u9700\u8981\u9032\u884c\u820a\u7248\u672c\u7684\u6539\u5beb\u548c\u642c\u79fb\u3002\u9019\u5c07\u5c0d\u516c\u53f8\u7db2\u9801\u7cfb\u7d71\u9020\u6210\u5f71\u97ff\uff0c\u56e0\u6b64\u9700\u8981\u8fa6\u7406\u7db2\u9801\u4f3a\u670d\u5668\u8edf\u9ad4\u66f4\u65b0\u3002\u5728\u9032\u884c\u66f4\u65b0\u524d\uff0c\u9700\u8981\u9032\u884c2\u500b\u6708\u7684\u6e2c\u8a66\u548c\u6539\u5beb\uff0c\u4ee5\u78ba\u4fdd\u7cfb\u7d71\u7684\u6b63\u5e38\u904b\u4f5c\u3002\u53e6\u5916\uff0c\u5982\u679c\u65b0\u7248\u672c\u78ba\u8a8d\u53ef\u884c\uff0c\u5c07\u53e6\u884c\u63a1\u8cfc\u8edf\u9ad4\u6388\u6b0a\uff0c\u7d04\u9700\u6bcf\u5e7420\u842c\u5143\u3002\n\n\u64ec\u8fa6\uff1a\n----\n\n1. \u64b0\u5beb\u66f4\u65b0\u8a08\u756b\u66f8\uff0c\u5305\u62ec\u66f4\u65b0\u7684\u76ee\u7684\u3001\u6b65\u9a5f\u548c\u6642\u9593\u8868\u7b49\u3002\n2. \u8fa6\u7406\u65b0\u7248\u672c\u7684\u8cfc\u8cb7\u548c\u6388\u6b0a\u3002\n3. \u9032\u884c\u66f4\u65b0\u7684\u6e2c\u8a66\u548c\u6539\u5beb\uff0c\u78ba\u4fdd\u7cfb\u7d71"
306/34: print (d)
306/35: !top
307/1: import mysqlclient
308/1: import mysqlclient
309/1: import mysqlclient
310/1: import mysqlclient
310/2: import MySQLdb
310/3:
db = MySQLdb.connect(host="200.200.12.191", user="kuang",
    passwd="yck4139", db="okmdb",
    charset='utf8',port=8000,unix_socket="/var/lib/mysql/mysql.sock")
310/4:
db = MySQLdb.connect(host="200.200.12.191", user="kuang",
    passwd="yck4139", db="okmdb",
    charset='utf8',port=3306,unix_socket="/var/lib/mysql/mysql.sock")
310/5:
db = MySQLdb.connect(host="200.200.12.191", user="kuang",
    passwd="yck4139", db="okmdb",
    charset='utf8',port=3306,unix_socket="/var/lib/mysql/mysql.sock")
310/6:
cursor = db.cursor()
cursor.execute("SELECT * from OKM_ACTIVITY")
data = cursor.fetchall()
cursor.execute("describe OKM_ACTIVITY")
cols = cursor.fetchall()
310/7: cols
310/8: from pandas import *
310/9:
cols=[cols[i][0] for i in range(len(cols))]
dd={}
for i in range(len(cols)):
  dd.update({cols[i]:[data[j][i] for j in range(len(data))]})
df=DataFrame(dd)
310/10: df.head()
310/11: df.tail()
310/12: data
311/1:
with open('proj_calss.html','r') as f:
    lines=[i for i in f]
311/2: lines[:5]
311/3: a=[i for i in lines if "value=" in i]
311/4: len(a)
311/5: a
311/6: a=[i for i in lines if "value=" in i and 'B' in i]
311/7: len(a)
311/8: a
311/9: import json
311/10: cat_Bnum=[i.split('"')[1] for i in a]
311/11: cat_Bnum[:5]
311/12: cat_Bnum[-5:]
311/13: cat_CNnam=[i.split('>1')[1].split('<')[0] for i in a]
311/14: cat_CNnam=[i.split('>1')[1].split('<')[0] for i in a if '<' in i and '>' in i]
311/15: b=[i for i in a if '<' in i and '>' in i]
311/16: len(b)
311/17: cat_CNnam=[i.split('>1')[1].split('<')[0] for i in b]
311/18: cat_CNnam=[i.split('>')[1].split('<')[0] for i in b]
311/19: cat_CNnam[:5]
311/20: cat_CNnam[-5:]
311/21: import os
311/22:
for i in range(len(b)):
    dd=cat_Bnum[i]+'_'+cat_CNnam[i]
    os.system('mkdir -p '+dd)
311/23:
for i in range(len(b)):
    dd=cat_Bnum[i]+'_'+cat_CNnam[i]
    dd.replace('(','\(').replace(')','\)')
    os.system('mkdir -p '+dd)
311/24:
for i in range(len(b)):
    dd=cat_Bnum[i]+'_'+cat_CNnam[i]
    dd.replace('(','（').replace(')','）')
    os.system('mkdir -p '+dd)
311/25:
for i in range(len(b)):
    dd=cat_Bnum[i]+'_'+cat_CNnam[i]
    dd=dd.replace('(','（').replace(')','）')
    os.system('mkdir -p '+dd)
311/26: ls
311/27: lst
311/28: dd={i:j for i,j in zip(cat_Bnum,cat_CNnam)}
311/29: dd
311/30:
with open('proj_calss.json','+w') as f:
    json.dump(dd, f)
311/31: !cat cat_class.json
311/32: !cat project_class.json
311/33: !cat proj_class.json
311/34: mv proj_calss.json proj_class.json
311/35: !cat project_class.json
311/36: !cat proj_class.json
311/37:
with open('proj_class.json','+w',coding='utf8') as f:
    json.dump(dd, f)
311/38:
with open('proj_class.json','+w',encoding='utf8') as f:
    json.dump(dd, f)
311/39: !cat proj_class.json
311/40:
with open('proj_class.json','r') as f:
    bb=json.load(f)
311/41: bb
311/42: pwd
312/1:
import requests
from bs4 import BeautifulSoup
import time
import sys

# 设置请求头
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"}

# 目标网站 URL
url = "https://eiadoc.epa.gov.tw/eiaweb/"

# 创建一个 Session
session = requests.Session()

# 发送 POST 请求，模拟选择条件并点击查询按钮
data = {
    "__VIEWSTATE": "/wEPDwUJNTU0MTU5NzEyZGQ=",
    "__EVENTTARGET": "ctl00$cphContent$lnkFtrAbstract",
    "__EVENTARGUMENT": "",
    "__VIEWSTATEGENERATOR": "F4F16D05",
    "ctl00$cphContent$ddlFtrDECAL": "B01"
}

response = session.post(url, headers=headers, data=data)
soup = BeautifulSoup(response.content, "html.parser")

# 获取表格数据
table = soup.find("table", {"id": "cphContent_gvAbstract"})

# 将第一页的数据保存到文件
with open("B01_page1.html", "w", encoding="utf-8") as f:
    f.write(str(table))
sys.exit()

# 寻找下一页按钮并点击
next_button = soup.find("div", {"id": "gvAbstractPager"}).find("a", {"title": "跳至第3頁"})
while next_button:
    next_url = url + next_button["href"]
    response = session.get(next_url, headers=headers)
    soup = BeautifulSoup(response.content, "html.parser")

    # 获取表格数据
    table = soup.find("table", {"id": "cphContent_gvAbstract"})

    # 保存当前页的数据到文件
    page_number = next_button.text.strip()
    with open(f"B01_page{page_number}.html", "w", encoding="utf-8") as f:
        f.write(str(table))

    # 寻找下一页按钮
    next_button = soup.find("div", {"id": "gvAbstractPager"}).find("a", {"title": f"跳至第{int(page_number) + 1}頁"})

    # 等待10秒
    time.sleep(10)

print("爬取完成")
312/2:
import requests
from bs4 import BeautifulSoup
import time
import sys

# 设置请求头
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"}

# 目标网站 URL
url = "https://eiadoc.epa.gov.tw/eiaweb/"

# 创建一个 Session
session = requests.Session()

# 发送 POST 请求，模拟选择条件并点击查询按钮
data = {
    "__VIEWSTATE": "/wEPDwUJNTU0MTU5NzEyZGQ=",
    "__EVENTTARGET": "ctl00$cphContent$lnkFtrAbstract",
    "__EVENTARGUMENT": "",
    "__VIEWSTATEGENERATOR": "F4F16D05",
    "ctl00$cphContent$ddlFtrDECAL": "B01"
}

response = session.post(url, headers=headers, data=data)
soup = BeautifulSoup(response.content, "html.parser")

# 获取表格数据
table = soup.find("table", {"id": "cphContent_gvAbstract"})

# 将第一页的数据保存到文件
with open("B01_page1.html", "w", encoding="utf-8") as f:
    f.write(str(table))
312/3:
mport requests
from bs4 import BeautifulSoup
import time
import sys

# 设置请求头
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"}

# 目标网站 URL
url = "https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode=1070121T&sFileName=C01.PDF"
session = requests.Session()
312/4:
import requests
from bs4 import BeautifulSoup
import time
import sys

# 设置请求头
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"}

# 目标网站 URL
url = "https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode=1070121T&sFileName=C01.PDF"
session = requests.Session()
312/5:
import requests
from bs4 import BeautifulSoup
import time
import sys

# 设置请求头
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"}

# 目标网站 URL
url = "https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode=1070121T&sFileName=C01.PDF"
session = requests.Session()
response = session.post(url, headers=headers)
312/6:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time

# 创建一个 Chrome 浏览器实例
driver = webdriver.Chrome()

# 循环遍历 URL 列表并下载文件
urls = [
    'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode=1070121T&sFileName=C0'+str(i)+'.PDF' for i in range(10)
]

for url in urls:
    driver.get(url)
    time.sleep(1)  # 等待加载完成，根据需要调整等待时间

# 关闭浏览器
driver.quit()
312/7: !pip install selenium
312/8:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time

# 创建一个 Chrome 浏览器实例
driver = webdriver.Chrome()

# 循环遍历 URL 列表并下载文件
urls = [
    'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode=1070121T&sFileName=C0'+str(i)+'.PDF' for i in range(10)
]

for url in urls:
    driver.get(url)
    time.sleep(1)  # 等待加载完成，根据需要调整等待时间

# 关闭浏览器
driver.quit()
312/9:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time

# 创建一个 Chrome 浏览器实例
driver = webdriver.Chrome()

# 循环遍历 URL 列表并下载文件
urls = [
    'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode=1070121T&sFileName=C1'+str(i)+'.PDF' for i in range(1,13)
]

for url in urls:
    driver.get(url)
    time.sleep(1)  # 等待加载完成，根据需要调整等待时间

# 关闭浏览器
driver.quit()
312/10:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time

# 创建一个 Chrome 浏览器实例
driver = webdriver.Chrome()

# 循环遍历 URL 列表并下载文件
urls = [
    'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode=1070121T&sFileName=A{i:02}'+'.PDF' for i in range(1,15)
]

import random
import time

# 生成5到20之间的随机秒数

for url in urls:
    driver.get(url)
    random_sleep = random.uniform(5, 20)
    time.sleep(random_sleep)  # 等待加载完成，根据需要调整等待时间

# 关闭浏览器
driver.quit()
312/11: urls
312/12:
urls = [
    'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode=1070121T&sFileName=A{:02d}'.format(i)+'.PDF' for i in range(1,15)
]
urls
312/13:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time

# 创建一个 Chrome 浏览器实例
driver = webdriver.Chrome()

# 循环遍历 URL 列表并下载文件
urls = [
    'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode=1070121T&sFileName=A{:02d}'.format(i)+'.PDF' for i in range(1,15)
]

import random
import time

# 生成5到20之间的随机秒数

for url in urls:
    driver.get(url)
    random_sleep = random.uniform(5, 20)
    time.sleep(random_sleep)  # 等待加载完成，根据需要调整等待时间

# 关闭浏览器
driver.quit()
313/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd
import os
import random
import glob
import shutil


df=pd.read_csv('B1.csv')
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例
driver = webdriver.Chrome()

source_directory = "~/Download/"

for i in [0,1]: #range(len(df)):
    id=df.iloc[i,0]
    cat=df['cat'][i]
    nam=df.iloc[i,2]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
# 循环遍历 URL 列表并下载文件
    for ac in 'CA':
        urls = [
        'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='+ac+'{:02d}'.format(i)+'.PDF' 
        for i in range(1,30)
]

# 生成5到20之间的随机秒数
        for url in urls:
            try:
                driver.get(url)
            except:
                print('file:'+id+url[-7:]+'not exist')
                continue
            random_sleep = random.uniform(5, 20)
            time.sleep(random_sleep)  # 等待加载完成，根据需要调整等待时间
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
        print(f"移动文件 {pdf_file} 到 {target_directory}")
# 关闭浏览器
driver.quit()
313/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd
import os
import random
import glob
import shutil


df=pd.read_csv('B1.csv')
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例
driver = webdriver.Chrome()

source_directory = "~/Download/"

for i in [0,1]: #range(len(df)):
    id=df.iloc[i,0]
    cat=df['cat'][i]
    nam=df.iloc[i,2]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
# 循环遍历 URL 列表并下载文件
    for ac in 'CA':
        urls = [
        'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='+ac+'{:02d}'.format(i)+'.PDF' 
        for i in range(1,30)
]

# 生成5到20之间的随机秒数
        for url in urls:
            try:
                driver.get(url)
            except:
                print('file:'+id+url[-7:]+'not exist')
                continue
            random_sleep = random.uniform(5, 20)
            time.sleep(random_sleep)  # 等待加载完成，根据需要调整等待时间
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
        print(f"移动文件 {pdf_file} 到 {target_directory}")
# 关闭浏览器
driver.quit()
314/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd
import os
import random
import glob
import shutil


df=pd.read_csv('B1.csv')
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例
driver = webdriver.Chrome()

source_directory = "~/Download/"

for i in [0,1]: #range(len(df)):
    id=df.iloc[i,0]
    cat=df['cat'][i]
    nam=df.iloc[i,2]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
# 循环遍历 URL 列表并下载文件
    for ac in 'CA':
        urls = [
        'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='+ac+'{:02d}'.format(i)+'.PDF' 
        for i in range(1,30)
]

# 生成5到20之间的随机秒数
        for url in urls:
            try:
                driver.get(url)
            except:
                print('file:'+id+url[-7:]+'not exist')
                continue
            random_sleep = random.uniform(5, 20)
            time.sleep(random_sleep)  # 等待加载完成，根据需要调整等待时间
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
        print(f"移动文件 {pdf_file} 到 {target_directory}")
# 关闭浏览器
driver.quit()
315/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd
import os
import random
import glob
import shutil


df=pd.read_csv('B1.csv')
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例
driver = webdriver.Chrome()

source_directory = "~/Download/"

for i in [0,1]: #range(len(df)):
    id=df.iloc[i,0]
    cat=df['cat'][i]
    nam=df.iloc[i,2]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
# 循环遍历 URL 列表并下载文件
    for ac in 'CA':
        urls = [
        'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='+ac+'{:02d}'.format(i)+'.PDF' 
        for i in range(1,30)
]

# 生成5到20之间的随机秒数
        for url in urls:
            try:
                driver.get(url)
            except:
                print('file:'+id+url[-7:]+'not exist')
                continue
            random_sleep = random.uniform(5, 20)
            time.sleep(random_sleep)  # 等待加载完成，根据需要调整等待时间
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
        print(f"移动文件 {pdf_file} 到 {target_directory}")
# 关闭浏览器
driver.quit()
315/2: !pip install selenium
315/3:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd
import os
import random
import glob
import shutil


df=pd.read_csv('B1.csv')
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例
driver = webdriver.Chrome()

source_directory = "~/Download/"

for i in [0,1]: #range(len(df)):
    id=df.iloc[i,0]
    cat=df['cat'][i]
    nam=df.iloc[i,2]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
# 循环遍历 URL 列表并下载文件
    for ac in 'CA':
        urls = [
        'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='+ac+'{:02d}'.format(i)+'.PDF' 
        for i in range(1,30)
]

# 生成5到20之间的随机秒数
        for url in urls:
            try:
                driver.get(url)
            except:
                print('file:'+id+url[-7:]+'not exist')
                continue
            random_sleep = random.uniform(5, 20)
            time.sleep(random_sleep)  # 等待加载完成，根据需要调整等待时间
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
        print(f"移动文件 {pdf_file} 到 {target_directory}")
# 关闭浏览器
driver.quit()
315/4: pdf_files
315/5: source_directory
315/6: glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
315/7: glob.glob(os.path.join(source_directory, "*.PDF"))
315/8: os.path.join(source_directory, "*.PDF")
315/9: glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
315/10: glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
315/11:
source_directory='/home/kuang'
glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
315/12:
source_directory='/home/kuang'
glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
315/13:
source_directory='/home/kuang'
glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
315/14:
source_directory='/home/kuang/Downloads'
glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
315/15:
source_directory='/home/kuang/Downloads'
glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
315/16:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd
import os
import random
import glob
import shutil


df=pd.read_csv('B1.csv')
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例
driver = webdriver.Chrome()

source_directory = "/home/kuang/Download/"

for i in [1,2]: #range(len(df)):
    id=df.iloc[i,0]
    cat=df['cat'][i]
    nam=df.iloc[i,2]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
# 循环遍历 URL 列表并下载文件
    for ac in 'CA':
        urls = [
        'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='+ac+'{:02d}'.format(i)+'.PDF' 
        for i in range(1,30)
]

# 生成5到20之间的随机秒数
        for url in urls:
            try:
                driver.get(url)
            except:
                print('file:'+id+url[-7:]+'not exist')
                continue
            random_sleep = random.uniform(5, 20)
            time.sleep(random_sleep)  # 等待加载完成，根据需要调整等待时间
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
        print(f"移动文件 {pdf_file} 到 {target_directory}")
# 关闭浏览器
driver.quit()
315/17:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd
import os
import random
import glob
import shutil


df=pd.read_csv('B1.csv')
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例

source_directory = "/home/kuang/Downloads"

for i in [3,4]: #range(len(df)):
    driver = webdriver.Chrome()
    id=df.iloc[i,0]
    cat=df['cat'][i]
    nam=df.iloc[i,2]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))    
    for ac in 'CA':
        urls = [
            'https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='+ac+'{:02d}'.format(i)+'.PDF' 
            for i in range(1,30)
]
        # 循环遍历 URL 列表并下载文件
        for url in urls:
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                print('file:'+id+url[-7:]+'not exist')
                continue
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
315/18:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd
import os
import random
import glob
import shutil


df=pd.read_csv('B1.csv')
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例

source_directory = "/home/kuang/Downloads"

for i in range(5,80): #range(5,len(df)):
    driver = webdriver.Chrome()
    id=df.iloc[i,0]
    cat=df['cat'][i]
    nam=df.iloc[i,2]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))    
    for ac in 'CA':
        pdf_file=ac+'{:02d}'.format(i)+'.PDF'
        urls = ['https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='+pdf_file
            for i in range(1,30)]
        # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=urls[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                continue
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
315/19:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd
import os
import random
import glob
import shutil


df=pd.read_csv('B1.csv')
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例

source_directory = "/home/kuang/Downloads"

for i in range(5,80): #range(5,len(df)):
    driver = webdriver.Chrome()
    id=df.iloc[i,0]
    cat=df['cat'][i]
    nam=df.iloc[i,2]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))    
    for ac in 'CA':
        pdf_file=ac+'{:02d}'.format(i)+'.PDF'
        urls = ['https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='+pdf_file
            for i in range(1,30)]
        # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=urls[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                continue
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
315/20:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd
import os
import random
import glob
import shutil


df=pd.read_csv('B01.csv')
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例

source_directory = "/home/kuang/Downloads"

for i in range(5,80): #range(5,len(df)):
    driver = webdriver.Chrome()
    id=df.iloc[i,0]
    cat=df['cat'][i]
    nam=df.iloc[i,2]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))    
    for ac in 'CA':
        pdf_file=ac+'{:02d}'.format(i)+'.PDF'
        urls = ['https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='+pdf_file
            for i in range(1,30)]
        # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=urls[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                continue
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
315/21: pdf_file
315/22: url[-7:]
315/23:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd
import os
import random
import glob
import shutil


df=pd.read_csv('B01.csv')
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例

source_directory = "/home/kuang/Downloads"

for i in range(5,80): #range(5,len(df)):
    driver = webdriver.Chrome()
    id=df.iloc[i,0]
    cat=df['cat'][i]
    nam=df.iloc[i,2]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))    
    for ac in 'CA':
        pdf_file=ac+'{:02d}'.format(i)+'.PDF'
        urls = ['https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='+pdf_file
            for i in range(1,30)]
        # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                continue
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
315/24:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd
import os
import random
import glob
import shutil


df=pd.read_csv('B01.csv')
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例

source_directory = "/home/kuang/Downloads"

for i in range(5,80): #range(5,len(df)):
    driver = webdriver.Chrome()
    id=df.iloc[i,0]
    cat=df['cat'][i]
    nam=df.iloc[i,2]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))    
    for ac in 'CA':
        pdf_file=ac+'{:02d}'.format(i)+'.PDF'
        urls = ['https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='+pdf_file
            for i in range(1,30)]
        # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                continue
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
315/25: url[-7:]
315/26: urls
316/1:
#!/home/kuang/.conda/envs/py39/bin/python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import pandas as pd
import os, sys, time, random, glob, shutil

'''
usage: get_eia.py B01 6 60 (kind of project, start and end of sequences)
'''

#df=pd.read_csv(sys.argv[1]+'.csv')
df=pd.read_csv('cat4.csv')
df=df.loc[df.cat!='cat'].reset_index(drop=True)
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例

source_directory = "." #"/home/kuang/Downloads"
chrome_driver="/home/kuang/.cache/selenium/chromedriver/linux64/119.0.6045.105/chromedriver"
chrome_options = Options()
chrome_options.add_argument("--headless")  # 启用无头模式
iend={'C':14,'A':31}
#ib,ie=[int(sys.argv[i]) for i in [2,3]]
ib=0;ie=86
for i in range(ib,ie): #range(5,len(df)):
    driver = webdriver.Chrome()#options=chrome_options)
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C': continue
                break
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
316/2: url_root
316/3: urls
317/1: ls *html
317/2:
import os

with open('proj_calss.html','r') as f:
    lines=[i for i in f]
a=[i for i in lines if "value=" in i and 'B' in i]
b=[i for i in a if '<' in i and '>' in i]
cat_Bnum=[i.split('"')[1] for i in a]
cat_CNnam=[i.split('>')[1].split('<')[0] for i in b
for i in range(len(b)):
    dd=cat_Bnum[i]+'_'+cat_CNnam[i]
    dd=dd.replace('(','▒~H').replace(')','▒~I')')

dd={i:j for i,j in zip(cat_Bnum,cat_CNnam)}
317/3:
with open('proj_calss.html','r') as f:
    lines=[i for i in f]
a=[i for i in lines if "value=" in i and 'B' in i]
b=[i for i in a if '<' in i and '>' in i]
cat_Bnum=[i.split('"')[1] for i in a]
cat_CNnam=[i.split('>')[1].split('<')[0] for i in b]
for i in range(len(b)):
    dd=cat_Bnum[i]+'_'+cat_CNnam[i]
317/4: dd
317/5: b
318/1: import json
318/2:
with open('proj_calss.json','r') as f:
    dd=json.load(f)
318/3:
with open('proj_class.json','r') as f:
    dd=json.load(f)
318/4: dd
318/5:
{'B01': '工廠之設立',
 'B02': '園區之開發',
 'B03': '道路之開發',
 'B04': '鐵路之開發',
 'B05': '大眾捷運系統之開發',
 'B06': '港灣之開發',
 'B07': '機場之開發',
 'B08': '土石採取之開發',
 'B09': '探礦、採礦之開發',
 'B10': '蓄水工程之開發',
 'B11': '供水、抽水或引水工程之開發',
 'B12': '防洪排水工程之開發',
 'B13': '農、林、漁、牧地之休閒農場或農產品加工場所之開發',
 'B14': '砍伐林木之開發',
 'B15': '魚塭或魚池之開發',
 'B16': '牧地之開發，興建畜牧場',
 'B17': '遊樂區、動物園之開發',
 'B18': '森林遊樂區之育樂設施區之開發',
 'B19': '旅館或觀光旅館之開發',
 'B20': '高爾夫球場、運動場地或運動公園之開發',
 'B21': '文教建設之開發',
 'B22': '醫療建設、護理機構、社會福利機構之開發',
 'B23': '新市區建設之開發',
 'B24': '高樓建築之開發',
 'B25': '舊市區更新之開發',
 'B26': '環境保護工程之興建',
 'B27': '能源或輸變電工程之開發',
 'B28': '放射性廢棄物貯存之開發',
 'B29': '工商綜合區或大型購物中心之開發',
 'B30': '展覽會(館)、博覽會或會展中心之開發',
 'B31': '殯葬設施之開發',
 'B33': '屠宰場之開發',
 'B34': '動物收容所之開發',
 'B35': '地下街工程',
 'B36': '輸電線路工程',
 'B37': '港區申請設置水泥儲庫',
 'B38': '設置液化天然氣接收站(港)',
 'B39': '天然氣或油品管線、貯存槽之開發',
 'B42': '工廠變更用地作為非工廠開發使用',
 'B43': '安養中心、護理機構或老人福利機構',
 'B44': '軍事營區、海岸（洋）巡防營區、飛彈試射場、靶場或雷達站之開發',
 'B46': '人工島嶼之興建或擴建工程',
 'B48': '核子反應器設施之除役',
 'B49': '超高壓變電所興建或擴建工程',
 'B50': '火化場之開發',
 'B51': '空中纜車之開發',
 'B52': '矯正機關、保安處分處所或其他以拘禁、感化為目的之收容機構之開發',
 'B53': '深層海水之開發',
 'B54': '設置氣象設施之開發',
 'B55': '於海域築堤排水填土造成陸地',
 'B56': '位於山坡地之露營區',
 'B98': '政策環評',
 'B99': '其他開發行為與經中央主管機關公告者'}
319/1: %matplotlib inline
319/2:
import torch
from torch import nn
from captcha_dataset import CaptchaDataset
import matplotlib.pyplot as plt
from model import NeuralNetwork
from utils import draw_boxes, decode_output
319/3:
img_width, img_height = 110, 40
positions = 13
digit_width = 20
digit_height = 40
position_width = img_width/positions
MODEL_FILE = "model.pt"
319/4:
model = NeuralNetwork()
checkpoint = torch.load(MODEL_FILE)
model.load_state_dict(checkpoint['model_state_dict'])
319/5:
model = NeuralNetwork()
checkpoint = torch.load(MODEL_FILE)
map_location=torch.device('cpu')
model.load_state_dict(checkpoint['model_state_dict'])
319/6:
map_location=torch.device('cpu')
checkpoint = torch.load(MODEL_FILE, map_location=map_location)
model = NeuralNetwork()
model.load_state_dict(checkpoint['model_state_dict'])
319/7:
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/8:
N=1000
model.eval()
e = 0
e_cnt = 0
for i in range(N):
    data = test_data[0]
    x, y, d = data[0], data[1], data[3]
    with torch.no_grad():
        pred = model(x[None, :])
        confidence = torch.clamp(pred[0,0,0,:], 0, 1)
        digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
        box_shift = torch.sigmoid(pred[0,11,0,:])
        (digits, digit_pos) = decode_output(confidence, box_shift, digit_predict.numpy(), position_width, positions, digit_width)
        if list(digits) != list(d):
            e+=1
        if len(digits) != len(d):
            e_cnt+=1
print(f"Accuracy {100-e/N*100:.1f}%")
print(f"Accuracy digit count {100-e_cnt/N*100:.1f}%")
319/9: test_data
319/10: type(test_data)
319/11: dir(test_data)
319/12: test_data.size
319/13: test_data[0]
319/14: type(test_data[0])
319/15: type(test_data.image)
319/16: test_data.image[0]
319/17: x, y, img=test_data.image_width,test_data.image_height,test_data.image
319/18: x, y, img=test_data.img_width,test_data.img_height,test_data.image
319/19: x,y
319/20:
test_data = CaptchaDataset(font_type='Arial', chars=6, height=60, width=200)
#test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/21:
test_data = CaptchaDataset(chars=6, height=60, width=200) #font_type='Arial', 
#test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/22:
test_data = CaptchaDataset(height=60, width=200) #font_type='Arial', chars=6, 
#test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/23:
test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
#test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/24: !pip install captcha_dataset
319/25:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data#[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/26: data
319/27: data.size
319/28:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(5, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/29:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/30: !pip install PIL
319/31: !pip install Pillow
319/32:
from PIL import Image, ImageDraw, ImageFont

# 載入字型
font = ImageFont.truetype("path/to/your/font.ttf", size=36)

# 創建一個空白圖像
image = Image.new("RGB", (200, 100), color="white")

# 在圖像上繪製文字
draw = ImageDraw.Draw(image)
draw.text((10, 10), "Hello, Pillow!", fill="black", font=font)

# 保存圖像
image.save("output.png")

# 顯示圖像
image.show()
319/33:
from PIL import Image, ImageDraw, ImageFont

ttf='./YOLOv8/lib/python3.9/site-packages/cv2/qt/fonts/DejaVuSans-BoldOblique.ttf'
# 載入字型
font = ImageFont.truetype(ttf, size=36)

# 創建一個空白圖像
image = Image.new("RGB", (200, 100), color="white")

# 在圖像上繪製文字
draw = ImageDraw.Draw(image)
draw.text((10, 10), "Hello, Pillow!", fill="black", font=font)

# 保存圖像
image.save("output.png")

# 顯示圖像
image.show()
319/34:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/35:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/36:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/37:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/38: %matplotlib inline
319/39:
import torch
from torch import nn
from captcha_dataset import CaptchaDataset
import matplotlib.pyplot as plt
from model import NeuralNetwork
from utils import draw_boxes, decode_output
319/40:
img_width, img_height = 110, 40
positions = 13
digit_width = 20
digit_height = 40
position_width = img_width/positions
MODEL_FILE = "model.pt"
319/41:
map_location=torch.device('cpu')
checkpoint = torch.load(MODEL_FILE, map_location=map_location)
model = NeuralNetwork()
model.load_state_dict(checkpoint['model_state_dict'])
319/42:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=0)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/43:
from PIL import Image, ImageDraw, ImageFont

ttf='/opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono.ttf'
# 載入字型
font = ImageFont.truetype(ttf, size=36)

# 創建一個空白圖像
image = Image.new("RGB", (200, 100), color="white")

# 在圖像上繪製文字
draw = ImageDraw.Draw(image)
draw.text((10, 10), "Hello, Pillow!", fill="black", font=font)

# 保存圖像
image.save("output.png")

# 顯示圖像
image.show()
319/44:
from PIL import Image, ImageDraw, ImageFont

ttf='/home/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-BoldOblique.ttf'
# 載入字型
font = ImageFont.truetype(ttf, size=36)

# 創建一個空白圖像
image = Image.new("RGB", (200, 100), color="white")

# 在圖像上繪製文字
draw = ImageDraw.Draw(image)
draw.text((10, 10), "Hello, Pillow!", fill="black", font=font)

# 保存圖像
image.save("output.png")

# 顯示圖像
image.show()
319/45:
from PIL import Image, ImageDraw, ImageFont

ttf='/home/anaconda3/envs/py37/fonts/UbuntuMono-R.ttf'
# 載入字型
font = ImageFont.truetype(ttf, size=36)

# 創建一個空白圖像
image = Image.new("RGB", (200, 100), color="white")

# 在圖像上繪製文字
draw = ImageDraw.Draw(image)
draw.text((10, 10), "Hello, Pillow!", fill="black", font=font)

# 保存圖像
image.save("output.png")

# 顯示圖像
image.show()
319/46:
from PIL import Image, ImageDraw, ImageFont

ttf='/home/anaconda3/envs/py37/fonts/UbuntuMono-BI.ttf'
# 載入字型
font = ImageFont.truetype(ttf, size=36)

# 創建一個空白圖像
image = Image.new("RGB", (200, 100), color="white")

# 在圖像上繪製文字
draw = ImageDraw.Draw(image)
draw.text((10, 10), "Hello, Pillow!", fill="black", font=font)

# 保存圖像
image.save("output.png")

# 顯示圖像
image.show()
319/47:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/48:
(digits, digit_pos) = decode_output(confidence, box_shift, digit_predict.numpy(), position_width, positions, digit_width)
img_pred = draw_boxes(img, digit_pos, digits, digit_width, digit_height)
plt.imshow(img_pred)
319/49:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/50:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
319/51:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
320/1: %matplotlib inline
320/2:
import torch
from torch import nn
from captcha_dataset import CaptchaDataset
import matplotlib.pyplot as plt
from model import NeuralNetwork
from utils import draw_boxes, decode_output
320/3:
img_width, img_height = 110, 40
positions = 13
digit_width = 20
digit_height = 40
position_width = img_width/positions
MODEL_FILE = "model.pt"
320/4:
map_location=torch.device('cpu')
checkpoint = torch.load(MODEL_FILE, map_location=map_location)
model = NeuralNetwork()
model.load_state_dict(checkpoint['model_state_dict'])
320/5:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
320/6:
(digits, digit_pos) = decode_output(confidence, box_shift, digit_predict.numpy(), position_width, positions, digit_width)
img_pred = draw_boxes(img, digit_pos, digits, digit_width, digit_height)
plt.imshow(img_pred)
320/7:
N=1000
model.eval()
e = 0
e_cnt = 0
for i in range(N):
    data = test_data[0]
    x, y, d = data[0], data[1], data[3]
    with torch.no_grad():
        pred = model(x[None, :])
        confidence = torch.clamp(pred[0,0,0,:], 0, 1)
        digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
        box_shift = torch.sigmoid(pred[0,11,0,:])
        (digits, digit_pos) = decode_output(confidence, box_shift, digit_predict.numpy(), position_width, positions, digit_width)
        if list(digits) != list(d):
            e+=1
        if len(digits) != len(d):
            e_cnt+=1
print(f"Accuracy {100-e/N*100:.1f}%")
print(f"Accuracy digit count {100-e_cnt/N*100:.1f}%")
320/8:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
320/9:
(digits, digit_pos) = decode_output(confidence, box_shift, digit_predict.numpy(), position_width, positions, digit_width)
img_pred = draw_boxes(img, digit_pos, digits, digit_width, digit_height)
plt.imshow(img_pred)
320/10:
#test_data = CaptchaDataset() #height=60, width=200) #font_type='Arial', chars=6, 
from captcha_dataset import CaptchaDataset
test_data = CaptchaDataset(1, digit_out=1)
model.eval()
data = test_data[0]
x, y, img = data[0], data[1], data[2]
with torch.no_grad():
    pred = model(x[None, :])
    confidence = torch.sigmoid(pred[0,0,0,:])
    digit_predict = torch.argmax(nn.functional.softmax(pred[0,1:11,0,:], 0), 0)
    box_shift = torch.sigmoid(pred[0,11,0,:])
320/11: plt.imshow(img)
320/12:
(digits, digit_pos) = decode_output(confidence, box_shift, digit_predict.numpy(), position_width, positions, digit_width)
img_pred = draw_boxes(img, digit_pos, digits, digit_width, digit_height)
plt.imshow(img_pred)
321/1:
%pip install captcha 
%pip install opencv-python
#!apt update && apt install -y libsm6 libxext6
!pip install opencv-python-headless
!pip install opencv-contrib-python-headless
%pip install keras


import argparse
import json
import string
import os
import shutil
import uuid
from captcha.image import ImageCaptcha

import itertools

import os
import cv2
import numpy as np
from random import random, randint, choices

import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, Input
import matplotlib.pyplot as plt
321/2:
alphabet_all = list('qwertyupasdfghjkzxcvbnm23456789QWERTYUPKJHGFDSAZXCVBNM') #(no 1,0,i,l,I,o,O )
alphabet = list('qwertyupasdfghjkzxcvbnm23456789')#QWERTYUIOPLKJHGFDSAZXCVBNM')
num_alphabet = len(alphabet)
def _gen_captcha(img_dir, num_of_letters, num_of_repetition, width, height):
    if os.path.exists(img_dir):
        shutil.rmtree(img_dir)
    if not os.path.exists(img_dir):
        os.makedirs(img_dir)

    image = ImageCaptcha(width=width, height=height)

    for counter in range(num_of_repetition):
        i = choices(alphabet_all, k=5)
        captcha = ''.join(i)
        fn = os.path.join(img_dir, '%s_%s.png' % (captcha, uuid.uuid4()))
        image.write(captcha, fn)


def gen_dataset(path, num_of_repetition, num_of_letters, width, height):
    _gen_captcha(os.path.join(path, 'data'), num_of_letters, num_of_repetition, width, height)
    print('Finished Data Generation')
321/3:
BATCH_SIZE = 128
NUM_OF_LETTERS = 5
EPOCHS = 50
IMG_ROW, IMG_COLS = 50, 135

# Non-configs
PATH = os.getcwd()
DATA_PATH = os.path.join(PATH, 'train')
321/4:
def load_data(path, test_split=0.1):
    print ('loading dataset...')
    y_train = []
    y_test = []
    x_train = []
    x_test = []

    # r=root, d=directories, f = files
    counter = 0
    for r, d, f in os.walk(path):
        for fl in f:
            if '.png' in fl:
                flr = fl.split('_')[0]
                counter += 1
                label = np.zeros((NUM_OF_LETTERS, num_alphabet))
                for i in range(NUM_OF_LETTERS):
                    label[i, alphabet.index(flr[i].lower())] = 1
#                 label = np.zeros((50, 1))
#                 for i in range(5):
#                     label[i*5+int(flr[i])] = 1

                img = cv2.imread(os.path.join(r, fl))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                img = cv2.resize(img, (int(135/2), int(50/2)), interpolation=cv2.INTER_AREA)
                img = np.reshape(img, (img.shape[0], img.shape[1], 1))

                if random() < test_split:
                    y_test.append(label)
                    x_test.append(img)
                else:
                    y_train.append(label)
                    x_train.append(img)

    print('dataset size:', counter, '(train=%d, test=%d)' % (len(y_train), len(y_test)))
    return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)
321/5:
if not os.path.exists(DATA_PATH):
    print('Generating Dataset')
    gen_dataset(DATA_PATH, 700*1000 , NUM_OF_LETTERS, IMG_COLS, IMG_ROW)
322/1:
import argparse
import json
import string
import os
import shutil
import uuid
from captcha.image import ImageCaptcha

import itertools

import os
import cv2
import numpy as np
from random import random, randint, choices

import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, Input
import matplotlib.pyplot as plt
322/2:
alphabet_all = list('qwertyupasdfghjkzxcvbnm23456789QWERTYUPKJHGFDSAZXCVBNM') #(no 1,0,i,l,I,o,O )
alphabet = list('qwertyupasdfghjkzxcvbnm23456789')#QWERTYUIOPLKJHGFDSAZXCVBNM')
num_alphabet = len(alphabet)
def _gen_captcha(img_dir, num_of_letters, num_of_repetition, width, height):
    if os.path.exists(img_dir):
        shutil.rmtree(img_dir)
    if not os.path.exists(img_dir):
        os.makedirs(img_dir)

    image = ImageCaptcha(width=width, height=height)

    for counter in range(num_of_repetition):
        i = choices(alphabet_all, k=5)
        captcha = ''.join(i)
        fn = os.path.join(img_dir, '%s_%s.png' % (captcha, uuid.uuid4()))
        image.write(captcha, fn)


def gen_dataset(path, num_of_repetition, num_of_letters, width, height):
    _gen_captcha(os.path.join(path, 'data'), num_of_letters, num_of_repetition, width, height)
    print('Finished Data Generation')
322/3:
alphabet_all = list('qwertyupasdfghjkzxcvbnm23456789QWERTYUPKJHGFDSAZXCVBNM') #(no 1,0,i,l,I,o,O )
alphabet = list('qwertyupasdfghjkzxcvbnm23456789')#QWERTYUIOPLKJHGFDSAZXCVBNM')
num_alphabet = len(alphabet)
def _gen_captcha(img_dir, num_of_letters, num_of_repetition, width, height):
    if os.path.exists(img_dir):
        shutil.rmtree(img_dir)
    if not os.path.exists(img_dir):
        os.makedirs(img_dir)

    image = ImageCaptcha(width=width, height=height)

    for counter in range(num_of_repetition):
        i = choices(alphabet_all, k=5)
        captcha = ''.join(i)
        fn = os.path.join(img_dir, '%s_%s.png' % (captcha, uuid.uuid4()))
        image.write(captcha, fn)


def gen_dataset(path, num_of_repetition, num_of_letters, width, height):
    _gen_captcha(os.path.join(path, 'data'), num_of_letters, num_of_repetition, width, height)
    print('Finished Data Generation')
322/4:
BATCH_SIZE = 128
NUM_OF_LETTERS = 5
EPOCHS = 50
IMG_ROW, IMG_COLS = 50, 135

# Non-configs
PATH = os.getcwd()
DATA_PATH = os.path.join(PATH, 'train')
322/5:
def load_data(path, test_split=0.1):
    print ('loading dataset...')
    y_train = []
    y_test = []
    x_train = []
    x_test = []

    # r=root, d=directories, f = files
    counter = 0
    for r, d, f in os.walk(path):
        for fl in f:
            if '.png' in fl:
                flr = fl.split('_')[0]
                counter += 1
                label = np.zeros((NUM_OF_LETTERS, num_alphabet))
                for i in range(NUM_OF_LETTERS):
                    label[i, alphabet.index(flr[i].lower())] = 1
#                 label = np.zeros((50, 1))
#                 for i in range(5):
#                     label[i*5+int(flr[i])] = 1

                img = cv2.imread(os.path.join(r, fl))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                img = cv2.resize(img, (int(135/2), int(50/2)), interpolation=cv2.INTER_AREA)
                img = np.reshape(img, (img.shape[0], img.shape[1], 1))

                if random() < test_split:
                    y_test.append(label)
                    x_test.append(img)
                else:
                    y_train.append(label)
                    x_train.append(img)

    print('dataset size:', counter, '(train=%d, test=%d)' % (len(y_train), len(y_test)))
    return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)
322/6:
if not os.path.exists(DATA_PATH):
    print('Generating Dataset')
    gen_dataset(DATA_PATH, 700*1000 , NUM_OF_LETTERS, IMG_COLS, IMG_ROW)
322/7:
import argparse
import json
import string
import os
import shutil
import uuid
from captcha.image import ImageCaptcha

import itertools

import os
import cv2
import numpy as np
from random import random, randint, choices

import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, Input
import matplotlib.pyplot as plt
323/1:
import argparse
import json
import string
import os
import shutil
import uuid
from captcha.image import ImageCaptcha

import itertools

import os
import cv2
import numpy as np
from random import random, randint, choices

import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, Input
import matplotlib.pyplot as plt
324/1:
import argparse
import json
import string
import os
import shutil
import uuid
from captcha.image import ImageCaptcha

import itertools

import os
import cv2
import numpy as np
from random import random, randint, choices

import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, Input
import matplotlib.pyplot as plt
324/2:
alphabet_all = list('qwertyupasdfghjkzxcvbnm23456789QWERTYUPKJHGFDSAZXCVBNM') #(no 1,0,i,l,I,o,O )
alphabet = list('qwertyupasdfghjkzxcvbnm23456789')#QWERTYUIOPLKJHGFDSAZXCVBNM')
num_alphabet = len(alphabet)
def _gen_captcha(img_dir, num_of_letters, num_of_repetition, width, height):
    if os.path.exists(img_dir):
        shutil.rmtree(img_dir)
    if not os.path.exists(img_dir):
        os.makedirs(img_dir)

    image = ImageCaptcha(width=width, height=height)

    for counter in range(num_of_repetition):
        i = choices(alphabet_all, k=5)
        captcha = ''.join(i)
        fn = os.path.join(img_dir, '%s_%s.png' % (captcha, uuid.uuid4()))
        image.write(captcha, fn)


def gen_dataset(path, num_of_repetition, num_of_letters, width, height):
    _gen_captcha(os.path.join(path, 'data'), num_of_letters, num_of_repetition, width, height)
    print('Finished Data Generation')
324/3:
BATCH_SIZE = 128
NUM_OF_LETTERS = 5
EPOCHS = 50
IMG_ROW, IMG_COLS = 50, 135

# Non-configs
PATH = os.getcwd()
DATA_PATH = os.path.join(PATH, 'train')
324/4:
def load_data(path, test_split=0.1):
    print ('loading dataset...')
    y_train = []
    y_test = []
    x_train = []
    x_test = []

    # r=root, d=directories, f = files
    counter = 0
    for r, d, f in os.walk(path):
        for fl in f:
            if '.png' in fl:
                flr = fl.split('_')[0]
                counter += 1
                label = np.zeros((NUM_OF_LETTERS, num_alphabet))
                for i in range(NUM_OF_LETTERS):
                    label[i, alphabet.index(flr[i].lower())] = 1
#                 label = np.zeros((50, 1))
#                 for i in range(5):
#                     label[i*5+int(flr[i])] = 1

                img = cv2.imread(os.path.join(r, fl))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                img = cv2.resize(img, (int(135/2), int(50/2)), interpolation=cv2.INTER_AREA)
                img = np.reshape(img, (img.shape[0], img.shape[1], 1))

                if random() < test_split:
                    y_test.append(label)
                    x_test.append(img)
                else:
                    y_train.append(label)
                    x_train.append(img)

    print('dataset size:', counter, '(train=%d, test=%d)' % (len(y_train), len(y_test)))
    return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)
324/5:
if not os.path.exists(DATA_PATH):
    print('Generating Dataset')
    gen_dataset(DATA_PATH, 700*1000 , NUM_OF_LETTERS, IMG_COLS, IMG_ROW)
324/6:
x_train, y_train, x_test, y_test = load_data(DATA_PATH)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
325/1:
import argparse
import json
import string
import os
import shutil
import uuid
from captcha.image import ImageCaptcha

import itertools

import os
import cv2
import numpy as np
from random import random, randint, choices

import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, Input
import matplotlib.pyplot as plt
325/2:
alphabet_all = list('qwertyupasdfghjkzxcvbnm23456789QWERTYUPKJHGFDSAZXCVBNM') #(no 1,0,i,l,I,o,O )
alphabet = list('qwertyupasdfghjkzxcvbnm23456789')#QWERTYUIOPLKJHGFDSAZXCVBNM')
num_alphabet = len(alphabet)
def _gen_captcha(img_dir, num_of_letters, num_of_repetition, width, height):
    if os.path.exists(img_dir):
        shutil.rmtree(img_dir)
    if not os.path.exists(img_dir):
        os.makedirs(img_dir)

    image = ImageCaptcha(width=width, height=height)

    for counter in range(num_of_repetition):
        i = choices(alphabet_all, k=5)
        captcha = ''.join(i)
        fn = os.path.join(img_dir, '%s_%s.png' % (captcha, uuid.uuid4()))
        image.write(captcha, fn)


def gen_dataset(path, num_of_repetition, num_of_letters, width, height):
    _gen_captcha(os.path.join(path, 'data'), num_of_letters, num_of_repetition, width, height)
    print('Finished Data Generation')
325/3:
BATCH_SIZE = 128
NUM_OF_LETTERS = 5
EPOCHS = 50
IMG_ROW, IMG_COLS = 50, 135

# Non-configs
PATH = os.getcwd()
DATA_PATH = os.path.join(PATH, 'train')
325/4:
def load_data(path, test_split=0.1):
    print ('loading dataset...')
    y_train = []
    y_test = []
    x_train = []
    x_test = []

    # r=root, d=directories, f = files
    counter = 0
    for r, d, f in os.walk(path):
        for fl in f:
            if '.png' in fl:
                flr = fl.split('_')[0]
                counter += 1
                label = np.zeros((NUM_OF_LETTERS, num_alphabet))
                for i in range(NUM_OF_LETTERS):
                    label[i, alphabet.index(flr[i].lower())] = 1
#                 label = np.zeros((50, 1))
#                 for i in range(5):
#                     label[i*5+int(flr[i])] = 1

                img = cv2.imread(os.path.join(r, fl))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                img = cv2.resize(img, (int(135/2), int(50/2)), interpolation=cv2.INTER_AREA)
                img = np.reshape(img, (img.shape[0], img.shape[1], 1))

                if random() < test_split:
                    y_test.append(label)
                    x_test.append(img)
                else:
                    y_train.append(label)
                    x_train.append(img)

    print('dataset size:', counter, '(train=%d, test=%d)' % (len(y_train), len(y_test)))
    return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)
325/5:
x_train, y_train, x_test, y_test = load_data(DATA_PATH)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
325/6:
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
325/7:
s_train = []
s_test = []
for i in range(NUM_OF_LETTERS):
    s_train.append(y_train[:, i, :])
    s_test.append(y_test[:, i, :])
325/8:
save_dir = os.path.join(PATH, 'saved_models')
model_name = 'keras_cifar10_trained_model.h5'
325/9:
input_layer = Input((25, 67, 1))
x = Conv2D(filters=32, kernel_size=(5, 5), padding='same', activation='relu')(input_layer)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Conv2D(filters=48, kernel_size=(5, 5), padding='same', activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Conv2D(filters=64, kernel_size=(5, 5), padding='same', activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Dropout(0.3)(x)
x = Flatten()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.3)(x)

out = [Dense(num_alphabet, name='digit%d' % i, activation='softmax')(x) for i in range(NUM_OF_LETTERS)]
# out = Dense(num_alphabet*5, activation='sigmoid')(x)

model = Model(inputs=input_layer, outputs=out)
325/10:
# model_path = os.path.join(save_dir, model_name)
# model = keras.models.load_model(model_path)
325/11:
# initiate Adam optimizer

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


model.summary()
325/12:
hist_train_loss_digit = {i:[] for i in range(5)}
hist_test_loss_digit = {i:[] for i in range(5)}

hist_train_acc_digit = {i:[] for i in range(5)}
hist_test_acc_digit = {i:[] for i in range(5)}

hist_train_loss = []
hist_test_loss = []

hist_train_acc = []
hist_test_acc = []
325/13:
digit_acc = [[] for _ in range(NUM_OF_LETTERS)]
val_digit_acc = [[] for _ in range(NUM_OF_LETTERS)]
loss = []
val_loss = []
325/14:
history = model.fit(x_train, s_train,
                    batch_size=BATCH_SIZE,
                    epochs=EPOCHS,
                    verbose=1,
                    validation_data=(x_test, s_test)
                   )
325/15:
digit_acc = [[] for _ in range(NUM_OF_LETTERS)]
val_digit_acc = [[] for _ in range(NUM_OF_LETTERS)]
loss = []
val_loss = []


def plot_diagram(digit_acc_now, val_digit_acc_now, loss_now, val_loss_now):
    global digit_acc, val_digit_acc, loss, val_loss
    
    
    for i in range(NUM_OF_LETTERS):
        digit_acc[i].extend(digit_acc_now[i])
        val_digit_acc[i].extend(val_digit_acc_now[i])
    loss.extend(loss_now)
    val_loss.extend(val_loss_now)
    
    for i in range(NUM_OF_LETTERS):
        s = {0:'First', 1:'Second', 2:'Third', 3:'Fourth', 4:'Fifth'}[i]
        # plt.plot(val_digit_acc[i], label='%s Digit Train' % s)
        plt.plot(digit_acc[i], label='%s Digit Test' % s)

    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()
    plt.show()

    for i in range(NUM_OF_LETTERS):
        s = {0:'First', 1:'Second', 2:'Third', 3:'Fourth', 4:'Fifth'}[i]
        plt.plot(val_digit_acc[i], label='%s Digit Train' % s)
        # plt.plot(digit_acc[i], label='%s Digit Test' % s)

    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()
    plt.show()



    # Plot training & validation loss values
    
    plt.plot(val_loss, label='Train')
    plt.plot(loss, label='Test')
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    plt.show()
325/16:
plot_diagram(
    
    [history.history['digit%d_accuracy' % i] for i in range(NUM_OF_LETTERS)],
    [history.history['val_digit%d_accuracy' % i] for i in range(NUM_OF_LETTERS)],
    history.history['loss'],
    history.history['val_loss'],
)
325/17:
# Save model and weights
if not os.path.isdir(save_dir):
    os.makedirs(save_dir)
model_path = os.path.join(save_dir, model_name)
model.save(model_path)
print('Saved trained model at %s ' % model_path)
325/18:
# Score trained model.
scores = model.evaluate(x_train, s_train, verbose=1)
print('Train loss:     %f' % np.mean(scores[0:5]))
acc = 1.
for i in range(5):
    acc *= scores[6+i]
print('Train accuracy: %.2f' % (acc * 100.))
325/19:
scores = model.evaluate(x_test, s_test, verbose=1)
print('Test loss:     %f' % np.mean(scores[0:5]))
acc = 1.
for i in range(5):
    acc *= scores[6+i]
print('Test accuracy: %.2f' % (acc * 100.))
325/20:
if not os.path.exists(DATA_PATH):
    print('Generating Dataset')
    gen_dataset(DATA_PATH, 700*1000 , NUM_OF_LETTERS, IMG_COLS, IMG_ROW)
325/21:
if not os.path.exists(DATA_PATH):
    print('Generating Dataset')
    gen_dataset(DATA_PATH, 700*1000 , NUM_OF_LETTERS, IMG_COLS, IMG_ROW)
325/22:
if not os.path.exists(DATA_PATH):
    print('Generating Dataset')
    gen_dataset(DATA_PATH, 700*1000 , NUM_OF_LETTERS, IMG_COLS, IMG_ROW)
326/1: run rd_xls.py
326/2: !vi rd_xls.py
326/3: run rd_xls.py
326/4: [sheet.cell_value(row, col) for col in range(8)]
326/5: row=4
326/6: [sheet.cell_value(row, col) for col in range(8)]
326/7: df.head()
326/8: !vi rd_xls.py
326/9: row=4
326/10: run rd_xls.py
326/11: len(df0)
326/12: df0.tail
326/13: df0.tail()
326/14: df['#7-NOx_C'].mean
326/15: df['#7-NOx_C'].mean()
326/16: import numpy as np
326/17: np.mean(df['#7-NOx_C'])
326/18: df.head()
326/19: col=list(df.columns)
326/20:
for c in col[1:-1]:
    df0[c]=[float(i) for i in df0[c]]
326/21:
for c in col[1:-1]:
    df.loc[df[c]=='保養',c]=np.nan
    df0[c]=[float(i) for i in df0[c]]
326/22:
for c in col[1:-1]:
    df0.loc[df0[c]=='保養',c]=np.nan
    df0[c]=[float(i) for i in df0[c]]
326/23: i
326/24: c
326/25: col[1:-1]
326/26:
for c in col[1:-2]:
    df0.loc[df0[c]=='保養',c]=np.nan
    df0[c]=[float(i) for i in df0[c]]
326/27: np.mean(df['#7-NOx_C'])
326/28: np.mean(df0['#7-NOx_C'])
326/29: np.mean(df0['#7-SOx_C'])
326/30: col[1:-1]
326/31: np.mean(df0['#7-SO2_C'])
326/32: df0.set_index('date').to_csv("cems.csv")
326/33: !vi rd_xls.py
326/34: !vi rd_xls.py
326/35: col=list(df.columns)
326/36: !vi rd_xls.py
326/37: np.mean(df0['#7-VEL'])
326/38: np.mean(df0['#7-TEMP'])
326/39: cd /nas1/TEDS/teds11/ptse
326/40: cems=pd.read_csv('point_cems.csv')
326/41: cems.head()
326/42: cno='L0200633P007'
326/43: a=cems.loc[cems.CP_NO==cno]
326/44: len(a)
326/45: a.head()
326/46: col
326/47: np.mean(df0['#7-FLOW_C'])
326/48: a.FLOW.mean()
326/49: pt=pd.read_csv('point.csv')
326/50: pt=pd.read_csv('point.csv',encoding='big5')
326/51: pt.columns
326/52: tzpp=pt.loc[pt.C_NO=="L0200633"]
326/53: tzpp.HEI.mean()
326/54: tzpp.HEI.max()
326/55: a=pt[pt.HEI>=240]
326/56: len(a)
326/57: a
326/58: cno='L0200473P007'
326/59: a=cems.loc[cems.CP_NO==cno]
326/60: len(a)
326/61: cno='L0200473P001'
326/62: a=cems.loc[cems.CP_NO==cno]
326/63: len(a)
326/64: a=cems.loc[cems.CP_NO.map(lambda x:'L0200473' in x)]
326/65: len(a)
326/66: a.head()
326/67: cno='L0200473P701'
326/68: a=cems.loc[cems.CP_NO==cno]
326/69: len(a)
326/70: a.head()
326/71: a.FLOW.mean()
326/72: a.NOX.sum()
326/73: a.SOX.sum()
326/74: a.PM.sum()
326/75: pt.columns
326/76: pt.NO_P[:5]
326/77: pt.NO_S[:5]
326/78: a=pt[(pt.C_NO=='L0200473') & (pt.NO_S=='P701')]
326/79: len(a)
326/80: a
326/81: em=[i for i in pt.columns if '_EMI' in i]
326/82: len(em)
326/83: [(c,a[c]) for c in em]
326/84: [(c,list(a[c])[0]) for c in em]
326/85: p7=pt[(pt.C_NO=='L0200473') & (pt.NO_S=='P701')]
326/86: a=cems.loc[cems.CP_NO==cno]
326/87: a.head()
326/88: b=a.loc[a.FLOW<100]
326/89: len(b)
326/90: b
326/91: b=a.loc[a.FLOW<10000]
326/92: len(b)
326/93: b=a.loc[a.FLOW<1000000]
326/94: len(b)
326/95: b.head()
326/96: b=a.loc[a.SOX==0.]
326/97: len(b)
326/98: 8760-1370
326/99: 7390/8760
325/23:
x_train, y_train, x_test, y_test = load_data(DATA_PATH)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
326/100: cd /nas3/cmaqruns/2019TZPP/output/2019-01/grid03/smoke
326/101: import netCDF4
326/102: fname='teds11.1901.timvar.nc'
326/103: nc = netCDF4.Dataset(fname,'r')
326/104: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
326/105: V
326/106: fname='teds11.1901.const.nc'
326/107: nc = netCDF4.Dataset(fname,'r')
326/108: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
326/109: V
325/24:
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
325/25:
s_train = []
s_test = []
for i in range(NUM_OF_LETTERS):
    s_train.append(y_train[:, i, :])
    s_test.append(y_test[:, i, :])
325/26:
save_dir = os.path.join(PATH, 'saved_models')
model_name = 'keras_cifar10_trained_model.h5'
325/27:
input_layer = Input((25, 67, 1))
x = Conv2D(filters=32, kernel_size=(5, 5), padding='same', activation='relu')(input_layer)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Conv2D(filters=48, kernel_size=(5, 5), padding='same', activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Conv2D(filters=64, kernel_size=(5, 5), padding='same', activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Dropout(0.3)(x)
x = Flatten()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.3)(x)

out = [Dense(num_alphabet, name='digit%d' % i, activation='softmax')(x) for i in range(NUM_OF_LETTERS)]
# out = Dense(num_alphabet*5, activation='sigmoid')(x)

model = Model(inputs=input_layer, outputs=out)
325/28:
# model_path = os.path.join(save_dir, model_name)
# model = keras.models.load_model(model_path)
325/29:
# initiate Adam optimizer

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


model.summary()
325/30:
hist_train_loss_digit = {i:[] for i in range(5)}
hist_test_loss_digit = {i:[] for i in range(5)}

hist_train_acc_digit = {i:[] for i in range(5)}
hist_test_acc_digit = {i:[] for i in range(5)}

hist_train_loss = []
hist_test_loss = []

hist_train_acc = []
hist_test_acc = []
325/31:
digit_acc = [[] for _ in range(NUM_OF_LETTERS)]
val_digit_acc = [[] for _ in range(NUM_OF_LETTERS)]
loss = []
val_loss = []
325/32:
history = model.fit(x_train, s_train,
                    batch_size=BATCH_SIZE,
                    epochs=EPOCHS,
                    verbose=1,
                    validation_data=(x_test, s_test)
                   )
326/110: !top
326/111: pwd
326/112: cd /nas3/cmaqruns/2019TZPP
326/113:
import numpy as np
import sys,os, subprocess, netCDF4
from pandas import *
from pyproj import Proj
326/114: mm='02'
326/115:
targ='/u01/cmaqruns/2019TZPP/output/2019-'+mm+'/grid03/smoke/'
root='/nas1/cmaqruns/2019base/data/ptse/twn/'
fname=root+'fortBE.413_teds11.ptse'+mm+'.nc'
nc = netCDF4.Dataset(fname,'r')
v='CP_NO'
nopt,ii=nc[v].shape
L='L0200473'
cp_no=[''.join([str(i, encoding='utf-8') for i in nc[v][t,:]]) for t in range(nopt)]
LL=[i for i in cp_no if L in i]
L=LL[0]
v='stkheight'
hei=nc[v][:]
df=DataFrame({'cp':cp_no,'he':hei})
tzpp=df.loc[(df.cp==L)&(df.he==250)]
l_tzpp=len(tzpp)
326/116: l_tzpp
326/117: tzpp
326/118:
fname='teds11.19'+mm+'.timvar.nc'
fnamO='teds11.19'+mm+'.timvar.nci'
ncks=subprocess.check_output('which ncks',shell=True).decode('utf8').strip('\n')
os.system(ncks+' -O -d ROW,1,'+str(l_tzpp)+' '+root+fname+' '+targ+fnamO)
nc0 = netCDF4.Dataset(root+fname,'r')
nc = netCDF4.Dataset(targ+fnamO,'r+')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
nv=len(V[3])
326/119: targ='/nas3/cmaqruns/2019TZPP/output/2019-'+mm+'/grid03/smoke/'
326/120:
os.system(ncks+' -O -d ROW,1,'+str(l_tzpp)+' '+root+fname+' '+targ+fnamO)
nc0 = netCDF4.Dataset(root+fname,'r')
nc = netCDF4.Dataset(targ+fnamO,'r+')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
nt,nlay,nrow,ncol=(nc[V[3][0]].shape[i] for i in range(4))
nv=len(V[3])
326/121: nv
326/122:
fname=root+'fortBE.413_teds11.ptse'+mm+'.nc'
pt_nc = netCDF4.Dataset(fname,'r')
326/123:
v='CP_NO'
nopt,ii=nc[v].shape
L='L0200473'
cp_no=[''.join([str(i, encoding='utf-8') for i in nc[v][t,:]]) for t in range(nopt)]
LL=[i for i in cp_no if L in i]
326/124:
nc = netCDF4.Dataset(fname,'r')
v='CP_NO'
nopt,ii=nc[v].shape
L='L0200473'
cp_no=[''.join([str(i, encoding='utf-8') for i in nc[v][t,:]]) for t in range(nopt)]
LL=[i for i in cp_no if L in i]
326/125: LL
326/126: len(LL)
326/127:
LL=[i for i in cp_no if L in i]
L=LL[0]
326/128: L
326/129: V=[list(filter(lambda x:nc.variables[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
326/130: V
326/131: v='ycoord'
326/132: ycoord=nc[v][:]
326/133: df=DataFrame({'cp':cp_no,'he':hei,'ycoord':ycoord})
326/134: tzpp=df.loc[(df.cp==L)&(df.he==250)]
326/135: tzpp
326/136: tzpp=pt.loc[pt.C_NO=="L0200633"]
326/137: tzpp.NO_S
326/138: len(tzpp)
326/139: set(tzpp.NO_S)
326/140: tzpp=pt.loc[pt.C_NO=="L0200473"]
326/141: len(tzpp)
326/142: set(tzpp.NO_S)
326/143: a=cems.loc[cems.CP_NO.map(lambda x:'L0200473' in x)]
326/144: set(a.CP_NO)
326/145: v='ycoord'['L0200473P{:1d}01'.format(i) for i in range(1,9)]
326/146: ['L0200473P{:1d}01'.format(i) for i in range(1,9)]
326/147: ['L0200473P01{:1d}'.format(i) for i in [1,2]]
326/148:
CP_NO=['L0200473P{:1d}01'.format(i) for i in range(1,9)]+['L0200473P01{:1d}'.format(i) for i in [1,2]]
tzpp['P_NO']=CP_NO
326/149:
CP_NO=['L0200473P{:1d}01'.format(i) for i in range(1,9)]+['L0200473P01{:1d}'.format(i) for i in [1,2]]
tzpp['P_NO']=CP_NO
326/150: tzpp=df.loc[(df.cp==L)&(df.hei==250)]
326/151:
v='stkheight';hei=nc[v][:]
v='ycoord';ycoord=nc[v][:]
df=DataFrame({'cp':cp_no,'hei':hei,'ycoord':ycoord,'CP_NO':CP_NO})
tzpp=df.loc[(df.cp==L)&(df.hei==250)]
CP_NO=['L0200473P{:1d}01'.format(i) for i in range(1,9)]+['L0200473P01{:1d}'.format(i) for i in [1,2]]
tzpp['P_NO']=CP_NO
l_tzpp=len(tzpp)
326/152:
df=DataFrame({'cp':cp_no,'hei':hei,'ycoord':ycoord)}
tzpp=df.loc[(df.cp==L)&(df.hei==250)]
CP_NO=['L0200473P{:1d}01'.format(i) for i in range(1,9)]+['L0200473P01{:1d}'.format(i) for i in [1,2]]
tzpp['P_NO']=CP_NO
l_tzpp=len(tzpp)
326/153:
df=DataFrame({'cp':cp_no,'hei':hei,'ycoord':ycoord})
tzpp=df.loc[(df.cp==L)&(df.hei==250)]
CP_NO=['L0200473P{:1d}01'.format(i) for i in range(1,9)]+['L0200473P01{:1d}'.format(i) for i in [1,2]]
tzpp['P_NO']=CP_NO
l_tzpp=len(tzpp)
326/154: l_tzpp
326/155: tzpp
326/156: np.mean(df0['#7-FLOW_C'])
326/157: a.head()
326/158: set(a.CP_NO)
326/159: np.mean(a.FLOW)
326/160: np.mean(a.FLOW>10000)
326/161: np.mean(a.loc[a.SOX>0,'FLOW'])
326/162: a=pt[(pt.C_NO=='L0200473') & (pt.NO_S=='P101')]
326/163: [(c,list(a[c])[0]) for c in em]
326/164: a.head()
326/165: a=cems.loc[cems.CP_NO.map(lambda x:'L0200473' in x)]
326/166: a.head()
326/167: a=cems.loc[cems.CP_NO.map(lambda x:'L0200473' in x) & cems.NOX>0]
326/168: a.head()
326/169: a=cems.loc[cems.CP_NO.map(lambda x:'L0200473' in x) & cems.NOX>0 & cems.SOX>0]
326/170: a=cems.loc[cems.CP_NO.map(lambda x:'L0200473' in x) & (cems.NOX>0) & (cems.SOX>0)]
326/171: a.head()
326/172: a.FLOW.mean()
326/173: V
326/174: v='stkdiam'
326/175: stkdiam=nc[v][:]
326/176: df=DataFrame({'cp':cp_no,'hei':hei,'ycoord':ycoord,'stkdiam':stkdiam})
326/177: tzpp=df.loc[(df.cp==L)&(df.hei==250)]
326/178: tzpp
326/179: np.mean(a.loc[a.SOX>0,'FLOW'])
326/180: a.head()
326/181: conc=[1,6.18*64/22.4,17.62*46/22.4]
326/182: conc
326/183: oldem=[180,1216,1292]
326/184: conc=np.array([1,6.18*64/22.4,17.62*46/22.4])
326/185: oldem=np.array([180,1216,1292])
326/186: newem=1485311.5792150644*8.76*0.84*conc
326/187: newem
326/188: newem=1485311.5792150644*8.76*0.84*conc/1000/1000
326/189: newem
326/190: (oldem-newem)/oldem*100
326/191: np.array((oldem-newem)/oldem*100,atype=int)
326/192: ans=(oldem-newem)/oldem*100
326/193: np.array(ans,atype=int)
326/194: np.array(ans,dtype=int)
326/195: tflag0[:]
326/196:
fname=targ+'cmaq.ncf'
nc00 = netCDF4.Dataset(fname,'r')
tflag0=nc00['TFLAG'][:,0,:]
326/197: tflag0[:5]
326/198: ls *py
326/199: ls ~/bin/con*py
326/200: ls ~/bin/*conv*py
326/201: cp /home/kuang/bin/dtconvertor.py .
326/202: !grep dtconverto ~/bin/*py|M
326/203: !grep jul2dt\( ~/bin/*py|M
326/204: tflag0[0]
326/205: list(tflag0[0])
326/206: from dtconvertor import dt2jul, jul2dt
326/207: dt0=[jul2dt[list(i)] for i in tflag0]
326/208: dt0=[jul2dt(list(i)) for i in tflag0]
326/209: dt0[:5]
326/210: teds_cems='/nas1/TEDS/teds11/ptse/point_cems.csv'
326/211: ms=read_csv(teds_cems)
326/212: ms.head()
326/213: '{:02d}{:02d}{:02d}'.format(dt0[0].month,dt0[0].day,dt0[0].hour)
326/214: int('{:02d}{:02d}{:02d}'.format(dt0[0].month,dt0[0].day,dt0[0].hour))
326/215: ms.loc[ms.MDH==12500]
326/216: mdh=[int('{:02d}{:02d}{:02d}'.format(i.month,i.day,i.hour)) for i in dt0]
326/217: mdh[:5]
326/218: mdh[-5:]
326/219: V
326/220: np.sum(nc['NO2'][:])
326/221: np.sum(nc['NO'][:])
326/222: 191049200.0/21227698.0
326/223:
nc = netCDF4.Dataset(targ+fnamO,'r+')
V=[list(filter(lambda x:nc[x].ndim==j, [i for i in nc.variables])) for j in [1,2,3,4]]
326/224: V
326/225: [i for i in V[3] if i[0]=='P']
326/226: flows=np.zeros(shape=(nt0,l_tzpp))
326/227: nt0=len(tflag0[:,0])
326/228: flows=np.zeros(shape=(nt0,l_tzpp))
326/229: flows.shape
326/230: ms.head()
326/231: s=cems.loc[cems.MDH.map(lambda x:x in set(mdh))].reset_index(drop=True)
326/232: s.head()
326/233: s.tail()
326/234: boo=(cems.MDH.map(lambda x:x in set(mdh))) & (cems.CP_NO.map(lambda x:x in CP_NO))
326/235: cems=cems.loc[boo].reset_index(drop=True)
326/236: len(cems)
326/237: cems.head()
326/238: len(mdh)
326/239: set(cems.CP_NO)
326/240: len(set(cems.CP_NO))
326/241: CP_NO
326/242: cems=pd.read_csv('point_cems.csv')
326/243: cems=pd.read_csv(teds_cems)
326/244: 'L0200473P012' in set(cems.CP_NO)
326/245: tzpp.tail()
326/246: len(tzpp)
326/247:
for cp in CP_NO:
  icp=CP_NO.index(cp)
  a=cems.loc[cems.CP_NO==cp,'FLOW'].reset_index(drop=True)
  if len(a)==0:continue #P012 data missing
  flows[:,icp]=np.array(a.FLOW)
326/248:
flows=np.zeros(shape=(nt0,l_tzpp))
for cp in CP_NO:
  icp=CP_NO.index(cp)
  a=cems.loc[cems.CP_NO==cp].reset_index(drop=True)
  if len(a)==0:continue #P012 data missing
  flows[:,icp]=np.array(a.FLOW)
326/249: cp
326/250: len(a)
326/251: cems=cems.loc[boo].reset_index(drop=True)
326/252:
flows=np.zeros(shape=(nt0,l_tzpp))
for cp in CP_NO:
  icp=CP_NO.index(cp)
  a=cems.loc[cems.CP_NO==cp].reset_index(drop=True)
  if len(a)==0:continue #P012 data missing
  flows[:,icp]=np.array(a.FLOW)
326/253: flows[:5,:5]
326/254: flows[0,:5]
326/255: flows[-5:,-5:]
326/256: sn_conc=np.array([6.16/22.4, 17.62/22.4])
326/257: sn_conc
326/258: targ
326/259:
tzpp['P_NO']=CP_NO
idx={cp:tzpp.loc[tzpp.P_NO==cp].index for cp in CP_NO}
326/260: idx
326/261: tzpp
326/262: targ
326/263: !lst /nas3/cmaqruns/2019TZPP/output/2019-02/grid03/smoke/
326/264: !lst /nas3/cmaqruns/2019TZPP/output/2019-02/grid03/smoke/
326/265: nc.close()
326/266: nc00.close()
325/33:
digit_acc = [[] for _ in range(NUM_OF_LETTERS)]
val_digit_acc = [[] for _ in range(NUM_OF_LETTERS)]
loss = []
val_loss = []


def plot_diagram(digit_acc_now, val_digit_acc_now, loss_now, val_loss_now):
    global digit_acc, val_digit_acc, loss, val_loss
    
    
    for i in range(NUM_OF_LETTERS):
        digit_acc[i].extend(digit_acc_now[i])
        val_digit_acc[i].extend(val_digit_acc_now[i])
    loss.extend(loss_now)
    val_loss.extend(val_loss_now)
    
    for i in range(NUM_OF_LETTERS):
        s = {0:'First', 1:'Second', 2:'Third', 3:'Fourth', 4:'Fifth'}[i]
        # plt.plot(val_digit_acc[i], label='%s Digit Train' % s)
        plt.plot(digit_acc[i], label='%s Digit Test' % s)

    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()
    plt.show()

    for i in range(NUM_OF_LETTERS):
        s = {0:'First', 1:'Second', 2:'Third', 3:'Fourth', 4:'Fifth'}[i]
        plt.plot(val_digit_acc[i], label='%s Digit Train' % s)
        # plt.plot(digit_acc[i], label='%s Digit Test' % s)

    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()
    plt.show()



    # Plot training & validation loss values
    
    plt.plot(val_loss, label='Train')
    plt.plot(loss, label='Test')
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    plt.show()
325/34:
plot_diagram(
    
    [history.history['digit%d_accuracy' % i] for i in range(NUM_OF_LETTERS)],
    [history.history['val_digit%d_accuracy' % i] for i in range(NUM_OF_LETTERS)],
    history.history['loss'],
    history.history['val_loss'],
)
325/35:
# Save model and weights
if not os.path.isdir(save_dir):
    os.makedirs(save_dir)
model_path = os.path.join(save_dir, model_name)
model.save(model_path)
print('Saved trained model at %s ' % model_path)
325/36:
# Score trained model.
scores = model.evaluate(x_train, s_train, verbose=1)
print('Train loss:     %f' % np.mean(scores[0:5]))
acc = 1.
for i in range(5):
    acc *= scores[6+i]
print('Train accuracy: %.2f' % (acc * 100.))
326/267: !lst /nas3/cmaqruns/2019TZPP/output/2019-02/grid03/smoke/
326/268: !crontab -e
326/269: !psg fcst
326/270: !kil fcst
326/271: !psg fcst
326/272: !lst /nas3/cmaqruns/2019TZPP/output/2019-02/grid03/smoke/
326/273: !vi improve_tzpp.py
325/37:
scores = model.evaluate(x_test, s_test, verbose=1)
print('Test loss:     %f' % np.mean(scores[0:5]))
acc = 1.
for i in range(5):
    acc *= scores[6+i]
print('Test accuracy: %.2f' % (acc * 100.))
327/1:
from selenium import webdriver
from selenium.webdriver.support.ui import Select
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
327/2:
href="https://law.moj.gov.tw/Hot/AddHotLaw.ashx?pcode=O0110017&amp;cur=Ln&amp;kw=%e6%b1%a1%e6%9f%93"
title="土壤污染評估調查及檢測作業管理辦法"+'.html'
driver = webdriver.Firefox()
driver.get(href)

with open(title, "w") as f:
    f.write(driver.page_source)
327/3:
href="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode=O0020126"
title="a"+'.html'
driver = webdriver.Firefox()
driver.get(href)

with open(title, "w") as f:
    f.write(driver.page_source)
327/4:
from bs4 import BeautifulSoup
import pandas as pd
with open(title,'r') as html:
#使用Beautiful Soup解析HTML
    soup = BeautifulSoup(html, 'html.parser')
table = soup.find('a', {'id': 'hlLawName'})
327/5: table
327/6: table.title
327/7: LawName = soup.find('a', {'id': 'hlLawName'})
327/8:
LawName = soup.find('a', {'id': 'hlLawName'})
dir(LawName)
327/9:
LawName = soup.find('a', {'id': 'hlLawName'}).getText
#dir(LawName)
327/10: LawName
327/11:
LawName = soup.find('a', {'id': 'hlLawName'})
dir(LawName)
327/12: LawName.getText()
327/13: LawDate = soup.find('tr',{'id': 'trLNODate'})
327/14: dir(LawDate)
327/15:
LawDate = soup.find('tr',{'id': 'trLNODate'}).getText()
LawDate
327/16:
LawDate = soup.find('tr',{'id': 'trLNODate'}).getText().split('\n')[1]
LawDate
327/17:
LawDate = soup.find('tr',{'id': 'trLNODate'}).getText().split('\n')[2]
LawDate
327/18:
a = soup.find('a', {'name': '1'})
dir(LawName)
327/19: a.name
327/20: a.getText()
327/21:
for i in range(1,999):
    try:
        a = soup.find('a', {'name': str(i)}).getText()
    except:
        break
    print(i)
327/22:
for i in range(1,999):
    try:
        a = soup.find('a', {'name': str(i)}).getText()
    except:
        break
    print(a)
327/23:
i=1
while True:
    try:
        a = soup.find('a', {'name': str(i)}).getText()
    except:
        break
    print(i,a)
    i+=1
327/24: article=soup.find('div', {'class': law-article})
327/25: article=soup.find('div', {'class': 'law-article'})
327/26: dir(article)
327/27: article.content
327/28: article.content()
327/29: article.contents
327/30: article=soup.findall('div', {'class': 'law-article'})
327/31: dir(soup)
327/32: article=soup.find_all('div', {'class': 'law-article'})
327/33: article.contents
327/34: article.contents()
327/35: article.getText()
327/36: len(article)
327/37: article[0].contents()
327/38: article[0]
327/39: article[0][1:]
327/40: article[0]
327/41: article[0].class
327/42: len(article[0])
327/43: len(article[1])
327/44: article[1][1:]
327/45: [i for i in article[1]]
327/46: article[1]
327/47: article[1][:]
327/48: list(article[1])[1:]
327/49: list(article[1])[1].getText()
327/50: article[2]
327/51: line(article[2])[1]
327/52: list(article[2])[1]
327/53: list(article[2])[1].class
327/54: list(article[2])[1].name
327/55: dir(list(article[2])[1])
327/56: list(article[2])[1].namespace()
327/57: list(article[2])[1].namespace
327/58: list(article[2])[1].find_all()
327/59: list(article[2])[1].find_all('div')
327/60: list(article[2])[1].find_all('div',{'class':'line-0004'})
327/61: a=list(article[2])[1].find_all('div',{'class':'line-0004'})
327/62: a
327/63: list(article[1])[1]
327/64: a=list(article[2])[1].find_all('div',{'class':'line-0000'})
327/65: a
327/66: a=list(article[2])[1].findAll('div',{'class':'line-0000'})
327/67: a
327/68: n=len(list(article[2])[1])
327/69: n
327/70: type(list(article[1])[1])
327/71: list(article[1])[1].text
327/72: list(article[1])[2].text
327/73: list(article[1])[3].text
327/74: list(article[1])
327/75: len(list(article[1]))
327/76: len(list(article[2]))
327/77: list(article[2])[1]
327/78: list(article[2])[1:]
327/79:
from bs4 import BeautifulSoup

# 您提供的HTML內容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div><div class="line-0004">一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。</div><div class="line-0004">二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div><div class="line-0000 show-number">審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON數據
json_data = {"parent": [], "child": []}
current_category = None

# 遍歷HTML內容
for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父層級
        current_category = "parent"
        json_data[current_category].append({"text": div.text})
    elif "line-0004" in div["class"]:
        # 子層級
        current_category = "child"
        if "children" not in json_data[current_category][-1]:
            json_data[current_category][-1]["children"] = []
        json_data[current_category][-1]["children"].append({"text": div.text})

# 將JSON數據轉為字符串
json_str = json.dumps(json_data, indent=4, ensure_ascii=False)

# 將JSON字符串寫入文件
with open("output.json", "w", encoding="utf-8") as json_file:
    json_file.write(json_str)

print("JSON文件已生成。")
327/80:
from bs4 import BeautifulSoup

# 您提供的HTML內容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div><div class="line-0004">一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。</div><div class="line-0004">二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div><div class="line-0000 show-number">審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON數據
json_data = {"parent": [], "child": []}
current_category = None

# 遍歷HTML內容
for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父層級
        current_category = "parent"
        json_data[current_category].append({"text": div.text})
    elif "line-0004" in div["class"]:
        # 子層級
        current_category = "child"
        if not json_data[current_category]:
            json_data[current_category].append({"text": ""})
        if "children" not in json_data[current_category][-1]:
            json_data[current_category][-1]["children"] = []
        json_data[current_category][-1]["children"].append({"text": div.text})

# 將JSON數據轉為字符串
327/81: json_data
327/82:
from bs4 import BeautifulSoup

# 您提供的HTML內容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div><div class="line-0004">一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。</div><div class="line-0004">二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div><div class="line-0000 show-number">審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON數據
json_data = {"parent": [], "child": []}
current_category = None
parent_counter = 0
child_counter = 0

# 遍歷HTML內容
for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父層級
        current_category = "parent"
        parent_counter += 1
        child_counter = 0
        json_data[current_category].append({"text": f"{parent_counter}. {div.text}"})
    elif "line-0004" in div["class"]:
        # 子層級
        current_category = "child"
        child_counter += 1
        if not json_data[current_category]:
            json_data[current_category].append({"text": ""})
        if "children" not in json_data[current_category][-1]:
            json_data[current_category][-1]["children"] = []
        json_data[current_category][-1]["children"].append({"text": f"{parent_counter}.{child_counter} {div.text}"})

# 將JSON數據轉為字符串
json_str = json.dumps(json_data, indent=4, ensure_ascii=False)

# 將JSON字符串寫入文件
with open("output.json", "w", encoding="utf-
327/83:
from bs4 import BeautifulSoup

# 您提供的HTML內容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div><div class="line-0004">一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。</div><div class="line-0004">二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div><div class="line-0000 show-number">審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON數據
json_data = {"parent": [], "child": []}
current_category = None
parent_counter = 0
child_counter = 0

# 遍歷HTML內容
for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父層級
        current_category = "parent"
        parent_counter += 1
        child_counter = 0
        json_data[current_category].append({"text": f"{parent_counter}. {div.text}"})
    elif "line-0004" in div["class"]:
        # 子層級
        current_category = "child"
        child_counter += 1
        if not json_data[current_category]:
            json_data[current_category].append({"text": ""})
        if "children" not in json_data[current_category][-1]:
            json_data[current_category][-1]["children"] = []
        json_data[current_category][-1]["children"].append({"text": f"{parent_counter}.{child_counter} {div.text}"})

# 將JSON數據轉為字符串
json_str = json.dumps(json_data, indent=4, ensure_ascii=False)

# 將JSON字符串寫入文件
with open("output.json", "w", encoding="utf-8") as json_file:
    json_file.write(json_str)

print("JSON文件已生成。")
327/84:
from bs4 import BeautifulSoup
import json

# 您提供的HTML內容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div><div class="line-0004">一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。</div><div class="line-0004">二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div><div class="line-0000 show-number">審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON數據
json_data = {"parent": [], "child": []}
current_category = None
parent_counter = 0
child_counter = 0

# 遍歷HTML內容
for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父層級
        current_category = "parent"
        parent_counter += 1
        child_counter = 0
        json_data[current_category].append({"text": f"{parent_counter}. {div.text}"})
    elif "line-0004" in div["class"]:
        # 子層級
        current_category = "child"
        child_counter += 1
        if not json_data[current_category]:
            json_data[current_category].append({"text": ""})
        if "children" not in json_data[current_category][-1]:
            json_data[current_category][-1]["children"] = []
        json_data[current_category][-1]["children"].append({"text": f"{parent_counter}.{child_counter} {div.text}"})

# 將JSON數據轉為字符串
json_str = json.dumps(json_data, indent=4, ensure_ascii=False)

# 將JSON字符串寫入文件
with open("output.json", "w", encoding="utf-8") as json_file:
    json_file.write(json_str)

print("JSON文件已生成。")
327/85: json_data
327/86:
from bs4 import BeautifulSoup
import json

# 您提供的HTML内容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div><div class="line-0004">一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。</div><div class="line-0004">二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div><div class="line-0000 show-number">審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON数据
json_data = {}

# 遍历HTML内容
current_key = None
current_counter = 0

for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父层级
        current_key = f"{current_counter + 1}"
        current_counter += 1
    elif "line-0004" in div["class"]:
        # 子层级
        current_key = f"{current_counter}.{len(json_data.get(str(current_counter), [])) + 1}"

    json_data[current_key] = div.text

# 将JSON数据转为字符串
json_str = json.dumps(json_data, indent=4, ensure_ascii=False)

# 将JSON字符串写入文件
with open("output.json", "w", encoding="utf-8") as json_file:
    json_file.write(json_str)

print("JSON文件已生成。")
327/87: json_data
327/88:
from bs4 import BeautifulSoup
import json

# 您提供的HTML内容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，
於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）
提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div>
<div class="line-0004">
一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合
附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。
</div><div class="line-0004">二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣
污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣
污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div>
<div class="line-0000 show-number">審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理
一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON数据
json_data = {}

# 遍历HTML内容
current_key = None
current_counter = 0

for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父层级
        current_key = f"{current_counter + 1}"
        current_counter += 1
    elif "line-0004" in div["class"]:
        # 子层级
        current_key = f"{current_counter}.{len(json_data.get(str(current_counter), [])) + 1}"

    json_data[current_key] = div.text

# 将JSON数据转为字符串
json_str = json.dumps(json_data, indent=4, ensure_ascii=False)

# 将JSON字符串写入文件
with open("output.json", "w", encoding="utf-8") as json_file:
    json_file.write(json_str)

print("JSON文件已生成。")
327/89:
from bs4 import BeautifulSoup
import json

# 您提供的HTML内容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div>
<div class="line-0004">
一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。</div>
<div class="line-0004">
二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div>
<div class="line-0000 show-number">
審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON数据
json_data = {}

# 遍历HTML内容
current_key = None
current_counter = 0

for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父层级
        current_key = f"{current_counter + 1}"
        current_counter += 1
    elif "line-0004" in div["class"]:
        # 子层级
        current_key = f"{current_counter}.{len(json_data.get(str(current_counter), [])) + 1}"

    json_data[current_key] = div.text

# 将JSON数据转为字符串
json_str = json.dumps(json_data, indent=4, ensure_ascii=False)

# 将JSON字符串写入文件
with open("output.json", "w", encoding="utf-8") as json_file:
    json_file.write(json_str)

print("JSON文件已生成。")
327/90: json_data
327/91:
from bs4 import BeautifulSoup
import json

# 您提供的HTML内容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div><div class="line-0004">一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。</div><div class="line-0004">二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div><div class="line-0000 show-number">審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON数据
json_data = {}

# 遍历HTML内容
current_category = None
current_counter = 0

for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父层级
        current_counter += 1
        current_category = str(current_counter)
        json_data[current_category] = {'text': div.text}
    elif "line-0004" in div["class"]:
        # 子层级
        sub_category = f"{current_category}.{len(json_data[current_category]) + 1}"
        json_data[current_category].setdefault('children', {})[sub_category] = {'text': div.text}

# 将JSON数据转为字符串
json_str = json.dumps(json_data, ensure_ascii=False, indent=2)

# 输出JSON字符串
print(json_str)
327/92:
from bs4 import BeautifulSoup
import json

# 您提供的HTML内容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div><div class="line-0004">一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。</div><div class="line-0004">二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div><div class="line-0000 show-number">審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON数据
json_data = {}
current_category = None
current_counter = 0

# 遍历HTML内容
for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父层级
        current_counter += 1
        current_category = str(current_counter)
        json_data[current_category] = {'text': div.text}
    elif "line-0004" in div["class"]:
        # 子层级
        sub_category = f"{current_category}.{len(json_data[current_category]) - 1}"
        if "children" not in json_data[current_category]:
            json_data[current_category]["children"] = {}
        json_data[current_category]["children"][sub_category] = {'text': div.text}

# 将JSON数据转为字符串
json_str = json.dumps(json_data, ensure_ascii=False, indent=2)

# 输出JSON字符串
print(json_str)
328/1:
from selenium import webdriver
from selenium.webdriver.support.ui import Select
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
328/2:
from bs4 import BeautifulSoup

# 您提供的HTML內容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div><div class="line-0004">一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。</div><div class="line-0004">二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div><div class="line-0000 show-number">審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON數據
json_data = {"parent": [], "child": []}
current_category = None

# 遍歷HTML內容
for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父層級
        current_category = "parent"
        json_data[current_category].append({"text": div.text})
    elif "line-0004" in div["class"]:
        # 子層級
        current_category = "child"
        if not json_data[current_category]:
            json_data[current_category].append({"text": ""})
        if "children" not in json_data[current_category][-1]:
            json_data[current_category][-1]["children"] = []
        json_data[current_category][-1]["children"].append({"text": div.text})

# 將JSON數據轉為字符串
328/3: len(json_data)
328/4:
from bs4 import BeautifulSoup
import json

# 您提供的HTML内容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div><div class="line-0004">一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。</div><div class="line-0004">二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div><div class="line-0000 show-number">審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON数据
json_data = {}
current_category = None
current_counter = 0

# 遍历HTML内容
for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父层级
        current_counter += 1
        current_category = str(current_counter)
        json_data[current_category] = {'text': div.text}
    elif "line-0004" in div["class"]:
        # 子层级
        sub_category = f"{current_category}.{len(json_data[current_category]) - 1}"
        if "children" not in json_data[current_category]:
            json_data[current_category]["children"] = {}
        json_data[current_category]["children"][sub_category] = {'text': div.text}

# 将JSON数据转为字符串
json_str = json.dumps(json_data, ensure_ascii=False, indent=2)

# 输出JSON字符串
print(json_str)
329/1:
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import pandas as pd
import os, sys, time, random, glob, shutil

'''
usage: get_eia.py B01 6 60 (kind of project, start and end of sequences)
'''

df=pd.read_csv(sys.argv[1]+'.csv')
df=df.loc[df.cat!='cat'].reset_index(drop=True)
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例

source_directory = "." #"/home/kuang/Downloads"
chrome_driver="/home/kuang/.cache/selenium/chromedriver/linux64/119.0.6045.105/chromedriver"
chrome_options = Options()
chrome_options.add_argument("--headless")  # 启用无头模式
iend={'C':14,'A':31}
ib,ie=180,181
329/2:
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import pandas as pd
import os, sys, time, random, glob, shutil

'''
usage: get_eia.py B01 6 60 (kind of project, start and end of sequences)
'''

df=pd.read_csv('cat4all.csv')
df=df.loc[df.cat!='cat'].reset_index(drop=True)
col=list(df.columns[:])
# 创建一个 Chrome 浏览器实例

source_directory = "." #"/home/kuang/Downloads"
chrome_driver="/home/kuang/.cache/selenium/chromedriver/linux64/119.0.6045.105/chromedriver"
chrome_options = Options()
chrome_options.add_argument("--headless")  # 启用无头模式
iend={'C':14,'A':31}
ib,ie=180,181
329/3:
for i in range(ib,ie): #range(5,len(df)):
    driver = webdriver.Chrome()#options=chrome_options)
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
329/4:
for i in range(ib,ie): #range(5,len(df)):
    driver = webdriver.Chrome(options=chrome_options)
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
329/5:
for i in range(ib,ie): #range(5,len(df)):
    driver = webdriver.Firefox() #Chrome(options=chrome_options)
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
329/6:
for i in range(ib,ie): #range(5,len(df)):
    driver = webdriver.Firefox() #Chrome(options=chrome_options)
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    #driver.quit()
329/7: ib,ie=181,182
329/8:
for i in range(ib,ie): #range(5,len(df)):
    driver = webdriver.Firefox() #Chrome(options=chrome_options)
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.epa.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
329/9: url
329/10: url.replace('epa','moenv')
329/11:
for i in range(ib,ie): #range(5,len(df)):
    driver = webdriver.Firefox() #Chrome(options=chrome_options)
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
329/12:
for i in range(ib,ie): #range(5,len(df)):
    driver = webdriver.Chrome(options=chrome_options)
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
329/13: ib,ie=181,182
329/14:
from selenium import webdriver
from selenium.webdriver.firefox.options import Options

# 创建一个无头Firefox浏览器选项对象
options = Options()
options.headless = True  # 设置为True以启用无头模式
329/15:
from selenium import webdriver
from selenium.webdriver.firefox.options import Options

# 创建一个无头Firefox浏览器选项对象
options = Options()
options.add_argument('--headless')  # 使用--headless参数启用无头模式
329/16:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    driver = webdriver.Firefox(options=options)
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
    # 关闭浏览器
    driver.quit()
329/17:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 关闭浏览器
                driver.quit()
                # 生成5到20之间的随机秒数
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/18: url
329/19:
from selenium import webdriver
from selenium.webdriver.firefox.options import Options

# 创建一个无头Firefox浏览器选项对象
options = Options()
options.add_argument('--headless')  # 使用--headless参数启用无头模式
options.add_argument('--port=46199')  # 设置WebDriver的端口号
329/20:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                #driver.get(url)
                # 关闭浏览器
                #driver.quit()
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/21: ib,ie=183,184
329/22: ib,ie=183,184
329/23:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 关闭浏览器
                driver.quit()
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/24:
from selenium import webdriver
from selenium.webdriver.firefox.options import Options

# 创建一个无头Firefox浏览器选项对象
options = Options()
options.add_argument('--headless')  # 使用--headless参数启用无头模式
options.add_argument('--port=46199')  # 设置WebDriver的端口号
329/25: ac
329/26: url
329/27: ib,ie=182,183
329/28:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 关闭浏览器
                driver.quit()
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/29:
from selenium import webdriver
from selenium.webdriver.firefox.options import Options

# 创建一个无头Firefox浏览器选项对象
options = Options()
options.add_argument('--headless')  # 使用--headless参数启用无头模式
options.add_argument('--port=46199')  # 设置WebDriver的端口号
329/30:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver.get(url)
                # 关闭浏览器
                driver.quit()
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/31:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                driver = webdriver.Firefox(options=options)
                driver.get(url)
                # 关闭浏览器
                driver.quit()
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/32: urls
329/33:
driver = webdriver.Firefox()
driver.get(urls[1])
driver.quit()
329/34:
driver = webdriver.Firefox()
driver.get(urls[2])
driver.close()  # 关闭当前窗口
driver.quit()
329/35:
import requests

pdf_url = urls[3]
response = requests.get(pdf_url)

with open("C04.pdf", "wb") as pdf_file:
    pdf_file.write(response.content)
329/36:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            try:
                response = requests.get(url)
                with open(pdf_file, "wb") as f:
                    f.write(response.content)
                # 关闭浏览器
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            except:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/37: type(response)
329/38: dir(response)
329/39: response.text
329/40: response.text[:10]
329/41: response.text[:20]
329/42: ib,ie=183,184
329/43:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            response = requests.get(url)
            if 'DOCTYPE' not in response.text[:20]:
                with open(pdf_file, "wb") as f:
                    f.write(response.content)
                # 关闭浏览器
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            else:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/44: ib,ie=184,185
329/45:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            response = requests.get(url)
            if 'DOCTYPE' not in response.text[:20]:
                with open(pdf_file, "wb") as f:
                    f.write(response.content)
                # 关闭浏览器
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            else:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                     if pdf_file=='C01.PDF':
                         url=url.replace(pdf_file,'000.PDF')
                         driver.get(url)
                         time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                         break #no more trying
                     else:
                         continue #try appendix files
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/46:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            response = requests.get(url)
            if 'DOCTYPE' not in response.text[:20]:
                with open(pdf_file, "wb") as f:
                    f.write(response.content)
                # 关闭浏览器
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            else:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                    if pdf_file!='C01.PDF':continue #try appendix files
                    url=url.replace(pdf_file,'000.PDF')
                    response = requests.get(url)
                    if 'DOCTYPE' not in response.text[:20]:
                        with open(pdf_file, "wb") as f:
                            f.write(response.content)
                        # 关闭浏览器
                        # 生成5到20之间的随机秒数
                        print(url)
                    time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                    break #no more trying
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/47: url
329/48: response.text[:20]
329/49: url
329/50:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            response = requests.get(url)
            if 'DOCTYPE' not in response.text[:20]:
                with open(pdf_file, "wb") as f:
                    f.write(response.content)
                # 关闭浏览器
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            else:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                    if pdf_file!='C01.PDF':continue #try appendix files
                    url=url.replace(pdf_file,'000.PDF')
                    pdf_file=url[-7:]
                    if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):break
                    response = requests.get(url)
                    if 'DOCTYPE' not in response.text[:20]:
                        with open(pdf_file, "wb") as f:
                            f.write(response.content)
                        # 关闭浏览器
                        # 生成5到20之间的随机秒数
                        print(url)
                    time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                    break #no more trying
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/51:
                with open(pdf_file, "wb") as f:
                    f.write(response.content)
329/52: response.header
329/53: response.header()
329/54: dir(response)
329/55: response.headers()
329/56: response.headers
329/57: urls[0]
329/58:
esponse1 = requests.get(urls[0])
response1.headers
329/59:
esponse1 = requests.get(urls[0])
esponse1.headers
329/60:
esponse1 = requests.get(urls[0])
esponse1.headers['Content-Type']
329/61: response.headers['Content-Type']
329/62: response.headers['Content-Type']=='text/html; charset=utf-8'
329/63: response.headers['Content-Type']
329/64: response.headers['Content-Type']=='application/download'
329/65:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            response = requests.get(url)
            if response.headers['Content-Type']=='application/download'
                with open(pdf_file, "wb") as f:
                    f.write(response.content)
                # 关闭浏览器
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            else:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                    if pdf_file!='C01.PDF':continue #try appendix files
                    url=url.replace(pdf_file,'000.PDF')
                    pdf_file=url[-7:]
                    if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):break
                    response = requests.get(url)
                    if response.headers['Content-Type']=='application/download'
                        with open(pdf_file, "wb") as f:
                            f.write(response.content)
                        # 关闭浏览器
                        # 生成5到20之间的随机秒数
                        print(url)
                    time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                    break #no more trying
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/66:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            response = requests.get(url)
            if response.headers['Content-Type']=='application/download':
                with open(pdf_file, "wb") as f:
                    f.write(response.content)
                # 关闭浏览器
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            else:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                    if pdf_file!='C01.PDF':continue #try appendix files
                    url=url.replace(pdf_file,'000.PDF')
                    pdf_file=url[-7:]
                    if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):break
                    response = requests.get(url)
                    if response.headers['Content-Type']=='application/download':
                        with open(pdf_file, "wb") as f:
                            f.write(response.content)
                        # 关闭浏览器
                        # 生成5到20之间的随机秒数
                        print(url)
                    time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                    break #no more trying
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/67: ib,ie=185,186
329/68:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            response = requests.get(url)
            if response.headers['Content-Type']=='application/download':
                with open(pdf_file, "wb") as f:
                    f.write(response.content)
                # 关闭浏览器
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            else:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                    if pdf_file!='C01.PDF':continue #try appendix files
                    url=url.replace(pdf_file,'000.PDF')
                    pdf_file=url[-7:]
                    if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):break
                    response = requests.get(url)
                    if response.headers['Content-Type']=='application/download':
                        with open(pdf_file, "wb") as f:
                            f.write(response.content)
                        # 关闭浏览器
                        # 生成5到20之间的随机秒数
                        print(url)
                    time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                    break #no more trying
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
329/69: ib,ie=186,187
329/70:
for i in range(ib,ie): #range(5,len(df)):
    # 使用geckodriver创建Firefox WebDriver并传递选项
    id=df['id'][i]
    cat=df['cat'][i]
    nam=df['name'][i]
    target_directory = "/nas2/sespub/epa_reports/"+cat+'/'+id+'_'+nam
    url_root='https://eiadoc.moenv.gov.tw/eiaweb/DownloadFiles.ashx?shcode='+id+'&sFileName='
    if not os.path.exists(os.path.expanduser(target_directory)):
        os.makedirs(os.path.expanduser(target_directory))
    for ac in 'CA':
        pdf_files=[ac+'{:02d}'.format(i)+'.PDF' for i in range(1,iend[ac])]
        urls = [url_root+p for p in pdf_files] # 循环遍历 URL 列表并下载文件
        for url in urls:
            pdf_file=url[-7:]
            if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):continue
            response = requests.get(url)
            if response.headers['Content-Type']=='application/download':
                with open(pdf_file, "wb") as f:
                    f.write(response.content)
                # 关闭浏览器
                # 生成5到20之间的随机秒数
                print(url)
                time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
            else:
                #print('file:'+id+url[-7:]+'not exist')
                if ac=='C':
                    if pdf_file!='C01.PDF':continue #try appendix files
                    url=url.replace(pdf_file,'000.PDF')
                    pdf_file=url[-7:]
                    if os.path.exists(os.path.join(os.path.expanduser(target_directory), pdf_file)):break
                    response = requests.get(url)
                    if response.headers['Content-Type']=='application/download':
                        with open(pdf_file, "wb") as f:
                            f.write(response.content)
                        # 关闭浏览器
                        # 生成5到20之间的随机秒数
                        print(url)
                    time.sleep(random.uniform(5, 20))  # 等待加载完成，根据需要调整等待时间
                    break #no more trying
                break #last appendix file
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.PDF"))
    if len(pdf_files)>0:
        for pdf_file in pdf_files:
            shutil.move(pdf_file, os.path.join(os.path.expanduser(target_directory), os.path.basename(pdf_file)))
            #print(f"移动文件 {pdf_file} 到 {target_directory}")
328/5:
html_content="<div class="row"><div class="col-no"> <a href="LawSingle.aspx?pcode=O0020001&flno=3" name="3">第 3 條</a></div><div class="col-data"><div class="law-article">
<div class="line-0000">本法用詞，定義如下：</div><div class="line-0004">一、空氣污染物：指空氣中足以直接或間接妨害國民健康或生活環境之物質。</div><div class="line-0004">二、污染源：指排放空氣污染物之物理或化學操作單元，其類別如下：</div><div class="line-0006">（一）移動污染源：指因本身動力而改變位置之污染源。</div><div class="line-0006">（二）固定污染源：指移動污染源以外之污染源。</div><div class="line-0004">三、汽車：指在道路上不依軌道或電力架設，而以原動機行駛之車輛，包括機車。</div><div class="line-0004">四、生活環境：指與人之生活有密切關係之財產、動、植物及其生育環境。</div><div class="line-0004">五、排放標準：指排放廢氣所容許混存各種空氣污染物之最高濃度、總量或單位原（物）料、燃料、產品之排放量。</div><div class="line-0004">六、空氣品質標準：指室外空氣中空氣污染物濃度限值。</div><div class="line-0004">七、空氣污染防制區（以下簡稱防制區）：指視地區土地利用對於空氣品質之需求，或依空氣品質現況，劃定之各級防制區。</div><div class="line-0004">八、自然保護（育）區：指生態保育區、自然保留區、野生動物保護區及國有林自然保護區。</div><div class="line-0004">九、總量管制：指在一定區域內，為有效改善空氣品質，對於該區域空氣污染物總容許排放數量所作之限制措施。</div><div class="line-0004">十、總量管制區：指依地形及氣象條件，按總量管制需求劃定之區域。</div><div class="line-0006">十一、控制技術：指固定污染源為減少空氣污染物所採取之污染減量技術，主要類別如下：</div><div class="line-0206">（一）最佳可行控制技術：指考量能源、環境、經濟之衝擊後，污染源應採取之已商業化並可行污染排放最大減量技術。</div><div class="line-0206">（二）最低可達成排放率控制技術：指考量能源、環境、經濟、健康等衝擊後，並依據科學方法，污染源應採取之減少污染物排放至最低排放率之技術。</div><div class="line-0006">十二、怠速：指汽車停車時，維持引擎持續運轉之情形。</div><div class="line-0006">十三、空氣品質維護區：指為維護空氣品質，得限制或禁止移動污染源使用之特定區域。</div><div class="line-0006">十四、含揮發性有機物化學製品：指經使用有排放揮發性有機物之任何物質、產品或物品。</div></div>
</div></div>"
328/6:
html_content='''<div class="row"><div class="col-no"> <a href="LawSingle.aspx?pcode=O0020001&flno=3" name="3">第 3 條</a></div><div class="col-data"><div class="law-article">
<div class="line-0000">本法用詞，定義如下：</div><div class="line-0004">一、空氣污染物：指空氣中足以直接或間接妨害國民健康或生活環境之物質。</div><div class="line-0004">二、污染源：指排放空氣污染物之物理或化學操作單元，其類別如下：</div><div class="line-0006">（一）移動污染源：指因本身動力而改變位置之污染源。</div><div class="line-0006">（二）固定污染源：指移動污染源以外之污染源。</div><div class="line-0004">三、汽車：指在道路上不依軌道或電力架設，而以原動機行駛之車輛，包括機車。</div><div class="line-0004">四、生活環境：指與人之生活有密切關係之財產、動、植物及其生育環境。</div><div class="line-0004">五、排放標準：指排放廢氣所容許混存各種空氣污染物之最高濃度、總量或單位原（物）料、燃料、產品之排放量。</div><div class="line-0004">六、空氣品質標準：指室外空氣中空氣污染物濃度限值。</div><div class="line-0004">七、空氣污染防制區（以下簡稱防制區）：指視地區土地利用對於空氣品質之需求，或依空氣品質現況，劃定之各級防制區。</div><div class="line-0004">八、自然保護（育）區：指生態保育區、自然保留區、野生動物保護區及國有林自然保護區。</div><div class="line-0004">九、總量管制：指在一定區域內，為有效改善空氣品質，對於該區域空氣污染物總容許排放數量所作之限制措施。</div><div class="line-0004">十、總量管制區：指依地形及氣象條件，按總量管制需求劃定之區域。</div><div class="line-0006">十一、控制技術：指固定污染源為減少空氣污染物所採取之污染減量技術，主要類別如下：</div><div class="line-0206">（一）最佳可行控制技術：指考量能源、環境、經濟之衝擊後，污染源應採取之已商業化並可行污染排放最大減量技術。</div><div class="line-0206">（二）最低可達成排放率控制技術：指考量能源、環境、經濟、健康等衝擊後，並依據科學方法，污染源應採取之減少污染物排放至最低排放率之技術。</div><div class="line-0006">十二、怠速：指汽車停車時，維持引擎持續運轉之情形。</div><div class="line-0006">十三、空氣品質維護區：指為維護空氣品質，得限制或禁止移動污染源使用之特定區域。</div><div class="line-0006">十四、含揮發性有機物化學製品：指經使用有排放揮發性有機物之任何物質、產品或物品。</div></div>
</div></div>'''
328/7:
from bs4 import BeautifulSoup
import json



# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON数据
json_data = {}
current_category = None
current_counter = 0

# 遍历HTML内容
for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if len(json_data)=0:
        current_counter += 1
        current_category = str(current_counter)
        json_data[current_category] = {'text': div.text}
        
    if "line-0000" in div["class"]:
        # 父层级
        current_counter += 1
        current_category = str(current_counter)
        json_data[current_category] = {'text': div.text}
    elif "line-0004" in div["class"]:
        # 子层级
        sub_category = f"{current_category}.{len(json_data[current_category]) - 1}"
        if "children" not in json_data[current_category]:
            json_data[current_category]["children"] = {}
        json_data[current_category]["children"][sub_category] = {'text': div.text}

# 将JSON数据转为字符串
json_str = json.dumps(json_data, ensure_ascii=False, indent=2)

# 输出JSON字符串
print(json_str)
328/8:
from bs4 import BeautifulSoup
import json



# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON数据
json_data = {}
current_category = None
current_counter = 0

# 遍历HTML内容
for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if len(json_data)==0:
        current_counter += 1
        current_category = str(current_counter)
        json_data[current_category] = {'text': div.text}
        
    if "line-0000" in div["class"]:
        # 父层级
        current_counter += 1
        current_category = str(current_counter)
        json_data[current_category] = {'text': div.text}
    elif "line-0004" in div["class"]:
        # 子层级
        sub_category = f"{current_category}.{len(json_data[current_category]) - 1}"
        if "children" not in json_data[current_category]:
            json_data[current_category]["children"] = {}
        json_data[current_category]["children"][sub_category] = {'text': div.text}

# 将JSON数据转为字符串
json_str = json.dumps(json_data, ensure_ascii=False, indent=2)

# 输出JSON字符串
print(json_str)
328/9:
law=''
for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    law+=div.text
print(law)
328/10:
law=''
soup2=soup.find_all('div', class_="law-article")
for div in soup2.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    law+=div.text
print(law)
328/11:
law=''
soup2=soup.find_all('div', class_="law-article")
for div in soup2.find('div', class_=["line-0000 show-number", "line-0004"]):
    law+=div.text
print(law)
328/12:
law=''
soup2=soup.find_all('div', class_="law-article")
for div in soup2.findAll('div', class_=["line-0000 show-number", "line-0004"]):
    law+=div.text
print(law)
328/13: type(soup2)
328/14: dir(soup2)
328/15:
from bs4 import BeautifulSoup

# 您提供的HTML內容
html_content = """
<div class="col-data"><div class="law-article">
<div class="line-0000 show-number">前條應削減排放量之公私場所，於向直轄市、縣（市）主管機關或中央主管機關委託之機關（以下簡稱審核機關）提出固定污染源操作許可證之展延申請時，應依下列規定辦理：</div><div class="line-0004">一、既存固定污染源之製程符合附表所列應符合條件者，應檢具最近一年檢測報告或其他足以證明符合附表所列之排放管道濃度或削減率之證明文件，併同固定污染源操作許可證展延申請所需檢附資料一併辦理。</div><div class="line-0004">二、既存固定污染源之製程未能符合附表所列應符合條件，需增加空氣污染防制設施者，應檢具其空氣污染物防制設施種類、構造、效能、流程、設計圖說、設置經費及進度之空氣污染防制計畫，向審核機關申請核定工程改善所需期限，改善期限不得逾中華民國一百十四年六月三十日。</div><div class="line-0000 show-number">審核機關受理前項第二款核定工程改善所需期限之申請，屬專責處理一般廢棄物之廢棄物焚化處理程序空氣污染防制計畫，經公私場所報請審核機關核准者，其改善期限不受前項第二款限制。</div></div>
</div>
"""

# 使用BeautifulSoup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 初始化JSON數據
json_data = {"parent": [], "child": []}
current_category = None

# 遍歷HTML內容
for div in soup.find_all('div', class_=["line-0000 show-number", "line-0004"]):
    if "line-0000" in div["class"]:
        # 父層級
        current_category = "parent"
        json_data[current_category].append({"text": div.text})
    elif "line-0004" in div["class"]:
        # 子層級
        current_category = "child"
        if not json_data[current_category]:
            json_data[current_category].append({"text": ""})
        if "children" not in json_data[current_category][-1]:
            json_data[current_category][-1]["children"] = []
        json_data[current_category][-1]["children"].append({"text": div.text})

# 將JSON數據轉為字符串
328/16:
href="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode=O0020126"
title="a"+'.html'
driver = webdriver.Firefox()
driver.get(href)

with open(title, "w") as f:
    f.write(driver.page_source)
328/17:
from bs4 import BeautifulSoup
import pandas as pd
with open(title,'r') as html:
#使用Beautiful Soup解析HTML
    soup = BeautifulSoup(html, 'html.parser')
LawName = soup.find('a', {'id': 'hlLawName'}).getText()
LawDate = soup.find('tr',{'id': 'trLNODate'}).getText().split('\n')[2]
328/18:
i=1
while True:
    try:
        a = soup.find('a', {'name': str(i)}).getText()
    except:
        break
    print(i,a)
    i+=1
328/19: article=soup.find_all('div', {'class': 'law-article'})
328/20: soup2
328/21: list(soup2)[1]
328/22: list(soup2)
328/23: soup2.div
328/24:
law=''
soup2=soup.find_all('div', class_="law-article")
for div in soup2.find('div', class_=["line-0000 show-number", "line-0004"]):
    law+=div.text
print(law)
328/25:
law=''
soup2=soup.find_all('div', class_="law-article")
all_text = ''.join(soup2.stripped_strings)
print(all_text)
328/26:
soup = BeautifulSoup(html_content, 'html.parser')
law=''
soup2=soup.find_all('div', class_="law-article")
all_text = ''.join(soup2.stripped_strings)
print(all_text)
328/27:
i=1
while True:
    try:
        a = soup.find('a', {'name': str(i)}).getText()
    except:
        break
    print(i,a)
    i+=1
articles = soup.find_all('a', href=True, name=True)

# 初始化一个字典来存储结果
result = {}

# 遍历每个條
for article in articles:
    article_name = article.text.strip()
    article_number = article['name']
    next_element = article.find_next('div', class_='line-0000')
    
    if next_element:
        article_text = next_element.text.strip()
        result[article_name] = article_text

# 打印结果
for key, value in result.items():
    print(f'{key}: {value}')
328/28:
i=1
while True:
    try:
        a = soup.find('a', {'name': str(i)}).getText()
    except:
        break
    print(i,a)
    i+=1
articles = soup.find('a', href=True, name=True)

# 初始化一个字典来存储结果
result = {}

# 遍历每个條
for article in articles:
    article_name = article.text.strip()
    article_number = article['name']
    next_element = article.find_next('div', class_='line-0000')
    
    if next_element:
        article_text = next_element.text.strip()
        result[article_name] = article_text

# 打印结果
for key, value in result.items():
    print(f'{key}: {value}')
328/29:
i=1
while True:
    try:
        a = soup.find('a', {'name': str(i)}).getText()
    except:
        break
    print(i,a)
    i+=1
articles = soup.find_all('a', href=True, name=True)

# 初始化一个字典来存储结果
result = {}

# 遍历每个條
for article in articles:
    article_name = article.text.strip()
    article_number = article['name']
    next_element = article.find_next('div', class_='line-0000')
    
    if next_element:
        article_text = next_element.text.strip()
        result[article_name] = article_text

# 打印结果
for key, value in result.items():
    print(f'{key}: {value}')
328/30:
i=1
while True:
    try:
        a = soup.find('a', {'name': str(i)}).getText()
    except:
        break
    print(i,a)
    i+=1
articles = soup.find_all('a', {'href': True, 'name': True})
# 初始化一个字典来存储结果
result = {}

# 遍历每个條
for article in articles:
    article_name = article.text.strip()
    article_number = article['name']
    next_element = article.find_next('div', class_='line-0000')
    
    if next_element:
        article_text = next_element.text.strip()
        result[article_name] = article_text

# 打印结果
for key, value in result.items():
    print(f'{key}: {value}')
328/31:
from bs4 import BeautifulSoup
import pandas as pd
with open(title,'r') as html:
#使用Beautiful Soup解析HTML
    soup = BeautifulSoup(html, 'html.parser')
LawName = soup.find('a', {'id': 'hlLawName'}).getText()
LawDate = soup.find('tr',{'id': 'trLNODate'}).getText().split('\n')[2]
328/32:
i=1
while True:
    try:
        a = soup.find('a', {'name': str(i)}).getText()
    except:
        break
    print(i,a)
    i+=1
articles = soup.find_all('a', {'href': True, 'name': True})
# 初始化一个字典来存储结果
result = {}

# 遍历每个條
for article in articles:
    article_name = article.text.strip()
    article_number = article['name']
    next_element = article.find_next('div', class_='line-0000')
    
    if next_element:
        article_text = next_element.text.strip()
        result[article_name] = article_text

# 打印结果
for key, value in result.items():
    print(f'{key}: {value}')
328/33:
i=1
while True:
    try:
        a = soup.find('a', {'name': str(i)}).getText()
    except:
        break
    print(i,a)
    i+=1
articles = soup.find_all('a', {'href': True, 'name': True})
# 初始化一个字典来存储结果
result = {}

# 遍历每个條
for article in articles:
    article_name = article.text.strip()
    if article_name=="::::":continue
    article_number = article['name']
    next_element = article.find_next('div', class_='line-0000')
    
    if next_element:
        article_text = next_element.text.strip()
        result[article_name] = article_text

# 打印结果
for key, value in result.items():
    print(f'{key}: {value}')
328/34:
i=1
while True:
    try:
        a = soup.find('a', {'name': str(i)}).getText()
    except:
        break
    print(i,a)
    i+=1
articles = soup.find_all('a', {'href': True, 'name': True})
# 初始化一个字典来存储结果
result = {}

# 遍历每个條
for article in articles:
    article_name = article.text.strip()
    if ":" in article_name:continue
    article_number = article['name']
    next_element = article.find_next('div', class_='line-0000')
    
    if next_element:
        article_text = next_element.text.strip()
        result[article_name] = article_text

# 打印结果
for key, value in result.items():
    print(f'{key}: {value}')
328/35:
with open(file='href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents:
    soup = BeautifulSoup(html_content, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'title': True})
328/36:
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents:
    soup = BeautifulSoup(html_content, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'title': True})
328/37: articles
328/38: articles.href
328/39: dir(articles)
328/40:
for i in articles:
    print(i)
328/41:
for i in articles:
    i.find('href')
    print(i)
328/42:
for i in articles:
    href=i.find('href')
    print(i,href)
328/43:
for i in articles:
    a=i.find('href')
    print(a)
328/44:
for i in articles:
    a=i.find('title')
    print(a)
328/45:
for i in articles:
    a=i.find('title')
    print(i)
328/46: type(articles)
328/47:
soup = BeautifulSoup(html_content, 'html.parser')

# 获取 <a> 元素
a_tag = soup.find('a')

# 提取 URL 和标题
url = a_tag['href']
title = a_tag['title']

# 打印结果
print(f'URL: {url}')
print(f'Title: {title}')
328/48:
from bs4 import BeautifulSoup
import requests
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href']
    title = a_tag['title']
    response = requests.get(url)

    with open(title+"html", "wb") as f:
        f.write(response.content)
    soup = BeautifulSoup(response, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
328/49:
from bs4 import BeautifulSoup
import requests
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href']
    title = a_tag['title']
    response = requests.get(url)
    with open(title+".html", "wb") as f:
        f.write(response.content)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
328/50: articles
328/51: url
328/52:
from bs4 import BeautifulSoup
import requests
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
    response = requests.get(url)
    with open(title+".html", "wb") as f:
        f.write(response.content)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
328/53: url
328/54: response.headers
328/55:
from bs4 import BeautifulSoup
import requests
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
    response = requests.get(url)
    with open(title+".html", "wb") as f:
        f.write(response.content)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
328/56:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#    with open(title+".html", "wb") as f:
#        f.write(response.content)
    os.system('/usr/bin/wget '+url+' -O '+title+'.html')
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    ith open(title+".json", "wb") as f:
        json.dump(f,result)
328/57:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#    with open(title+".html", "wb") as f:
#        f.write(response.content)
    os.system('/usr/bin/wget '+url+' -O '+title+'.html')
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/58:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#    with open(title+".html", "wb") as f:
#        f.write(response.content)
    os.system('url="'+url+'";/usr/bin/wget $url'+' -O '+title+'.html')
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/59:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#    with open(title+".html", "wb") as f:
#        f.write(response.content)
    os.system('url="'+url+'";/usr/bin/wget -q $url'+' -O '+title+'.html')
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/60: url
328/61: url.replace('&','\&')
328/62: url.replace('?','\?')
328/63: os.system('url="'+url.replace('?','\?')+'";/usr/bin/wget -q $url'+' -O '+title+'.html')
328/64: os.system('url="'+url.replace('?','\?')+'";/usr/bin/wget -q $url'+' -O '+title+'.html')
328/65:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#    with open(title+".html", "wb") as f:
#        f.write(response.content)
    os.system('url="'+url.replace('?','\?')+'";/usr/bin/wget -q $url'+' -O '+title+'.html')
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/66: result
328/67: articles
328/68:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#    with open(title+".html", "wb") as f:
#        f.write(response.content)
    os.system('url="'+url.replace('?','\?')+'";/usr/bin/wget -q $url'+' -O '+title+'.html')
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/69:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#    with open(title+".html", "wb") as f:
#        f.write(response.content)
    os.system('/usr/bin/wget -q '+url.replace('?','\?')+' -O '+title+'.html')
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/70:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#        f.write(response.content)
#    os.system('/usr/bin/wget -q '+url.replace('?','\?')+' -O '+title+'.html')
    driver = webdriver.Firefox()
    driver.get(href)
    with open(title+".html", "wb") as f:
        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/71:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#        f.write(response.content)
#    os.system('/usr/bin/wget -q '+url.replace('?','\?')+' -O '+title+'.html')
    driver = webdriver.Firefox()
    driver.get(url)
    with open(title+".html", "wb") as f:
        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/72: url
328/73:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = '"'+a_tag['href'].split('&')[0]+'""'
    title = a_tag['title']
    response = requests.get(url)
    with open(title+".html", "wb") as f:
        f.write(response.content)
#    os.system('/usr/bin/wget -q '+url.replace('?','\?')+' -O '+title+'.html')
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/74:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = '"'+a_tag['href'].split('&')[0]+'"'
    title = a_tag['title']
    response = requests.get(url)
    with open(title+".html", "wb") as f:
        f.write(response.content)
#    os.system('/usr/bin/wget -q '+url.replace('?','\?')+' -O '+title+'.html')
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/75:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
    response = requests.get(url)
    with open(title+".html", "wb") as f:
        f.write(response.content)
#    os.system('/usr/bin/wget -q '+url.replace('?','\?')+' -O '+title+'.html')
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/76:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
    with open('a.cs','w') as f:
        f.write('url='+url+';/usr/bin/wget -q $url -O '+title+'.html\n')
    os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/77:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
    url2=url.replace('?','\?').replace('=','\=')
    with open('a.cs','w') as f:
        f.write('url='+url2+';/usr/bin/wget -q $url -O '+title+'.html\n')
    os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/78:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
    url2=url.replace('?','\?').replace('=','\=')
    with open('a.cs','w') as f:
        f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
    os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/79:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0]
    title = a_tag['title']
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
    url2=url.replace('?','\\?').replace('=','\\=')
    with open('a.cs','w') as f:
        f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
    os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/80:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
    url2=url.replace('?','\\?').replace('=','\\=')
    with open('a.cs','w') as f:
        f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
    os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/81: url
328/82: articles
328/83:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:1]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
    url2=url.replace('?','\\?').replace('=','\\=')
    with open('a.cs','w') as f:
        f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
    os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
#        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/84:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[5:6]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
    url2=url.replace('?','\\?').replace('=','\\=')
    with open('a.cs','w') as f:
        f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
    os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
#        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "wb") as f:
        json.dump(f,result)
328/85:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[5:6]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
    url2=url.replace('?','\\?').replace('=','\\=')
    with open('a.cs','w') as f:
        f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
    os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
#        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "wb") as f:
        json.dump(result,f)
328/86: result
328/87:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[5:6]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
    url2=url.replace('?','\\?').replace('=','\\=')
    with open('a.cs','w') as f:
        f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
    os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
#        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
328/88:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
    url2=url.replace('?','\\?').replace('=','\\=')
    with open('a.cs','w') as f:
        f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
    os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
328/89:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser(./), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': 'trLNODate'}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
328/90:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': 'trLNODate'}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
328/91: title
328/92:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate'}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
328/93:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate']}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
330/1: import json
330/2:
with open('土壤及地下水污染整治法.json','r') as f:
    dd=json.load(f)
330/3: dd
328/94:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href_w.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate']}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
328/95:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href_n.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate']}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
328/96:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href_e.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate']}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
331/1:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href_s.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate']}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
331/2:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href_s.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate']}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
331/3:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href_s.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate']}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
331/4:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href_s.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate']}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
331/5:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href_s.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate']}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
331/6:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href_s.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate']}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
331/7: title
331/8: title
331/9:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href_s.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate']}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
331/10: title
331/11: url
331/12:
from bs4 import BeautifulSoup
import requests
import os, json
with open('href_s.txt', 'r') as f:
    html_contents=[i for i in f]
for html_content in html_contents[:]:
    soup = BeautifulSoup(html_content, 'html.parser')
    # ▒~N▒▒~O~V <a> ▒~E~C▒|
    a_tag = soup.find('a')

    # ▒~O~P▒~O~V URL ▒~R~L▒| ~G▒~X
    url = a_tag['href'].split('&')[0].split('=')[1]
    url="https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode="+url
    title = a_tag['title']
    if not os.path.exists(os.path.join(os.path.expanduser("./"), title+'.html')):
#    response = requests.get(url)
#        f.write(response.content)
#    with open(title+".html", "wb") as f:
        url2=url.replace('?','\\?').replace('=','\\=')
        with open('a.cs','w') as f:
            f.write('url='+url2+'\n/usr/bin/wget -q $url -O '+title+'.html\n')
        os.system('chmod u+x ./a.cs;./a.cs' )
#    driver = webdriver.Firefox()
#    driver.get(url)
#        f.write(driver.page_source)
    with open(title+".html",'r') as html:
#使用Beautiful Soup解析HTML
        soup = BeautifulSoup(html, 'html.parser')
    LawName = soup.find('a', {'id': 'hlLawName'}).getText()
    LawDate = soup.find('tr',{'id': ['trLNODate','trLNNDate']}).getText().split('\n')[2]
    articles = soup.find_all('a', {'href': True, 'name': True})
    result = {'LawName':LawName,'LawDate':LawDate}

    # 遍历每个條
    for article in articles:
        article_name = article.text.strip()
        if ":" in article_name:continue
        article_number = article['name']
        next_element = article.find_next('div', class_='line-0000')
    
        if next_element:
            article_text = next_element.text.strip()
            result[article_name] = article_text
    if len(result)==-0:continue
    with open(title+".json", "w") as f:
        json.dump(result,f)
332/1: from pandas import *
332/2: df=read_csv('C07.csv')
332/3: set(df.lev)
332/4: pwd
332/5: ls *csv
333/1:
import PyPDF2
import sys, glob
333/2: !pip install PyPDF2
333/3:
import PyPDF2
import sys, glob
333/4:
def dots(l):
    tdic={'':'.','(':')','▒~H':'▒~I'}
    for p in ['','(','(']:
        for i in range(1,11):
            a=p+str(i)+tdic[p]
            n=len(a)
            if l[:n]==a:return True
    return False
def CNnum(l):
    num=['▒~@','▒~L','▒~I','▒~[~[','▒~T','▒~E▒','▒~C','▒~E▒','▒~]','▒~M~A',]
    tdic={'':'▒~@~A','(':')','▒~H':'▒~I'}
    for p in ['','(','(']:
        for i in num:
            a=p+i+tdic[p]
            n=len(a)
            if l[:n]==a:return True
    return False
def ENnum(l):
    num='ABCDEFGHabcdefgh'
    tdic={'':['.','▒~@~A'],'(':')','▒~H':'▒~I'}
    for p in ['','(']:
        for i in num:
            t=tdic[p]
            if type(t)==list:
                for tt in t:
                    a=p+i+tt
                    n=len(a)
                    if l[:n]==a:return True
            else:
                a=p+i+t
                n=len(a)
                if l[:n]==a:return True
    return False
333/5: lst ../B27_能源或輸變電工程之開發/
333/6: !lst ../B27_能源或輸變電工程之開發/
333/7: !lst "../B27_能源或輸變電工程之開發/*環境影響說明書*"
333/8: ls -ld "../B27_能源或輸變電工程之開發/*環境影響說明書*"
333/9: ls -ld ../B27_能源或輸變電工程之開發/*環境影響說明書*
333/10: ll -ld ../B27_能源或輸變電工程之開發/*環境影響說明書*
334/1:
import PyPDF2
import sys, glob
334/2:
def dots(l):
    tdic={'':'.','(':')','▒~H':'▒~I'}
    for p in ['','(','(']:
        for i in range(1,11):
            a=p+str(i)+tdic[p]
            n=len(a)
            if l[:n]==a:return True
    return False
def CNnum(l):
    num=['▒~@','▒~L','▒~I','▒~[~[','▒~T','▒~E▒','▒~C','▒~E▒','▒~]','▒~M~A',]
    tdic={'':'▒~@~A','(':')','▒~H':'▒~I'}
    for p in ['','(','(']:
        for i in num:
            a=p+i+tdic[p]
            n=len(a)
            if l[:n]==a:return True
    return False
def ENnum(l):
    num='ABCDEFGHabcdefgh'
    tdic={'':['.','▒~@~A'],'(':')','▒~H':'▒~I'}
    for p in ['','(']:
        for i in num:
            t=tdic[p]
            if type(t)==list:
                for tt in t:
                    a=p+i+tt
                    n=len(a)
                    if l[:n]==a:return True
            else:
                a=p+i+t
                n=len(a)
                if l[:n]==a:return True
    return False
334/3: ll -ld ../B27_能源或輸變電工程之開發/*環境影響說明書*
334/4: ls -lrth --color=auto --show-control-chars -hF --color=tty -d ../B27_能源或輸變電工程之開發/*環境影響說明書*
334/5: ls -lrth --color=auto --show-control-chars -hF --color=tty -d ../B27_能源或輸變電工程之開發/*環境影響說明書*|grep -v 差異
334/6: ls -lrth --color=auto --show-control-chars -hF --color=tty -d ../B27_能源或輸變電工程之開發/*環境影響說明書*|grep -v 差異|grep -v 變更
334/7: ls -lrth --color=auto --show-control-chars -hF --color=tty -d ../B27_能源或輸變電工程之開發/*環境影響說明書*|grep -v 差異|grep -v 變更|grep -v 調查
334/8: source_directory="../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書"
334/9: pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "C*.PDF"))
334/10: import os
334/11: pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "C*.PDF"))
334/12: pdf_files
334/13: fname=pdf_files[5]
334/14: int(fname.split('/')[-1][1:3])
334/15: lst
334/16: !lst
334/17: run pp.py ../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書
334/18: run pp.py ../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書
334/19: run pp.py ../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書
334/20: run pp.py ../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書
334/21: run pp.py ../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書
334/22: run pp.py ../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書
334/23: run pp.py ../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書
334/24: !vi pdf2txt.py
334/25: run level.py ../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書
334/26: run level.py ../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書
334/27: run level.py ../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書
334/28: len(df)
334/29: df
334/30: len(a)
334/31: len(lines)
334/32: lines
334/33: a[:50]
334/34: nlines
334/35: i
334/36: fname
334/37: lines
334/38: df
334/39: mnlev,mxlev
334/40: run level.py ../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書
334/41: fname
334/42: run level.py ../B27_能源或輸變電工程之開發/1070061A_協和發電廠更新改建計畫環境影響說明書
335/1:
from selenium import webdriver
from selenium.webdriver.support.ui import Select
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def click_wait(pth,itm):
    # 点击元素
    element = driver.find_element(pth,itm)
    element.click()
# 等待直到页面更新
    wait = WebDriverWait(driver, 10)  # 设置至少等待时间
    wait.until(EC.staleness_of(element))  # 等待直到元素变为陈旧，即页面更新
    return 0
335/2:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.ID,"DaataTables_Table_0_length"))
select.select_by_value("50")
335/3:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.NAME,"DaataTables_Table_0_length"))
select.select_by_value("50")
335/4:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.NAME,"DataTables_Table_0_length"))
select.select_by_value("50")
335/5:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.id,"DataTables_Table_0_length"))
select.select_by_value("50")
335/6:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.ID,"DataTables_Table_0_length"))
select.select_by_value("50")
335/7:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
select.select_by_value('50')
335/8:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

wait_time = 10

# 创建 WebDriverWait 实例
wait = WebDriverWait(driver, wait_time)
select = wait.until(EC.presence_of_element_located((By.XPATH, '//select[@name="DataTables_Table_0_length"]')))

# 选择计划类别为"工厂的设立"
#select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
select.select_by_value('50')
335/9:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

wait_time = 10

# 创建 WebDriverWait 实例
wait = WebDriverWait(driver, wait_time)
element = wait.until(EC.presence_of_element_located((By.XPATH, '//select[@name="DataTables_Table_0_length"]')))

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
select.select_by_value('50')
335/10:
from bs4 import BeautifulSoup

# 你提供的 HTML 内容
html_content = '<li class="paginate_button page-item "><a href="https://epq.moenv.gov.tw/Query/ResultList?Classification=01#" aria-controls="DataTables_Table_0" data-dt-idx="7" tabindex="0" class="page-link">117</a></li>'

# 使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find('li', {'class': 'paginate_button page-item '})

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/11: dir(page_element)
335/12: page_element
335/13:
from bs4 import BeautifulSoup

# 你提供的 HTML 内容
html_content = '''
<li class="paginate_button page-item ">
<a href="https://epq.moenv.gov.tw/Query/ResultList?Classification=01#" aria-controls="DataTables_Table_0" data-dt-idx="7" tabindex="0" class="page-link">117</a>
</li>
'''

# 使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find('li', {'class': 'paginate_button page-item '})

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/14:
from bs4 import BeautifulSoup

# 你提供的 HTML 内容
html_content = '''
<li class="paginate_button page-item ">
<a href="https://epq.moenv.gov.tw/Query/ResultList?Classification=01#" aria-controls="DataTables_Table_0" data-dt-idx="7" tabindex="0" class="page-link">117</a>
</li>
'''

# 使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find_all('li', {'class': 'paginate_button page-item '})

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/15:
from bs4 import BeautifulSoup

# 你提供的 HTML 内容
html_content = '''
<li class="paginate_button page-item ">
<a href="https://epq.moenv.gov.tw/Query/ResultList?Classification=01#" aria-controls="DataTables_Table_0" data-dt-idx="7" tabindex="0" class="page-link">117</a>
</li>
'''

# 使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.findAll('li', {'class': 'paginate_button page-item '})

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/16:
from bs4 import BeautifulSoup

# 你提供的 HTML 内容
html_content = '''
<li class="paginate_button page-item ">
<a href="https://epq.moenv.gov.tw/Query/ResultList?Classification=01#" aria-controls="DataTables_Table_0" data-dt-idx="7" tabindex="0" class="page-link">117</a>
</li>
'''

# 使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find('li', {'class': 'paginate_button page-item '})

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/17:
from bs4 import BeautifulSoup

# 你提供的 HTML 内容
html_content = '''
<li class="paginate_button page-item ">
<a href="https://epq.moenv.gov.tw/Query/ResultList?Classification=01#" aria-controls="DataTables_Table_0" data-dt-idx="7" tabindex="0" class="page-link">117</a>
</li>
'''

# 使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find('li', {'class': 'paginate_button page-item '} {'aria-controls':"DataTables_Table_0})

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/18:
from bs4 import BeautifulSoup

# 你提供的 HTML 内容
html_content = '''
<li class="paginate_button page-item ">
<a href="https://epq.moenv.gov.tw/Query/ResultList?Classification=01#" aria-controls="DataTables_Table_0" data-dt-idx="7" tabindex="0" class="page-link">117</a>
</li>
'''

# 使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find('li', [{'class': 'paginate_button page-item '}, {'aria-controls':"DataTables_Table_0}])

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/19:
from bs4 import BeautifulSoup

# 你提供的 HTML 内容
html_content = '''
<li class="paginate_button page-item ">
<a href="https://epq.moenv.gov.tw/Query/ResultList?Classification=01#" aria-controls="DataTables_Table_0" data-dt-idx="7" tabindex="0" class="page-link">117</a>
</li>
'''

# 使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find('li', [{'class': 'paginate_button page-item '}, {'aria-controls':"DataTables_Table_0"}])

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/20:
from bs4 import BeautifulSoup

# 你提供的 HTML 内容
html_content = '''
<li class="paginate_button page-item ">
<a href="https://epq.moenv.gov.tw/Query/ResultList?Classification=01#" aria-controls="DataTables_Table_0" data-dt-idx="7" tabindex="0" class="page-link">117</a>
</li>
'''

# 使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find('li', [{'class': 'paginate_button page-item '},])

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/21: page_element
335/22: len(page_element)
335/23:
from bs4 import BeautifulSoup

# 你提供的 HTML 内容
html_content = '''
<li class="paginate_button page-item ">
<a href="https://epq.moenv.gov.tw/Query/ResultList?Classification=01#" aria-controls="DataTables_Table_0" data-dt-idx="7" tabindex="0" class="page-link">117</a>
</li>
'''

# 使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find('li', [{'class': 'paginate_button'},]) # page-item '

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/24:
from bs4 import BeautifulSoup

# 你提供的 HTML 内容
html_content = '''
<li class="paginate_button page-item ">
<a href="https://epq.moenv.gov.tw/Query/ResultList?Classification=01#" aria-controls="DataTables_Table_0" data-dt-idx="7" tabindex="0" class="page-link">117</a>
</li>
'''

# 使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find('li', {'class': 'paginate_button'}) # page-item '

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/25:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

wait_time = 10

# 创建 WebDriverWait 实例
wait = WebDriverWait(driver, wait_time)
element = wait.until(EC.presence_of_element_located((By.XPATH, '//select[@name="DataTables_Table_0_length"]')))

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
select.select_by_value('50')
fnames=['p1.html']
with open(fnames[1], "w") as f:
    f.write(driver.page_source)
with open(fnames[1], "r") as f:
    f.write(driver.page_source)
html_content=driver.page_source
#使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find('li', {'class': 'paginate_button'}) # page-item '

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/26:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

wait_time = 10

# 创建 WebDriverWait 实例
wait = WebDriverWait(driver, wait_time)
element = wait.until(EC.presence_of_element_located((By.XPATH, '//select[@name="DataTables_Table_0_length"]')))

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
select.select_by_value('50')
fnames=['p1.html']
with open(fnames[0], "w") as f:
    f.write(driver.page_source)
with open(fnames[1], "r") as f:
    f.write(driver.page_source)
html_content=driver.page_source
#使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find('li', {'class': 'paginate_button'}) # page-item '

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/27:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

wait_time = 10

# 创建 WebDriverWait 实例
wait = WebDriverWait(driver, wait_time)
element = wait.until(EC.presence_of_element_located((By.XPATH, '//select[@name="DataTables_Table_0_length"]')))

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
select.select_by_value('50')
fnames=['p1.html']
with open(fnames[0], "w") as f:
    f.write(driver.page_source)
html_content=driver.page_source
#使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find('li', {'class': 'paginate_button'}) # page-item '

# 提取页码文本
page_number = page_element.text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/28: len(page_element)
335/29: page_element = soup.find_all('li', {'class': 'paginate_button'}) # page-item '
335/30: len(page_element)
335/31: page_element[-1]
335/32: page_element[-1].text.strip()
335/33: page_element[-2].text.strip()
335/34:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

wait_time = 10

# 创建 WebDriverWait 实例
wait = WebDriverWait(driver, wait_time)
element = wait.until(EC.presence_of_element_located((By.XPATH, '//select[@name="DataTables_Table_0_length"]')))

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
select.select_by_value('50')
fnames=['p1.html']
with open(fnames[0], "w") as f:
    f.write(driver.page_source)
html_content=driver.page_source
#使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find_all('li', {'class': 'paginate_button'}) # page-item '

# 提取页码文本
page_number = page_element[-2].text.strip()

# 输出最后一页的页码
print("最后一页的页码是:", page_number)
335/35:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

wait_time = 10

# 创建 WebDriverWait 实例
wait = WebDriverWait(driver, wait_time)
element = wait.until(EC.presence_of_element_located((By.XPATH, '//select[@name="DataTables_Table_0_length"]')))

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
select.select_by_value('50')
fnames=['p1.html']
with open(fnames[0], "w") as f:
    f.write(driver.page_source)
html_content=driver.page_source
#使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find_all('li', {'class': 'paginate_button'}) # page-item '

# 提取页码文本
npage = int(page_element[-2].text.strip())
fnames+=['p'+str(i)+'.html' for i in range(2,npage+1)]
for i in range(2,npage+1):
    pth=By.LINK_TEXT;ii=str(i)
    result=click_wait(pth,ii)
    if os.path.exists(fnames[i]):continue
    with open(fnames[i], "w") as f:
        f.write(driver.page_source)   
    time.sleep(10)
335/36:
from selenium import webdriver
from selenium.webdriver.support.ui import Select
import time, os
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def click_wait(pth,itm):
    # 点击元素
    element = driver.find_element(pth,itm)
    element.click()
# 等待直到页面更新
    wait = WebDriverWait(driver, 10)  # 设置至少等待时间
    wait.until(EC.staleness_of(element))  # 等待直到元素变为陈旧，即页面更新
    return 0
335/37:
driver = webdriver.Firefox()
driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification=01#")

wait_time = 10

# 创建 WebDriverWait 实例
wait = WebDriverWait(driver, wait_time)
element = wait.until(EC.presence_of_element_located((By.XPATH, '//select[@name="DataTables_Table_0_length"]')))

# 选择计划类别为"工厂的设立"
select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
select.select_by_value('50')
fnames=['p1.html']
with open(fnames[0], "w") as f:
    f.write(driver.page_source)
html_content=driver.page_source
#使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找包含页码的元素
page_element = soup.find_all('li', {'class': 'paginate_button'}) # page-item '

# 提取页码文本
npage = int(page_element[-2].text.strip())
fnames+=['p'+str(i)+'.html' for i in range(2,npage+1)]
for i in range(2,npage+1):
    pth=By.LINK_TEXT;ii=str(i)
    result=click_wait(pth,ii)
    if os.path.exists(fnames[i]):continue
    with open(fnames[i], "w") as f:
        f.write(driver.page_source)   
    time.sleep(10)
335/38:
from selenium.webdriver.firefox.options import Options
firefox_options = Options()
firefox_options.add_argument("--headless")  # 启用无头模式

driver = webdriver.Firefox(options=firefox_options)
wait_time = 10
for cat in ['{:02d}'.format(i) for i in range(1,2)]:
    driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification="+cat+"#")


    # 创建 WebDriverWait 实例
    wait = WebDriverWait(driver, wait_time)
    element = wait.until(EC.presence_of_element_located((By.XPATH, '//select[@name="DataTables_Table_0_length"]')))

    # 选择计划类别为"工厂的设立"
    select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
    select.select_by_value('50')
    fnames=['p1.html']
    with open(fnames[0], "w") as f:
        f.write(driver.page_source)
    html_content=driver.page_source
    #使用 BeautifulSoup 解析 HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # 查找包含页码的元素
    page_element = soup.find_all('li', {'class': 'paginate_button'}) # page-item '

    # 提取页码文本
    npage = int(page_element[-2].text.strip())
    fnames+=['p'+cat+'_'+str(i)+'.html' for i in range(2,npage+1)]
    for i in range(2,npage+1):
        pth=By.LINK_TEXT;ii=str(i)
        result=click_wait(pth,ii)
        if os.path.exists(fnames[i]):continue
        with open(fnames[i], "w") as f:
            f.write(driver.page_source)   
        time.sleep(10)    
    driver.quit()
335/39:
from selenium.webdriver.firefox.options import Options
firefox_options = Options()
firefox_options.add_argument("--headless")  # 启用无头模式

driver = webdriver.Firefox(options=firefox_options)
wait_time = 10
for cat in ['{:02d}'.format(i) for i in range(1,20)]:
    driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification="+cat+"#")


    # 创建 WebDriverWait 实例
    wait = WebDriverWait(driver, wait_time)
    element = wait.until(EC.presence_of_element_located((By.XPATH, '//select[@name="DataTables_Table_0_length"]')))

    # 选择计划类别为"工厂的设立"
    select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
    select.select_by_value('50')
    fnames=['p'+cat+'_'+str(1)+'.html' ]
    with open(fnames[0], "w") as f:
        f.write(driver.page_source)
    html_content=driver.page_source
    #使用 BeautifulSoup 解析 HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # 查找包含页码的元素
    page_element = soup.find_all('li', {'class': 'paginate_button'}) # page-item '

    # 提取页码文本
    npage = int(page_element[-2].text.strip())
    fnames+=['p'+cat+'_'+str(i)+'.html' for i in range(2,npage+1)]
    for i in range(1,npage):
        pth=By.LINK_TEXT;ii=str(i)
        result=click_wait(pth,ii)
        if os.path.exists(fnames[i]):continue
        with open(fnames[i], "w") as f:
            f.write(driver.page_source)   
        time.sleep(10)    
    driver.quit()
335/40:
from selenium.webdriver.firefox.options import Options
firefox_options = Options()
firefox_options.add_argument("--headless")  # 启用无头模式

driver = webdriver.Firefox(options=firefox_options)
wait_time = 10
for cat in ['{:02d}'.format(i) for i in range(1,20)]:
    driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification="+cat+"#")


    # 创建 WebDriverWait 实例
    wait = WebDriverWait(driver, wait_time)
    element = wait.until(EC.presence_of_element_located((By.XPATH, '//select[@name="DataTables_Table_0_length"]')))

    # 选择计划类别为"工厂的设立"
    select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
    select.select_by_value('50')
    fnames=['p'+cat+'_'+str(1)+'.html' ]
    with open(fnames[0], "w") as f:
        f.write(driver.page_source)
    html_content=driver.page_source
    #使用 BeautifulSoup 解析 HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # 查找包含页码的元素
    page_element = soup.find_all('li', {'class': 'paginate_button'}) # page-item '

    # 提取页码文本
    npage = int(page_element[-2].text.strip())
    fnames+=['p'+cat+'_'+str(i)+'.html' for i in range(2,npage+1)]
    for i in range(2,npage+1):
        pth=By.LINK_TEXT;ii=str(i)
        result=click_wait(pth,ii)
        if os.path.exists(fnames[i-1]):continue
        with open(fnames[i-1], "w") as f:
            f.write(driver.page_source)   
        time.sleep(10)    
    driver.quit()
335/41:
from selenium.webdriver.firefox.options import Options
firefox_options = Options()
firefox_options.add_argument("--headless")  # 启用无头模式

wait_time = 10
for cat in ['{:02d}'.format(i) for i in range(2,20)]:
    driver = webdriver.Firefox(options=firefox_options)
    driver.get("https://epq.moenv.gov.tw/Query/ResultList?Classification="+cat+"#")


    # 创建 WebDriverWait 实例
    wait = WebDriverWait(driver, wait_time)
    element = wait.until(EC.presence_of_element_located((By.XPATH, '//select[@name="DataTables_Table_0_length"]')))

    # 选择计划类别为"工厂的设立"
    select = Select(driver.find_element(By.XPATH, '//select[@name="DataTables_Table_0_length"]'))
    select.select_by_value('50')
    fnames=['p'+cat+'_'+str(1)+'.html' ]
    with open(fnames[0], "w") as f:
        f.write(driver.page_source)
    html_content=driver.page_source
    #使用 BeautifulSoup 解析 HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # 查找包含页码的元素
    page_element = soup.find_all('li', {'class': 'paginate_button'}) # page-item '

    # 提取页码文本
    npage = int(page_element[-2].text.strip())
    fnames+=['p'+cat+'_'+str(i)+'.html' for i in range(2,npage+1)]
    for i in range(2,npage+1):
        pth=By.LINK_TEXT;ii=str(i)
        result=click_wait(pth,ii)
        if os.path.exists(fnames[i-1]):continue
        with open(fnames[i-1], "w") as f:
            f.write(driver.page_source)   
        time.sleep(10)    
    driver.quit()
336/1: !lst
336/2: from pandas import *
336/3: df=read_csv('df0WithBasic.csv')
336/4: df.head()
336/5: df.columns
336/6: col=df.columns
336/7: a=set(df.專案計畫編號)
336/8: a
336/9: a=[i for i in df.專案計畫編號 if 'EPA' not in i]
336/10: a=[i for i in df.專案計畫編號 if 'EPA' not in i and type(i)==str]
336/11: a=[i for i in list(df.專案計畫編號) if 'EPA' not in i and type(i)==str]
336/12: a=[i for i in list(df.專案計畫編號) if type(i)==str]
336/13: a=[i for i in a if 'EPA' not in i]
336/14: len(a)
336/15: len(set(a))
336/16: a=list(set(a))
336/17: a[:20]
336/18: a=[i for i in a if 'EPB' not in i]
336/19: len(a)
336/20: a[:20]
336/21: a
336/22: df.head()
336/23: col
336/24: set(df.經費年度)
336/25: set(df.專案開始日期)
336/26: a=[i for i in list(df.專案開始日期) if type(i)==str]
336/27: a=[i for i in a if '/' not in i]
336/28: a
336/29: df.head()
336/30: a=[i for i in list(df.計畫聯絡信箱) if type(i)==str]
336/31: a=[i for i in a if '@' not in i]
336/32: a
336/33: df.loc[df.計畫聯絡信箱=='02-89195049']
336/34: df.loc[4647]
336/35: pwd
336/36: cd ..
336/37: !lst
336/38: !head \*.txt
336/39: import os, glob
336/40:
source_directory="./"
pdfs = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
336/41:
for pdf in pdfs:
    pdfFileObj = open(pdf, 'rb')
    pdfReader = PyPDF2.PdfReader(pdfFileObj)
    a=''
    for pageObj in pdfReader.pages:
        a+=pageObj.extract_text()
    i+=1
    lines=a.split('\n')
    fname=pdf.replace('pdf'.txt)
    with open(fname,'w', encoding='utf-8') as f:
    for i in lines:
        f.write(i+'\n')
336/42:
for pdf in pdfs:
    pdfFileObj = open(pdf, 'rb')
    pdfReader = PyPDF2.PdfReader(pdfFileObj)
    a=''
    for pageObj in pdfReader.pages:
        a+=pageObj.extract_text()
    i+=1
    lines=a.split('\n')
    fname=pdf.replace('pdf'.txt)
    with open(fname,'w', encoding='utf-8') as f:
        for i in lines:
            f.write(i+'\n')
336/43: import PyPDF2
336/44:
for pdf in pdfs:
    pdfFileObj = open(pdf, 'rb')
    pdfReader = PyPDF2.PdfReader(pdfFileObj)
    a=''
    for pageObj in pdfReader.pages:
        a+=pageObj.extract_text()
    i+=1
    lines=a.split('\n')
    fname=pdf.replace('pdf'.txt)
    with open(fname,'w', encoding='utf-8') as f:
        for i in lines:
            f.write(i+'\n')
336/45:
for pdf in pdfs:
    pdfFileObj = open(pdf, 'rb')
    pdfReader = PyPDF2.PdfReader(pdfFileObj)
    a=''
    for pageObj in pdfReader.pages:
        a+=pageObj.extract_text()    
    lines=a.split('\n')
    fname=pdf.replace('pdf'.txt)
    with open(fname,'w', encoding='utf-8') as f:
        for i in lines:
            f.write(i+'\n')
336/46:
for pdf in pdfs:
    pdfFileObj = open(pdf, 'rb')
    pdfReader = PyPDF2.PdfReader(pdfFileObj)
    a=''
    for pageObj in pdfReader.pages:
        a+=pageObj.extract_text()    
    lines=a.split('\n')
    fname=pdf.replace('pdf','txt')
    with open(fname,'w', encoding='utf-8') as f:
        for i in lines:
            f.write(i+'\n')
336/47:
html_content='''<div id="chiSubject" class="sheet" style="">
<table class="table table-hover">
    <tbody>
        <tr>
            <th scope="row" style="width:10em">中▒~V~G▒~Q~X▒~A</th>
            <td>
                <div class="row">
                    <div class="col-md-12" style="text-align: justify;">
                                水污費111年徵收家數為10,881~10,977家，徵收金額約4億7,713萬元，
                    </div>
                </div>
            </td>
        </tr>
        <tr>
            <th scope="row">中文關鍵字</th>
            <td>
                <div class="row">
                    <div class="col-md-12">
                        水污染防治費、申報案件審查、勾稽檢核、申報暨查詢系統
                    </div>
                </div>
            </td>
        </tr>
    </tbody>
</table>
</div>'''
336/48:
from bs4 import BeautifulSoup
import pandas as pd
336/49:
soup = BeautifulSoup(html_content, 'html.parser')

# 提取表格数据
table_rows = soup.select('#chiSubject table tbody tr')
data = {}
for row in table_rows:
    columns = row.find_all('td')
    if columns:
        key = row.find('th').text.strip()
        value = columns[0].text.strip()
        data[key] = [value]

# 创建 DataFrame
df = pd.DataFrame(data)

# 打印结果
print(df)
336/50: df.columns
336/51:
with open('detail.html') as html_content:
    soup = BeautifulSoup(html_content, 'html.parser')

# 提取表格数据
    table_rows = soup.select('#chiSubject table tbody tr')
    data = {}
    for row in table_rows:
        columns = row.find_all('td')
        if columns:
            key = row.find('th').text.strip()
            value = columns[0].text.strip()
            data[key] = [value]

# 创建 DataFrame
    df = pd.DataFrame(data)

# 打印结果
    print(df)
336/52: cd works
336/53:
with open('detail.html') as html_content:
    soup = BeautifulSoup(html_content, 'html.parser')

# 提取表格数据
    table_rows = soup.select('#chiSubject table tbody tr')
    data = {}
    for row in table_rows:
        columns = row.find_all('td')
        if columns:
            key = row.find('th').text.strip()
            value = columns[0].text.strip()
            data[key] = [value]

# 创建 DataFrame
    df = pd.DataFrame(data)

# 打印结果
    print(df)
336/54: lst
336/55: !lst
336/56:
df0=read_csv('df0.csv')
df=DataFrame({})
columns= ['proj_id','chiAbs','engAbs' ]
df['proj_id']=list(df0.proj_id)
for c in columns[1:]:
    df[c]=np.nan
336/57: !lst
336/58:
df0=read_csv('df0WithBasic.csv')
df=DataFrame({})
columns= ['proj_id','chiAbs','engAbs' ]
df['proj_id']=list(df0.proj_id)
for c in columns[1:]:
    df[c]=np.nan
336/59:
import requests
from bs4 import BeautifulSoup
from pandas import *
import numpy as np
336/60:
df0=read_csv('df0WithBasic.csv')
df=DataFrame({})
columns= ['proj_id','chiAbs','engAbs' ]
df['proj_id']=list(df0.proj_id)
for c in columns[1:]:
    df[c]=np.nan
336/61: df.head()
336/62: len(df)
336/63:
with open('proj_link.txt') as f:
    htmls=[i for i in f]
i=0
336/64: html=htmls[0]
336/65:
    soup = BeautifulSoup(html, 'html.parser')

    # ▒~_▒▒~I▒▒~L~E▒~P▒project_id▒~Z~D▒~S▒▒~N▒
    link = soup.find('a', href=True)

    # ▒~N▒▒~O~Vproject_id
    project_id = link['href'].split('proj_id=')[1].split('&')[0]
    if project_id not in list(df.proj_id):continue

    # ▒~Z▒~I▒~[▒▒| ~GURL
    url = "https://epq.moenv.gov.tw"+link['href']  # 请▒~FURL▒~[
336/66:
    soup = BeautifulSoup(html, 'html.parser')

    # ▒~_▒▒~I▒▒~L~E▒~P▒project_id▒~Z~D▒~S▒▒~N▒
    link = soup.find('a', href=True)

    # ▒~N▒▒~O~Vproject_id
    project_id = link['href'].split('proj_id=')[1].split('&')[0]
    if project_id not in list(df.proj_id):continue

    # ▒~Z▒~I▒~[▒▒| ~GURL
    url = "https://epq.moenv.gov.tw"+link['href']
336/67:
    soup = BeautifulSoup(html, 'html.parser')

    # ▒~_▒▒~I▒▒~L~E▒~P▒project_id▒~Z~D▒~S▒▒~N▒
    link = soup.find('a', href=True)

    # ▒~N▒▒~O~Vproject_id
    project_id = link['href'].split('proj_id=')[1].split('&')[0]
336/68: project_id not in list(df.proj_id)
336/69: url = "https://epq.moenv.gov.tw"+link['href']
336/70: 'keyword' not in url
336/71:     response = requests.get(url)
336/72: response.status_code != 200
336/73:
    html_content=response.content
    soup = BeautifulSoup(html_content, 'html.parser')
    data = {}
336/74: lang='chi'
336/75:
        table_rows = soup.select('#'+lang+'Subject table tbody tr')
        for row in table_rows:
            column = row.find_all('td')
            if column:
                key = row.find('th').text.strip()
                value = column[0].text.strip()
                data[key] = [value]
336/76: data
336/77: lang='eng'
336/78:
        table_rows = soup.select('#'+lang+'Subject table tbody tr')
        for row in table_rows:
            column = row.find_all('td')
            if column:
                key = row.find('th').text.strip()
                value = column[0].text.strip()
                data[key] = [value]
336/79: data
336/80: idx=df.loc[df.proj_id==project_id].index
336/81: idx
336/82: df.head()
336/83: project_idx
336/84: project_id
336/85: html
336/86: cname={'中▒~V~G▒~Q~X▒~A':'chi','▒~Q~X▒~A':eng}
336/87: cname={'中▒~V~G▒~Q~X▒~A':'chi','▒~Q~X▒~A':'eng'}
336/88: cname
336/89:
    idx=df.loc[df.proj_id==project_id].index
    for c in cname:
        df.loc[idx,c]=data[cname[c]]
336/90:
    for c in cname:
        df.loc[idx,cname[c]]=data[c]
336/91: cname
336/92: cname={'中▒~V~G▒~Q~X▒~A':'chi','▒~Q~X▒~A':'eng'}
336/93:
    for c in cname:
        df.loc[idx,cname[c]]=data[c]
336/94:
    for c in cname:
        print(c)
        df.loc[idx,cname[c]]=data[c]
336/95: column
336/96: key
336/97: cname={'中文摘要':'chi','摘要':'eng'}
336/98:
    for c in cname:
        print(c)
        df.loc[idx,cname[c]]=data[c]
336/99: df.head()
336/100: !lst
336/101: run rd_Abs.py
336/102: idx
336/103: df.loc[idx]
336/104: df0=read_csv('df0WithBasic.csv')
336/105: df0.loc[idx]
336/106: len(df)
336/107: len(set(df.proj_id))
336/108: 12462-11781
336/109: c
336/110: len(data[c])
336/111: type(data[c])
336/112: df.head()
336/113: run rd_Abs.py
336/114: df.head()
336/115: run rd_Abs.py
337/1: run rd_Abs.py
339/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# 设置 Chrome 驱动器的路径
chrome_driver_path = '/path/to/chromedriver'

# 创建 Chrome 驱动器
driver = webdriver.Firefox()

# 打开网页
url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id=1111564042&group_id=22357"
driver.get(url)

try:
    # 输入验证码（如果需要）
    captcha_input = driver.find_element(By.ID, "CaptchaCode")
    captcha_input.send_keys("ccQ1")

    # 点击 "我同意"
    agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
    agree_button.click()

    # 等待文件下载完成，您可能需要根据实际情况调整等待时间
    wait = WebDriverWait(driver, 10)
    wait.until(EC.url_contains("YourDownloadedFileName"))

finally:
    # 关闭浏览器
    driver.quit()
339/2:
driver = webdriver.Firefox()
url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id=1111564042&group_id=22357"
driver.get(url)
driver.save_screenshot("screenshot.png")
339/3:
from PIL import Image

# 打开截图文件
screenshot_path = "screenshot.png"
screenshot = Image.open(screenshot_path)

# 设置裁剪区域的位置和大小
x = 100  # 距左边界的像素
y = 200  # 距顶部的像素
w = 800  # 宽度
h = 600  # 高度

# 裁剪图像
cropped_image = screenshot.crop((x, y, x + w, y + h))

# 保存裁剪后的图像
cropped_image.save("cropped_image.png")
339/4:
from PIL import Image

# 打开截图文件
screenshot_path = "screenshot.png"
screenshot = Image.open(screenshot_path)

# 设置裁剪区域的位置和大小
x = 100  # 距左边界的像素
y = 1200  # 距顶部的像素
w = 80  # 宽度
h = 60  # 高度

# 裁剪图像
cropped_image = screenshot.crop((x, y, x + w, y + h))

# 保存裁剪后的图像
cropped_image.save("cropped_image.png")
339/5:
from PIL import Image

# 打开截图文件
screenshot_path = "screenshot.png"
screenshot = Image.open(screenshot_path)

# 设置裁剪区域的位置和大小
x = 100  # 距左边界的像素
y = 1200  # 距顶部的像素
w = 80  # 宽度
h = 60  # 高度

# 裁剪图像
cropped_image = screenshot.crop((x, y, x + w, y + h))

# 保存裁剪后的图像
cropped_image.save("cropped_image.png")
339/6:
from PIL import Image

# 打开截图文件
screenshot_path = "screenshot.png"
screenshot = Image.open(screenshot_path)

# 设置裁剪区域的位置和大小
x = 100  # 距左边界的像素
y = 1200  # 距顶部的像素
w = 80  # 宽度
h = 60  # 高度

# 裁剪图像
cropped_image = screenshot.crop((x, y, x + w, y + h))

# 保存裁剪后的图像
cropped_image.save("cropped_image.png")
339/7:
from PIL import Image

# 打开截图文件
screenshot_path = "screenshot.png"
screenshot = Image.open(screenshot_path)

# 设置裁剪区域的位置和大小
x = 100  # 距左边界的像素
y = 1000  # 距顶部的像素
w = 80  # 宽度
h = 60  # 高度

# 裁剪图像
cropped_image = screenshot.crop((x, y, x + w, y + h))

# 保存裁剪后的图像
cropped_image.save("cropped_image.png")
339/8:
from PIL import Image

# 打开截图文件
screenshot_path = "screenshot.png"
screenshot = Image.open(screenshot_path)

# 设置裁剪区域的位置和大小
x = 100  # 距左边界的像素
y = 1000  # 距顶部的像素
w = 80  # 宽度
h = 60  # 高度

# 裁剪图像
cropped_image = screenshot.crop((x, y, x + w, y + h))

# 保存裁剪后的图像
cropped_image.save("cropped_image.png")
339/9:
from PIL import Image

# 打开截图文件
screenshot_path = "screenshot.png"
screenshot = Image.open(screenshot_path)

# 设置裁剪区域的位置和大小
x = 100  # 距左边界的像素
y = 500  # 距顶部的像素
w = 80  # 宽度
h = 60  # 高度

# 裁剪图像
cropped_image = screenshot.crop((x, y, x + w, y + h))

# 保存裁剪后的图像
cropped_image.save("cropped_image.png")
339/10:
from PIL import Image

# 打开截图文件
screenshot_path = "screenshot.png"
screenshot = Image.open(screenshot_path)

# 设置裁剪区域的位置和大小
x = 100  # 距左边界的像素
y = 800  # 距顶部的像素
w = 80  # 宽度
h = 60  # 高度

# 裁剪图像
cropped_image = screenshot.crop((x, y, x + w, y + h))

# 保存裁剪后的图像
cropped_image.save("cropped_image.png")
cropped_image.show()
339/11:
from PIL import Image

# 打开截图文件
screenshot_path = "screenshot.png"
screenshot = Image.open(screenshot_path)

# 设置裁剪区域的位置和大小
x = 100  # 距左边界的像素
y = 740  # 距顶部的像素
w = 80  # 宽度
h = 60  # 高度

# 裁剪图像
cropped_image = screenshot.crop((x, y, x + w, y + h))

# 保存裁剪后的图像
cropped_image.save("cropped_image.png")
cropped_image.show()
339/12:
from PIL import Image

# 打开截图文件
screenshot_path = "screenshot.png"
screenshot = Image.open(screenshot_path)

# 设置裁剪区域的位置和大小
x = 100  # 距左边界的像素
y = 750  # 距顶部的像素
w = 80  # 宽度
h = 50  # 高度

# 裁剪图像
cropped_image = screenshot.crop((x, y, x + w, y + h))

# 保存裁剪后的图像
cropped_image.save("cropped_image.png")
cropped_image.show()
340/1:
save_dir = os.path.join(PATH, 'saved_models')
model_name = 'keras_cifar10_trained_model.h5'
340/2:
import argparse
import json
import string
import os
import shutil
import uuid
from captcha.image import ImageCaptcha

import itertools

import os
import cv2
import numpy as np
from random import random, randint, choices

import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, Input
import matplotlib.pyplot as plt
340/3:
save_dir = os.path.join(PATH, 'saved_models')
model_name = 'keras_cifar10_trained_model.h5'
340/4:
BATCH_SIZE = 128
NUM_OF_LETTERS = 5
EPOCHS = 50
IMG_ROW, IMG_COLS = 50, 135

# Non-configs
PATH = os.getcwd()
DATA_PATH = os.path.join(PATH, 'train')
340/5:
save_dir = os.path.join(PATH, 'saved_models')
model_name = 'keras_cifar10_trained_model.h5'
340/6:
model_path = os.path.join(save_dir, model_name)
model = keras.models.load_model(model_path)
340/7:
model_path = os.path.join(save_dir, model_name)
model = keras.models.load_model(model_path)
model.summary()
341/1:
import argparse
import json
import string
import os
import shutil
import uuid
from captcha.image import ImageCaptcha

import itertools

import os
import cv2
import numpy as np
from random import random, randint, choices

import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, Input
import matplotlib.pyplot as plt
341/2:
BATCH_SIZE = 128
NUM_OF_LETTERS = 5
EPOCHS = 50
IMG_ROW, IMG_COLS = 50, 135

# Non-configs
PATH = os.getcwd()
DATA_PATH = os.path.join(PATH, 'train')
341/3:
save_dir = os.path.join(PATH, 'saved_models')
model_name = 'keras_cifar10_trained_model.h5'
341/4:
model_path = os.path.join(save_dir, model_name)
model = keras.models.load_model(model_path)
model.summary()
341/5: dir(model)
341/6:
png='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
predictions = model.predict(png)
341/7:
from PIL import Image
import numpy as np
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
img = image.load_img(image_path, target_size=(YOUR_TARGET_SIZE))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)
predictions = model.predict(img_array)
341/8:
from PIL import Image
import numpy as np
from keras.preprocessing import image
from keras.applications.mobilenet import preprocess_input

image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
img = image.load_img(image_path, target_size=(YOUR_TARGET_SIZE))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)
predictions = model.predict(img_array)
341/9:
from PIL import Image
import numpy as np
from keras.preprocessing import image
from keras.applications.mobilenet import preprocess_input

image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
YOUR_TARGET_SIZE=(int(135/2), int(50/2)))
img = image.load_img(image_path, target_size=(YOUR_TARGET_SIZE))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)
predictions = model.predict(img_array)
341/10:
from PIL import Image
import numpy as np
from keras.preprocessing import image
from keras.applications.mobilenet import preprocess_input

image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
YOUR_TARGET_SIZE=(int(135/2), int(50/2))
img = image.load_img(image_path, target_size=(YOUR_TARGET_SIZE))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)
predictions = model.predict(img_array)
341/11:
from PIL import Image
import numpy as np
from keras.preprocessing import image
from keras.applications.mobilenet import preprocess_input

image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
YOUR_TARGET_SIZE=(int(50/2),int(135/2),1)
img = image.load_img(image_path, target_size=(YOUR_TARGET_SIZE))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)
predictions = model.predict(img_array)
341/12:
from PIL import Image
import numpy as np
from keras.preprocessing import image
from keras.applications.mobilenet import preprocess_input

image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
YOUR_TARGET_SIZE=(int(50/2),int(135/2))
img = image.load_img(image_path, target_size=(YOUR_TARGET_SIZE))
img_array_gray = np.dot(img_array[YOUR_TARGET_SIZE[0],YOUR_TARGET_SIZE[1], :3], [0.2989, 0.5870, 0.1140])  # 转为灰度图像
img_array_gray = np.expand_dims(img_array_gray, axis=-1)  # 添加一个通道
img_array_gray = np.expand_dims(img_array_gray, axis=0)  # 添加一个 batch 维度

img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)
predictions = model.predict(img_array)
341/13:
from PIL import Image
import numpy as np
from keras.preprocessing import image
from keras.applications.mobilenet import preprocess_input

image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
YOUR_TARGET_SIZE=(int(50/2),int(135/2))
img = image.load_img(image_path, target_size=(YOUR_TARGET_SIZE))
img_array_gray = np.dot(img_array[:YOUR_TARGET_SIZE[0],:YOUR_TARGET_SIZE[1], :3], [0.2989, 0.5870, 0.1140])  # 转为灰度图像
img_array_gray = np.expand_dims(img_array_gray, axis=-1)  # 添加一个通道
img_array_gray = np.expand_dims(img_array_gray, axis=0)  # 添加一个 batch 维度

img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)
predictions = model.predict(img_array)
341/14:
from PIL import Image
import numpy as np
from keras.preprocessing import image
from keras.applications.mobilenet import preprocess_input

image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
YOUR_TARGET_SIZE=(int(50/2),int(135/2))
img_array = image.load_img(image_path, target_size=(YOUR_TARGET_SIZE))
img_array_gray = np.dot(img_array[:YOUR_TARGET_SIZE[0],:YOUR_TARGET_SIZE[1], :3], [0.2989, 0.5870, 0.1140])  # 转为灰度图像
img_array_gray = np.expand_dims(img_array_gray, axis=-1)  # 添加一个通道
img_array_gray = np.expand_dims(img_array_gray, axis=0)  # 添加一个 batch 维度
img_array = preprocess_input(img_array_gray)
predictions = model.predict(img_array)
341/15:
from PIL import Image
import numpy as np
from keras.preprocessing import image
from keras.applications.mobilenet import preprocess_input

image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
YOUR_TARGET_SIZE=(int(50/2),int(135/2))
img = image.load_img(image_path, target_size=(YOUR_TARGET_SIZE))
img_array = image.img_to_array(img)
img_array_gray = np.dot(img_array[:YOUR_TARGET_SIZE[0],:YOUR_TARGET_SIZE[1], :3], [0.2989, 0.5870, 0.1140])  # 转为灰度图像
img_array_gray = np.expand_dims(img_array_gray, axis=-1)  # 添加一个通道
img_array_gray = np.expand_dims(img_array_gray, axis=0)  # 添加一个 batch 维度
predictions = model.predict(img_array_gray)
341/16: predictions
341/17: predictions.shape
341/18: len(predictions)
341/19: predictions[0]
341/20: len(predictions[0])
341/21: len(predictions[0][0])
341/22: predictions[0][0]
341/23: predictions[1][0]
341/24: len(predictions[1][0])
341/25:
alphabet_all = list('qwertyupasdfghjkzxcvbnm23456789QWERTYUPKJHGFDSAZXCVBNM') #(no 1,0,i,l,I,o,O )
alphabet = list('qwertyupasdfghjkzxcvbnm23456789')#QWERTYUIOPLKJHGFDSAZXCVBNM')
341/26: len(alphabet_all)
341/27: len(alphabet)
341/28:
s=""
for i in range(5):
    mx=np.max(predictions[i][0])
    ix=np.where(predictions[i][0]==mx)
    s+=alphabet[ix]
print(s)
341/29:
s=""
for i in range(5):
    mx=np.max(predictions[i][0])
    ix=np.where(predictions[i][0]==mx)
    s+=alphabet[0][ix]
print(s)
341/30: alphabet
341/31:
s=""
for i in range(5):
    mx=np.max(predictions[i][0])
    ix=np.where(predictions[i][0]==mx)
    s+=alphabet[ix]
print(s)
341/32: ix
341/33:
s=""
for i in range(5):
    mx=np.max(predictions[i][0])
    ix=np.where(predictions[i][0]==mx)[0]
    s+=alphabet[ix]
print(s)
341/34: ix
341/35:
s=""
for i in range(5):
    mx=np.max(predictions[i][0])
    ix=np.where(predictions[i][0]==mx)[0][0]
    s+=alphabet[ix]
print(s)
341/36:
from PIL import Image
import numpy as np
from keras.preprocessing import image
from keras.applications.mobilenet import preprocess_input

image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
YOUR_TARGET_SIZE=(int(50/2),int(135/2))
img = image.load_img(image_path, target_size=(YOUR_TARGET_SIZE))
img_array = image.img_to_array(img)
img_array_gray = np.dot(img_array[:YOUR_TARGET_SIZE[0],:YOUR_TARGET_SIZE[1], :3], [0.2989, 0.5870, 0.1140])  # 转为灰度图像
img_array_gray = np.expand_dims(img_array_gray, axis=-1)  # 添加一个通道
img_array_gray = np.expand_dims(img_array_gray, axis=0)  # 添加一个 batch 维度
predictions = model.predict(img_array_gray)
341/37: len(predictions)
341/38: len(predictions[0])
341/39:
s=""
for i in range(5):
    mx=np.max(predictions[i][0])
    ix=np.where(predictions[i][0]==mx)[0][0]
    s+=alphabet[ix]
print(s)
342/1:
import argparse
import json
import string
import os
import shutil
import uuid
from captcha.image import ImageCaptcha

import itertools

import os
import cv2
import numpy as np
from random import random, randint, choices

import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, Input
import matplotlib.pyplot as plt
341/40: len(alphabet_all)
342/2:
alphabet_all = list('qwertyupasdfghjkzxcvbnm23456789QWERTYUPKJHGFDSAZXCVBNM') #(no 1,0,i,l,I,o,O )
alphabet = list('qwertyupasdfghjkzxcvbnm23456789')#QWERTYUIOPLKJHGFDSAZXCVBNM')
num_alphabet = len(alphabet_all)
def _gen_captcha(img_dir, num_of_letters, num_of_repetition, width, height):
    if os.path.exists(img_dir):
        shutil.rmtree(img_dir)
    if not os.path.exists(img_dir):
        os.makedirs(img_dir)

    image = ImageCaptcha(width=width, height=height)

    for counter in range(num_of_repetition):
        i = choices(alphabet_all, k=NUM_OF_LETTERS)
        captcha = ''.join(i)
        fn = os.path.join(img_dir, '%s_%s.png' % (captcha, uuid.uuid4()))
        image.write(captcha, fn)


def gen_dataset(path, num_of_repetition, num_of_letters, width, height):
    _gen_captcha(os.path.join(path, 'data'), num_of_letters, num_of_repetition, width, height)
    print('Finished Data Generation')
342/3:
BATCH_SIZE = 128
NUM_OF_LETTERS = 4
EPOCHS = 50
IMG_ROW, IMG_COLS = 50, 135

# Non-configs
PATH = os.getcwd()
DATA_PATH = os.path.join(PATH, 'train')
342/4:
def load_data(path, test_split=0.1):
    print ('loading dataset...')
    y_train = []
    y_test = []
    x_train = []
    x_test = []

    # r=root, d=directories, f = files
    counter = 0
    for r, d, f in os.walk(path):
        for fl in f:
            if '.png' in fl:
                flr = fl.split('_')[0]
                counter += 1
                label = np.zeros((NUM_OF_LETTERS, num_alphabet))
                for i in range(NUM_OF_LETTERS):
                    label[i, alphabet.index(flr[i])] = 1
#                    label[i, alphabet.index(flr[i].lower())] = 1
#                 label = np.zeros((50, 1))
#                 for i in range(5):
#                     label[i*5+int(flr[i])] = 1

                img = cv2.imread(os.path.join(r, fl))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                img = cv2.resize(img, (int(135/2), int(50/2)), interpolation=cv2.INTER_AREA)
                img = np.reshape(img, (img.shape[0], img.shape[1], 1))

                if random() < test_split:
                    y_test.append(label)
                    x_test.append(img)
                else:
                    y_train.append(label)
                    x_train.append(img)

    print('dataset size:', counter, '(train=%d, test=%d)' % (len(y_train), len(y_test)))
    return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)
342/5:
if not os.path.exists(DATA_PATH):
    print('Generating Dataset')
    gen_dataset(DATA_PATH, 700*1000 , NUM_OF_LETTERS, IMG_COLS, IMG_ROW)
342/6:
if not os.path.exists(DATA_PATH):
    print('Generating Dataset')
    gen_dataset(DATA_PATH, 700*1000 , NUM_OF_LETTERS, IMG_COLS, IMG_ROW)
343/1: p='/nas2/sespub/epa_reports/B27_能源或輸變電工程之開發/1110651A_美森離岸風力發電場興建計畫環境影響說明書'
343/2: txt=p+'/C05.txt'
343/3:
    with open(txt,'r', encoding=codings[1]) as f:
      lines=[l.strip().replace('\n','') for l in f]
343/4:
codings={0:'big5'}
codings.update({i:'utf-8' for i in range(1,5)})
343/5:
    with open(txt,'r', encoding=codings[1]) as f:
      lines=[l.strip().replace('\n','') for l in f]
343/6: lines[:5]
343/7: from pandas import *
343/8:
  df=DataFrame({'tit':lines,'lev':0})
  df['id']=df.index
  df.loc[df.tit.map(lambda x:x.count('.')==2 and x[0].isdigit() and x[2].isdigit() and x[4].isdigit()),'lev']=2
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[2].isdigit() and x[1]=='.')),'lev']=1
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0] in chnum)),'lev']=3
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1] in chnum)),'lev']=4
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].isdigit())),'lev']=5
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].isdigit() and x[0]=='(' and x[2]==')')),'lev']=6
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].isupper())),'lev']=7
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].isupper() and x[0]=='(' and x[2]==')')),'lev']=8
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].islower())),'lev']=9
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].islower() and x[0]=='(' and x[2]==')')),'lev']=10
  df['parent_id'] = None
  mnlev,mxlev=df.lev.min(),df.lev.max()
343/9: chnum=['一','二','三','四','五','六','七','八','九','十',]
343/10:
  df=DataFrame({'tit':lines,'lev':0})
  df['id']=df.index
  df.loc[df.tit.map(lambda x:x.count('.')==2 and x[0].isdigit() and x[2].isdigit() and x[4].isdigit()),'lev']=2
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[2].isdigit() and x[1]=='.')),'lev']=1
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0] in chnum)),'lev']=3
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1] in chnum)),'lev']=4
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].isdigit())),'lev']=5
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].isdigit() and x[0]=='(' and x[2]==')')),'lev']=6
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].isupper())),'lev']=7
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].isupper() and x[0]=='(' and x[2]==')')),'lev']=8
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].islower())),'lev']=9
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].islower() and x[0]=='(' and x[2]==')')),'lev']=10
  df['parent_id'] = None
  mnlev,mxlev=df.lev.min(),df.lev.max()
343/11: mnlev,mxlev
343/12:
    for l in range(mnlev,mxlev):
      a=df.loc[df.lev==l]
      n=len(a)
      if n==0:continue
      ai=list(a.index)
      aj=ai[1:]+[len(df)]
      b=df.loc[(df.lev>l) & (df.parent_id.isnull())]
      for i in range(n):
        boo=(b.index>ai[i]) & (b.index<aj[i] )
        c=b.loc[boo]
        mn=c.lev.min()
        df.loc[c.loc[c.lev==mn].index,'parent_id']=ai[i]
343/13: df.head()
343/14: pdf=p+'/C05.PDF'
343/15:
  csv=pdf.replace('.PDF','.csv')
  df.set_index('id').to_csv(csv) # 暫存
343/16: p
343/17: p='/nas2/sespub/epa_reports/B27_能源或輸變電工程之開發/1120451A_國光電廠興建先進燃氣複循環機組計畫環境影響說明書'
343/18: txt=p+'/C05.txt'
343/19:
    with open(txt,'r', encoding=codings[1]) as f:
      lines=[l.strip().replace('\n','') for l in f]
343/20:
  df=DataFrame({'tit':lines,'lev':0})
  df['id']=df.index
  df.loc[df.tit.map(lambda x:x.count('.')==2 and x[0].isdigit() and x[2].isdigit() and x[4].isdigit()),'lev']=2
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[2].isdigit() and x[1]=='.')),'lev']=1
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0] in chnum)),'lev']=3
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1] in chnum)),'lev']=4
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].isdigit())),'lev']=5
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].isdigit() and x[0]=='(' and x[2]==')')),'lev']=6
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].isupper())),'lev']=7
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].isupper() and x[0]=='(' and x[2]==')')),'lev']=8
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].islower())),'lev']=9
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].islower() and x[0]=='(' and x[2]==')')),'lev']=10
  df['parent_id'] = None
  mnlev,mxlev=df.lev.min(),df.lev.max()
343/21:
    for l in range(mnlev,mxlev):    
      a=df.loc[df.lev==l]
      n=len(a)
      if n==0:continue
      ai=list(a.index)
      aj=ai[1:]+[len(df)]
      b=df.loc[(df.lev>l) & (df.parent_id.isnull())]
      for i in range(n):
        boo=(b.index>ai[i]) & (b.index<aj[i] )
        c=b.loc[boo]
        mn=c.lev.min()
        df.loc[c.loc[c.lev==mn].index,'parent_id']=ai[i]
342/7:
x_train, y_train, x_test, y_test = load_data(DATA_PATH)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
342/8:
def load_data(path, test_split=0.1):
    print ('loading dataset...')
    y_train = []
    y_test = []
    x_train = []
    x_test = []

    # r=root, d=directories, f = files
    counter = 0
    for r, d, f in os.walk(path):
        for fl in f:
            if '.png' in fl:
                flr = fl.split('_')[0]
                counter += 1
                label = np.zeros((NUM_OF_LETTERS, num_alphabet))
                for i in range(NUM_OF_LETTERS):
                    label[i, alphabet_all.index(flr[i])] = 1
#                    label[i, alphabet.index(flr[i].lower())] = 1
#                 label = np.zeros((50, 1))
#                 for i in range(5):
#                     label[i*5+int(flr[i])] = 1

                img = cv2.imread(os.path.join(r, fl))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                img = cv2.resize(img, (int(135/2), int(50/2)), interpolation=cv2.INTER_AREA)
                img = np.reshape(img, (img.shape[0], img.shape[1], 1))

                if random() < test_split:
                    y_test.append(label)
                    x_test.append(img)
                else:
                    y_train.append(label)
                    x_train.append(img)

    print('dataset size:', counter, '(train=%d, test=%d)' % (len(y_train), len(y_test)))
    return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)
342/9:
x_train, y_train, x_test, y_test = load_data(DATA_PATH)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
342/10:
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
342/11:
s_train = []
s_test = []
for i in range(NUM_OF_LETTERS):
    s_train.append(y_train[:, i, :])
    s_test.append(y_test[:, i, :])
342/12:
save_dir = os.path.join(PATH, 'saved_models')
model_name = 'keras_cifar10_trained_model.h'+str(NUM_OF_LETTERS)
342/13:
input_layer = Input((25, 67, 1))
x = Conv2D(filters=32, kernel_size=(NUM_OF_LETTERS, NUM_OF_LETTERS), padding='same', activation='relu')(input_layer)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Conv2D(filters=48, kernel_size=(NUM_OF_LETTERS, NUM_OF_LETTERS), padding='same', activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Conv2D(filters=64, kernel_size=(NUM_OF_LETTERS, NUM_OF_LETTERS), padding='same', activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Dropout(0.3)(x)
x = Flatten()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.3)(x)

out = [Dense(num_alphabet, name='digit%d' % i, activation='softmax')(x) for i in range(NUM_OF_LETTERS)]
# out = Dense(num_alphabet*5, activation='sigmoid')(x)

model = Model(inputs=input_layer, outputs=out)
342/14:
model_path = os.path.join(save_dir, model_name)
model = keras.models.load_model(model_path)
model.summary()
342/15:
# initiate Adam optimizer

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


model.summary()
342/16:
hist_train_loss_digit = {i:[] for i in range(NUM_OF_LETTERS)}
hist_test_loss_digit = {i:[] for i in range(NUM_OF_LETTERS)}

hist_train_acc_digit = {i:[] for i in range(NUM_OF_LETTERS)}
hist_test_acc_digit = {i:[] for i in range(NUM_OF_LETTERS)}

hist_train_loss = []
hist_test_loss = []

hist_train_acc = []
hist_test_acc = []
342/17:
digit_acc = [[] for _ in range(NUM_OF_LETTERS)]
val_digit_acc = [[] for _ in range(NUM_OF_LETTERS)]
loss = []
val_loss = []
342/18:
history = model.fit(x_train, s_train,
                    batch_size=BATCH_SIZE,
                    epochs=EPOCHS,
                    verbose=1,
                    validation_data=(x_test, s_test)
                   )
343/22:
  csv=pdf.replace('.PDF','.csv')
  df.set_index('id').to_csv(csv) # 暫存
343/23: df.head()
343/24: pdf
343/25: p='/nas2/sespub/epa_reports/B27_能源或輸變電工程之開發/1120121A_新風離岸風力發電計畫環境影響說明書'
343/26: txt=p+'/C05.txt'
343/27:
    with open(txt,'r', encoding=codings[1]) as f:
      lines=[l.strip().replace('\n','') for l in f]
343/28:
  df=DataFrame({'tit':lines,'lev':0})
  df['id']=df.index
  df.loc[df.tit.map(lambda x:x.count('.')==2 and x[0].isdigit() and x[2].isdigit() and x[4].isdigit()),'lev']=2
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[2].isdigit() and x[1]=='.')),'lev']=1
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0] in chnum)),'lev']=3
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1] in chnum)),'lev']=4
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].isdigit())),'lev']=5
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].isdigit() and x[0]=='(' and x[2]==')')),'lev']=6
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].isupper())),'lev']=7
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].isupper() and x[0]=='(' and x[2]==')')),'lev']=8
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].islower())),'lev']=9
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].islower() and x[0]=='(' and x[2]==')')),'lev']=10
  df['parent_id'] = None
  mnlev,mxlev=df.lev.min(),df.lev.max()
343/29:
    for l in range(mnlev,mxlev):    
      a=df.loc[df.lev==l]
      n=len(a)
      if n==0:continue
      ai=list(a.index)
      aj=ai[1:]+[len(df)]
      b=df.loc[(df.lev>l) & (df.parent_id.isnull())]
      for i in range(n):
        boo=(b.index>ai[i]) & (b.index<aj[i] )
        c=b.loc[boo]
        mn=c.lev.min()
        df.loc[c.loc[c.lev==mn].index,'parent_id']=ai[i]
343/30:
  csv=pdf.replace('.PDF','.csv')
  df.set_index('id').to_csv(csv) # 暫存
343/31: pdf=p+'/C05.PDF'
343/32:
  csv=pdf.replace('.PDF','.csv')
  df.set_index('id').to_csv(csv) # 暫存
343/33: run level.py ../B27_能源或輸變電工程之開發//1120121A_新風離岸風力發電計畫環境影響說明書
343/34: iold.nlines
343/35: jj
343/36: len(df)
343/37: nlines
343/38: lines
343/39: mnlev,mxlev
343/40: mxlev>mnlev
343/41: mnlev == mnlev
343/42: np.isnan(mnlev)
343/43: import numpy as np
343/44: np.isnan(mnlev)
343/45: txt
343/46:
  df=DataFrame({'tit':lines,'lev':0})
  df['id']=df.index
  df.loc[df.tit.map(lambda x:x.count('.')==2 and x[0].isdigit() and x[2].isdigit() and x[4].isdigit()),'lev']=2
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[2].isdigit() and x[1]=='.')),'lev']=1
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0] in chnum)),'lev']=3
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1] in chnum)),'lev']=4
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].isdigit())),'lev']=5
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].isdigit() and x[0]=='(' and x[2]==')')),'lev']=6
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].isupper())),'lev']=7
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].isupper() and x[0]=='(' and x[2]==')')),'lev']=8
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].islower())),'lev']=9
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].islower() and x[0]=='(' and x[2]==')')),'lev']=10
  df['parent_id'] = None
  mnlev,mxlev=df.lev.min(),df.lev.max()
343/47: len(df)
343/48: df.tit
343/49: run level.py ../B27_能源或輸變電工程之開發/1120121A_新風離岸風力發電計畫環境影響說明書
343/50: len(df)
343/51: len(lines)
343/52:
  df=DataFrame({'tit':lines,'lev':0})
  df['id']=df.index
  df.loc[df.tit.map(lambda x:x.count('.')==2 and x[0].isdigit() and x[2].isdigit() and x[4].isdigit()),'lev']=2
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[2].isdigit() and x[1]=='.')),'lev']=1
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0] in chnum)),'lev']=3
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1] in chnum)),'lev']=4
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].isdigit())),'lev']=5
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].isdigit() and x[0]=='(' and x[2]==')')),'lev']=6
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].isupper())),'lev']=7
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].isupper() and x[0]=='(' and x[2]==')')),'lev']=8
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[0].islower())),'lev']=9
  df.loc[(df.lev==0)&(df.tit.map(lambda x:x[1].islower() and x[0]=='(' and x[2]==')')),'lev']=10
  df['parent_id'] = None
343/53: df
343/54: df.tit[9]
343/55: run level.py ../B27_能源或輸變電工程之開發/1120121A_新風離岸風力發電計畫環境影響說明書
343/56: csv
343/57: ls -lh ../B27_能源或輸變電工程之開發/1120121A_新風離岸風力發電計畫環境影響說明書/C01.csv
343/58: cat ../B27_能源或輸變電工程之開發/1120121A_新風離岸風力發電計畫環境影響說明書/C01.csv
343/59: txt
343/60:
    with open(txt,'r', encoding=codings[1]) as f:
      lines=[l.strip(' ').replace('\n','') for l in f]
343/61: lines
343/62: !cat ../B27_能源或輸變電工程之開發/1120121A_新風離岸風力發電計畫環境影響說明書/C01.txt
343/63: !lst ../B27_能源或輸變電工程之開發/1120121A_新風離岸風力發電計畫環境影響說明書
343/64: ls
343/65: run pdf2txt.py ../B27_能源或輸變電工程之開發/1120121A_新風離岸風力發電計畫環境影響說明書
343/66: run level.py ../B27_能源或輸變電工程之開發/1120121A_新風離岸風力發電計畫環境影響說明書
342/19:
digit_acc = [[] for _ in range(NUM_OF_LETTERS)]
val_digit_acc = [[] for _ in range(NUM_OF_LETTERS)]
loss = []
val_loss = []


def plot_diagram(digit_acc_now, val_digit_acc_now, loss_now, val_loss_now):
    global digit_acc, val_digit_acc, loss, val_loss
    
    
    for i in range(NUM_OF_LETTERS):
        digit_acc[i].extend(digit_acc_now[i])
        val_digit_acc[i].extend(val_digit_acc_now[i])
    loss.extend(loss_now)
    val_loss.extend(val_loss_now)
    
    for i in range(NUM_OF_LETTERS):
        s = {0:'First', 1:'Second', 2:'Third', 3:'Fourth', 4:'Fifth'}[i]
        # plt.plot(val_digit_acc[i], label='%s Digit Train' % s)
        plt.plot(digit_acc[i], label='%s Digit Test' % s)

    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()
    plt.show()

    for i in range(NUM_OF_LETTERS):
        s = {0:'First', 1:'Second', 2:'Third', 3:'Fourth', 4:'Fifth'}[i]
        plt.plot(val_digit_acc[i], label='%s Digit Train' % s)
        # plt.plot(digit_acc[i], label='%s Digit Test' % s)

    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()
    plt.show()



    # Plot training & validation loss values
    
    plt.plot(val_loss, label='Train')
    plt.plot(loss, label='Test')
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    plt.show()
342/20:
plot_diagram(
    
    [history.history['digit%d_accuracy' % i] for i in range(NUM_OF_LETTERS)],
    [history.history['val_digit%d_accuracy' % i] for i in range(NUM_OF_LETTERS)],
    history.history['loss'],
    history.history['val_loss'],
)
342/21:
# Save model and weights
if not os.path.isdir(save_dir):
    os.makedirs(save_dir)
model_path = os.path.join(save_dir, model_name)
model.save(model_path)
print('Saved trained model at %s ' % model_path)
342/22:
# Score trained model.
scores = model.evaluate(x_train, s_train, verbose=1)
print('Train loss:     %f' % np.mean(scores[0:NUM_OF_LETTERS]))
acc = 1.
for i in range(NUM_OF_LETTERS):
    acc *= scores[NUM_OF_LETTERS+1+i]
print('Train accuracy: %.2f' % (acc * 100.))
342/23:
scores = model.evaluate(x_test, s_test, verbose=1)
print('Test loss:     %f' % np.mean(scores[0:NUM_OF_LETTERS]))
acc = 1.
for i in range(NUM_OF_LETTERS):
    acc *= scores[NUM_OF_LETTERS+1+i]
print('Test accuracy: %.2f' % (acc * 100.))
341/41:
save_dir = os.path.join(PATH, 'saved_models')
model_name = 'keras_cifar10_trained_model.h4'
341/42:
model_path = os.path.join(save_dir, model_name)
model = keras.models.load_model(model_path)
model.summary()
341/43:
from PIL import Image
import numpy as np
from keras.preprocessing import image
from keras.applications.mobilenet import preprocess_input

image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
YOUR_TARGET_SIZE=(int(50/2),int(135/2))
img = image.load_img(image_path, target_size=(YOUR_TARGET_SIZE))
img_array = image.img_to_array(img)
img_array_gray = np.dot(img_array[:YOUR_TARGET_SIZE[0],:YOUR_TARGET_SIZE[1], :3], [0.2989, 0.5870, 0.1140])  # 转为灰度图像
img_array_gray = np.expand_dims(img_array_gray, axis=-1)  # 添加一个通道
img_array_gray = np.expand_dims(img_array_gray, axis=0)  # 添加一个 batch 维度
predictions = model.predict(img_array_gray)
341/44:
s=""
for i in range(5):
    mx=np.max(predictions[i][0])
    ix=np.where(predictions[i][0]==mx)[0][0]
    s+=alphabet[ix]
print(s)
341/45:
s=""
for i in range(4):
    mx=np.max(predictions[i][0])
    ix=np.where(predictions[i][0]==mx)[0][0]
    s+=alphabet_all[ix]
print(s)
341/46: predictions
341/47: img_array
341/48: img_array.shape[2]
341/49: unique_colors = np.unique(img_array.reshape(-1, img_array.shape[2]), axis=0)
341/50: len(unique_colors)
341/51: unique_colors[:5]
341/52: unique_colors[-5:]
341/53:
import cv2
import numpy as np
from sklearn.cluster import KMeans

def extract_colors(image_path, num_colors):
    # 读取图像
    img = cv2.imread(image_path)
    
    # 将图像转换为RGB格式
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # 将图像转换为一维数组
    pixels = img.reshape((-1, 3))
    
    # 使用K-means聚类算法
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    
    # 获取聚类中心，即主要颜色
    colors = kmeans.cluster_centers_.astype(int)
    
    return colors

# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 5  # 可根据需要调整颜色数量

# 提取主要颜色
main_colors = extract_colors(image_path, num_colors)

# 打印主要颜色
print("Main Colors:")
for color in main_colors:
    print(color)
341/54:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = "Unknown"
    return color_name

def extract_colors(image_path, num_colors):
    # 读取图像
    img = cv2.imread(image_path)
    
    # 将图像转换为RGB格式
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # 将图像转换为一维数组
    pixels = img.reshape((-1, 3))
    
    # 使用K-means聚类算法
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    
    # 获取聚类中心，即主要颜色
    colors = kmeans.cluster_centers_.astype(int)
    
    return colors

# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 5  # 可根据需要调整颜色数量

# 提取主要颜色
main_colors = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")
341/55:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = "Unknown"
    return color_name

def extract_colors(image_path, num_colors):
    # 读取图像
    img = cv2.imread(image_path)
    
    # 将图像转换为RGB格式
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # 将图像转换为一维数组
    pixels = img.reshape((-1, 3))
    
    # 使用K-means聚类算法
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    
    # 获取聚类中心，即主要颜色
    colors = kmeans.cluster_centers_.astype(int)
    
    return colors

# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 5  # 可根据需要调整颜色数量

# 提取主要颜色
main_colors = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")
341/56:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = "Unknown"
    return color_name

def extract_colors(image_path, num_colors):
    # 读取图像
    img = cv2.imread(image_path)
    
    # 将图像转换为RGB格式
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # 将图像转换为一维数组
    pixels = img.reshape((-1, 3))
    
    # 使用K-means聚类算法
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    
    # 获取聚类中心，即主要颜色
    colors = kmeans.cluster_centers_.astype(int)
    
    return colors

# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
main_colors = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")
341/57:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors

def rgb_to_color_name(rgb):
    try:
        # 尝试找到确切的颜色
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        # 如果找不到确切的颜色，则返回最接近的颜色
        color_name = webcolors.closest_match(rgb).name
    return color_name

def extract_colors(image_path, num_colors):
    # 读取图像
    img = cv2.imread(image_path)
    
    # 将图像转换为RGB格式
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # 将图像转换为一维数组
    pixels = img.reshape((-1, 3))
    
    # 使用K-means聚类算法
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    
    # 获取聚类中心，即主要颜色
    colors = kmeans.cluster_centers_.astype(int)
    
    return colors

# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 5  # 可根据需要调整颜色数量

# 提取主要颜色
main_colors = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")
341/58: !pip install colorthief
341/59:
import cv2
import numpy as np
from sklearn.cluster import KMeans
from colorthief import ColorThief

def rgb_to_color_name(rgb):
    # 获取最接近的颜色
    color_name = ColorThief.get_color_name(rgb)
    return color_name

def extract_colors(image_path, num_colors):
    # 读取图像
    img = cv2.imread(image_path)
    
    # 将图像转换为RGB格式
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # 将图像转换为一维数组
    pixels = img.reshape((-1, 3))
    
    # 使用K-means聚类算法
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    
    # 获取聚类中心，即主要颜色
    colors = kmeans.cluster_centers_.astype(int)
    
    return colors

# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 5  # 可根据需要调整颜色数量

# 提取主要颜色
main_colors = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")
341/60:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    
    return colors

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 5  # 可根据需要调整颜色数量

# 提取主要颜色
main_colors = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")
341/61:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    
    return colors

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
main_colors = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")
341/62:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    
    return colors

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

def save_colored_theme(image_path, theme_color, output_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 创建一个掩码，将主题颜色以外的像素置为白色
    mask = np.all(img != theme_color, axis=-1)
    img[mask] = [255, 255, 255]

    # 保存结果
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    
# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
main_colors = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")

# 为每个主要颜色创建一个文件
for i, theme_color in enumerate(main_colors):
    # 将 RGB 值转换为颜色名称
    color_name = rgb_to_color_name(theme_color)
    
    # 保存主题颜色的文件
    output_path = f"output_{i}_{color_name}.png"
    save_colored_theme(image_path, theme_color, output_path)

    print(f"Saved {color_name} theme as {output_path}")
337/2: df.head()
337/3: df.tail()
341/63:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    kmeans_labels = kmeans.predict(pixels)

    return colors, kmeans_labels

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

def save_colored_theme(image_path, kmeans_labels, theme_label, output_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 创建一个掩码，将主题标签以外的像素置为白色
    mask = (kmeans_labels != theme_label).reshape(img.shape[:2] + (1,))
    img[mask] = [255, 255, 255]

    # 保存结果
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    
# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
main_colors, kmeans_labels = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")

# 为每个主要颜色创建一个文件
for i, theme_color in enumerate(main_colors):
    # 获取主题颜色的标签
    theme_label = kmeans.predict(np.array([theme_color]))
    
    # 将 RGB 值转换为颜色名称
    color_name = rgb_to_color_name(theme_color)
    
    # 保存主题颜色的文件
    output_path = f"output_{i}_{color_name}.png"
    save_colored_theme(image_path, kmeans_labels, theme_label, output_path)

    print(f"Saved {color_name} theme as {output_path}")
341/64:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    kmeans_labels = kmeans.predict(pixels)

    return kmeans, colors, kmeans_labels

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

def save_colored_theme(image_path, kmeans_labels, theme_label, output_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 创建一个掩码，将主题标签以外的像素置为白色
    mask = (kmeans_labels != theme_label).reshape(img.shape[:2] + (1,))
    img[mask] = [255, 255, 255]

    # 保存结果
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    
# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
kmeans, main_colors, kmeans_labels = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")

# 为每个主要颜色创建一个文件
for i, theme_color in enumerate(main_colors):
    # 获取主题颜色的标签
    theme_label = kmeans.predict(np.array([theme_color]))
    
    # 将 RGB 值转换为颜色名称
    color_name = rgb_to_color_name(theme_color)
    
    # 保存主题颜色的文件
    output_path = f"output_{i}_{color_name}.png"
    save_colored_theme(image_path, kmeans_labels, theme_label, output_path)

    print(f"Saved {color_name} theme as {output_path}")
341/65:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    kmeans_labels = kmeans.predict(pixels)

    return kmeans, colors, kmeans_labels

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

def save_colored_theme(image_path, kmeans_labels, theme_label, output_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 创建一个掩码，将主题标签以外的像素置为白色
    mask = np.broadcast_to((kmeans_labels != theme_label).reshape(img.shape[:2] + (1,)), img.shape)
    img[mask] = [255, 255, 255]

    # 保存结果
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    
# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
kmeans, main_colors, kmeans_labels = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")

# 为每个主要颜色创建一个文件
for i, theme_color in enumerate(main_colors):
    # 获取主题颜色的标签
    theme_label = kmeans.predict(np.array([theme_color]))
    
    # 将 RGB 值转换为颜色名称
    color_name = rgb_to_color_name(theme_color)
    
    # 保存主题颜色的文件
    output_path = f"output_{i}_{color_name}.png"
    save_colored_theme(image_path, kmeans_labels, theme_label, output_path)

    print(f"Saved {color_name} theme as {output_path}")
341/66:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    kmeans_labels = kmeans.predict(pixels)

    return kmeans, colors, kmeans_labels

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

def save_colored_theme(image_path, kmeans_labels, theme_label, output_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 创建一个掩码，将主题标签以外的像素置为白色
    mask = (kmeans_labels != theme_label).reshape(img.shape[:2])

    # 使用 np.where 创建条件数组
    img = np.where(np.expand_dims(mask, axis=-1), img, [255, 255, 255])

    # 保存结果
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    
# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
kmeans, main_colors, kmeans_labels = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")

# 为每个主要颜色创建一个文件
for i, theme_color in enumerate(main_colors):
    # 获取主题颜色的标签
    theme_label = kmeans.predict(np.array([theme_color]))
    
    # 将 RGB 值转换为颜色名称
    color_name = rgb_to_color_name(theme_color)
    
    # 保存主题颜色的文件
    output_path = f"output_{i}_{color_name}.png"
    save_colored_theme(image_path, kmeans_labels, theme_label, output_path)

    print(f"Saved {color_name} theme as {output_path}")
341/67:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    kmeans_labels = kmeans.predict(pixels)

    return kmeans, colors, kmeans_labels

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

def save_colored_theme(image_path, kmeans_labels, theme_label, output_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 创建一个掩码，将主题标签以外的像素置为白色
    mask = np.broadcast_to((kmeans_labels != theme_label).reshape(img.shape[:2] + (1,)), img.shape)
    img[mask] = [255]*3

    # 保存结果
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    
# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
kmeans, main_colors, kmeans_labels = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")

# 为每个主要颜色创建一个文件
for i, theme_color in enumerate(main_colors):
    # 获取主题颜色的标签
    theme_label = kmeans.predict(np.array([theme_color]))
    
    # 将 RGB 值转换为颜色名称
    color_name = rgb_to_color_name(theme_color)
    
    # 保存主题颜色的文件
    output_path = f"output_{i}_{color_name}.png"
    save_colored_theme(image_path, kmeans_labels, theme_label, output_path)

    print(f"Saved {color_name} theme as {output_path}")
341/68:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    kmeans_labels = kmeans.predict(pixels)

    return kmeans, colors, kmeans_labels

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

def save_colored_theme(image_path, kmeans_labels, theme_label, output_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 创建一个掩码，将主题标签以外的像素置为白色
    mask = np.broadcast_to((kmeans_labels != theme_label).reshape(img.shape[:2] + (1,)), img.shape)
    img[mask] = 255

    # 保存结果
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    
# 指定图像路径和主要颜色的数量
image_path='/nas2/sespub/EPA_PrjReports/works/cropped_image.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
kmeans, main_colors, kmeans_labels = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")

# 为每个主要颜色创建一个文件
for i, theme_color in enumerate(main_colors):
    # 获取主题颜色的标签
    theme_label = kmeans.predict(np.array([theme_color]))
    
    # 将 RGB 值转换为颜色名称
    color_name = rgb_to_color_name(theme_color)
    
    # 保存主题颜色的文件
    output_path = f"output_{i}_{color_name}.png"
    save_colored_theme(image_path, kmeans_labels, theme_label, output_path)

    print(f"Saved {color_name} theme as {output_path}")
341/69:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    kmeans_labels = kmeans.predict(pixels)

    return kmeans, colors, kmeans_labels

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

def save_colored_theme(image_path, kmeans_labels, theme_label, output_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 创建一个掩码，将主题标签以外的像素置为白色
    mask = np.broadcast_to((kmeans_labels != theme_label).reshape(img.shape[:2] + (1,)), img.shape)
    img[mask] = 255

    # 保存结果
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    
# 指定图像路径和主要颜色的数量
image_path='./input.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
kmeans, main_colors, kmeans_labels = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")

# 为每个主要颜色创建一个文件
for i, theme_color in enumerate(main_colors):
    # 获取主题颜色的标签
    theme_label = kmeans.predict(np.array([theme_color]))
    
    # 将 RGB 值转换为颜色名称
    color_name = rgb_to_color_name(theme_color)
    
    # 保存主题颜色的文件
    output_path = f"output_{i}_{color_name}.png"
    save_colored_theme(image_path, kmeans_labels, theme_label, output_path)

    print(f"Saved {color_name} theme as {output_path}")
341/70: !pip install colorthief
341/71:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    kmeans_labels = kmeans.predict(pixels)

    return kmeans, colors, kmeans_labels

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

def save_colored_theme(image_path, kmeans_labels, theme_label, output_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 创建一个掩码，将主题标签以外的像素置为白色
    mask = np.broadcast_to((kmeans_labels != theme_label).reshape(img.shape[:2] + (1,)), img.shape)
    img[mask] = 255

    # 保存结果
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    
# 指定图像路径和主要颜色的数量
image_path='./input.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
kmeans, main_colors, kmeans_labels = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")

# 为每个主要颜色创建一个文件
for i, theme_color in enumerate(main_colors):
    # 获取主题颜色的标签
    theme_label = kmeans.predict(np.array([theme_color]))
    
    # 将 RGB 值转换为颜色名称
    color_name = rgb_to_color_name(theme_color)
    
    # 保存主题颜色的文件
    output_path = f"output_{i}_{color_name}.png"
    save_colored_theme(image_path, kmeans_labels, theme_label, output_path)

    print(f"Saved {color_name} theme as {output_path}")
341/72: !pip install pytesseract
341/73:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter
import pytesseract 

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    kmeans_labels = kmeans.predict(pixels)

    return kmeans, colors, kmeans_labels

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

def save_colored_theme(image_path, kmeans_labels, theme_label, output_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 创建一个掩码，将主题标签以外的像素置为白色
    mask = np.broadcast_to((kmeans_labels != theme_label).reshape(img.shape[:2] + (1,)), img.shape)
    img[mask] = 255

    # 保存结果
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    return pytesseract.image_to_string(img)

    
# 指定图像路径和主要颜色的数量
image_path='./input.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
kmeans, main_colors, kmeans_labels = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")

# 为每个主要颜色创建一个文件
for i, theme_color in enumerate(main_colors):
    # 获取主题颜色的标签
    theme_label = kmeans.predict(np.array([theme_color]))
    
    # 将 RGB 值转换为颜色名称
    color_name = rgb_to_color_name(theme_color)
    
    # 保存主题颜色的文件
    output_path = f"output_{i}_{color_name}.png"
    s=save_colored_theme(image_path, kmeans_labels, theme_label, output_path)

    print(f"Saved {color_name} theme as {output_path}",'ans=',s)
341/74:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter
import pytesseract 

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    kmeans_labels = kmeans.predict(pixels)

    return kmeans, colors, kmeans_labels

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

def save_colored_theme(image_path, kmeans_labels, theme_label, output_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 创建一个掩码，将主题标签以外的像素置为白色
    mask = np.broadcast_to((kmeans_labels != theme_label).reshape(img.shape[:2] + (1,)), img.shape)
    img[mask] = 255
    mask = np.broadcast_to((kmeans_labels == theme_label).reshape(img.shape[:2] + (1,)), img.shape)
    img[mask] = 0    

    # 保存结果
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    return 

    
# 指定图像路径和主要颜色的数量
image_path='./input.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
kmeans, main_colors, kmeans_labels = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")

# 为每个主要颜色创建一个文件
for i, theme_color in enumerate(main_colors):
    # 获取主题颜色的标签
    theme_label = kmeans.predict(np.array([theme_color]))
    
    # 将 RGB 值转换为颜色名称
    color_name = rgb_to_color_name(theme_color)
    
    # 保存主题颜色的文件
    output_path = f"output_{i}_{color_name}.png"
    save_colored_theme(image_path, kmeans_labels, theme_label, output_path)

    print(f"Saved {color_name} theme as {output_path}",'ans=',s)
341/75: !pip install pytesseract
341/76:
import cv2
import numpy as np
from sklearn.cluster import KMeans
import webcolors
from collections import Counter
import pytesseract 

def closest_color(requested_color):
    min_colors = {}
    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(key)
        rd = (r_c - requested_color[0]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[2]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

def extract_colors(image_path, num_colors):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(pixels)
    colors = kmeans.cluster_centers_.astype(int)
    kmeans_labels = kmeans.predict(pixels)

    return kmeans, colors, kmeans_labels

def rgb_to_color_name(rgb):
    try:
        color_name = webcolors.rgb_to_name(rgb)
    except ValueError:
        color_name = closest_color(rgb)
    return color_name

def save_colored_theme(image_path, kmeans_labels, theme_label, output_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 创建一个掩码，将主题标签以外的像素置为白色
    mask = np.broadcast_to((kmeans_labels != theme_label).reshape(img.shape[:2] + (1,)), img.shape)
    img[mask] = 255
    mask = np.broadcast_to((kmeans_labels == theme_label).reshape(img.shape[:2] + (1,)), img.shape)
    img[mask] = 0    

    # 保存结果
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    return 

    
# 指定图像路径和主要颜色的数量
image_path='./input.png'
num_colors = 6  # 可根据需要调整颜色数量

# 提取主要颜色
kmeans, main_colors, kmeans_labels = extract_colors(image_path, num_colors)

# 将 RGB 值转换为颜色名称
color_names = [rgb_to_color_name(rgb) for rgb in main_colors]

# 打印主要颜色及其名称
print("Main Colors:")
for rgb, color_name in zip(main_colors, color_names):
    print(f"RGB: {rgb}, Color Name: {color_name}")

# 为每个主要颜色创建一个文件
for i, theme_color in enumerate(main_colors):
    # 获取主题颜色的标签
    theme_label = kmeans.predict(np.array([theme_color]))
    
    # 将 RGB 值转换为颜色名称
    color_name = rgb_to_color_name(theme_color)
    
    # 保存主题颜色的文件
    output_path = f"output_{i}_{color_name}.png"
    save_colored_theme(image_path, kmeans_labels, theme_label, output_path)

    print(f"Saved {color_name} theme as {output_path}",'ans=',s)
341/77:
import requests
api_url = "https://aip.baidubce.com/rest/2.0/ocr/v1/accurate_basic" 

payload = {"images": 'input.png'}
headers = {
   "Content-Type": "application/x-www-form-urlencoded"  
}
response = requests.post(api_url, data=payload, headers=headers)
341/78: response
341/79:
import requests
api_url = "https://aip.baidubce.com/rest/2.0/ocr/v1/accurate_basic" 
img = cv2.imread(image_path)
payload = {"images": img}
headers = {
   "Content-Type": "application/x-www-form-urlencoded"  
}
response = requests.post(api_url, data=payload, headers=headers)
341/80: response
341/81:
import requests
api_url = "https://aip.baidubce.com/rest/2.0/ocr/v1/accurate_basic" 
img = cv2.imread(image_path)
base64_data = base64.b64encode(img)  
base64_str = str(base64_data, 'utf-8')

payload = {"images": base64_str}
headers = {
   "Content-Type": "application/x-www-form-urlencoded"  
}
response = requests.post(api_url, data=payload, headers=headers)
341/82:
import requests
import base64

api_url = "https://aip.baidubce.com/rest/2.0/ocr/v1/accurate_basic" 
img = cv2.imread(image_path)
base64_data = base64.b64encode(img)  
base64_str = str(base64_data, 'utf-8')

payload = {"images": base64_str}
headers = {
   "Content-Type": "application/x-www-form-urlencoded"  
}
response = requests.post(api_url, data=payload, headers=headers)
341/83: response
341/84:
import requests
import base64

api_url = "https://aip.baidubce.com/rest/2.0/ocr/v1/accurate_basic" 
img = cv2.imread(image_path)
base64_data = base64.b64encode(img)  
base64_str = str(base64_data, 'utf-8')

payload = {"images": base64_str}
headers = {
   "Content-Type": "application/x-www-form-urlencoded"  
}
import json

# ...code to request api 

resp = requests.post(api_url, data=payload, headers=headers)
print(resp.text)

# 解析结果
data = json.loads(resp.text)  
for text in data['words_result']:
    print(text['words'])
341/85: image_path
341/86:
import cv2

# 读取图像
image = cv2.imread('./input.png', 0)

# 二值化
_, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
text = pytesseract.image_to_string(Image.fromarray(binary_image))

print(text)
344/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
driver = webdriver.Firefox()
source_directory="/Users/kuang/Downloads"

for i in [1]: #range(len(df0)):
    # 打开网页
    proj_id=df0.proj_id[i]
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=df0.group_id[i]
    target_directory="/nas2/ses_pub/EPAReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory))
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print(ii,CaptchaCode,title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            break
    # 关闭浏览器
driver.quit()
344/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
driver = webdriver.Firefox()
source_directory="/Users/kuang/Downloads"

for i in [1]: #range(len(df0)):
    # 打开网页
    proj_id=df0.proj_id[i]
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=df0.group_id[i]
    target_directory="/nas2/ses_pub/EPAReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory))
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print(ii,CaptchaCode,title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            break
    # 关闭浏览器
driver.quit()
344/3:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
driver = webdriver.Firefox()
source_directory="/Users/kuang/Downloads"

for i in [1]: #range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=str(df0.group_id[i])
    target_directory="/nas2/ses_pub/EPAReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory))
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print(ii,CaptchaCode,title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            break
    # 关闭浏览器
driver.quit()
344/4:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/Users/kuang/Downloads"

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=str(df0.group_id[i])
    target_directory="/nas2/ses_pub/EPAReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory))
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            break
    # 关闭浏览器
    driver.quit()
344/5:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/Users/kuang/Downloads"

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=str(df0.group_id[i])
    target_directory="/nas2/ses_pub/EPAReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory))
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            break
    # 关闭浏览器
    driver.quit()
344/6:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/Users/kuang/Downloads"

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=str(df0.group_id[i])
    target_directory="/nas2/ses_pub/EPAReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory))
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            break
    # 关闭浏览器
    driver.quit()
344/7:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/Users/kuang/Downloads"

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=str(df0.group_id[i])
    target_directory="/nas2/ses_pub/EPAReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            break
    # 关闭浏览器
    driver.quit()
344/8:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
344/9:
source_directory="/home/kuang/Downloads"
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
344/10:
source_directory="/home/kuang/Downloads"
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
if len(pdf_files)>1:
    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
    os.system("rm "+sfile)
344/11:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
344/12:
source_directory="/home/kuang/Downloads"
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
344/13:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
344/14:
source_directory="/home/kuang/Downloads"
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
344/15:
source_directory="/home/kuang/Downloads"
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
344/16:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
344/17:
source_directory="/home/kuang/Downloads"
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
344/18:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
344/19:
source_directory="/home/kuang/Downloads"
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
344/20:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
344/21:
source_directory="/home/kuang/Downloads"
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
344/22:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
344/23:
source_directory="/home/kuang/Downloads"
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
344/24:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i]
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
344/25:
source_directory="/home/kuang/Downloads"
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
344/26: tfile
344/27:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
344/28:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

firefox_options = Options()
firefox_options.add_argument("--headless")  

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
344/29:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
344/30:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

firefox_options = Options()
firefox_options.add_argument("--headless")  

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
344/31:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
344/32:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

firefox_options = Options()
firefox_options.add_argument("--headless")  

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=firefox_options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
344/33:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

firefox_options = Options()
firefox_options.add_argument("--headless")  

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()#options=firefox_options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
344/34:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
345/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

firefox_options = Options()
firefox_options.add_argument("--headless")  

for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox()#options=firefox_options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
345/2:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
345/3:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")


for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            if len(pdf_files)==0:sys.exit()
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
    driver.close()
345/4: tfile
345/5:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")


for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            if len(pdf_files)==0:sys.exit()
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
345/6:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")


for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 20)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            if len(pdf_files)==0:sys.exit()
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
345/7:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")


for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            wait = WebDriverWait(driver, 30)
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            if len(pdf_files)==0:sys.exit()
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()
            break
    # 关闭浏览器
345/8:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")


for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            wait = WebDriverWait(driver, 30)
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            if len(pdf_files)==0:break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            break
        driver.quit()    # 关闭浏览器
345/9:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")


for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            wait = WebDriverWait(driver, 30)
            pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
345/10:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")


for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                wait = WebDriverWait(driver, 10)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
345/11:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")


for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 10)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                wait = WebDriverWait(driver, 10)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
345/12:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")


for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
346/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    if j==1:break
346/2: tfile
346/3: proj_id
346/4:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    if j==1:break
346/5:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
346/6:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    if j==1:break
346/7: proj_id, title
346/8:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(3)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        sys.exit()
        wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                wait = WebDriverWait(driver, 30)
                
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    if j==1:break
347/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)').replace('/','-')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    if j==1:break
347/2:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
347/3:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)').replace('/','-')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    if j==1:break
347/4:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)').replace('/','-')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    #if j==1:break
348/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)').replace('/','-')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    if j==1:break
348/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)').replace('/','-')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
#    if j==1:break
348/3:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('(','\(').replace(')','\)').replace('/','-')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    if j==1:break
348/4: os.path.exists(tfile)
348/5: tfile
348/6: title
348/7: df.title[i]
348/8: df0.title[i]
348/9:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    if j==1:break
348/10:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    if j==1:break
348/11:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    if j==1:break
348/12:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    #if j==1:break
348/13:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
        if ii==10:driver.quit()
    #if j==1:break
348/14:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    #if j==1:break
349/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            driver.quit()    # 关闭浏览器
            break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    #if j==1:break
349/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()    # 关闭浏览器
            break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    #if j==1:break
349/3:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        sys.exit()
        try:
            ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
            # 点击 OK 按钮
            ok_button.click()
            continue
        except:
            pdf_files=[]
            iii=0
            while iii<10 and len(pdf_files)==0:
                time.sleep(10) #wait = WebDriverWait(driver, 30)
                pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                iii+=1
            if len(pdf_files)==0:
                driver.quit()    # 关闭浏览器
                break
            pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
            tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
            sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
            os.system("mv "+sfile+" "+tfile)
            if len(pdf_files)>1:
                sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                os.system("rm "+sfile)
            driver.quit()    # 关闭浏览器
            break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    #if j==1:break
349/4:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 10).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过10秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    if j==1:break
349/5:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 10).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过10秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
349/6:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 10).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过10秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
349/7:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
350/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
351/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
351/2:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
351/3:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
352/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
353/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
354/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
354/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
355/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

j=1
for i in range(600):#len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
355/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
           '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫']
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
356/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
           '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫']
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
357/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫']
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
358/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫']
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
359/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/kuang/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫']
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
360/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/yizhen/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
360/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
source_directory="/home/yizhen/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫"
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
361/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫"
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
364/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫"
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
364/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=110].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫"
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
365/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=100].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫"
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
366/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=100].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫"
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
367/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=100].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫"
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
368/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=100].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市",
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"'])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
371/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=100].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
372/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=100].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
373/1: from pandas import *
373/2: df0=read_csv('df0WithBasic.csv')
373/3: pid='1090106299'
373/4: df0.loc[df.proj_id==pid]
373/5: df0.loc[df0.proj_id==pid]
373/6: a=df0.loc[df0.proj_id==pid,'title']
374/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=100].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
375/1: from pandas import *
375/2: df0=read_csv('df0WithBasic.csv')
375/3: df0.head()
375/4: df0[0]
375/5: df0.loc[0]
375/6: df=df0.loc[df0.執行單位.map(lambda x:"環興" in x)].reset_index(drop=True)
375/7: df=df0.loc[df0.執行單位.map(lambda x:"環興" in x and type(x)==str)].reset_index(drop=True)
375/8: df1=df0.loc[df0.執行單位.map(lambda x:type(x)==str)].reset_index(drop=True)
375/9: df=df1.loc[df1.執行單位.map(lambda x:"環興" in x and type(x)==str)].reset_index(drop=True)
375/10: len(df)
375/11: df.head()
375/12: df.loc[0]
375/13: pwd
375/14: cd ..
375/15: !ll */*.txt
375/16: !ls */*.txt
375/17: cd works
375/18: tit=[i..replace(' ','').replace('/','-').replace('\t','') for i in df.title]
375/19: tit=[i.replace(' ','').replace('/','-').replace('\t','') for i in df.title]
375/20: tit[:5]
375/21: pid=[i.replace(' ','').replace('/','-').replace('\t','') for i in df.proj_id]
375/22: gid=[i.replace(' ','').replace('/','-').replace('\t','') for i in df.group_id]
375/23: gid=[str(i).replace(' ','').replace('/','-').replace('\t','') for i in df.group_id]
375/24: gid[:5]
375/25: df.loc[0]
375/26: cat=[str(i).replace(' ','').replace('/','-').replace('\t','') for i in df.cat_nam]
375/27: pwd
375/28: fname=["../"+g+'_'+c+'/'+p+'_'+t+'.pdf' for g,n,p,t in zip(gid,cat,pid,tit)]
375/29: fname=["../"+g+'_'+c+'/'+p+'_'+t+'.pdf' for g,c,p,t in zip(gid,cat,pid,tit)]
375/30: fname[:5]
375/31: fname[-5:]
375/32: pwd
375/33: import os
375/34:
for f in fname:
    if not os.path.exists(f):continue
    os.system('ln -s '+f+' .')
375/35: ls *pdf|wc
375/36: history -f his.py
376/1: import os
376/2: from pandas import *
376/3: df0=read_csv('df0WithBasic.csv')
376/4: df1=df0.loc[df0.執行單位.map(lambda x:type(x)==str)].reset_index(drop=True)
376/5: df=df1.loc[df1.執行單位.map(lambda x:"環興" in x and type(x)==str)].reset_index(drop=True)
376/6: tit=[i.replace(' ','').replace('/','-').replace('\t','') for i in df.title]
376/7: cat=[str(i).replace(' ','').replace('/','-').replace('\t','') for i in df.cat_nam]
376/8: pid=[i.replace(' ','').replace('/','-').replace('\t','') for i in df.proj_id]
376/9: gid=[str(i).replace(' ','').replace('/','-').replace('\t','') for i in df.group_id]
376/10: fname=["../"+g+'_'+c+'/'+p+'_'+t+'.pdf' for g,c,p,t in zip(gid,cat,pid,tit)]
376/11: a=[]
376/12:
for f in fname:
    if not os.path.exists(f):continue
    a.append(f)
376/13: a
376/14: rm *pdf
376/15:
for f in fname:
    if not os.path.exists(f):continue
    os.system('ln '+f+' .')
376/16: ls -lh *.pdf
377/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=100].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    if j==1:break
377/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[df0.yr_mg>=100].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i])
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
    if os.path.exists(tfile):continue

    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    if j==100:break
    j+=1
378/1: cd work
378/2: cd works/
378/3: from pandas import *
378/4: df0=read_csv('df0WithBasic.csv')
378/5: pid=list(df0.proj_id)
378/6: [i for i in pid if ' ' in i]
380/1: from pandas import *
380/2: df0=read_csv('df0WithBasic.csv')
380/3: df0=read_csv('works/df0WithBasic.csv')
380/4: df=df0.loc[df0.proj_id=="1074920430"]
380/5: df
380/6: df.title
380/7: df.title[0]
380/8: list(df.title)[0]
384/1:
html_txt='''<!-- 書件摘要 -->
<div role="tabpanel" class="tab-pane active" id="divTabAbstract">
    <div class="table-responsive" style="margin: 2px 2px 2px 2px;">
        <table class="table table-condensed table-hover">
            <tbody>
                <tr class="active">
                    <th id="a" style="width: 15%;">
                        <label for="lbDOCTY">書件類別</label>
                    </th>
                    <td headers="a">
                        <span id="lbDOCTYP" tabindex="700" Name="lbDOCTY" class="form-control input-sm" style="width: 150px;">說明書</span>
                    </td>
                    <th id="l" style="width: 15%;">
                        <label for="txDST">基地行政區</label>
                    </th>
                    <td headers="l">
                        <input name="ctl00$txDST" type="text" value="高雄市" readonly="readonly" id="txDST" tabindex="711" Name="txDST" class="form-control input-sm" />
                    </td>
                </tr>
                <tr>
                    <th id="b">
                        <label for="txDEPN">開發單位名稱</label>
                    </th>
                    <td headers="b" colspan="3">
                        <input name="ctl00$txDEPN" type="text" value="國巨股份有限公司" id="txDEPN" tabindex="701" Name="txDEPN" class="form-control input-sm" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="c">
                        <label for="txDIRORG">目的事業主管機關</label>
                    </th>
                    <td headers="c" colspan="3">
                        <input name="ctl00$txDIRORG" type="text" value="高雄市政府經濟發展局" id="txDIRORG" tabindex="702" Name="txDIRORG" class="form-control input-sm" />
                    </td>
                </tr>
                <tr>
                    <th id="d" style="width: 15%;">

                        <label id="lbCONSULTI" for="txCONSULTI">顧問機構名稱</label>
                    </th>
                    <td headers="d" colspan="3" id="d1">
                        <input name="ctl00$txCONSULTI" type="text" value="傳閔工程顧問有限公司" id="txCONSULTI" tabindex="703" Name="txCONSULTI" class="form-control input-sm" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="e">
                        <label for="txDAREA">基地面積</label>
                    </th>
                    <td headers="e">
                        <input name="ctl00$txDAREA" type="text" value="0.1582公頃" id="txDAREA" tabindex="704" Name="txDAREA" class="form-control input-sm" />
                    </td>
                    <th id="m">
                        <label for="txDSIZE">開發規模</label>
                    </th>
                    <td headers="m">
                        <input name="ctl00$txDSIZE" type="text" value="1582平方公尺" id="txDSIZE" tabindex="712" Name="txDSIZE" class="form-control input-sm" />
                    </td>
                </tr>
                <tr>
                    <th id="f">
                        <label for="txDECAL">開發計畫類別</label>
                    </th>
                    <td headers="f" colspan="3">
                        <input name="ctl00$txDECAL" type="text" value="工廠之設立" id="txDECAL" tabindex="705" Name="txDECAL" class="form-control input-sm" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="g">
                        <label for="txDSTNAME">環保主管機關</label>
                    </th>
                    <td headers="g">
                        <input name="ctl00$txDSTNAME" type="text" value="高雄市" id="txDSTNAME" tabindex="706" Name="txDSTNAME" class="form-control input-sm" />
                    </td>
                    <th id="n">                              
                    </th>
                    <td headers="n">                              
                    </td>
                </tr>
                <tr>
                    <th id="h">
                        <label for="txSEDAT">繳費日期</label>
                    </th>
                    <td headers="h">
                        <input name="ctl00$txSEDAT" type="text" value="0831215" id="txSEDAT" tabindex="707" Name="txSEDAT" class="form-control input-sm" />
                    </td>
                    <th id="o">
                        <label for="txPORCS">處理情形</label>
                        <label for="txMSNO" style="color: white; opacity: 0.1;">處理情形</label>
                    </th>
                    <td headers="o">
                        <input name="ctl00$txPORCS" type="text" value="辦理結案" id="txPORCS" tabindex="714" Name="txPORCS" class="form-control input-sm" />                              
                    </td>
                </tr>
                <tr class="active">
                    <th id="i">
                        <label for="txTRIA">初審會日期</label>
                    </th>
                    <td headers="i">
                        <input name="ctl00$txTRIA" type="text" value="-" id="txTRIA" tabindex="708" Name="txTRIA" class="form-control input-sm" />
                    </td>
                    <th id="p">
                        <label for="txEXTP">審查結論別</label>
                        <label for="Comment2" style="color: #f5f2f2; opacity: 0.1;">我有意見</label>
                    </th>
                    <td headers="p">
                        <div>
                            <div class="FloatLeft">
                                <input name="ctl00$txEXTP" type="text" value="通過環境影響評估審查" id="txEXTP" tabindex="716" Name="txEXTP" class="form-control input-sm" />
                            </div>
                            <div class="pull-right">
                                <p>
                                    <a id="Comment2" href="#" class="btn btn-primary" data-toggle="modal" data-target="#modalComment2" tabindex="717">
                                        <span class="glyphicon glyphicon-envelope"></span>&nbsp;&nbsp;我有意見
                                    </a>
                                </p>
                            </div>
                        </div>
                    </td>
                </tr>
                <tr>
                    <th id="j">
                        <label for="txCOMIT">委員會日期</label>
                    </th>
                    <td headers="j" colspan="3">
                        <input name="ctl00$txCOMIT" type="text" value="-" id="txCOMIT" tabindex="709" Name="txCOMIT" class="form-control input-sm" style="width: 150px;" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="k">
                        <label for="txNOTES">備註</label>
                    </th>
                    <td headers="k" colspan="3">
                        <input name="ctl00$txNOTES" type="text" value="定稿本 84.10" id="txNOTES" tabindex="710" Name="txNOTES" class="form-control input-sm" />
                    </td>
                </tr>
            </tbody>
        </table>
    </div>
</div>
<!-- 審查結論 -->
<div role="tabpanel" class="tab-pane" id="divTabExct">
    <div class="table-responsive" style="margin: 5px 5px 5px 5px;">
        <table class="table table-condensed table-hover">
            <tbody>
                <tr class="active">
                    <td>
                        <div>
                            <label for="txEXCT" style="color: #f5f2f2; opacity: 0.1;">審查結論</label>
                        </div>
                        <div>
                            <textarea name="ctl00$txEXCT" rows="2" cols="20" id="txEXCT" tabindex="717" Name="txEXCT" class="form-control input-sm" style="height:245px;width:99%;">
「國巨股份有限公司高雄廠毗鄰地變更擴建計畫環境影響說明書」審查結論

發文日期:中華民國85年9月7日
發文字號:(85)府環一字第172491號

一、請補充開發前後廢棄物（含污泥、廢液、溶劑）之排放量及其比較表，並說明處理方案，以不增加污染排放量為原則。
二、圖3.3-1 請將基地現況（含警衛室、廠內之道路、增建廠房部分）明確標示。
三、同意開發公司開發後，遇有三級以上地震震度十(即定期)，請合格公司檢查是否變形或發生傾斜等狀況，並將結果送環保局及隨時備查。
四、建議依意見補充說明後，同意開發。
五、位置圖要列出比例，施工、營運路線（用顏色表示）。
六、噪音、振動現況在文章內也要說明，同時要列出使用儀器及校正方法；又噪音、振動的測定值有特別變化的時段要說明其原因，如 P.4-26 噪音 8: 00、振動 11: 00（此時段的噪音沒有特別變化，為什麼只有振動特別大），P.4-27振動 7 : 00（此時段的噪音不足特別低，但振動特別小）等等，以後請注意。
七、請列出 PCU 計算之參考文獻。
八、參考文獻中有著作者姓名的要列出著作者姓名（如P.6-6)。
九、振動的預估須加施工道路的部份，使用△Ldn之變化評估。
十、噪音、振動的實測日期（表內有，文章內沒有）及開始時問要列在文章內。
十一、監測計畫表（(P.7-10)之監測頻率，營運期間內要加幾年內（如一年內、二年內等等，不然如工廠運轉期間永久要做）。
十二、噪音量測定值之表內，由下第一行，夜間時間 22:00-05:00 請改 00:00-05:00 及 22:00-24:00。(附件六) 十三、本案如前次意見及本次意見確實有修改時，不必再審，但通知時請加承諾事項確實實行，監測報告按期送環保局，不然依環境影響評估法第十七條，第二十三條處分。
十四、本計畫如予執行，應依環境影響說明書承諾事項，歷次審查意見辦理。若有差異部份，以本府審查意見為主。
十五、本案如經許可，應依環評法第七條及同法施行細則第十八條、第二十二條規定於動工前在當地適當地點舉行公開 說明會。
十六、本案於取得目的事業主管機關核發之開發單位許可後，逾三年始實施開發行為應依環境影響評估法施行細則第四十二條規定辦理。</textarea>
                        </div>
                    </td>
                </tr>
            </tbody>
        </table>
    </div>
</div>    '''
384/2:
from bs4 import BeautifulSoup
import pandas as pd
384/3: html_content = html_txt
384/4: len(html_content )
384/5:
soup = BeautifulSoup(html_content, 'html.parser')

# Find the table within the HTML content
table = soup.find('table', {'class': 'table-condensed'})

# Extract table data into a list of dictionaries
data = []
for row in table.find_all('tr'):
    columns = row.find_all(['th', 'td'])
    row_data = [col.get_text(strip=True) for col in columns]
    data.append(row_data)

# Define column names (assuming the first row contains column headers)
columns = data[0]

# Create a pandas DataFrame
df = pd.DataFrame(data[1:], columns=columns)

# Display the DataFrame
print(df)
385/1:
from bs4 import BeautifulSoup
import pandas as pd
385/2:
html_txt='''<!-- 書件摘要 -->
<div role="tabpanel" class="tab-pane active" id="divTabAbstract">
    <div class="table-responsive" style="margin: 2px 2px 2px 2px;">
        <table class="table table-condensed table-hover">
            <tbody>
                <tr class="active">
                    <th id="a" style="width: 15%;">
                        <label for="lbDOCTY">書件類別</label>
                    </th>
                    <td headers="a">
                        <span id="lbDOCTYP" tabindex="700" Name="lbDOCTY" class="form-control input-sm" style="width: 150px;">說明書</span>
                    </td>
                    <th id="l" style="width: 15%;">
                        <label for="txDST">基地行政區</label>
                    </th>
                    <td headers="l">
                        <input name="ctl00$txDST" type="text" value="高雄市" readonly="readonly" id="txDST" tabindex="711" Name="txDST" class="form-control input-sm" />
                    </td>
                </tr>
                <tr>
                    <th id="b">
                        <label for="txDEPN">開發單位名稱</label>
                    </th>
                    <td headers="b" colspan="3">
                        <input name="ctl00$txDEPN" type="text" value="國巨股份有限公司" id="txDEPN" tabindex="701" Name="txDEPN" class="form-control input-sm" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="c">
                        <label for="txDIRORG">目的事業主管機關</label>
                    </th>
                    <td headers="c" colspan="3">
                        <input name="ctl00$txDIRORG" type="text" value="高雄市政府經濟發展局" id="txDIRORG" tabindex="702" Name="txDIRORG" class="form-control input-sm" />
                    </td>
                </tr>
                <tr>
                    <th id="d" style="width: 15%;">

                        <label id="lbCONSULTI" for="txCONSULTI">顧問機構名稱</label>
                    </th>
                    <td headers="d" colspan="3" id="d1">
                        <input name="ctl00$txCONSULTI" type="text" value="傳閔工程顧問有限公司" id="txCONSULTI" tabindex="703" Name="txCONSULTI" class="form-control input-sm" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="e">
                        <label for="txDAREA">基地面積</label>
                    </th>
                    <td headers="e">
                        <input name="ctl00$txDAREA" type="text" value="0.1582公頃" id="txDAREA" tabindex="704" Name="txDAREA" class="form-control input-sm" />
                    </td>
                    <th id="m">
                        <label for="txDSIZE">開發規模</label>
                    </th>
                    <td headers="m">
                        <input name="ctl00$txDSIZE" type="text" value="1582平方公尺" id="txDSIZE" tabindex="712" Name="txDSIZE" class="form-control input-sm" />
                    </td>
                </tr>
                <tr>
                    <th id="f">
                        <label for="txDECAL">開發計畫類別</label>
                    </th>
                    <td headers="f" colspan="3">
                        <input name="ctl00$txDECAL" type="text" value="工廠之設立" id="txDECAL" tabindex="705" Name="txDECAL" class="form-control input-sm" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="g">
                        <label for="txDSTNAME">環保主管機關</label>
                    </th>
                    <td headers="g">
                        <input name="ctl00$txDSTNAME" type="text" value="高雄市" id="txDSTNAME" tabindex="706" Name="txDSTNAME" class="form-control input-sm" />
                    </td>
                    <th id="n">                              
                    </th>
                    <td headers="n">                              
                    </td>
                </tr>
                <tr>
                    <th id="h">
                        <label for="txSEDAT">繳費日期</label>
                    </th>
                    <td headers="h">
                        <input name="ctl00$txSEDAT" type="text" value="0831215" id="txSEDAT" tabindex="707" Name="txSEDAT" class="form-control input-sm" />
                    </td>
                    <th id="o">
                        <label for="txPORCS">處理情形</label>
                        <label for="txMSNO" style="color: white; opacity: 0.1;">處理情形</label>
                    </th>
                    <td headers="o">
                        <input name="ctl00$txPORCS" type="text" value="辦理結案" id="txPORCS" tabindex="714" Name="txPORCS" class="form-control input-sm" />                              
                    </td>
                </tr>
                <tr class="active">
                    <th id="i">
                        <label for="txTRIA">初審會日期</label>
                    </th>
                    <td headers="i">
                        <input name="ctl00$txTRIA" type="text" value="-" id="txTRIA" tabindex="708" Name="txTRIA" class="form-control input-sm" />
                    </td>
                    <th id="p">
                        <label for="txEXTP">審查結論別</label>
                        <label for="Comment2" style="color: #f5f2f2; opacity: 0.1;">我有意見</label>
                    </th>
                    <td headers="p">
                        <div>
                            <div class="FloatLeft">
                                <input name="ctl00$txEXTP" type="text" value="通過環境影響評估審查" id="txEXTP" tabindex="716" Name="txEXTP" class="form-control input-sm" />
                            </div>
                            <div class="pull-right">
                                <p>
                                    <a id="Comment2" href="#" class="btn btn-primary" data-toggle="modal" data-target="#modalComment2" tabindex="717">
                                        <span class="glyphicon glyphicon-envelope"></span>&nbsp;&nbsp;我有意見
                                    </a>
                                </p>
                            </div>
                        </div>
                    </td>
                </tr>
                <tr>
                    <th id="j">
                        <label for="txCOMIT">委員會日期</label>
                    </th>
                    <td headers="j" colspan="3">
                        <input name="ctl00$txCOMIT" type="text" value="-" id="txCOMIT" tabindex="709" Name="txCOMIT" class="form-control input-sm" style="width: 150px;" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="k">
                        <label for="txNOTES">備註</label>
                    </th>
                    <td headers="k" colspan="3">
                        <input name="ctl00$txNOTES" type="text" value="定稿本 84.10" id="txNOTES" tabindex="710" Name="txNOTES" class="form-control input-sm" />
                    </td>
                </tr>
            </tbody>
        </table>
    </div>
</div>
<!-- 審查結論 -->
<div role="tabpanel" class="tab-pane" id="divTabExct">
    <div class="table-responsive" style="margin: 5px 5px 5px 5px;">
        <table class="table table-condensed table-hover">
            <tbody>
                <tr class="active">
                    <td>
                        <div>
                            <label for="txEXCT" style="color: #f5f2f2; opacity: 0.1;">審查結論</label>
                        </div>
                        <div>
                            <textarea name="ctl00$txEXCT" rows="2" cols="20" id="txEXCT" tabindex="717" Name="txEXCT" class="form-control input-sm" style="height:245px;width:99%;">
「國巨股份有限公司高雄廠毗鄰地變更擴建計畫環境影響說明書」審查結論

發文日期:中華民國85年9月7日
發文字號:(85)府環一字第172491號

一、請補充開發前後廢棄物（含污泥、廢液、溶劑）之排放量及其比較表，並說明處理方案，以不增加污染排放量為原則。
二、圖3.3-1 請將基地現況（含警衛室、廠內之道路、增建廠房部分）明確標示。
三、同意開發公司開發後，遇有三級以上地震震度十(即定期)，請合格公司檢查是否變形或發生傾斜等狀況，並將結果送環保局及隨時備查。
四、建議依意見補充說明後，同意開發。
五、位置圖要列出比例，施工、營運路線（用顏色表示）。
六、噪音、振動現況在文章內也要說明，同時要列出使用儀器及校正方法；又噪音、振動的測定值有特別變化的時段要說明其原因，如 P.4-26 噪音 8: 00、振動 11: 00（此時段的噪音沒有特別變化，為什麼只有振動特別大），P.4-27振動 7 : 00（此時段的噪音不足特別低，但振動特別小）等等，以後請注意。
七、請列出 PCU 計算之參考文獻。
八、參考文獻中有著作者姓名的要列出著作者姓名（如P.6-6)。
九、振動的預估須加施工道路的部份，使用△Ldn之變化評估。
十、噪音、振動的實測日期（表內有，文章內沒有）及開始時問要列在文章內。
十一、監測計畫表（(P.7-10)之監測頻率，營運期間內要加幾年內（如一年內、二年內等等，不然如工廠運轉期間永久要做）。
十二、噪音量測定值之表內，由下第一行，夜間時間 22:00-05:00 請改 00:00-05:00 及 22:00-24:00。(附件六) 十三、本案如前次意見及本次意見確實有修改時，不必再審，但通知時請加承諾事項確實實行，監測報告按期送環保局，不然依環境影響評估法第十七條，第二十三條處分。
十四、本計畫如予執行，應依環境影響說明書承諾事項，歷次審查意見辦理。若有差異部份，以本府審查意見為主。
十五、本案如經許可，應依環評法第七條及同法施行細則第十八條、第二十二條規定於動工前在當地適當地點舉行公開 說明會。
十六、本案於取得目的事業主管機關核發之開發單位許可後，逾三年始實施開發行為應依環境影響評估法施行細則第四十二條規定辦理。</textarea>
                        </div>
                    </td>
                </tr>
            </tbody>
        </table>
    </div>
</div>    '''
385/3: html_content = html_txt
385/4:
from bs4 import BeautifulSoup
import pandas as pd

html_content = """
<div class="panel-body">
    <!-- your HTML content goes here -->
</div>
"""

soup = BeautifulSoup(html_content, 'html.parser')

# Find the table within the HTML content
table = soup.find('table', {'class': 'table-condensed'})

# Extract table data into a list of dictionaries
data = []
for row in table.find_all('tr'):
    columns = row.find_all(['th', 'td'])
    row_data = [col.get_text(strip=True) for col in columns]
    data.append(row_data)

# Define column names (assuming the first row contains column headers)
columns = data[0]

# Create a pandas DataFrame
df = pd.DataFrame(data[1:], columns=columns)

# Display the DataFrame
print(df)
385/5:
soup = BeautifulSoup(html_content, 'html.parser')

# Find the table within the HTML content
table = soup.find('table', {'class': 'table-condensed'})

# Extract table data into a list of dictionaries
data = []
for row in table.find_all('tr'):
    columns = row.find_all(['th', 'td'])
    row_data = [col.get_text(strip=True) for col in columns]
    data.append(row_data)

# Define column names (assuming the first row contains column headers)
columns = data[0]

# Create a pandas DataFrame
df = pd.DataFrame(data[1:], columns=columns)

# Display the DataFrame
print(df)
385/6:
html_txt='''<!-- 書件摘要 -->
<div role="tabpanel" class="tab-pane active" id="divTabAbstract">
    <div class="table-responsive" style="margin: 2px 2px 2px 2px;">
        <table class="table table-condensed table-hover">
            <tbody>
                <tr class="active">
                    <th id="a" style="width: 15%;">
                        <label for="lbDOCTY">書件類別</label>
                    </th>
                    <td headers="a">
                        <span id="lbDOCTYP" tabindex="700" Name="lbDOCTY" class="form-control input-sm" style="width: 150px;">說明書</span>
                    </td>
                    <th id="l" style="width: 15%;">
                        <label for="txDST">基地行政區</label>
                    </th>
                    <td headers="l">
                        <input name="ctl00$txDST" type="text" value="高雄市" readonly="readonly" id="txDST" tabindex="711" Name="txDST" class="form-control input-sm" />
                    </td>
                </tr>
                <tr>
                    <th id="b">
                        <label for="txDEPN">開發單位名稱</label>
                    </th>
                    <td headers="b" colspan="3">
                        <input name="ctl00$txDEPN" type="text" value="國巨股份有限公司" id="txDEPN" tabindex="701" Name="txDEPN" class="form-control input-sm" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="c">
                        <label for="txDIRORG">目的事業主管機關</label>
                    </th>
                    <td headers="c" colspan="3">
                        <input name="ctl00$txDIRORG" type="text" value="高雄市政府經濟發展局" id="txDIRORG" tabindex="702" Name="txDIRORG" class="form-control input-sm" />
                    </td>
                </tr>
                <tr>
                    <th id="d" style="width: 15%;">

                        <label id="lbCONSULTI" for="txCONSULTI">顧問機構名稱</label>
                    </th>
                    <td headers="d" colspan="3" id="d1">
                        <input name="ctl00$txCONSULTI" type="text" value="傳閔工程顧問有限公司" id="txCONSULTI" tabindex="703" Name="txCONSULTI" class="form-control input-sm" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="e">
                        <label for="txDAREA">基地面積</label>
                    </th>
                    <td headers="e">
                        <input name="ctl00$txDAREA" type="text" value="0.1582公頃" id="txDAREA" tabindex="704" Name="txDAREA" class="form-control input-sm" />
                    </td>
                    <th id="m">
                        <label for="txDSIZE">開發規模</label>
                    </th>
                    <td headers="m">
                        <input name="ctl00$txDSIZE" type="text" value="1582平方公尺" id="txDSIZE" tabindex="712" Name="txDSIZE" class="form-control input-sm" />
                    </td>
                </tr>
                <tr>
                    <th id="f">
                        <label for="txDECAL">開發計畫類別</label>
                    </th>
                    <td headers="f" colspan="3">
                        <input name="ctl00$txDECAL" type="text" value="工廠之設立" id="txDECAL" tabindex="705" Name="txDECAL" class="form-control input-sm" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="g">
                        <label for="txDSTNAME">環保主管機關</label>
                    </th>
                    <td headers="g">
                        <input name="ctl00$txDSTNAME" type="text" value="高雄市" id="txDSTNAME" tabindex="706" Name="txDSTNAME" class="form-control input-sm" />
                    </td>
                    <th id="n">                              
                    </th>
                    <td headers="n">                              
                    </td>
                </tr>
                <tr>
                    <th id="h">
                        <label for="txSEDAT">繳費日期</label>
                    </th>
                    <td headers="h">
                        <input name="ctl00$txSEDAT" type="text" value="0831215" id="txSEDAT" tabindex="707" Name="txSEDAT" class="form-control input-sm" />
                    </td>
                    <th id="o">
                        <label for="txPORCS">處理情形</label>
                        <label for="txMSNO" style="color: white; opacity: 0.1;">處理情形</label>
                    </th>
                    <td headers="o">
                        <input name="ctl00$txPORCS" type="text" value="辦理結案" id="txPORCS" tabindex="714" Name="txPORCS" class="form-control input-sm" />                              
                    </td>
                </tr>
                <tr class="active">
                    <th id="i">
                        <label for="txTRIA">初審會日期</label>
                    </th>
                    <td headers="i">
                        <input name="ctl00$txTRIA" type="text" value="-" id="txTRIA" tabindex="708" Name="txTRIA" class="form-control input-sm" />
                    </td>
                    <th id="p">
                        <label for="txEXTP">審查結論別</label>
                        <label for="Comment2" style="color: #f5f2f2; opacity: 0.1;">我有意見</label>
                    </th>
                    <td headers="p">
                        <div>
                            <div class="FloatLeft">
                                <input name="ctl00$txEXTP" type="text" value="通過環境影響評估審查" id="txEXTP" tabindex="716" Name="txEXTP" class="form-control input-sm" />
                            </div>
                            <div class="pull-right">
                                <p>
                                    <a id="Comment2" href="#" class="btn btn-primary" data-toggle="modal" data-target="#modalComment2" tabindex="717">
                                        <span class="glyphicon glyphicon-envelope"></span>&nbsp;&nbsp;我有意見
                                    </a>
                                </p>
                            </div>
                        </div>
                    </td>
                </tr>
                <tr>
                    <th id="j">
                        <label for="txCOMIT">委員會日期</label>
                    </th>
                    <td headers="j" colspan="3">
                        <input name="ctl00$txCOMIT" type="text" value="-" id="txCOMIT" tabindex="709" Name="txCOMIT" class="form-control input-sm" style="width: 150px;" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="k">
                        <label for="txNOTES">備註</label>
                    </th>
                    <td headers="k" colspan="3">
                        <input name="ctl00$txNOTES" type="text" value="定稿本 84.10" id="txNOTES" tabindex="710" Name="txNOTES" class="form-control input-sm" />
                    </td>
                </tr>
            </tbody>
        </table>
    </div>
</div>
<!-- 審查結論 -->
<div role="tabpanel" class="tab-pane" id="divTabExct">
    <div class="table-responsive" style="margin: 5px 5px 5px 5px;">
        <table class="table table-condensed table-hover">
            <tbody>
                <tr class="active">
                    <td>
                        <div>
                            <label for="txEXCT" style="color: #f5f2f2; opacity: 0.1;">審查結論</label>
                        </div>
                        <div>
                            <textarea name="ctl00$txEXCT" rows="2" cols="20" id="txEXCT" tabindex="717" Name="txEXCT" class="form-control input-sm" style="height:245px;width:99%;">
「國巨股份有限公司高雄廠毗鄰地變更擴建計畫環境影響說明書」審查結論

發文日期:中華民國85年9月7日
發文字號:(85)府環一字第172491號

一、請補充開發前後廢棄物（含污泥、廢液、溶劑）之排放量及其比較表，並說明處理方案，以不增加污染排放量為原則。
二、圖3.3-1 請將基地現況（含警衛室、廠內之道路、增建廠房部分）明確標示。
三、同意開發公司開發後，遇有三級以上地震震度十(即定期)，請合格公司檢查是否變形或發生傾斜等狀況，並將結果送環保局及隨時備查。
四、建議依意見補充說明後，同意開發。
五、位置圖要列出比例，施工、營運路線（用顏色表示）。
六、噪音、振動現況在文章內也要說明，同時要列出使用儀器及校正方法；又噪音、振動的測定值有特別變化的時段要說明其原因，如 P.4-26 噪音 8: 00、振動 11: 00（此時段的噪音沒有特別變化，為什麼只有振動特別大），P.4-27振動 7 : 00（此時段的噪音不足特別低，但振動特別小）等等，以後請注意。
七、請列出 PCU 計算之參考文獻。
八、參考文獻中有著作者姓名的要列出著作者姓名（如P.6-6)。
九、振動的預估須加施工道路的部份，使用△Ldn之變化評估。
十、噪音、振動的實測日期（表內有，文章內沒有）及開始時問要列在文章內。
十一、監測計畫表（(P.7-10)之監測頻率，營運期間內要加幾年內（如一年內、二年內等等，不然如工廠運轉期間永久要做）。
十二、噪音量測定值之表內，由下第一行，夜間時間 22:00-05:00 請改 00:00-05:00 及 22:00-24:00。(附件六) 十三、本案如前次意見及本次意見確實有修改時，不必再審，但通知時請加承諾事項確實實行，監測報告按期送環保局，不然依環境影響評估法第十七條，第二十三條處分。
十四、本計畫如予執行，應依環境影響說明書承諾事項，歷次審查意見辦理。若有差異部份，以本府審查意見為主。
十五、本案如經許可，應依環評法第七條及同法施行細則第十八條、第二十二條規定於動工前在當地適當地點舉行公開 說明會。
十六、本案於取得目的事業主管機關核發之開發單位許可後，逾三年始實施開發行為應依環境影響評估法施行細則第四十二條規定辦理。</textarea>
                        </div>
                    </td>
                </tr>
            </tbody>
        </table>
    </div>
</div>    '''
385/7: html_content = html_txt
385/8:
soup = BeautifulSoup(html_content, 'html.parser')

# Find the table within the HTML content
table = soup.find('table', {'class': 'table-condensed'})

# Extract table data into a list of dictionaries
data = []
for row in table.find_all('tr'):
    columns = row.find_all(['th', 'td'])
    row_data = [col.get_text(strip=True) for col in columns]
    data.append(row_data)

# Define column names (assuming the first row contains column headers)
columns = data[0]

# Create a pandas DataFrame
df = pd.DataFrame(data[1:], columns=columns)

# Display the DataFrame
print(df)
385/9: table.find_all('label')
385/10: table.find_all({'label':'for'})
385/11: table.find_all('label').for
385/12: type(table)
385/13: a=table.find_all('label')
385/14: a[0].get()
385/15: a[0]
385/16: a[0].get('for')
385/17: col=[i.get('for') for i in table.find_all('label')]
385/18: col
385/19: len(col)
385/20: table.find_all(col[0])
385/21: table.find_all({id:col[0]})
385/22: table.find_all({id=col[0]})
385/23: table.find_all({'id':col[0]})
385/24: ids=table.find_all('id')
385/25: len(ids)
385/26: table
385/27: ids=table.find_all('class':'id')
385/28: ids=table.find_all({'class':'id'})
385/29: ids
385/30: ids=table.find_all({'input':'id'})
385/31: ids
385/32: ids[0].get('value')
385/33: ids[0].get('id')
385/34: dd={}
385/35:
for i in ids:
    dd.update({i.get('id'):i.get('value')})
385/36: df=pd.DataFrame(dd)
385/37: dd
385/38:
for i in ids:
    dd.update({i.get('id'):[i.get('value')]})
385/39: df=pd.DataFrame(dd)
385/40: dd
385/41:
soup = BeautifulSoup(html_content, 'html.parser')

# Find the table within the HTML content
table = soup.find('table', {'class': 'table-condensed'})

# Extract table data into a list of dictionaries

ids=table.find_all({'input':'id'})
for i in ids:
    dd.update({i.get('id'):[i.get('value')]})
df=pd.DataFrame(dd)
385/42: df
386/1:
from bs4 import BeautifulSoup
import pandas as pd
386/2:
html_txt='''<!-- 書件摘要 -->
<div role="tabpanel" class="tab-pane active" id="divTabAbstract">
    <div class="table-responsive" style="margin: 2px 2px 2px 2px;">
        <table class="table table-condensed table-hover">
            <tbody>
                <tr class="active">
                    <th id="a" style="width: 15%;">
                        <label for="lbDOCTY">書件類別</label>
                    </th>
                    <td headers="a">
                        <span id="lbDOCTYP" tabindex="700" Name="lbDOCTY" class="form-control input-sm" style="width: 150px;">說明書</span>
                    </td>
                    <th id="l" style="width: 15%;">
                        <label for="txDST">基地行政區</label>
                    </th>
                    <td headers="l">
                        <input name="ctl00$txDST" type="text" value="高雄市" readonly="readonly" id="txDST" tabindex="711" Name="txDST" class="form-control input-sm" />
                    </td>
                </tr>
                <tr>
                    <th id="b">
                        <label for="txDEPN">開發單位名稱</label>
                    </th>
                    <td headers="b" colspan="3">
                        <input name="ctl00$txDEPN" type="text" value="國巨股份有限公司" id="txDEPN" tabindex="701" Name="txDEPN" class="form-control input-sm" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="c">
                        <label for="txDIRORG">目的事業主管機關</label>
                    </th>
                    <td headers="c" colspan="3">
                        <input name="ctl00$txDIRORG" type="text" value="高雄市政府經濟發展局" id="txDIRORG" tabindex="702" Name="txDIRORG" class="form-control input-sm" />
                    </td>
                </tr>
                <tr>
                    <th id="d" style="width: 15%;">

                        <label id="lbCONSULTI" for="txCONSULTI">顧問機構名稱</label>
                    </th>
                    <td headers="d" colspan="3" id="d1">
                        <input name="ctl00$txCONSULTI" type="text" value="傳閔工程顧問有限公司" id="txCONSULTI" tabindex="703" Name="txCONSULTI" class="form-control input-sm" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="e">
                        <label for="txDAREA">基地面積</label>
                    </th>
                    <td headers="e">
                        <input name="ctl00$txDAREA" type="text" value="0.1582公頃" id="txDAREA" tabindex="704" Name="txDAREA" class="form-control input-sm" />
                    </td>
                    <th id="m">
                        <label for="txDSIZE">開發規模</label>
                    </th>
                    <td headers="m">
                        <input name="ctl00$txDSIZE" type="text" value="1582平方公尺" id="txDSIZE" tabindex="712" Name="txDSIZE" class="form-control input-sm" />
                    </td>
                </tr>
                <tr>
                    <th id="f">
                        <label for="txDECAL">開發計畫類別</label>
                    </th>
                    <td headers="f" colspan="3">
                        <input name="ctl00$txDECAL" type="text" value="工廠之設立" id="txDECAL" tabindex="705" Name="txDECAL" class="form-control input-sm" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="g">
                        <label for="txDSTNAME">環保主管機關</label>
                    </th>
                    <td headers="g">
                        <input name="ctl00$txDSTNAME" type="text" value="高雄市" id="txDSTNAME" tabindex="706" Name="txDSTNAME" class="form-control input-sm" />
                    </td>
                    <th id="n">                              
                    </th>
                    <td headers="n">                              
                    </td>
                </tr>
                <tr>
                    <th id="h">
                        <label for="txSEDAT">繳費日期</label>
                    </th>
                    <td headers="h">
                        <input name="ctl00$txSEDAT" type="text" value="0831215" id="txSEDAT" tabindex="707" Name="txSEDAT" class="form-control input-sm" />
                    </td>
                    <th id="o">
                        <label for="txPORCS">處理情形</label>
                        <label for="txMSNO" style="color: white; opacity: 0.1;">處理情形</label>
                    </th>
                    <td headers="o">
                        <input name="ctl00$txPORCS" type="text" value="辦理結案" id="txPORCS" tabindex="714" Name="txPORCS" class="form-control input-sm" />                              
                    </td>
                </tr>
                <tr class="active">
                    <th id="i">
                        <label for="txTRIA">初審會日期</label>
                    </th>
                    <td headers="i">
                        <input name="ctl00$txTRIA" type="text" value="-" id="txTRIA" tabindex="708" Name="txTRIA" class="form-control input-sm" />
                    </td>
                    <th id="p">
                        <label for="txEXTP">審查結論別</label>
                        <label for="Comment2" style="color: #f5f2f2; opacity: 0.1;">我有意見</label>
                    </th>
                    <td headers="p">
                        <div>
                            <div class="FloatLeft">
                                <input name="ctl00$txEXTP" type="text" value="通過環境影響評估審查" id="txEXTP" tabindex="716" Name="txEXTP" class="form-control input-sm" />
                            </div>
                            <div class="pull-right">
                                <p>
                                    <a id="Comment2" href="#" class="btn btn-primary" data-toggle="modal" data-target="#modalComment2" tabindex="717">
                                        <span class="glyphicon glyphicon-envelope"></span>&nbsp;&nbsp;我有意見
                                    </a>
                                </p>
                            </div>
                        </div>
                    </td>
                </tr>
                <tr>
                    <th id="j">
                        <label for="txCOMIT">委員會日期</label>
                    </th>
                    <td headers="j" colspan="3">
                        <input name="ctl00$txCOMIT" type="text" value="-" id="txCOMIT" tabindex="709" Name="txCOMIT" class="form-control input-sm" style="width: 150px;" />
                    </td>
                </tr>
                <tr class="active">
                    <th id="k">
                        <label for="txNOTES">備註</label>
                    </th>
                    <td headers="k" colspan="3">
                        <input name="ctl00$txNOTES" type="text" value="定稿本 84.10" id="txNOTES" tabindex="710" Name="txNOTES" class="form-control input-sm" />
                    </td>
                </tr>
            </tbody>
        </table>
    </div>
</div>
<!-- 審查結論 -->
<div role="tabpanel" class="tab-pane" id="divTabExct">
    <div class="table-responsive" style="margin: 5px 5px 5px 5px;">
        <table class="table table-condensed table-hover">
            <tbody>
                <tr class="active">
                    <td>
                        <div>
                            <label for="txEXCT" style="color: #f5f2f2; opacity: 0.1;">審查結論</label>
                        </div>
                        <div>
                            <textarea name="ctl00$txEXCT" rows="2" cols="20" id="txEXCT" tabindex="717" Name="txEXCT" class="form-control input-sm" style="height:245px;width:99%;">
「國巨股份有限公司高雄廠毗鄰地變更擴建計畫環境影響說明書」審查結論

發文日期:中華民國85年9月7日
發文字號:(85)府環一字第172491號

一、請補充開發前後廢棄物（含污泥、廢液、溶劑）之排放量及其比較表，並說明處理方案，以不增加污染排放量為原則。
二、圖3.3-1 請將基地現況（含警衛室、廠內之道路、增建廠房部分）明確標示。
三、同意開發公司開發後，遇有三級以上地震震度十(即定期)，請合格公司檢查是否變形或發生傾斜等狀況，並將結果送環保局及隨時備查。
四、建議依意見補充說明後，同意開發。
五、位置圖要列出比例，施工、營運路線（用顏色表示）。
六、噪音、振動現況在文章內也要說明，同時要列出使用儀器及校正方法；又噪音、振動的測定值有特別變化的時段要說明其原因，如 P.4-26 噪音 8: 00、振動 11: 00（此時段的噪音沒有特別變化，為什麼只有振動特別大），P.4-27振動 7 : 00（此時段的噪音不足特別低，但振動特別小）等等，以後請注意。
七、請列出 PCU 計算之參考文獻。
八、參考文獻中有著作者姓名的要列出著作者姓名（如P.6-6)。
九、振動的預估須加施工道路的部份，使用△Ldn之變化評估。
十、噪音、振動的實測日期（表內有，文章內沒有）及開始時問要列在文章內。
十一、監測計畫表（(P.7-10)之監測頻率，營運期間內要加幾年內（如一年內、二年內等等，不然如工廠運轉期間永久要做）。
十二、噪音量測定值之表內，由下第一行，夜間時間 22:00-05:00 請改 00:00-05:00 及 22:00-24:00。(附件六) 十三、本案如前次意見及本次意見確實有修改時，不必再審，但通知時請加承諾事項確實實行，監測報告按期送環保局，不然依環境影響評估法第十七條，第二十三條處分。
十四、本計畫如予執行，應依環境影響說明書承諾事項，歷次審查意見辦理。若有差異部份，以本府審查意見為主。
十五、本案如經許可，應依環評法第七條及同法施行細則第十八條、第二十二條規定於動工前在當地適當地點舉行公開 說明會。
十六、本案於取得目的事業主管機關核發之開發單位許可後，逾三年始實施開發行為應依環境影響評估法施行細則第四十二條規定辦理。</textarea>
                        </div>
                    </td>
                </tr>
            </tbody>
        </table>
    </div>
</div>    '''
386/3: html_content = html_txt
386/4:
soup = BeautifulSoup(html_content, 'html.parser')

# Find the table within the HTML content
table = soup.find('table', {'class': 'table-condensed'})

# Extract table data into a list of dictionaries

ids=table.find_all({'input':'id'})
for i in ids:
    dd.update({i.get('id'):[i.get('value')]})
df=pd.DataFrame(dd)
386/5: dd={}
386/6:
soup = BeautifulSoup(html_content, 'html.parser')

# Find the table within the HTML content
table = soup.find('table', {'class': 'table-condensed'})

# Extract table data into a list of dictionaries

ids=table.find_all({'input':'id'})
for i in ids:
    dd.update({i.get('id'):[i.get('value')]})
df=pd.DataFrame(dd)
386/7: df
386/8: table = soup.find('table', {'class':['table','table-condensed','table-condensed']})
386/9: dd={}
386/10: ids[0].get('id')
386/11: ids=table.find_all({'input':'id'})
386/12:
for i in ids:
    dd.update({i.get('id'):[i.get('value')]})
386/13: df=pd.DataFrame(dd)
386/14: df
386/15: len(ids)
386/16: extr=table.find_all({'textarea':'id'})
386/17: extr
386/18: table
386/19: table2 = soup.find('div', {'role':'tabpanel'})
386/20: table2
386/21: ids=table2.find_all({'t':'id'})
386/22: table = soup.find_all('table', {'class':['table','table-condensed','table-condensed']})
386/23: ids=table.find_all({'input':'id'})
386/24: len(table)
386/25: ids=table[1].find_all({'textarea':'id'})
386/26: ids
386/27: ids[0].text
386/28:
ids=table[0].find_all({'input':'id'})
dd={}
for i in ids:
    dd.update({i.get('id'):[i.get('value')]})
ids=table[1].find_all({'textarea':'id'})
dd.update({ids[0].get('id'):[i.text]})
df=pd.DataFrame(dd)
386/29: df
386/30:
dd.update({ids[0].get('id'):[ids[0].text]})
df=pd.DataFrame(dd)
386/31: df
386/32: df.txEXCT
386/33: df.txEXCT[0]
386/34: ls *csv
386/35: head cat4call.csv
386/36: !head cat4call.csv
386/37: !head cat4all.csv
386/38: !grep cat cat4all.csv
386/39: dfi=pd.read_csv('cat4all.csv')
386/40: dfi=df.loc[dfi.cat!='cat'].reset_index(drop=True)
386/41: len(dfi)
386/42: dfi
386/43: dfi=dfi.loc[dfi.cat!='cat'].reset_index(drop=True)
386/44: dfi=dfi.loc[dfi.cat!='cat'].reset_index(drop=True)
386/45: dfi=pd.read_csv('cat4all.csv')
386/46: dfi=dfi.loc[dfi.cat!='cat'].reset_index(drop=True)
386/47: len(dfi)
386/48: i=0
386/49:
    id=dfi['id'][i]
    cat=dfi['cat'][i]
    nam=dfi['name'][i]
    url='https://eiadoc.moenv.gov.tw/eiaweb/10.aspx?hcode=o'+id+'&srctype=0'
    response = requests.get(url)
386/50:
import os, sys, time, random, glob, shutil
import requests
386/51:
    id=dfi['id'][i]
    cat=dfi['cat'][i]
    nam=dfi['name'][i]
    url='https://eiadoc.moenv.gov.tw/eiaweb/10.aspx?hcode=o'+id+'&srctype=0'
    response = requests.get(url)
386/52: id
386/53:
    html_content = requests.get(url)
    soup = BeautifulSoup(html_content, 'html.parser')
    # Find the table within the HTML content
    table = soup.find_all('table', {'class': 'table-condensed'})
    # Extract table data into a list of dictionaries
    ids=table[0].find_all({'input':'id'})
    dd={}
    for i in ids:
        dd.update({i.get('id'):[i.get('value')]})
    ids=table[1].find_all({'textarea':'id'})
    dd.update({ids[0].get('id'):[ids[0].text]})
    df=pd.DataFrame(dd)
386/54: html_content[:100]
386/55: 1grep BeautifulSoup *py
386/56: !grep BeautifulSoup *py
386/57: response.headers
386/58: response.text
386/59: html_content = requests.get(url).text
386/60: soup = BeautifulSoup(html_content, 'html.parser')
386/61:     table = soup.find_all('table', {'class': 'table-condensed'})
386/62: len(table)
386/63: table[0]
386/64: url
386/65: table = soup.find_all('div', {'role':'tabpanel'})
386/66: len(table)
386/67: table = soup.find_all('table', {'class':'table-condensed'})
386/68: len(table)
386/69: len(html_content)
386/70: html_content[500:550]
386/71: html_content[500:1550]
386/72: soup = BeautifulSoup(html_content, 'html.parser')
386/73: table = soup.find_all('table', {'class':'table-condensed'})
386/74: len(html_content)
386/75: len(table)
386/76: table
386/77: url
386/78: id
386/79:
    url='https://eiadoc.moenv.gov.tw/eiaweb/10.aspx?hcode='+id+'&srctype=0'
    html_content = requests.get(url).text
    soup = BeautifulSoup(html_content, 'html.parser')
    # Find the table within the HTML content
    table = soup.find_all('table', {'class': 'table-condensed'})
386/80: len(table)
386/81: table[0]
386/82: table[1]
386/83: table[2]
386/84:
    ids=table[0].find_all({'input':'id'})
    dd={}
    for i in ids:
        dd.update({i.get('id'):[i.get('value')]})
    ids=table[1].find_all({'textarea':'id'})
    dd.update({ids[0].get('id'):[ids[0].text]})
    df=pd.DataFrame(dd)
386/85: df
386/86: df.txEXCT[0]
386/87:
for i in range(10): #len(dfi)):
    id=dfi['id'][i]
    cat=dfi['cat'][i]
    nam=dfi['name'][i]
    url='https://eiadoc.moenv.gov.tw/eiaweb/10.aspx?hcode='+id+'&srctype=0'
    html_content = requests.get(url).text
    soup = BeautifulSoup(html_content, 'html.parser')
    # Find the table within the HTML content
    table = soup.find_all('table', {'class': 'table-condensed'})
    # Extract table data into a list of dictionaries
    ids=table[0].find_all({'input':'id'})
    dd={}
    for i in ids:
        dd.update({i.get('id'):[i.get('value')]})
    ids=table[1].find_all({'textarea':'id'})
    dd.update({ids[0].get('id'):[ids[0].text]})
    if i==0:
        df=pd.DataFrame(dd)
    else:
        combined_df = pd.concat([df, pd.DataFrame(dd)], ignore_index=True)
        df = combined_df
df.to_csv('detail.csv')
386/88: len(df)
386/89: df
386/90: df.loc[8]
386/91: dfi.loc[8]
386/92: df.loc[9]
386/93: ls *csv
386/94: !lst
386/95: run get_eiaDetail.py cat4all.csv
386/96: run get_eiaDetail.py cat4all
386/97: len(df)
386/98: i
386/99: df.tail()
386/100: df['tail']=[i[-1:] for i in dfi.id]
386/101: dfi=pd.read_csv('cat4all.csv')
386/102: len(dfi)
386/103: len(df)
386/104: !lst
386/105: df=read_csv('detail.csv')
386/106: df=pd.read_csv('detail.csv')
386/107: df=pd.read_csv('detail.csv')
386/108: len(df)
386/109: len(dfi)
386/110: dfi.head()
386/111: df.head()
386/112: df.loc[0]
386/113: df['id']=list(dfi.id)
386/114: dfi.loc[0]
386/115: df.loc[0]
386/116: df['cat']=[i[:3] for i in dfi.cat]
386/117: df.loc[0]
386/118: df['tail']=[i[-1:] for i in dfi.id]
386/119: df.loc[0]
386/120: d={i:j for i,j in zip(df.tail,df.txDST)}
386/121: d={i:j for i,j in zip(list(df.tail),list(df.txDST))}
386/122: d={i:j for i,j in zip(list(df['tail']),list(df.txDST))}
386/123: d
386/124: pwd
386/125: !lst
386/126: df.head()
386/127: df.set_index('id').to_csv('detail.csv')
387/1: ls *csv
387/2:
from bs4 import BeautifulSoup
import pandas as pd
387/3: dfi=pd.read_csv('cat.csv')
387/4:
dfi=dfi.loc[dfi.cat!='cat'].reset_index(drop=True)
col=list(dfi.columns)
387/5: col
387/6: 'id' not in col
387/7: 'id' in col
387/8: '案號' in col
387/9: '▒~H▒~Y~_' in col
387/10: dfi.columns[1]='id'
387/11: df=pd.read_csv('cat4all.csv')
387/12: df.columns
387/13: col=list(df.columns)
387/14: dfi.columns=col
387/15: dfi.set_index('cat').to_csv('cat.csv')
388/1: !lst
388/2: run get_eiaDetail.py cat
389/1: run get_eiaDetail.py cat
389/2: dfa
389/3: dfa.id
389/4: !tail detail-cat.csv
389/5: !tail -n30 detail-cat.csv
389/6: col
389/7: !vi header.txt
389/8: !lst
389/9: !vi get_eiaDetail.py
389/10: df=pd.read_csv('detail.csv')
389/11: df.head()
389/12: df[col].head()
389/13: df[col].to_csv('detail.csv',index=False)
389/14: !head detail.csv
389/15: !lst
389/16: rm detail-cat.csv
389/17: id
389/18: id in list(df['id'])
389/19: run get_eiaDetail.py cat
389/20: !head detail.csv
389/21: run get_eiaDetail.py cat
390/1: from pandas import *
390/2: df=read_csv('detail-cat.csv')
390/3: len(set(df.id))
390/4: len(df)
390/5: df.head()
390/6: dfid=list(df.id)
390/7:
for i in set(df.id):
    if dfid.count(i)>1:print (i)
390/8: !grep 1080821A detail-cat.csv
390/9: '1080821A' in dfid
390/10: '1080821A' in df.id
390/11: !vi get_eiaDetail.py
390/12: !cat get_eiaDetail.py
390/13:
import pandas as pd
import os, sys, time, random, glob, shutil
import requests
from bs4 import BeautifulSoup
390/14: dfi=pd.read_csv('detail-cat.csv')
390/15: i=10
390/16: id=dfi['id'][i]
390/17: id in list(df['id'])
390/18: df=read_csv('detail-cat.csv')
390/19: len(df)
390/20: a=df.drop_duplicate().reset_index(drop=True)
390/21: a=df.drop_duplicates().reset_index(drop=True)
390/22: len(a)
390/23: len(df)
390/24: !lst
390/25: a.head()
390/26: a.to_csv('detail-cat.csv',index=False)
390/27: df=read_csv('detail-cat.csv')
390/28: len(df)
390/29: df=read_csv('detail-cat.csv')
390/30: len(df)
390/31: dfi=pd.read_csv('cat.csv')
390/32: len(dfi)
391/1: import openai
391/2: !pip install --upgrade openai
391/3: import openai
391/4: openai.api_key = "EMPTY"
391/5: openai.api_base = "http://200.200.32.195:55083/v1"
391/6: model="vicuna-7b-v1.5-16k"
391/7: prompt = "Once upon a time"
391/8: completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=64)
391/9: dir openai)
391/10: dir(openai)
391/11: completion = openai.completions.create(model=model, prompt=prompt, max_tokens=64)
391/12: openai.api_key = "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"
391/13: completion = openai.completions.create(model=model, prompt=prompt, max_tokens=64)
391/14: openai.api_base
391/15: pwd
391/16: completion = openai.completions.create(model=model, prompt=prompt, max_tokens=64,api_base=openai.base_url)
391/17: completion = openai.completions.create(model=model, prompt=prompt, max_tokens=64,base_url=openai.base_url)
391/18: dir(create)
391/19: dir(openai.completions.create)
391/20: completion = openai.completions(model=model, prompt=prompt, max_tokens=64,base_url=openai.base_url)
391/21: openai.api_key
391/22: openai.api_key = "EMPTY"
391/23: openai.default_headers = {"x-foo": "true"}
391/24: model
391/25:
completion = openai.chat.completions.create(
    model= model, messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
391/26: openai.api_key = "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"
391/27:
completion = openai.chat.completions.create(
    model= model, messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
391/28: from openai import OpenAI
391/29: client = OpenAI()
392/1: from openai import OpenAI
392/2: client = OpenAI()
392/3: import openai
392/4: openai.default_headers = {"x-foo": "true"}
392/5: model="vicuna-7b-v1.5-16k"
392/6:
completion = openai.chat.completions.create(model=model, messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
393/1: from openai import OpenAI
393/2: import openai
393/3: model="vicuna-7b-v1.5-16k"
393/4: client = OpenAI()
393/5: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
394/1: import openai
394/2: from openai import OpenAI
394/3: model="vicuna-7b-v1.5-16k"
394/4: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
395/1: from openai import OpenAI
395/2: import openai
395/3: model="vicuna-7b-v1.5-16k"
395/4: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
395/5: openai.api_base
395/6: openai.api_base
395/7: import openai
395/8: [i for i in dir(openai) if 'base' in i]
395/9: openai.base_url="http://200.200.32.195:55083/"
395/10: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
395/11: openai.base_url="http://200.200.32.195:55083"
395/12: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
395/13: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
395/14: openai.api_base = "http://200.200.32.195"
395/15: openai.base_url="http://200.200.32.195:55083"
395/16: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
395/17: openai.api_key = "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"
395/18: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
396/1:
import pkg_resources

openai_version = pkg_resources.get_distribution("openai").version
print(openai_version)
396/2:
import os
from openai import OpenAI
396/3: client = OpenAI(base_url="http://200.200.32.195:55083",api_key="EMPTY")
396/4: stream = client.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
396/5: model="vicuna-7b-v1.5-16k"
396/6: stream = client.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
396/7: stream = client.chat.completions.create(base_url="http://200.200.32.195:55083",model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
396/8: openai.base_url
396/9: import openai
396/10: client.base_url
396/11:
import httpx
from openai import OpenAI
396/12: client = OpenAI(base_url="http://200.200.32.195:55083",api_key="EMPTY", http_client=httpx.Client(proxies="http://200.200.32.195", transport=httpx.HTTPTransport(local_address="0.0.0.0")))
396/13: stream = client.chat.completions.create(base_url="http://200.200.32.195:55083",model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
396/14: stream = client.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
396/15: stream = client.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?"}])
396/16: client = OpenAI(base_url="http://200.200.32.195:55083",api_key="EMPTY", http_client=httpx.Client(proxies="http://200.200.32.195", transport=httpx.HTTPTransport(local_address="200.200.32.195")))
396/17: stream = client.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?"}])
396/18:
!curl http://200.200.32.195:55083/v1/completions -H "Content-Type: application/json" -d '{
    "model": "vicuna-7b-v1.3",
    "prompt": "Once upon a time",
    "max_tokens": 41,
    "temperature": 0.5
  }'
396/20: !curl http://200.200.32.195:55083/v1/completions -H "Content-Type: application/json" -d '{ "model": "vicuna-7b-v1.3",    "prompt": "Once upon a time",    "max_tokens": 41,    "temperature": 0.5 }'
396/21: !curl http://200.200.32.195:55083/v1/completions -H "Content-Type: application/json" -d '{ "model": model,    "prompt": "Once upon a time",    "max_tokens": 41,    "temperature": 0.5 }'
396/22: !curl http://200.200.32.195:55083/v1/chat/completions -H "Content-Type: application/json" -d '{ "model": model,    "prompt": "Once upon a time",    "max_tokens": 41,    "temperature": 0.5 }'
397/1: import openai
397/2: from openai import OpenAI
397/3: model="vicuna-7b-v1.5-16k"
397/4: openai.base_url="http://200.200.32.195:55083"
397/5: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
397/6: openai.base_url="http://200.200.32.195:21001"
397/7: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
397/8: !vi b
397/9: openai.base_url="http://200.200.32.195"
397/10: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
397/11: !vi b
397/12: openai.base_url="http://200.200.32.195:55083/v1"
397/13: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
397/14: !vi b
397/15: client = OpenAI(base_url="http://200.200.32.195:55083",api_key="EMPTY")
397/16: completion = client.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
397/17: client = OpenAI()
397/18: completion = client.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
398/1: from openai import OpenAI
398/2: import openai
398/3: client = OpenAI()
398/4: completion = client.chat.completions.create(messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
398/5: model="gpt-3.5-turbo"
398/6: completion = client.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
398/7: print(completion)
398/8: client = OpenAI(base_url="http://200.200.32.195:55083",api_key="EMPTY", http_client=httpx.Client(proxies="http://200.200.32.195", transport=httpx.HTTPTransport(local_address="200.200.32.195")))
398/9: import httpx
398/10: client = OpenAI(base_url="http://200.200.32.195:55083",api_key="EMPTY", http_client=httpx.Client(proxies="http://200.200.32.195", transport=httpx.HTTPTransport(local_address="200.200.32.195")))
398/11: completion = client.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
399/1: from openai import OpenAI
399/2: import openai
399/3: openai.base_url="http://200.200.32.195/v1"
399/4: model="vicuna-7b-v1.5-16k"
399/5: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
399/6: openai.base_url="http://200.200.32.195/v1/"
399/7: completion = openai.chat.completions.create(model=model, messages=[{"role": "user","content": "How do I output all files in a directory using Python?",}])
399/8: model
400/1:
with open('rar2.txt','r') as f:
    fnames=[i.strip('\n') for i in f]
fnames[:5]
400/2:
import os
for i in fnames:
    fname=i.replace(' ','_')
    os.system('sudo mv /home/yizhen/Downloads/'+i+' '+fname)
400/3:
import os
for i in fnames:
    inp=i.replace('(','\(').replace(')','\)')..replace(' ','\ ')
    fname=inp.replace(' ','')
    os.system('sudo mv /home/yizhen/Downloads/'+inp+' '+fname)
400/4:
import os
for i in fnames:
    inp=i.replace('(','\(').replace(')','\)').replace(' ','\ ')
    fname=inp.replace(' ','')
    os.system('sudo mv /home/yizhen/Downloads/'+inp+' '+fname)
400/5:
with open('zip.txt','r') as f:
    fnames=[i.strip('\n') for i in f]
fnames[:5]
400/6:
import os
for i in fnames:
    inp=i.replace('(','\(').replace(')','\)').replace(' ','\ ')
    fname=inp.replace(' ','')
    os.system('sudo mv /home/yizhen/Downloads/'+inp+' '+fname)
400/7:
with open('PDF.txt','r') as f:
    fnames=[i.strip('\n') for i in f]
fnames[:5]
400/8:
import os
for i in fnames:
    inp=i.replace('(','\(').replace(')','\)').replace(' ','\ ')
    fname=inp.replace(' ','')
    os.system('sudo mv /home/yizhen/Downloads/'+inp+' '+fname)
400/9:
with open('part.txt','r') as f:
    fnames=[i.strip('\n') for i in f]
fnames[:5]
400/10:
import os
for i in fnames:
    inp=i.replace('(','\(').replace(')','\)').replace(' ','\ ')
    fname=inp.replace(' ','')
    os.system('sudo mv /home/yizhen/Downloads/'+inp+' '+fname)
401/1:
df0=read_csv('df0.csv')
df0[9288]
401/2:
from pandas import *
df0=read_csv('df0.csv')
df0[9288]
401/3:
from pandas import *
#df0=read_csv('df0.csv')
df0.loc[9288]
401/4:
from pandas import *
#df0=read_csv('df0.csv')
df0.loc[9287]
401/5:
from pandas import *
#df0=read_csv('df0.csv')
df0.loc[df0.proj_id=='0994783662']
401/6: df.loc[9286]
401/7: df0.loc[9286]
401/8:
withFile=.false.
print(withFile)
401/9:
withFile=false
print(withFile)
401/10:
withFile=False
print(withFile)
401/11:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(9286,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
401/12:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(9286,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
401/13:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(9286,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

#    if j==1:break
402/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(9286,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    if j==1:break
402/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(9286,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    if j==1:break
402/3:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
402/4:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(9286,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    if j==1:break
402/5: len(df0)
402/6:
from pandas import *
df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)
df0.loc[df0.proj_id=='0994783662']
402/7:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    if j==1:break
403/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    if j==1:break
404/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    if j==1:break
404/2:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
404/3:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=1
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")

    if j==1:break
404/4:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==3:break
404/5:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
404/6:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
404/7:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
404/8:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")
not_found=['111年度雲林縣石化業專用監測車操作維護計畫','111年度土壤及地下水污染調查及查證工作計畫-臺南市',
        '111年度土壤污染評估調查及檢測資料勾稽查核計畫','111年綠島鄉及蘭嶼鄉環境衛生計畫',
        '111年度產品碳足跡資訊揭露與發展雲端審查標示制度專案工作計畫',
           '屏東縣-屏東縣殺蛇溪水質淨化場設計案',
           '110年度嘉義縣固定空氣污染源許可暨揮發性有機物稽查管制計畫',
               "110年度臺東縣營建工程污染管制暨電子e化系統設置計畫",
           "110年度土壤及地下水污染調查及查證工作計畫-雲林縣",
           "110年土壤及地下水相關計畫檢測作業品保監督查核計畫",
           "110年度非農地環境雜草管理計畫__臺北市"
          ]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==10:break
404/9:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==10:break
404/10:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==10:break
404/11:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==10:break
404/12:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
print(not_found)
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==10:break
404/13:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
print(not_found)
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==10:break
404/14:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    print(i)
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==10:break
404/15:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:
        print(i)
        continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==10:break
404/16:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(539,len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:
        print(i)
        continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:
        print(i,i)
        continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==10:break
404/17:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==10:break
405/1:
from pandas import *
df0=read_csv('df0.csv')
405/2: len(df0)
404/18:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
404/19:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
406/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
406/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                if '.' not in pdf_files[0]:
                    pdf_files[0]+='.rar'
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
406/3:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
406/4:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                if '.' not in pdf_files[0]:
                    pdf_files[0]+='.rar'
                ext=pdf_files[0][-4:].split('.')[-1]
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
406/5:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='.rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
406/6:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
406/7:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(20) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
406/8:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
406/9:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==10:break
406/10:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
#    if j==10:break
406/11:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)

# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
#    if j==10:break
406/12:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
#    if j==10:break
406/13:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
#    if j==10:break
406/14:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
#    if j==10:break
406/15:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-') #replace('(','\(').replace(')','\)')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
406/16:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-')
    title=title.replace('<','(').replace('>',')')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
406/17:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
406/18:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-')
    title=title.replace('<','(').replace('>',')')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
406/19:
source_directory="/home/kuang/Downloads"
title=df0.title[i].replace(' ','')
group_id=str(df0.group_id[i])
target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title+".pdf")
pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
os.system("mv "+sfile+" "+tfile)
406/20:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-')
    title=title.replace('<','(').replace('>',')')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==1:break
406/21:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-')
    title=title.replace('<','(').replace('>',')')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    if j==10:break
407/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-')
    title=title.replace('<','(').replace('>',')')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    #if j==1:break
407/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
#df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-')
    title=title.replace('<','(').replace('>',')')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    #if j==1:break
407/3:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
#df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-')
    title=title.replace('<','(').replace('>',')')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    #if j==1:break
408/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
#df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-')
    title=title.replace('<','(').replace('>',')')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    #if j==1:break
408/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
#df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-')
    title=title.replace('<','(').replace('>',')')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    #if j==1:break
409/1:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
#df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-')
    title=title.replace('<','(').replace('>',')')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    #if j==1:break
409/2:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
#df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF','doc','DOC','docx','DOCX']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\t','').replace('\x0b','-')
    title=title.replace('<','(').replace('>',')')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    #if j==1:break
409/3:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import cv2, os, time, glob, sys, subprocess
import pytesseract
from PIL import Image
from pandas import *
from selenium.webdriver.firefox.options import Options

def get_captcha(i):

    # 打开截图文件
    screenshot_path = "./screenshot.png"
    screenshot = Image.open(screenshot_path)

    # 设置裁剪区域的位置和大小
    # linux firefox
    x = 100  # 距左边界的像素
    y = 750  # 距顶部的像素
    w = 80  # 宽度
    h = 50  # 高度
# mac Firefox    
#    x = 200  # 距左边界的像素
#    y = 1500  # 距顶部的像素
#    w = 150  # 宽度
#    h = 100  # 高度
    # 裁剪图像
    cropped_image = screenshot.crop((x, y, x + w, y + h))

    # 保存裁剪后的图像
    cropped_image.save("./cropped_image"+str(i)+".png")
    cropped_image=cv2.imread("./cropped_image"+str(i)+".png")
    _, binary_image = cv2.threshold(cropped_image, 128, 255, cv2.THRESH_BINARY)

# 使用 Tesseract 进行 OCR
    CaptchaCode = pytesseract.image_to_string(Image.fromarray(binary_image))
    CaptchaCode=CaptchaCode.replace(' ','').strip('\n')
    print(CaptchaCode,len(CaptchaCode))
    return CaptchaCode

def print_single_line(msg):
    sys.stdout.write('\033[F')  # 光标移动到上一行
    sys.stdout.write('\033[K')  # 清除行
    print(msg)

df0=read_csv('df0.csv')
#df0=df0.loc[(df0.yr_mg >= 90)].reset_index(drop=True)
#df0=df0.loc[(df0.yr_mg >= 90) & (df0.yr_mg <= 99)].reset_index(drop=True)


# 创建 Chrome 驱动器
home=subprocess.check_output('echo $HOME',shell=True).decode('utf8').strip('\n')
source_directory=home+"/Downloads"

#firefox_options = Options()
#firefox_options.add_argument("--headless")  
options = webdriver.FirefoxOptions()

# 禁用內建的 PDF 開啟器
options.set_preference("pdfjs.disabled", True)

# 禁用內建的 PDF 下載器
#options.set_preference("plugin.disable_full_page_plugin_for_types", "application/pdf")

with open('not_found.txt','r') as f:
    not_found=[i.strip('\n') for i in f]
j=0
tails=['.pdf','.rar','.zip','.7z','.PDF','doc','DOC','docx','DOCX']
for i in range(len(df0)):
    # 打开网页
    proj_id=str(df0.proj_id[i]).replace(' ','')
    cat_nam=df0.cat_nam[i]
    title=df0.title[i].replace(' ','').replace('/','-').replace('\&amp;','＆').replace('\t','').replace('\x0b','-')
    title=title.replace('<','(').replace('>',')')
    if title in not_found:continue
    group_id=str(df0.group_id[i])
    target_directory="/nas2/sespub/EPA_PrjReports/"+group_id+"_"+cat_nam
    os.makedirs(os.path.expanduser(target_directory), exist_ok=True)
    tfile=os.path.join(os.path.expanduser(target_directory), proj_id+"_"+title) #+".pdf")
    
    withFile=False
    for tail in tails:
        if os.path.exists(tfile+tail):withFile=True
    if withFile:continue
        
    driver = webdriver.Firefox(options=options)
    url = "https://epq.moenv.gov.tw/ProjectDoc/FileDownload?proj_id="+proj_id+"&group_id="+group_id

    ii=0
    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*.pdf"))
    if len(pdf_files)>1:
        sfile=os.path.join(os.path.expanduser(source_directory), '*.pdf')
        os.system("rm "+sfile)

    while ii<10:
        iwrite=False
        ii+=1
        driver.get(url)
        driver.save_screenshot("./screenshot.png")
        CaptchaCode=get_captcha(ii)
        if len(CaptchaCode)!=4:continue
        if len(set(list(CaptchaCode))-set(list('/?,.!@#$%^&*()[]|')+['\n']+["'"]+['"']+["`"])) != len(CaptchaCode):continue
        # 输入验证码（如果需要）
        captcha_input = driver.find_element(By.ID, "CaptchaCode")
        print_single_line('{:d} '.format(ii)+CaptchaCode+title)
        captcha_input.send_keys(CaptchaCode)
        time.sleep(1)
        # 点击 "我同意"
        agree_button = driver.find_element(By.XPATH, "//input[@value='我同意']")
        agree_button.click()
        time.sleep(1) #wait = WebDriverWait(driver, 30)
        try:
            WebDriverWait(driver, 1).until(
            EC.text_to_be_present_in_element((By.XPATH, "//div[@id='swal2-content']"), "本文件系統調整中，暫不提供下載；請聯絡業務單位")
            )
            # 如果找到元素，表示弹窗出现，处理你的逻辑
            break
        except TimeoutException:
            # 如果等待时间超过1秒，或者找不到元素，表示弹窗未出现，继续执行后续逻辑        
            try:
                ok_button = driver.find_element(By.XPATH, "//button[text()='OK']")
                # 点击 OK 按钮
                ok_button.click()
                continue
            except:
                pdf_files=[]
                iii=0
                while iii<10 and len(pdf_files)==0:
                    time.sleep(10) #wait = WebDriverWait(driver, 30)
                    pdf_files = glob.glob(os.path.join(os.path.expanduser(source_directory), "*"))
                    iii+=1
                if len(pdf_files)==0:
                    driver.quit()    # 关闭浏览器
                    break
                pdf_files[0]=pdf_files[0].replace('(','\(').replace(')','\)').replace(' ','\ ')
                pdf_files[0]=pdf_files[0].replace('<','\<').replace('>','\>')
                if '.' in pdf_files[0][-4:]:
                    ext=pdf_files[0][-4:].split('.')[-1]
                else:
                    ext='rar'
                tfile=tfile.replace('(','\(').replace(')','\)').replace(' ','\ ')+'.'+ext
                sfile=os.path.join(os.path.expanduser(source_directory), pdf_files[0])
                os.system("mv "+sfile+" "+tfile)
                iwrite=True
                if len(pdf_files)>1:
                    sfile=os.path.join(os.path.expanduser(source_directory), '*')
#                    os.system("rm "+sfile)
                driver.quit()    # 关闭浏览器
                break
    if not iwrite:
        with open('not_found.txt','a') as f:
            f.write(title+'\n')
    # 判断 WebDriver 是否存在并关闭
    if 'driver' in locals() or 'driver' in globals():
        try:
            driver.quit()
            print("Selenium WebDriver 已关闭。")
        except:
            print("关闭 Selenium WebDriver 时出现错误。")
        else:
            print("Selenium WebDriver 未找到。")
    j+=1
    #if j==1:break
410/1: cols='Coupon_number,declaration_date,removal_date,receipt_date,processing_completion_date,recycling_completion_date,business_organization_management_number,business_organization_name,removal_organization_management_code,removal_organization_name,transport_vehicle_number,removal_confirmation,processing_organization_Management_code,processing_agency_name,processing_confirmation,reuse_agency_management_code,reuse_agency_name,reuse_confirmation,final_disposal_agency_management_code,final_disposal_agency_name,final_disposal_confirmation,output_agency_confirmation,recipient_clearance,Name_of_recipient_clearance,confirmation_of_recipient_clearance,waste_code,waste_name,industry_code,industry_Chinese_name,process_code,process_Chinese_name,species_code,species_Chinese_name,physical_property_code,physics_Property_Chinese_name,cleanup_method_code,cleanup_method_Chinese_name,main_hazardous_ingredient_code,main_hazardous_ingredient_Chinese_name,harmful_characteristic_code,harmful_characteristic_Chinese_name,declaration_amount,declaration_method,industrial_zone_code,industrial_zone_name'.split(',')
410/2: len(cols)
410/3: s=''
410/4:
for c in cols:
    s+=' '+c+' VARCHAR(255) NULL,'
410/5: s
411/1: import MySQLdb
411/2: !pip install MySQLdb
411/3: !pip install PyMySQL
411/4: !pip install PyMySQL
411/5: import PyMySQL
411/6: import pymysql.cursors
411/7:
connection = pymysql.connect(host='localhost',
                             user='kuang',
                             password='sinotec2',
                             database='iwr',
                             cursorclass=pymysql.cursors.DictCursor)
411/8: cols='Coupon_number,declaration_date,removal_date,receipt_date,processing_completion_date,recycling_completion_date,business_organization_management_number,business_organization_name,removal_organization_management_code,removal_organization_name,transport_vehicle_number,removal_confirmation,processing_organization_Management_code,processing_agency_name,processing_confirmation,reuse_agency_management_code,reuse_agency_name,reuse_confirmation,final_disposal_agency_management_code,final_disposal_agency_name,final_disposal_confirmation,output_agency_confirmation,recipient_clearance,Name_of_recipient_clearance,confirmation_of_recipient_clearance,waste_code,waste_name,industry_code,industry_Chinese_name,process_code,process_Chinese_name,species_code,species_Chinese_name,physical_property_code,physics_Property_Chinese_name,cleanup_method_code,cleanup_method_Chinese_name,main_hazardous_ingredient_code,main_hazardous_ingredient_Chinese_name,harmful_characteristic_code,harmful_characteristic_Chinese_name,declaration_amount,declaration_method,industrial_zone_code,industrial_zone_name'.split(',')
411/9:
with connection:
    with connection.cursor() as cursor:
        sql="SELECT `%s`,`%s`,`%s`,`%s` FROM raw".format(cols[3],cols[13],cols[23],cols[33])
        result=cursor.execute(sql)
411/10:
with connection:
    with connection.cursor() as cursor:
        sql="SELECT '%s','%s','%s','%s' FROM raw".format(cols[3],cols[13],cols[23],cols[33])
        result=cursor.execute(sql)
411/11: cols[3],cols[13],cols[23],cols[33]
411/12:
with connection:
    with connection.cursor() as cursor:
        sql="SELECT `receipt_date`,`processing_agency_name`,`Name_of_recipient_clearance`,`physical_property_code` FROM raw"
        result=cursor.execute(sql)
411/13:
connection = pymysql.connect(host='localhost',
                             user='kuang',
                             password='sinotec2',
                             database='iwr',
                             cursorclass=pymysql.cursors.DictCursor)
411/14:
with connection:
    with connection.cursor() as cursor:
        sql="SELECT `receipt_date`,`processing_agency_name`,`Name_of_recipient_clearance`,`physical_property_code` FROM raw"
        result=cursor.execute(sql)
411/15: type(result)
411/16: result
411/17:
connection = pymysql.connect(host='localhost',
                             user='kuang',
                             password='sinotec2',
                             database='iwr',
                             cursorclass=pymysql.cursors.DictCursor)
413/1:
with open('y2.txt','r') as f:
    fnames=[i.strip('\n') for i in f]
fnames[:5]
413/2:
with open('y2.txt','r',encoding='utf7') as f:
    fnames=[i.strip('\n') for i in f]
fnames[:5]
413/3:
with open('y2.txt','r',encoding='utf8') as f:
    fnames=[i.strip('\n') for i in f]
fnames[:5]
413/4:
with open('y2.txt','r',encoding='big5') as f:
    fnames=[i.strip('\n') for i in f]
fnames[:5]
414/1:
from pandas import *
df0=read_csv('path2.csv')
414/2: ls *csv
414/3:
from pandas import *
df0=read_csv('paths2.csv')
414/4: df0.head()
414/5: set([i.split('.')[-1] for i in list(df.Filename)])
414/6: set([i.split('.')[-1] for i in list(df0.Filename)])
414/7: set([i.split('.')[-1] for i in list(df0.Filename) if '.' in i])
414/8: set([i.split('.')[-1] for i in list(df0.Filename) if '.' in i[-10:]])
414/9: s=set([i.split('.')[-1] for i in list(df0.Filename) if '.' in i[-10:]])
414/10: df1=df0.loc[df0.Filename.map(lambda i:'.' in i[-10:])]
414/11: len(df1)
414/12: df1['ext']=[i.split('.')[-1] for i in list(df0.Filename) if '.' in i[-10:]]
414/13: l=[i.split('.')[-1] for i in list(df0.Filename) if '.' in i[-10:]]
414/14: len(l),len(df1)
414/15: df0['ext']=''
414/16: df1=df0.loc[df0.Filename.map(lambda i:'.' in i[-10:])]
414/17: df1['ext']=l
414/18: df1.head()
414/19: len(df1)
414/20: len(df1.loc[df1.ext==''])
414/21:
for i in s:
    print(i,len(df1.loc[df1.ext==i]))
416/1:
import os

from langchain.chat_models import ChatOpenAI
416/2: !pip install langchain
416/3:
import os

from langchain.chat_models import ChatOpenAI
416/4:
from langchain.document_loaders import TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.indexes import VectorstoreIndexCreator
416/5:
def test_chain():
    embedding = OpenAIEmbeddings(model="text-embedding-ada-002")
    loader = TextLoader("state_of_the_union.txt")
    index = VectorstoreIndexCreator(embedding=embedding).from_loaders([loader])

    llm = ChatOpenAI(model="gpt-3.5-turbo")

    questions = [
        "Who is the speaker",
        "What did the president say about Ketanji Brown Jackson",
        "What are the threats to America",
        "Who are mentioned in the speech",
        "Who is the vice president",
        "How many projects were announced",
    ]

    for query in questions:
        print("Query:", query)
        print("Answer:", index.query(query, llm=llm))
416/6: test_chain()
416/7: ls *txt
416/8: pwd
416/9: !wget https://raw.githubusercontent.com/hwchase17/langchain/v0.0.200/docs/modules/state_of_the_union.txt
416/10: test_chain()
416/11: %pip install chromadb
417/1:
import os

from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.indexes import VectorstoreIndexCreator


def test_chain():
    embedding = OpenAIEmbeddings(model="text-embedding-ada-002")
    loader = TextLoader("state_of_the_union.txt")
    index = VectorstoreIndexCreator(embedding=embedding).from_loaders([loader])

    llm = ChatOpenAI(model="gpt-3.5-turbo")

    questions = [
        "Who is the speaker",
        "What did the president say about Ketanji Brown Jackson",
        "What are the threats to America",
        "Who are mentioned in the speech",
        "Who is the vice president",
        "How many projects were announced",
    ]

    for query in questions:
        print("Query:", query)
        print("Answer:", index.query(query, llm=llm))
417/2: test_chain()
417/3: !lst
417/4: loader = TextLoader("state_of_the_union.txt")
417/5: index = VectorstoreIndexCreator(embedding=embedding).from_loaders([loader])
417/6: embedding = OpenAIEmbeddings(model="text-embedding-ada-002")
417/7: index = VectorstoreIndexCreator(embedding=embedding).from_loaders([loader])
417/8:
import os

from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.indexes import VectorstoreIndexCreator


def test_chain():
    embedding = OpenAIEmbeddings(model="vicuna-7b-v1.5-16k")
    loader = TextLoader("state_of_the_union.txt")
    index = VectorstoreIndexCreator(embedding=embedding).from_loaders([loader])

    llm = ChatOpenAI(model="vicuna-7b-v1.5-16k")

    questions = [
        "Who is the speaker",
        "What did the president say about Ketanji Brown Jackson",
        "What are the threats to America",
        "Who are mentioned in the speech",
        "Who is the vice president",
        "How many projects were announced",
    ]

    for query in questions:
        print("Query:", query)
        print("Answer:", index.query(query, llm=llm))
417/9: test_chain()
417/10: test_chain()
417/11:
import openai
# to get proper authentication, make sure to use a valid key that's listed in
# the --api-keys flag. if no flag value is provided, the `api_key` will be ignored.
openai.api_key = "EMPTY"
openai.api_base = "http://200.200.32.195:55083/v1"
417/12:
model = "vicuna-7b-v1.3"
prompt = "Once upon a time"

# create a completion
completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=64)
# print the completion
print(prompt + completion.choices[0].text)

# create a chat completion
completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
print(completion.choices[0].message.content)
417/13: %pip install openai==0.28
417/14:
model = "vicuna-7b-v1.3"
prompt = "Once upon a time"

# create a completion
completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=64)
# print the completion
print(prompt + completion.choices[0].text)

# create a chat completion
completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
print(completion.choices[0].message.content)
418/1:
model = "vicuna-7b-v1.3"
prompt = "Once upon a time"

# create a completion
completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=64)
# print the completion
print(prompt + completion.choices[0].text)

# create a chat completion
completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
print(completion.choices[0].message.content)
418/2: import openai
418/3:
model = "vicuna-7b-v1.3"
prompt = "Once upon a time"

# create a completion
completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=64)
# print the completion
print(prompt + completion.choices[0].text)

# create a chat completion
completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
print(completion.choices[0].message.content)
418/4:
import openai
# to get proper authentication, make sure to use a valid key that's listed in
# the --api-keys flag. if no flag value is provided, the `api_key` will be ignored.
openai.api_key = "EMPTY"
openai.api_base = "http://200.200.32.195:55083/v1"
418/5: model = "vicuna-7b-v1.3"
418/6: prompt = "Once upon a time"
418/7:
model = "vicuna-7b-v1.3"
prompt = "Once upon a time"

# create a completion
completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=64)
# print the completion
print(prompt + completion.choices[0].text)

# create a chat completion
completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
print(completion.choices[0].message.content)
418/8: model = "vicuna-7b-v1.5-16k"
418/9:
prompt = "Once upon a time"

# create a completion
completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=64)
# print the completion
print(prompt + completion.choices[0].text)

# create a chat completion
completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
print(completion.choices[0].message.content)
418/10: prompt="介紹最好的古典音樂"
418/11:
# create a completion
completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=64)
# print the completion
print(prompt + completion.choices[0].text)

# create a chat completion
completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
print(completion.choices[0].message.content)
418/12:
def testp(prompt):# create a completion
    completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=64)
# print the completion
    print(prompt + completion.choices[0].text)

# create a chat completion
    completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
    print(completion.choices[0].message.content)
    return
418/13: testp(prompt)
418/14:
def testp(prompt):# create a completion
    completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=1064)
# print the completion
    print(prompt + completion.choices[0].text)

# create a chat completion
    completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
    print(completion.choices[0].message.content)
    return
418/15: testp(prompt)
418/16:
def testp(prompt):# create a completion
    completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=640)
# print the completion
    print(prompt + completion.choices[0].text)

# create a chat completion
    completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
    print(completion.choices[0].message.content)
    return
418/17: testp(prompt)
418/18: testp(prompt)
418/19: prompt
418/20:
def testp(prompt):# create a completion
    completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=64)
# print the completion
    print(prompt + completion.choices[0].text)

# create a chat completion
    completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
    print(completion.choices[0].message.content)
    return
418/21: testp(prompt)
418/22: testp(prompt)
418/23: testp("我會給你一段文章，請給我600字中文的摘要")
418/24:
def testp(prompt):# create a completion
    completion = openai.generate(model=model, prompt=prompt, max_tokens=64)
# print the completion
    print(prompt + completion.choices[0].text)

# create a chat completion
    completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
    print(completion.choices[0].message.content)
    return
418/25: testp("我會給你一段文章，請給我600字中文的摘要")
418/26: dir(openai)
418/27:
def testp(prompt):# create a completion
    completion = openai.ChatCompletion.create(model=model, prompt=prompt, max_tokens=64)
# print the completion
    print(prompt + completion.choices[0].text)

# create a chat completion
    completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
    print(completion.choices[0].message.content)
    return
418/28: testp("我會給你一段文章，請給我600字中文的摘要")
418/29:
def testp(prompt):# create a completion
    completion = openai.ChatCompletion(model=model, prompt=prompt, max_tokens=64)
# print the completion
    print(prompt + completion.choices[0].text)

# create a chat completion
    completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
    print(completion.choices[0].message.content)
    return
418/30: testp("我會給你一段文章，請給我600字中文的摘要")
418/31:
def testp(prompt):# create a completion
    completion = openai.ChatCompletion(model=model, prompt=prompt, max_tokens=64)
# print the completion
    print(prompt + completion.text)

# create a chat completion
    completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "Hello! What is your name?"}]
)
# print the completion
    print(completion.choices[0].message.content)
    return
418/32: testp("我會給你一段文章，請給我600字中文的摘要")
418/33: completion = openai.ChatCompletion(model=model, prompt=prompt, max_tokens=64)
418/34: print(completion)
418/35: prompt
418/36: model
418/37: chat = openai.ChatCompletion(model=model, prompt=prompt, max_tokens=64)
418/38: chat.prompt
418/39: chat.prompt()
418/40: chat
418/41: chat = openai.ChatCompletion()
418/42: chat.prompt()
418/43: chat
419/1: ls  dlist\(2021-01-03\)\(300639\).csv
419/2: fname='dlist\(2021-01-03\)\(300639\).csv'
419/3:
from pandas import *
df0=read_csv(fname)
419/4: fname='dlist(2021-01-03)(300639).csv'
419/5: df0=read_csv(fname)
419/6: cols=list(dfo.columns)
419/7: cols=list(df0.columns)
419/8: cols
419/9: [i for i in cols if '日期' in i]
419/10: d=[i for i in cols if '日期' in i]
419/11: df0.loc[1,d]
419/12: df0['清運日期'][1]
419/13: type(df0['清運日期'][1])
419/14: d
419/15: for i in d[1:]
419/16:
for i in d[1:]:
    print(i,len(df0.loc[df0[i].map(lambda x:x[0]!='2')]))
419/17:
for i in d[1:]:
    print(i,len(df0.loc[df0[i].map(lambda x:type(x)==float)]))
419/18: len(df)
419/19: len(df0)
419/20: !wc df0.csv
419/21: df1=df0.loc[df0[i].map(lambda x:type(x)==float)]
419/22: i
419/23: i='清運日期'
419/24: df2=df1.loc[df1[i].map(lambda x:type(x)==float)]
419/25: i='收受日期'
419/26: df3=df2.loc[df2[i].map(lambda x:type(x)==float)]
419/27: i='處理完成日期'
419/28: df4=df3.loc[df3[i].map(lambda x:type(x)==float)]
419/29: len(df4)
419/30: df0.loc[0]
419/31: i='申報日期'
419/32: df1=df0.loc[df0[i].map(lambda x:type(x)==float)]
419/33: len(df1)
419/34: df0=read_csv('df0.csv')
419/35: df1=df0.loc[df0[i].map(lambda x:type(x)==float)]
419/36: len(df1)
419/37: from datetime import datetime
419/38:
def convert_date_format(old_format_date_str):
    # 解析舊格式日期字符串
    old_format_date = datetime.strptime(old_format_date_str, "%m %d %Y %I:%M%p")

    # 將日期轉換為新格式
    new_format_date_str = old_format_date.strftime("%Y-%m-%d %H:%M:%S.%f")

    return new_format_date_str
419/39:
old_date_str = "01 18 2021 4:10PM"
new_date_str = convert_date_format(old_date_str)
print(new_date_str)
419/40: df0['申報日期2']=[convert_date_format(i) for i in list(df0['申報日期'])]
419/41: df0.loc[0]
419/42: df0.to_csv('df0.csv',index=False)
419/43: pwd
419/44: !top
419/45: history
420/1:
from pandas import *
df0=read_csv('df0.csv')
420/2: cols=list(df0.columns)
420/3: cols
420/4: nset=set(df0['事業機構管編'])
420/5: nset=set(df0['事業機構名稱'])
420/6: nam_set=set(df0['事業機構名稱'])
420/7: num_set=set(df0['事業機構管編'])
420/8: len(nam_set),len(num_set)
420/9: num_nam={i:j for i,j in zip(list(df0['事業機構管編']),list(df0['事業機構名稱']))}
420/10: import json
420/11:
with open('FacilityNumNam.json','w') as f:
    f.dump(num_nam)
420/12:
with open('FacilityNumNam.json','w') as f:
    json.dump(num_nam,f)
420/13: !lst
420/14: !vi FacilityNumNam.json
420/15:
with open('FacilityNumNam.json','w',encoding='utf8') as f:
    json.dump(num_nam,f)
420/16: !vi FacilityNumNam.json
420/17:
with open('FacilityNumNam.json','w',encoding='big5') as f:
    json.dump(num_nam,f)
420/18: !vi FacilityNumNam.json
420/19:
with open('FacilityNumNam.json','w') as f:
    json.dump(num_nam,f)
420/20: del df0['事業機構名稱']
420/21: cols
421/1:
from pandas import *
df0=read_csv('df0.csv')
421/2: cols=list(df0.columns)
421/3: [i for i in cols if '名稱' in i]
421/4: d=[i for i in cols if '名稱' in i]
421/5: e=[i for i in cols if i[-2]=='日期']
421/6: e
421/7: e=[i for i in cols if i[-4]=='日期']
421/8: e=[i for i in cols if i[-2:]=='日期']
421/9: e
421/10:
for c in d+e:
    del df0[c]
421/11: col2=list(df0.columns)
421/12: col2
421/13: df0.to_csv('df0_clean.csv',index=False)
421/14: pwd
421/15: !top
421/16: col23
421/17: col2
421/18:
for s in set(df0['廢棄物代碼']):
    print(s,list(df0['廢棄物代碼']).count(s))
421/19:
for s in set(df0['廢棄物代碼']):
    print(s,list(df0['廢棄物代碼']).count(s))
421/20: col2
421/21: pv=pivot_table(df0,indexc='行業別代碼',values='行業別代碼',aggfunc=count).reset_index()
421/22: pv=pivot_table(df0,indexc='行業別代碼',values='行業別代碼',aggfunc='count').reset_index()
421/23: pv=pivot_table(df0,index='行業別代碼',values='行業別代碼',aggfunc='count').reset_index()
421/24: pv=pivot_table(df0,index='行業別代碼',values='申報量',aggfunc='count').reset_index()
421/25: pv
   1: history -f his.py
   2: vi his.py
   3: !vi his.py
   4: history -g -f his.py
